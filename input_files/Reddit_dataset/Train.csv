Summary,Document
Randomness has a lot to do with success. Keep rolling the dice if you eventually want to be successful.,"One more thing, that few people fail to realize, is that success is often extremely random. Maybe if you're lucky your project will get recognized and get into a positive feedback loop of adaptation and recognition. Sensitive dependence on initial conditions.  But probably not. If you want to be successful with any kind of creative venture, you've got to be willing to do  a lot  of work that no one will ever see or recognize. Maybe if you're lucky you will hit on something that catches on. But the only thing you can do is keep rolling those dice and try not to be deterred by your failures.  tl;dr:  Randomness has a lot to do with success. Keep rolling the dice if you eventually want to be successful. "
There are a ton of people in the world who work in tech.,"So, these numbers (or my math!) might be totally off, but based on  wikipedia  are in technology.  So, hypothetically, if (big if) my numbers are right, and if (another big if) I've got these numbers right, then even if everyone at google+amazon was a rockstar, that would still only be .08% of the total labor force.  That seems plausible.  So, I'm not saying everyone at google/amazon IS a rockstar.  But I AM saying that your argument, ""tons of people work at those companies, they can't ALL be rockstars, there are just too many"" is not a good argument, as far as I can tell.  TL;DR: There are a ton of people in the world who work in tech. "
"Either extreme can work, it's the middle that's painful.","It depends on whether the DB is a ""first-class citizen"" in how the application works, which will vary depending on the organization. If you actually have ""real DBA"" who can write, test, debug, and profile all that stuff, and feature-negotiation regularly happens between the database and application-layer teams... then it's probably OK, because at that point the DB is itself one of your ""battle-tested tools for logic"".  Conversely, if you don't need/want to go in that direction, it's better to treat the DB as a data-store that happens to have too-cool-not-to-use query-functionality and integrity assertions. If the database-behavior is too complex to mock in your tests, that's a warning flag.  TLDR:  Either extreme can work, it's the middle that's painful. "
"don't overengineer"" is the first piece of essential advice, but we still have to engineer.","I'm going to both agree with you, but also respectfully disagree.  On the one hand, the basic self discipline that includes YAGNI and straightforwardness is both the most important and the hardest part. Expert musicians practice scales and athletes run sprints because doing the basics right is the foundation.  On the other hand, I'm certain it's not the  only  thing. Even the history of programming languages and operating systems show that it is not. Human readable assembly is easier to maintain than machine code. Structured languages are easier than assembly. Objects, pure functions, and garbage collection add even more advantages. Operating systems with memory isolation and preemptive multitasking are less bug prone than those without.  The two big themes are readability and modularity. It's great when we figure out how to build those into the systems themselves, but surely there is a range of choices we can make in the systems we have today that have different pros and cons to balance. Design is about tradeoffs, I'd like to know more about what ones are available.  Edit: TL;DR ""don't overengineer"" is the first piece of essential advice, but we still have to engineer. "
"edit: to the downmodders, yes, I'm (painfully) aware PHP has plenty of flaws)","I think this is really cool - I've used PHP for several years, but my JavaScript is really weak.  JavaScript is a wonderful language with so many fantastic features that I'll never get around to learning, because all I need it for is to popup the confirmation dialog and modify some CSS on-the-fly. I usually end up with an ugly script full of cut+paste code, simply because I don't know enough of JavaScript to cope (although I know enough of other languages to know what I'm doing is horrible, which is even worse!)  Although I hate how long it takes to download and render websites with too many javascript dependencies (spend twenty seconds trying to load digg or mashable recently, anyone?) so I would probably just pick a few of the best functions, rather than include them all in one go.  (edit: Sorry for the wall of text. Feel free to tl;dr!)  (edit: to the downmodders, yes, I'm (painfully) aware PHP has plenty of flaws) "
"The report is junk. And Google and Apple haven't figured out how to play the vulnerability reporting game yet, so their numbers are artificially terrible.","This report's a disaster. I've found and reported critical security bugs in all the browsers on the list, so I'll limit my criticism to those. Here goes.  Apple Safari and Google Chrome are getting  unfairly punished for being more transparent , and Mozilla lumps any internally discovered memory corruption (as in remote code execution) under a single CVE for each release.  I'm also dubious of the quality of these ratings. Many of the Chrome bug reports are restricted and I can't read them, but of the ones I can all seem to be mis-scored. All the Chrome WebKit vulnerabilities I've been able to look at are listed as full compromises of integrity and confidentiality. However, the Chrome sandbox would mitigate those to partial at the most under the CVSS scoring guidelines. So, based on my read, the vast majority of the Chrome bugs should be ranked below the 7-10 threshold described in the report.  One other thing is odd. You'll notice that most of the vendor's CVEs are clustered in groups. This is because any decent sized software project or company will generally get a block of CVEs allocated to them, so they can do the vulnerability reporting themselves to ensure it's correct. Chrome's CVEs are all over the place and the rankings diverge wildly (WebKit is clustered, however). I'm guessing that Google doesn't participate in CVE reporting, and all these entries are coming from third parties, which explains the disproportionately large number of errors. Google really should get off their asses and start doing this themselves, otherwise they look like idiots.  TLDR: The report is junk. And Google and Apple haven't figured out how to play the vulnerability reporting game yet, so their numbers are artificially terrible. "
"yes, go get a design/ux buddy in a large corp, make lots of cash","There will always be a place for a full on js person. You forget the sheer amount of js a large corporation manages. The idea of a person taking care of one part of a stack of technologies is not only common, but in many cases the way a team might split themselves optionally if given the change.  The web requires that people be a jack of all trades, but if you get those people in one group, often times the skillsets skew heavily towards one part of the stack or another.  Never fear, people will love you even if you can't design/hate css/wouldn't want to test your stuff on people... Just be prepared to take lots of constructive criticism, as the needs of the end user might be a lot more work than what you'd be able to do if you ""just did it this way instead"" and as a straight js person you won't be immersed in that side.  tl:dr, yes, go get a design/ux buddy in a large corp, make lots of cash "
go easy on him. he didnt have the benefit of 15 years more experience in game design and software design paradigms in general.,"from a proper design ""best practices"" standpoint, i totally see what you're saying and agree. at the same time, i feel like the author is really just trying to get across to the modern reader how different things were 15 years ago in a somewhat informal way.  not only were the conditions much more harsh (holy shit 20% loss on a wired connection), but this was basically the first online multiplayer game of it's kind ever. it's understandable that even a great developer could have made these kinds of oversights when the area was totally uncharted. inevitably there are some problems that you will find only after you start building your code.  tl;dr: go easy on him. he didnt have the benefit of 15 years more experience in game design and software design paradigms in general. "
Java is the best language for performant functional programming!  Who knew?,"So much hate for something that hasn't been mentioned in either the article or the comments up until now!  Of course it will be slower, no-one had claimed that Clojure emits more efficient bytecode than Java.  Having a quick go at a parallel sum-of-squares-even function, however:  (-&gt;&gt; v    (r/filter #(zero? (mod % 2)))    (r/map #(* % %))    (r/fold +)))  Yields much better results than Scala, C# or F#.  It's not really the cost of Clojure functions that's the difference, that's just a method call.  It's the fact that Clojure doesn't really (only for Java interop) work with native types.  It would be interesting to do a similar set of comparisons where strings were returned.  TL;DR:  Java is the best language for performant functional programming!  Who knew? "
"if it quacks like a Data Scientist, and gets paid like a Data Scientist...","I think it may just be a lack of standardization, or slowness to adopt the title of ""data scientist."" After all, people have been in job roles like this since before the term ""data science"" was popular.  For example, I just left a project where we were building a recommender. There was a guy on that team who did exclusively tasks that we would call data science: cleaning data, choosing what variables to model from, designing visualizations, etc. His job title is something like ""Senior solutions architect."" No programming background except SAS; he's clearly a Data Scientist, but our HR dept. hasn't caught up with that trend.  TL;DR  - if it quacks like a Data Scientist, and gets paid like a Data Scientist... "
"jQuery is a library, not a framework. You call jQuery, unlike Angular which calls you.","> jQuery encourages you to use global variables  jQuery encourages you to use global variables as much as Javascript encourages you to use global variables. You can store references to DOM components in a Javascript class just as easily you can repeated evaluate selectors.  > jQuery doesn't help to build a maintainable code in a way that was > proven since many years in UI development : the mvc architecture  You can write MVC code just fine  which uses  jQuery, but you will have to write the MVC scaffolding yourself. This might be desirable because that MVC scaffolding you write will be the minimum necessary for your application, rather than Angular (for example) which must cater to all use cases. For example, with Angular you are stuck with the digest cycle even if you are not using two-way bindings.  > It doesn't encourage you to write reusable code  jQuery doesn't encourage you to write reusable code any more than Javascript doesn't encourage you to write reusable code.  TL;DR jQuery is a library, not a framework. You call jQuery, unlike Angular which calls you. "
Nothing new. Same old bullshit you see in your everyday interview.,"1. Ask about the company vision.  My personal experience is definitely skewed. But usually the ""vision"" is just some marketing bullshit passed down from top to bottom. Managers and engineers rarely see eye to eye.  2. Ask what you will be doing in 3 months.  This is good advice.  3. Ask engineers about cool projects.  Job projects are never cool, especially in larger companies. Expect bored engineers that are already sick and tired with the existing codebase. Want cool, try a startup.  4. Ask about the company culture.  Most software companies have the same culture:  ""Here are some useless perks and shitty pay. Now go do some work because we have a deadline.""  The only way to check if you're compatible is to work with them. And by then it's already too late.  5. Ask about performance review.  Nobody cares. You won't get a raise. You won't get a promotion. If they're going to fire you, the evaluation process is not going to matter much.  6. Ask what they would change in the company.  Nothing. Everything is perfect. Only a rookie badmouths the company in the presence of strangers.  7. Ask about coding vs meetings balance.  Don't bother. It's the same in every company.  8. Ask about how junior/med/senior positions are perceived.  You already know. Everybody wants 3 years exp. for junior and a rock star ninja for senior.  TLDR: Nothing new. Same old bullshit you see in your everyday interview. "
"libertarians aren't helping, but the situation isn't the fault of the free market.","I'm not a libertarian, but for the sake of argument; the H-1B visa program is a subversion of the free market. In a free market, scarce things in demand rise in cost. In this case, programmer wages would rise. This would cause the field to be more attractive and more people would go into the field. Any 'shortage', in the long term, would be corrected.  The H-1B program is bringing in people that normally wouldn't be in the US labor market, artificially inflating supply. This is artificially depressing wages, and subverting normal market correcting forces.  Additionally, the particulars of the program make H-1Bs more attractive than native programmers. No matter how much you dump on them, they can't go elsewhere for a job. This only subverts the market more.  The libertarian allergy to anything union means, of course, workers have no say when companies and the government are making these rules. But in this case, if there unions opposing the H-1B program, the net effect would be a more free market, not a less free market.  tl;dr libertarians aren't helping, but the situation isn't the fault of the free market. "
"Thanks || [Haters gonna hate]( 
 Edit : Damn crazy formatting.","Jokes aside. To those of you who gave me positive feedback; Thanks a lot. It's appreciated. :)First off I'm gonna lay my cards on the table and say that the actual swirly bit at the top is in fact ripped from a (legally licensed) stock illustration. I regularly have problems getting started with a design and need a starting point. The rest of the site is my work.  Secondly I want to reiterate what I said below about this template. I still like it ok in spite of its obvious short-comings, but having graduated from the school of geocities my site is perpetually ""Under Construction"". My regular pattern is to release the templates after ""I'm done with them"" under a creative common license ([The License]( However since I'm a bit anal I got done with configuring options and making sure it could accept multiple side bar widgets without looking terrible and so forth. I'll probably finish it some day, and when I do I might finally redesign my own blog. Shoemakers. Children. Lack of proper footwear.  tl;dr: Thanks || [Haters gonna hate](  Edit : Damn crazy formatting. "
Gifted people look like cheaters to everyone else. No one wants to help cheaters.,"I think there's this kind of odd disdain for the gifted. It goes against the fantasy that all people are equal; if some have innate abilities that others don't, that means that some were born with an edge they didn't have to work for. People  hate  that. It feels like they've cheated and no one wants to help a cheater. We wind up with programs for the less-advantaged because everyone wants the slow kids to catch up while the fast kids are left to tap their feet and stare at the ceiling. No one wants to give them even  more  of an edge than they've already got.  Rather than see the range of possibilities for the gifted and how their abilities could be used for the betterment of all, they're seen as a threat to the less-advantaged. Best to let them fend for themselves...if they're smart enough, they'll manage to entertain themselves.  It's a shame, really. I know quite a few highly intelligent people that spent their high school years being bullied and bored senseless. They wind up with a general antipathy towards people because they feel they were thrown away and spit on. Why use their brains to help people when they're the same sorts of people who shoved their heads into toilets and beat them up that would be benefiting?  Tl;dr: Gifted people look like cheaters to everyone else. No one wants to help cheaters. "
"Buy into whatever beliefs you want, but don't PUSH your beliefs onto others.","Sure, and everyone is ""owed"" a house, a car, a TV, refrigerator, free-food for life, etc.  May as well add in a ""pony"" while you're at it.  Sorry, but the universe doesn't work that way. The natural world does NOT guarantee you or ""owe"" you a damn thing.  And if you were told that you were ""owed"" that and more -- well, you were lied to and someone sold you a ""bill of goods"".  If you refuse to accept that and think OTHER PEOPLE owe you things... produce the contract, heck sue them for ""breaching"" it.  Maybe you'll even succeed and get a windfall  -- but you will only do so at someone ELSE'S expense (i.e. you will be exploiting others for your own benefit).  But ultimately, the world is a harsh place. If you are  truly  ethical, then you will realize that whatever you have received by ""windfall"" or other people's generosity, you ought to be damned grateful for.  If you  believe  a metaphysical burden has been placed on you to ""pay it forward"" that's fine.  But just as you were not ""owed"" anything, you likewise have no right to impose your belief that your personally-derived burdens should be automatically shouldered and carried by everyone else as well.  TL;DR; Buy into whatever beliefs you want, but don't PUSH your beliefs onto others. "
"The influx of Digg users are retarded, the whole 'Intel app store' garbage is a blatant and unprovoked lie.","I watched the Keynote speach, it's available on Intel's website.  Arstechnia over-exaggerated what Intel's CEO actually said, to an extent that is beyond unreasonable. My impression is that Intel will offer a wide variety of options for different devices in different markets.  People are assuming Intel is suicidal and stupid, these are the guys that lead the development of this industry, they are not that stupid. Everyone should show Intel more respect by not letting Arstechnica provoke the kind of drama now churning.  In reality I feel Intel won't be forcing consumers to bend to Intel's will. It makes more sense if Intel were developing this as an optional framework for various other companies/people to employ should they choose to. Other HW technologies (on the CPU) people irrationally feared in the past (Speed Step, etc) were options that could be disabled, no one has reason to believe Intel won't do the same with additional security features.  People need to shut up, sit patiently, quietly, and wait to see what actually happens. It'd save hundreds of Terabytes of garbage from flowing through the tubes.  An enterprise environment has the right to tell their employees what software cannot be run on their hardware, no individual has the place to tell a company how it should be run.  And everyone deserves the option to build their own trusted sources to more effectively protect themselves against 0day attacks, etc.  I'm willing to bet only certain family of CPU's will initially offer these types of features anyway, maybe specific to portable devices.  TLDR; The influx of Digg users are retarded, the whole 'Intel app store' garbage is a blatant and unprovoked lie. "
"Pike is right: if you want to write Java code, write it in Java rather than complaining that Go isn't Java.","Not scrolling down all the way, but am I the only one who thought this was a legitimate, reasonable, and correct post?  Most 'programmers' know only a handful of languages, all extremely similar, and are resistant to learning new languages. From the perspective of someone who is interested only in getting paid, this is reasonable if myopic. From the perspective of someone interested in improving one's ability in the abstract, it is stupid -- the best way to improve your ability is to master the languages that are hard, unfamiliar, useless, and crazy rather than to learn yet another variation on a language you already know.  Furthermore, complaining about a programming language is itself counterproductive. If your complaint is that the language lacks feature x, either use a language that has feature x or get off your ass and implement it -- after all, you're a programmer, and thus should be (in the abstract) capable of implementing such things. If you consider the language to be in some ways useful but in some ways objectionable, write your own language -- while it's not trivial, it's by no means impossible, and any competent programmer can write a mediocre interpreter or a mediocre compiler or a mediocre preprocessor.  So, yes, Rob Pike is a little butthurt. Whether or not it's justifiable for him to be butthurt is up in the air. Nevertheless, the content of his rant is legitimate: people on the internet complain a lot, many programmers are shitty, and many shitty programmers write stupid internet complaints rather than fixing the problems they complain about. The languages I've written are much worse than Go, and certainly if people cared about me the way they care about Pike I'd be pretty butthurt about their (probably entirely legitimate) complaints.  tl;dr: Pike is right: if you want to write Java code, write it in Java rather than complaining that Go isn't Java. "
"XML, HTML, XHTML were designed for humans, not machines.","As I recall, things like HTML, XML, XHTML, etc., are the way they are because of practices adopted by editors and publishers of books and newspapers who used a (mostly) standard format to mark up documents for print layout, as well as editorial or typographical changes that would need to be made by the printer/type-setter. These markup techniques were in many ways similar to the various markup languages we see on the web, and many publishers/editors today use an XML schema for markup. The point is that it's very  human  readable, but the cost on the web is that it isn't particularly  machine  readable without a lot of what some would call unnecessary overhead.  tl;dr: XML, HTML, XHTML were designed for humans, not machines. "
the grass is always greener on the other side of the fence.,"When I was doing my final year project, I had to work with some FIPA-compliant multi-agent systems. In fact, I had to get agents from one framework talking to another, and in my stupidity I wrote another. The agents used the ACL wrapped up in various transport formats called Content Langauges. The most commonly implemented one is a form of [modified S-expressions](  Anyway, because I made a project-limiting move and worked outside of the two given frameworks, I had to try to reimplement a parser for S-expressions. For various reasons, it didn't work so well for me. In the end, I wished they just used JSON as a content language, because it seemed much easier for me to parse (not least because there were json libraries available, but nothing easily yoinkable for S-expressions, or at least not enough for me to easily modify it for FIPA-SL).  In the end I went for parsing the xml format.  tl;dr - the grass is always greener on the other side of the fence. "
"Modern language implementations do strange optimizations. Optimization is now an  empirical  science, you must perform experiments and measurements. The days of reasoning from first principles are over.","It's important to note that this is only  historically  true. Modern browsers (at least V8 in Chrome and I think Firefox) use ""cons strings"" for this. Strings have a few different internal implementations. One is your usual ""array of characters"", but another is ""pointers to two strings"". So you get:  a = ""string1"" // allocate string1a += ""string2"" // create a cons string pointing to ""string1"" and ""string2""  Creating a cons string is trivial: it's just a tiny fixed-size object.  Whenever you do something like index into the string where it needs to act like a normal string, it will flatten the tree of cons strings out all in one lump. That should give you the same performance of joining an array of strings.  What you may have to be careful of is making sure it doesn't flatten prematurely. For example, this is likely fast:  var s = '';for (var i = 0; i &lt; 100000; i++) {  s += 'abc';}  While this is probably slow:  var s = '';var c;for (var i = 0; i &lt; 100000; i++) {  s += 'abc';  c = s[s.length - 1]; // Oops, probably flattens.}  TL;DR: Modern language implementations do strange optimizations. Optimization is now an  empirical  science, you must perform experiments and measurements. The days of reasoning from first principles are over. "
"Objects have a scope. Dependency Injection is meant for application-scope (services). As soon as you 'leave' the application, you need other services to step in.","This is one of the advantages to using 'plain old objects' for model classes. Where the model objects just holds data, and other services request the data and send it to other services to manipulate and/or present that data (like a controller). Yes, the objects that request the data ARE dependent on a service classes, but it isn't a service locator. The scope of the container ends as soon as you leave your application code (input, output, databases).  The database is a service. Your Kitten and Cat are not services, although you may have a KittenService and a CatService that loads and manipulates them.  Ideally, the Kitten and Cat do not need to know anything about those services. Realistically, it ends up more complicated. Some ORMs like Doctrine2 help you cheat and keep your code decoupled (it transparently creates a proxy so that your Kitten is really DoctrineAwareKitten, but all of your code just uses Kitten).  You may  try  to make your container do these things, but that comes at a cost: your container is no longer static. Its bigger. It can no longer be cached as easily. Likely slower as well.  The container, as it is now, is meant to be something that only changes with developers/code-- a limited scope.  tl;dr;  Objects have a scope. Dependency Injection is meant for application-scope (services). As soon as you 'leave' the application, you need other services to step in. "
There's a lot more to getting beneficial community involvement in software then just dumping the source somewhere.,"> If the problems are related to bugs in the code (as opposed to problems with the infrastructure) then the community could theoretically submit patches to fix bugs and improve performance.  I'm very pro-FOSS, and in favor of software work done with public money being open-sourced or made Freely available.  But it's hard to imagine many bugs getting fixed if healthcare.gov is open-sourced.  I imagine the feds would get tons and tons of patches/pull-requests, most of them either trivial - people fixing typos or style issues - or malicious - tea-bagger types looking to introduce bugs and sabotage the site.  I've used some government web services and APIs at my job, and they often live up to the worst stereotype of code written by government contractors: verbose, shitty, cantakerous, and more.  I imagine that at worst, healthcare.gov is filled with code like that, and at best, is filled with workarounds for all the code like that that it has to talk to.  Given all that, it's almost certainly a huge task to smoke-test a fix for something like healthcare.gov given just the source.  How are you going to set up a dev environment and validate that your fix does the right thing?  Are you going to set up a mock IRS web service, or whatever else?  I guess you  could , but it seems pretty unlikely.  I've worked on projects that were nominally open-sourced but had very high barriers to participation.  The result was no outside devs chipped in anything meaningful, unsurprisingly.  TLDR: There's a lot more to getting beneficial community involvement in software then just dumping the source somewhere. "
"As far as security is concerned, the only reason not to release source code is if the original designers were terrible at their jobs.","All the major points I came to say are mentioned here. Only thing is to add on the concept of security through obscurity: it is largely considered a bad practice and not widely accepted. Common phrases used with security through obscurity include ""obscurity is not security"" ""security through obscurity is false security"" etc. the NIST specifically says ""System security should not depend on the secrecy of the implementation or its components."" and lastly, if any project should use sto it should be an ""obscure"" project, not a huge one like this which would be subject to much scrutiny by attackers.  tl;dr: As far as security is concerned, the only reason not to release source code is if the original designers were terrible at their jobs. "
"Open leads to more secure systems, because open means more good guys. Closed doesn't keep out the bad guys.","> But without the source code, they're just taking shots in the dark, right?  No. There's a whole discipline devoted to finding these holes. It's a helluva lot easier when there's only a few people looking to stop you, too.  > Once the source code gets released, I imagine it would become much easier for a hacker to find potential weak points, and concentrate their efforts there.  Very, very briefly.  > If I may use a Star Wars analogy: the rebels were only able to destroy the Death Star because they had the schematics for it, and were able to use the schematics to identify a weak point. Without the schematics, they probably would have been circling the thing for weeks or months, which is time the empire could have used to find and reinforce the weak point themselves.  To continue the metaphor, if someone other than just the Rebels had been looking, that design flaw would have been spotted and fixed much earlier. Time isn't worth as much as you think, because anyone can make a system so secure they can't see any flaws.  > Additionally, in the case of healthcare.gov, it seems pretty apparent that there's several bugs in the system. I would say it's a near certainty that there is more than one avenue for attack. For the good guys to ""win"", they would have to find and fix most (or all) of them. For the bad guys to ""win"", they would really only need to find and exploit one to cause some serious damage.  Yes. And the history of computer security shows that the more good guys you have looking at code, the sooner things will be found and fixed. The bad guys are already there, and giving them code doesn't actually change much.  tl;dr: Open leads to more secure systems, because open means more good guys. Closed doesn't keep out the bad guys. "
"Think of nearly everything in JS as an object, with children and parents.","See, with JS, we don't have classes. We already have objects with certain behaviours (methods/properties). We only 'borrow' their behaviours when possible. In other OOP languages, we only have a base structure with which we use to build objects. For instance, in python, class Animal doesn't mean there's an object called Animal. But with JS, we actually have an object Animal with it's own private & public properties. We can interact with it, we can decide to not use it and it will still function as any other object. But we can also use it to instantiate other objects. This is because functions act as objects too (they are objects!).  So when you call a property of an object, it first checks if it's in the object, then it checks if it's in the first class prototype (object's constructor), then the constructor's constructor prototype and all the way to the native Object.  TL;DR:  Think of nearly everything in JS as an object, with children and parents. "
Using the same texture for color lightmapping gives us greater flexibility  and  quality. And it works just fine.,"I know very well how detail mapping can improve the impression of otherwise blurry textures. It was used very nicely in UT 2003.  However, if I was to use repeating textures, the game would instead look very bland and repetitive. And to get sharp edges on the road, we would need to model the road with a mesh, wasting a lot of our much more limited vertex budget. It isn't even very relevant with the camera angle we have chosen, since objects on the screen are always on more-or-less the same distance.  Instead, we can focus on painting a nice ground texture, which for our main graphican is better use of his time, having a background in print and web design.  Also, have you seen how terrible low-res lightmapping looks in tight creases?  tl;dr: Using the same texture for color lightmapping gives us greater flexibility  and  quality. And it works just fine. "
used the earth's orbit as a solution for a problem involving arbitrary length ladders. Received full credit.,"A couple of my classmates and I once used this answer on a homework.  We needed a better than O(n) algorithm for dropping graduate students off of a ladder and learning the maximum survivable height; and the algorithm was further constrained by only being allowed to 'kill' two 'graduate students'. (It's just a theory; no one was harmed.)  We put the first 'graduate student' into orbit; and argued that all other graduate students would have the same effective result. Then we pointed out that our solution was O(1), and only 'killed' one 'graduate student'.  Got full credit. It was a Friday and we made the graduate student grader laugh.  tl;dr - used the earth's orbit as a solution for a problem involving arbitrary length ladders. Received full credit. "
Users are human and aren't going to backup properly. Having backups on the server is useless if server crashes and should therefore be stored elsewhere.,"Well, if you want this to become a successful backup software, you need to stop trusting the user. The user isn't going to download those backups (especially not hourly) because the user is sloppy, lazy and  human .  If you want it to become successful, then bake in a feature that allows them to upload the backup automatically. Some suggestions would be having the script :   e-mail the backup.  upload to a separate FTP account.  upload to cloud services.   Storing the backups on the server would be utterly useless. If the server crashes, what good would the backups be if you can't access them?  tl;dr : Users are human and aren't going to backup properly. Having backups on the server is useless if server crashes and should therefore be stored elsewhere. "
"you're not alone and if you figure it out, please inform the rest of us.","For what it's worth, I fully relate to where you're at. It's been like this for a couple of years and I initially thought it might be burnout, but from what I've read and heard about burnout since, it seems like something else.I still have a passion for software development, but I'm having a really hard time staying focused and my attempts to get back to blogging have all failed miserably. Also feels like it's starting to affect my work.  Instead of switching your career, maybe you should try changing things up a little. Perhaps do a bit of teaching, or hold a few presentations on topics that you're passionate about.  TL;DR: you're not alone and if you figure it out, please inform the rest of us. "
Not everyone has the vision that you have. You must get them to see the vision as you see it. Plant the seed.,"Show your vision, then build it. Sometimes you need to get the glitz and glam in the face first to be green lit (green lighted?? bah...approved) to do the important back-end stuff; No matter what business you are in. Presentation is key. It's part of your company's brand and if you can't get that part down, then you may never be able to keep your project off the chopping block.  Especially long term projects.  I've been given the go ahead on many projects because I mocked something up and have had one project cancelled when we failed to do this.  TLDR: Not everyone has the vision that you have. You must get them to see the vision as you see it. Plant the seed. "
The generalized Bezier curve formula is the same as the expectancy of a binomial distribution.,"Very useful in game development. I remember the first time I stumbled upon these Bezier curves was when we needed a way to draw a rope with eight control points. The Bezier curve doesn't actually pass through pre-calculated points, but that's ok as we only needed to draw something that could pass as a rope.  I was studying probability at the time, and it's interesting because the generalized formula for the bezier curve can be looked at from a probabilistic point of view. Essentially, it is  exactly  calculating the expectancy (E) of a binomial distribution for any given t. Say, for a t of 0.25 you wish to get the point on the curve. Simply create a binomial distribution with the same number of control points as the curve, a probability equal to 0.25, then calculate the expectancy per axis and that will be the correct coordinate.  It even makes sense. For each axis, you essentially calculate the average between all control points. Once you do that for every axis, you get a coordinate that is on the curve for any given t. I was especially interested in this as it wasn't in the Wikipedia article on Bezier curves, and I couldn't even find something solid with my Google-fu. I just kind of figured it out myself staring at the formulas and graphing things out.  TL;DR - The generalized Bezier curve formula is the same as the expectancy of a binomial distribution. "
"Radix sort is awesome, also there is this theoretical method of sorting in parallel called enumeration sort that's kind of unrealistic, but interesting.","I did a presentation on Radix sort in my Algorithms class.  I was rather proud of it, seeing as I was an undergrad.  It's a really powerful sorting algorithm for sequential systems, O(n), but it's impossible to divide for parallelism. As such, Quick sort and Merge sort outperform it for massively parallel systems.  I did manage to find an O(1) algorithm for super parallel systems (enumeration sort). But it basically required p, the number of processors, to be equal to ( (n-1)^2 ) / 2.  Hardly likely in most systems, but possible on gpu based system.  It's also resource locked to O(n) unless you have memory that can be incremented a large number of times simultaneously.  Essentially you have a counter for every element in the array.  Then, you compare every element to every other element in turn and increment the corresponding counter. The idea is that each element will then have a counter associated with it's position in the final array. If you handle conflicts by favoring one of the elements in the comparison, and ensure no comparisons are performed twice, each counter should be unique.  This can be massively divisible between many different processes. pulling one process from the pool for each comparison and increment of the counter. Given a perfect system, (i.e. ignoring Amdahl's law), and given a system that can write a large amount of data to a certain memory location simultaneously , this could be O(1).  Finally, you pull one thread for each element to write it to the array position specified by it's counter, which is O(1)  However, since a system like the one described earlier simply does not exist, the algorithm turns out to be able to run exactly as fast as takes to allocate enough threads ( ( (n-1)^2 ) / 2 ), run one comparison, write all the increments to the memory location of the largest/smallest element (O(n)) and perform the chunk of memory writing that occurs in the last step, the algorithm ends up being O( n^2 ).  The only thing this succeeds in doing is minimizing T_p (time spent running in parallel), but it fails to take care of the T_s (the time spent performing tasks which must be carried out sequentially), which is increasing exponentially.  Still, I found it interesting.  TL;DR Radix sort is awesome, also there is this theoretical method of sorting in parallel called enumeration sort that's kind of unrealistic, but interesting. "
"Yep, major issue for us teaching compsci in Ireland. To the extent where we almost ASSUME an Indian student is cheating ><","As a student and now TA in a college in Ireland in compsci (why I had to make this alt) I am utterly, utterly, unsurprised. My biggest issue in correcting assignments etc has always been plagiarism and most often it comes from our Indian & Pakistani students.  When they are confronted, they really dont care or are surprised that not everyone else is doing it. If there is a woman in the group, they will all blame her for copying (yeah not so fond of shit like that) OFC since the college doesnt like to kick students out we just fail the assignment and they learn nothing from the whole process.  Fuck, as it is a class has a summer assignment beginning. A number of the Indian students have openly bragged to classmates (who then told us) that theyll just pay someone to do their work for em. Gonna have fun when we get to pull all the students in midway to examine their code and get them to explain it. Should catch majority of cheaters there. Last year that it was done, out of 8 Indian nationals attending, 1 could actually talk about his code as if he had written it himself. Others were dropped and are back again repeating and havent changed.  TL;DR: Yep, major issue for us teaching compsci in Ireland. To the extent where we almost ASSUME an Indian student is cheating >< "
when more businesses start to embrace higher languages then you may see CS programs start to change.,"> I just think that building a curriculum around Java these days shows how dated the program really is.  There are two real issues with that attitude, IMO.   Like it or not, Java is probably still the most (or one of the most) widely used language(s) in the business world.  A CS program that is producing graduates who don't have a solid grasp of Java will be doing them a huge disservice when they start job hunting, IMO.  I think a CS program really does need a ""core"" language which can be used in most of its classes.  If Java isn't your core language, then you'll have issues with the above point, since the majority of students will work with the languages they're required to and little else.  Though Java does have more than its share of flaws, it does occupy a nice middle-ground position from which most people can easily transition to higher level languages or, with a bit more effort, go lower-level to C or C++.  If you if you have students that are mostly focused on higher level languages like Ruby/Python/etc I think it's going to take more work to get them to learn lower level stuff.   tl;dr - when more businesses start to embrace higher languages then you may see CS programs start to change. "
sometimes good intentions is being forced to replace existing architecture just to offer new opportunities.,"I agree with this but I would add that fault is not always to play. I work on a 10 year old app written against now deprecated frameworks, or forked versions. For instance we have shims (or adapter pattern if you prefer) to run code written for mod perl + apache behind a newer psgi webserver. You shouldn't stick with anti-patterns or bad code just because. One of our more frustrating problems is how the business has changed it means architectural problems have naturally occurred. For example, unique identifiers aren't unique enough once we start selling new product lines in foreign countries.  I love our code base, its 10 year operation makes me feel like I have a better understanding of the business against those who can't read it.  But we're trying to embrace that difference since its more just a side effect of time. Were trying an SOA architecture where many small intercommunicating parts mean more various technologies can be mixed, replaced or cut apart as they grow, or just made redundant. It has a cost for each new service that we have to fight against (the business want to throw it in the legacy app for speed). I'm not sure if it will become a soup as well, but hopefully it will by having each piece be small we remove the lock in of legacy which is forced upon us.  TL;DR sometimes good intentions is being forced to replace existing architecture just to offer new opportunities. "
"Isn't the same thing as"" is a way of saying ""isn't as simple as"", not ""doesn't include"".","> it implies that you don't use cryptographic hash functions to hash passwords  Only if you read that into it.  It actually only says they aren't equivalent.  It doesn't say that you don't or shouldn't use cryptographic hash functions in the implementations of a proper system.  To make an analogy, I could say ""a proper filesystem isn't 'just a bunch of writes to the hard disk'"".  Instead, you need some stuff like journaling or other tricks to guarantee consistency.  But that doesn't mean you don't  use  a bunch of writes writes to the disk in a proper filesystem.  Of course you use them; it's just that you don't use  just  them, so they aren't the same thing.  TL;DR:  ""Isn't the same thing as"" is a way of saying ""isn't as simple as"", not ""doesn't include"". "
Two sets of functions aren't equal.  That doesn't imply that members of one set can't call members of the other.,"> we're talking about sets  Why?  I thought were talking about software and algorithms.  There's no reason those have to be treated as sets.  EDIT :  OK, the set thing was a red herring and didn't make a lot of sense.  So, let me try and phrase my objection better now that I have a free second.  You said:  >  So the title is saying that you don't use cryptographic functions to hash passwords. That's the interpretation I presented in my post  The title is saying that the two aren't equivalent.  Whether you want to talk about sets or not, it means that the two things (cryptographic hashes and password hashes) aren't the same.  That implies that you don't  directly  use crypotgraphic hashes as password hashes.  But it does not imply that you don't  indirectly  use them.  Your objection was the form, ""I don't like title X because X implies Y, and Y isn't true.""  I'm saying X doesn't imply Y.  If two sets A (cryptographic hashes) and B (password hashes) aren't equal, that doesn't imply that members of set B must not have a certain property (in this case, the property is ""calls functions from set A"").  TL;DR:  Two sets of functions aren't equal.  That doesn't imply that members of one set can't call members of the other. "
Misinformation is a bad thing in the world of computer security.,"If he had said something like ""A rainbow table is a list of precomputed hashes, which helps save crackers processing power"" that would be one thing. Instead, this supposed security expert went into detail on how to build a rainbow table. But, the thing he was building is not even a rainbow table!  It would not surprise me to learn that he did not know the difference, as many do not. That is a serious issue, because a security expert has to be knowledgeable about threats. If he does not even know what a threat is, he can make grievous errors in defending against it. As an example, what if he ran some numbers and realized that 10 character hash database would be 3 petabytes in size? He could then conclude that ""rainbow tables"" are no threat against random password 10 character passwords. In reality an actual rainbow table for 10 characters would be about 170GB... and is a very real threat.  tl;dr  Misinformation is a bad thing in the world of computer security. "
"What you can actually do matters more than anything else.  School is helpful for lots of people, but is not required.","It will help a lot, but it's by no means required.  As others have mentioned, what you know matters a lot more than what your credentials are, but going to school is a good way to know things.  The CS program I went to was pretty heavy on theory.  I chafed at this a lot when I was a student, but years later, I'm grateful for the theoretical knowledge I gained in school.  If you can get that same knowledge without college, go for it, by all means.  I had to learn to be a software engineer on the job, however, but that would have been true no matter what; a greenhorn developer from the most practically-oriented program around would still have a lot to learn once they started working.  Now that I hire people, I don't care what degree someone has, or even (within reason) what their resume says.  I've found that the best predictor of how someone will do on the job is to have them actually program at the interview.  I don't know if this is common, but I bet it is.  There are tons of stories out there about how many candidates can't do the equivalent of fizzbuzz at an interview.  TLDR: What you can actually do matters more than anything else.  School is helpful for lots of people, but is not required. "
"it's always a people problem.  Spring is just a technology, it's actually quite flexible and solid, but entirely unnecessary 99 times out of 100.","It kind of is though.  If you're using Spring and you think you shouldn't, then there are many root-causes:   It does actually do a good job at what it's for, but you don't see it.  (This is very common actually, always worth validation your preferences against what you see.  There's usually many reasons behind these decisions.)   It used to do a good job, but it's now the centre-piece of a tangle of technical debt.  (Also very common, but this would likely to have been the result regardless of technology; unless you could go back in time and also change the team, management, the market it moves in, and any other external factors.)   The people you work with are just idiots, and they choose heavy weight solutions for no valid reason.  (Again, this is the team's fault; they're not going to make better decisions if they moved to whatever your preferred choice is.)    TL;DR: it's always a people problem.  Spring is just a technology, it's actually quite flexible and solid, but entirely unnecessary 99 times out of 100. "
"if you point a newbie at this, you'll deliberately teach them incorrectly.","This is wrong. The very first program uses  gets  - function so dangerous that it has been actually  removed  from the C standard and is not legit C anymore ([click](  In the later ones, they're using magic numbers to change a character's case (and the C standard doesn't enforce ASCII - any encoding is viable, as long as values for  '0' ... '9'  are consecutive), completely ignoring the fact that C does provide this functionality in the  ctype.h  header (actually, it's better -  toupper('8')  will not return garbage).  There's also a program that claims to convert a string to uppercase, but only handles a single character. I didn't bother checking later programs after this.'  tl;dr: if you point a newbie at this, you'll deliberately teach them incorrectly. "
the offended girl in your anecdote was just looking for a reason to be offended.,"fellow is not always gender specific. fella is.  googling ""fellow"" turns up:  > 1 : comrade, associate 2 a : an equal in rank, power, or character : peer b : one of a pair : mate 3 : a member of a group having common characteristics; specifically : a member of an incorporated literary or scientific society 4 a obsolete : a person of one of the lower social classes; b archaic : a worthless man or boy; c : man, boy; d : boyfriend, beau 5 : an incorporated member of a college or collegiate foundation especially in a British university 6 : a person appointed to a position granting a stipend and allowing for advanced study or research  and  >A fellow in the broadest sense is someone who is an equal or a comrade. The term fellow is also used to describe a person, particularly by those in the upper social classes. It is most often used in an academic context: a fellow is often part of an elite group of learned people who work together as peers in the pursuit of knowledge or practice.  So, Merriam Webster's definition 4.c. refers to man/boy, but in general it refers to peer, associate, comrade, etc. Academic meaning aside, of course.  EDIT: TL;DR the offended girl in your anecdote was just looking for a reason to be offended. "
"Meaningful names and clean code are more readable, and therefore easier to grok, fix, and less likely to contain subtle bugs.","It's not about redundancy, it's about clarity.  That code you've grown to understand well today will be the strange stuff you have to maintain two years from now,  unless  it's well written ;)  Besides, a well factored API will cut down on redundancy, and unless you're the only person who ever sees your code your personal understanding is kinda irrelevant to the next guy...  I'm not arguing that you don't have to understand what you're writing.  I'm just saying that research (and experience), show that readability helps reduce bugs.  Magic numbers/strings/bools do not improve readability, and are a well known source of bugs and support issues.  TL;DR: Meaningful names and clean code are more readable, and therefore easier to grok, fix, and less likely to contain subtle bugs. "
I'd appreciate specifics additions or corrections if possible.  I'm trying to make this repeatable.,"However  both  Ubuntu and Windows can require OS-based package installation that use the OS, outside of the PIP or setuptools infrastructure.  And the user doesn't know in advance whether a package will require this.    For Python on Windows, for instance, I have had to install numpy, scipy,  pyzmq using a Windows installer.  But for Ubuntu, you may need to pre-install various packages before you build Python.  For instance, if you want to use a Python 2.6 version which was built without SSL support, but you need to include SSLsupport for your App.  For what we're using in a Django app, this requires  sudo apt-get install opensslsudo apt-get install python-devsudo apt-get install libxslt-devsudo apt-get install build-essentialsudo apt-get install libldap2-devsudo apt-get install libssl-devsudo apt-get install libsasl2-dev  And  then  you get to build and install Python 2.6.  EDIT: If whoever decided this was worth down-moding could also explain the ""one Stackoverflow"" they found or ""one page"" in the mythical user guide they read to find all this out, I'd be very happy and more encouraged  than I am by down-voting what was difficult information to gather.  My interest is in creating a repeatable process so that new developers can be added to a Python/WSGI project more easily. It took a while for me to find all this.   Here: [Python: can't install python-ldap](  Here [pip install lxml error] (  Here [Python - Install OpenSSL] (  Here [Adding SSL support to Python 2.6](  And here: [Installing easy_install… to get to installing lxml](   tl;dr I'd appreciate specifics additions or corrections if possible.  I'm trying to make this repeatable. "
"I don't hate PHP, but to deny its criticisms is really not a good idea.","Yes, language fanboyism is stupid and unproductive, but it does not mitigate some of PHPs problems, which must be addressed if top tier development shops are supposed to take it seriously.  And by top-tier I mean ""enterprise"".  The inconsistent function naming/parameters are just the low hanging fruit, there's plenty more where that came from.  But whatever...this isn't news.  Here's my issue with the PHP community: if you have the audacity to speak out about the language, you're written off as yet another python/ruby fanboy who just wants to pile on.  Fuck that.  I've been writing PHP almost as long as there's been a PHP (since 3.0), and I have no plans on abandoning the years of knowledge I've accumulated during that time.  If I critique the language, it's because it's justified.  But I also think PHP doesn't fill every niche well.  In fact, there are some things PHP is just not good at, despite all Herculean efforts to fit a square peg into a round hole.  I'm still of the opinion that PHP is a supercharged templating or glue language, and anyone who uses it for anything critical is insane.  But that's just like, my opinion man.  TL;DR I don't hate PHP, but to deny its criticisms is really not a good idea. "
"while great for learning a language by itself, this is risky advice for practical applications of languages.","This advice sounds good for learning a new language for fun or purely educational purposes, but not for evaluating new languages to be applied to practical problems immediately.  If you have some technique that works for your problem domain, and you run into trouble applying it in this new language, it's a lot to ask to just ""forget about the problem while you learn the language"". You can only kick the can down the road so far. Some 6 months later, you'll need a solution, but might find that not only does your favorite technique not work, you've learned the language inside and out only to find that the language designers  failed  at providing any other tools to solving the problem.  Asynchronous programing is an example that's popular right now. I might switch from C# to Go, and find the author's advice works: I don't have async promises, but Go says to use goroutines. I practice for a while, figure out how the techniques work, and will find that it solves a similar problem. That's a case where the OP's advice works.  However, I could switch to Node/JavaScript. The community will tell me ""give callbacks a chance"". On the surface, my kneejerk reaction is that this is just as bad as the mess created by IAsyncResult, but I persevere, like the OP suggests. 6 months later, I've realized that Node's async facilities are even  less  flexible than IAsyncResult, have the same callback nightmares and worse, lack pattern reuse as each async API in Node is subtly different. That's 6 months wasted.  tl;dr : while great for learning a language by itself, this is risky advice for practical applications of languages. "
"Drawing conclusions from timeit isnt wrong, but its important to understand that it's naieve.","I mean, this blog is just pointing out the obvious. Yes, operators are faster than function calls. They always are. Just like {} is faster than dict(). This isnt specific to string formatting this is a basic property of the python language.  Also, he doesnt break down the python bytecode so his analysis is generally baseless.  For those looking to investigate stuff like this, by all means do it. This blog is step 1, do some timeits. But PLEASE dont stop there. Try to understand WHY. The next step is to decompile the code into python byte code and see what the codes actually DOING.  In this case, with format, Python sees the word and then looks it up in the context of the string namespace and then calls the function. Where as % is a builtin operator and doesnt incur the lookup penalty.  tldr: Drawing conclusions from timeit isnt wrong, but its important to understand that it's naieve. "
"programming work isn't the only important work out there, and programmers are just as guilty sometimes when it comes to eyes glazing over about someone else's work","This is probably not even going to be read by anyone, being at the bottom and all.  I'm the girlfriend of a programmer/computer engineer, myself being an environmental science major.  While I fully understand the differences between what his jobs are and what he does when working, I have not been able to wrap my head around exactly how to do what he does.  This means I can't help him, I can't do the dual-programming he speaks of (although the possible pros of bouncing off another person makes total sense to me), and I know the many languages there are in programming, however I do not know the little nuance differences between them all.  But it's not for lack of trying; I retain a good amount of what he's telling me (about processor wars between Intel and AMD and things like that).  My problem is that this isn't enough for him.  I have friends from college that were also programmers and their view of people who don't know how to program or don't know the languages are they're a little simpler.  While my own boyfriend is excellent at holding back verbal condescension, it is kind of hurtful when I discuss my own projects about invasive species eradication or soil erosion control he just kind of smiles and goes blank.  My projects are important to ME while his are important to him, though he doesn't seem to think so.  It's as though to him ""if SHE can do it, then it can't be that difficult or important, can it?""  tl;dr programming work isn't the only important work out there, and programmers are just as guilty sometimes when it comes to eyes glazing over about someone else's work "
I read this story to my girlfriend and hope it improves my relationship,"I was about to go to bed, my girlfriend was lying down in the futon next to my desk. I told her ""one more submission"" before going to bed. So this one drew my interest (it's a good title). I clicked it and saw that it was a lengthy article, so I asked my girlfriend to come read it with me not knowing what it would be about. She didn't want to read it because she was too tired so I read it to her while she sat next to me. This was a very eye opening story. I've always tried to get my girlfriend more interested in what I do, programming, chess, biking, whatever. She never really shows any interest though, but I do stuff she likes, like running. I love her and she's great, but it really got me thinking. I spend more than 50% of my day doing stuff she has no idea about. She never asks, I usually tell her what I do some days. Right after the article she asked what my favourite programming language was - I answered but also told her she should be genuinely interested. Hopefully she gains some interest, because I think to be in a strong relationship, you should know and understand (at least at a high level) what your partner does.  TL;DR  I read this story to my girlfriend and hope it improves my relationship "
My boyfriend and I support each others different hobbies :),"I LOVED this article. I mean LOVED it. I really think that that is a very important part of a relationship. Now, I'm not a computer programmer (although I've dabbled in HTML,) and neither is my boyfriend of nearly five years, but we both have very different interests and we support each other 100%. I love dance, and while my boyfriend is not about to get onstage and perform a ballet, he has been to every recital, competition, and entertainment affair I have had. He can tell the difference between steps and knows how to distinguish between a good and bad dancer. My boyfriend is a weather nerd. I mean LOVES the weather. He tracks hurricanes and snowstorms for fun, and has his own weather modules. While I don't actually give a flying flip about the weather in such great detail, when he talks about it or tells me about a new storm, I hear him out--and I usually learn something I didn't know! I also accompany him on every single trip he takes to go pick baseball cards out to add to his collection. We share some of the same interests, but when it comes to stuff like this, we really ARE interested even if it isn't our own hobby, because we know how happy it makes each other!  TL;DR: My boyfriend and I support each others different hobbies :) "
Joomla is OPs best choice from his given options and what we know about him.,"I hope that you're right about 1.6 being out so soon, I've not found the expected release date anywhere, do you have a link?I'm using 1.6 on several internal websites now as it solves so many weaknesses of Joomla.That said, given the OPs list, I'd choose Joomla.  I know nothing of PHP-Fusion and I cannot stand the php4 put-it-all-in-one-table architecture of Drupal.  One shouldn't code on a framework that makes one feel ill every time they look at the architecture.Codeigniter (Expression Engine) would be nice, if they ever updated it or even addressed long standing bugs.  I really enjoy writing small projects in it anyway.  For larger projects where bug fixes and updates are important, Symfony is my choice and there are two CMSs that are built on it, I like [Diem]( although I've never actually built a project on it.  tl;dr  Joomla is OPs best choice from his given options and what we know about him. "
"essentially I'm pointing out that in C and C++, const is an annotation, not a guarantee.","There are a couple reasons you might  do something stupid like that .  The first is accidentally.  Since C and C++ are not, in fact, type safe, it's not hard to write a bug that accidentally mutates your ""immutable string"", for example perhaps by overstepping the bounds of some array.  Another case is initialization of a C++ object.  Say a base type has a const field and you're a derived class that  really would like  to set the field to some value that can't be known before the call to the base type constructor... in that case this is  something stupid  you can do to get what you want.  In either case, if you believe a  const  object is really constant, you're going to be in for a surprise.  *edit, TL;DR, essentially I'm pointing out that in C and C++, const is an annotation, not a guarantee. "
"With good tools, unit tests are practically the same effort as running your code before you check it in in the first place.","I've been slowly developing the testing religion while working on my startup. We've got pretty decent coverage for our models layer, but views and controllers are all but completely untested. The unit tests at the model level are extremely targeted and, as a result, often need to be completely re-written when internals change. However, I wind up testing my code in the REPL anyway, so I often just do it in VIM with autotest instead. It's practically identical, except that when I'm done, I check in a log of what I tried & proved to work in the form of unit tests. It takes some practice to get used to it, but eventually I've started to find it to be more convenient and predictable than the REPL. As a bonus, it's become entirely obvious that writing testable code is writing  better  code.  tldr: With good tools, unit tests are practically the same effort as running your code before you check it in in the first place. "
"The code is not C99, ANSI C, or K&R.  I assume it is not c++98 either.",">If this is C99, the exit value is defined to indicatesuccess to the runtime environment, just like inC++98, but for older versions of C, like ANSI Cand K&R, the exit value from this program willbe some undefined garbage value.|  Are we discussing compilers or standards?  If compilers, gcc (and I suspect most other compilers) can be made as forgiving or as hard-ass as you care to make it.  If standards, The White Book clearly states;  >You may have noticed that there is a return statement at the end of main. Since main is a function like any other, it may return a value to its caller, which is in effect the environment in which the program was executed.  Typically, a return value of zero implies normal termination; non-zero values signal unusual or erroneoustermination conditions.  >In the interests of simplicity, we have omitted return statements from our main functions up to this point, but we will include them hereafter, as a reminder that programs should return status to their environment.|  tl;dr The code is not C99, ANSI C, or K&R.  I assume it is not c++98 either. "
"New to Liberty Basic, Game for Class Final, Its already done, would still like to improve my game for fun!!","I use Liberty Basic for our Computer Science class (HS) and our Final Project is to create a game!! Cool!! I know! I decided to make Space invaders, If you would like to try it out so far I can email you a ZIP file of it all. I have the main game finished, Movement, graphics, shooting, and a pretty background. It would be nice if someone here could help me -make the hitboxes larger, make a cooler moving background, complete with a flashing name of your choice (Similar to Minecraft's yellow text in the main menu) and cool stuff you would like to implement  Oh I turned it in, but showing him a pimped out version would be cool too!!  -TLDR New to Liberty Basic, Game for Class Final, Its already done, would still like to improve my game for fun!! "
"I hated it and found it to be awkward, slow and unreliable in production.","These are my first hand experiences...others experience my differ so take this with a grain of salt.  I worked with it for about four years on a legacy system that was the company's cash-cow.   I'd prefer to never use it again.  First off, fail-over and clustering were awkward and unreliable.    If you need to build a distributed system or even a webapp that has to be scaled horizontally you will likely need to look to [ZEO] ( which in all honesty was the weakest link in our product due to almost non-existent instrumentation or management tools and reliability issues we experienced.   If you use it be prepared to spend time parsing logs manually or with tools like splunk or sentry.  Also, as the number of users  we added to the system grew (not a lot really...maybe a few thousand concurrent), we saw more and more ZODB conflict issues.  Next, it was damn slow for everything except reads (which it really excels at) under heavy load.  Finally, nobody uses it figuratively speaking....so it has a tiny user community and finding people that want to work with ZODB becomes more difficult.  TLDR;I hated it and found it to be awkward, slow and unreliable in production. "
"Inbox applies Sturgeon's rule to my email. I get ~30-50 emails on a bad day, only about 3 I care about. Seriously.","My solution to you?  Get a Nokia N900. Install *nix, use mutt. Or, install K9 and go tweak.  The whitespace is 80% for touchability. I have big fingers, and on some smaller screens, touchability sucks on most email apps. Touch-interface devices need bigger hit zones than most are used to, and you have to account for error.  Inbox substantially lowers the number of emails i have to care about. eBay? It's spam. Amazon? I probably bought something. I probably don't need those in my main inbox. Yes, Gmail's tabbed interface has that, but it also can't figure out that when Amazon does send me something and it has tracking info, I want it. [I can't ask Gmail ""Flight to MSP""]( and get details that make sense. I can use Inbox as a GTD tool that doesn't suck.  TL;DR: Inbox applies Sturgeon's rule to my email. I get ~30-50 emails on a bad day, only about 3 I care about. Seriously. "
"summary, the guy's a whiner but the issues he initially mentions don't surprise me, and they come from Apple.","If I understand things right, it seems the problems come -- at least in part -- from apple:   default system libraries don't allow for PHP compilation, that might be a problem with Apple, PHP or more than likely both   OSX bundles PHP out of the box (as well as Ruby and Python), but the symbols are stripped from the binary, which means PECL extensions can't be used. This is clearly Apple's fault, even though the retardation that are PHP's extension mechanism and PECL extensions doesn't help they're not the culprits here.    Then there's the end of the post, where he whines that Apple doesn't help them and doesn't give them free hardware. Baa whaaa whaaa. But the first paragraphs actually seem to match the headline.  FWIW other communities have had the same problem:   For a long time, the bundled Python and Ruby were woefully out of date. They were updated with 10.5 (and some nice libraries were bundled), but there's been no update since (my apple-provided Python 2.5 is  2.5 (r25:51918) , the Macports one is  2.5.1 (r251:54863) , I've yet to see any mention of an apple-provided Python 2.6 (Macports?  2.6.1 (r261:67515) ), and I know the Ruby side isn't any better. It also tends not to work very well with e.g. the MacPython installer, but that's less of an issue   Java isn't exactly up to date: Java5 is the update 16 (latest is update 17) and Java5's already been EOL'D. Meanwhile on the Java 6 side Apple took their sweet time to provide one (to the point that some guys started porting FreeBSD's Java 6 to OSX, and they got the time to port all of it), when they finally did they basically told the PPC and Core buyers to go fuck themselves (they only provide 64bits java binaries, and as far as I know only the x86-64 ones) and they didn't provide any update since the initial binary (OSX, java6u7, official is 6u12)    And re the Java stuff, it's not like it's Sun's fault, Apple explicitly told them they wanted to handle OSX's java (to make the GUI toolkits look less shitty on the platform)  tl;dr summary, the guy's a whiner but the issues he initially mentions don't surprise me, and they come from Apple. "
"Executable diagrams"" are a trap for any nontrivial project. They look good, but will turn your code into spaghetti.","Oh dear god, not this again. I mean we've had this promise of, ""We can generate running programs directly from diagrams,"" for more than twenty years now. First it was the CASE tools of the Eighties, then Rational tried it with Rational Rose, and now this.  In every case so far, the tool works great as long as your logic is simple and your relations are straightforward. Unfortunately, the only projects where those conditions apply are either in classrooms or tech. demos. Once you start trying to generate executable code for applications that have complex multi-way relations or complex business logic, the executable code has always turned to be harder to maintain in the long run.  I worked for a company that tried to move to a more formal methodology. There was a huge project to put all the data model classes and their relations into UML diagrams and feed those diagrams into Rational Rose. From that point on, developers were not supposed to modify modeled classes directly, but rather update the Rose model, which would then autogenerate the code.  It might have worked for a couple of years, but by the time I got there, pretty much everyone had acknowledged that the attempt to put everything into Rose was an unmitigated disaster. The classes generated were obtuse and hard to use. Code generation from the model took an hour. Everything seemed to be much more difficult than it should have been. Piled on top of this was the fact that the egregiously long generation times perverted incentives for the developers. A developer was more likely to abuse an existing model class to hold new data rather than modify or create a new class. Why? It was literally easier and faster (in terms of development time) to take an ""almost right"" class and add functionality to it via reflection than it was to just go ahead and create a new class with the necessary functions.  At the time I joined, the company was in the midst of a multi-year project to strip out all of the Rational Rose classes and replace them with hand-made classes that would do the same things, but be easier to use and modify.  tl;dr ""Executable diagrams"" are a trap for any nontrivial project. They look good, but will turn your code into spaghetti. "
"this is fun"" is fine. Except don't use ""this is fun"" now. Because everybody knows it.","""this is fun"" is a very secure password against brute force, and when stored as a salted hash, even if the attacker knows that the password is  [a-z ]{11} .  Based on that knowledge of the limited character set used, there are 27^11 possible passwords to try.  5,559,060,566,555,523  That's 5.5 million billion passwords.  It is secure against brute forcing the login page, just because that is slow. We know that.  Now to take the hash (salted, so rainbow tables are right out, and assume you know the salting algorithm) and work it out? Let's say you have an expensive GPU-powered hash cracker, running at 1 billion hashes per second (a few hundred times faster than my MacBook Pro, here). That's still 5.5 million seconds, or 2 months, to exhaust them all (of course you've got a 50/50 shot at finding it in half the time or less).  Now you could devote your thousand-node zombie botnet to cracking this hash in a few hours, maybe, but then you've just got one password from one hash (since it was salted, remember). Good job. Congratulations.  The article also says that dictionary words are bad, but what about three dictionary words separated by spaces? And again, this is limiting our scope... you could join words with any of "" _,.;:-/ "" etc.  519,098,151,596,032 passwords. Only .5 million billion, but still not bad, as demonstrated.  Add word separators "" _,.;:-/ "" to the mix, and you get:  33,222,281,702,146,048  Oh my.  Add variations in capitalization and it's absolutely out of the question.  tl;dr  ""this is fun"" is fine. Except don't use ""this is fun"" now. Because everybody knows it. "
YAGNI is preached because developers are more clever than lazy usually. Just because it isn't always true doesn't mean it is a bad idea.,"As you said YAGNI isn't set in stone, but it shouldn't be ignored just to allow flexibility either. Here is a simplified version of the situation. Lets say you are writing something and thinking about expanding it.   If your code is never expanded then the X hours spent making it flexible is wasted  If your code is expanded but wasn't flexible then the Y-X hours spent making it flexible afterwards is wasted   This is a form of probability. Which is greater the chance of not expanding times X or the chance of expanding times the difference? Obviously we don't know how long these will actually take, but you can usually guess what X is.  If X is small then you should just implement it, adding an interface to an area that could change based on business rules takes next to no time and makes future expansion possible without major rehaul, so flexibility wins. In contrast if X is enormous then it isn't worth it because the time for Y likely isn't much bigger, if you are building a factory method to control creation of factories just in case there is a need to indirectly create a factory then YAGNI wins.  The reason that most people say YAGNI and leave it at that is because developers are notoriously clever. Why use a hammer when you can use a bowling ball, thread, knife and catapult. Sure it takes longer but it is more fun and allows defining the length of the thread at runtime.  tl;dr - YAGNI is preached because developers are more clever than lazy usually. Just because it isn't always true doesn't mean it is a bad idea. "
"Neither Microsoft nor Webkit are being hypocritical. But who's this ""we"" that's being hypocritical? Was there something that stood out in my post?","So, thanks for the clarification regarding ARS. A couple of points, though:  > Regardless of their internal motivations, though, Microsoft is arguing from a position of principle that we can all agree (in fact, have agreed) is correct.  Not on the page you linked to. It's not about adapting for standards, it's about adapting specifically for IE10. For example, the blog says this:  > The following WebKit-prefixed properties also have the same behavior in Internet Explorer 10 but require Microsoft vendor-prefixing (for example, with the prefix “-ms”) because the corresponding standards have not progressed far enough at the W3C to be unprefixed.  It's not an argument at all, or any sort of political statement. It's not really good, or bad. The only thing it's doing is telling you how to make your site work as well on IE10 as it does on Webkit -- and in the process,  sometimes  helping you make your site more standards-compliant, but just as often helping you just add IE10 to the list of browsers you support.  I'm also not sure how you get to your TL;DR. Neither Microsoft nor Webkit are being hypocritical. But who's this ""we"" that's being hypocritical? Was there something that stood out in my post? "
All web scripts have their ups and downs. Think OScommerce is bad? Let's talk VirtueMart.,"I've had my fair share of commercial Magento websites. Let me tell you, if you never plan on making a single code change, Magento can work wonders. However, once your customer requests a new feature for their Magento-powered website... Have fun. Have fun opening tons upon tons of files, only to see that the file is including another function in some obcsurely-loaded file.  It is beatifully written, in terms of standards, design patterns, etc. However, it is not something that can just be picked up and hacked away. When you charge a flat-rate per hour, you really don't want to eat away at your profits just sifting through files or docs just to do a simple change.  Also: this Magento experience is from 2 years ago. Back then, there wasn't any ""free"" support or docs. And much less support forums. I don't know about know, and quite franky, I'm never looking back.  TL:DR: All web scripts have their ups and downs. Think OScommerce is bad? Let's talk VirtueMart. "
"I agree with you, the concept of /this shouldn't be a class/ being labled as ""oh it only has 2 methods"" is a limited worldview","I was wondering: doesn't this advice completely break the concepts of ""this class has ONE purpose"" and in general RAII?  Say I have a class (C++ example) that is supposed to output things in a certain manner in a Draw() method.  I COULD have some member function that changes cout.flags() and cout.precision(), and another that restores them. Maybe these are even private member functions.  Now Draw looks like this, in the best case:  void MyClass::Draw(){    save_cout_format();    //do work, dick around with flags as need be    restore_cout_format();}  But now, that class is  partially responsible  for maintaining proper status of cout's formatting, and any time I call ""change"" I have to also call ""restore"", and I have to have member variables to put these all in. And if I don't do this JUST RIGHT, other parts of my program will suffer.  OR I could have a format_saver class, that on construction saves existing flags. On destruction, it restores them. It's destructed when any method leaves scope.  Cool, now my Draw() looks like this:  void MyClass::Draw(){     format_saver fs;     //do work, dick around with flags as need be}  MyClass  is no longer responsible  for  caring  about the cout flags.It just has to do it's job of drawing, with the knowledge that if it needs to dick around with flags, it better tell the class responsible for maintaining the state of cout to maintain that state.  tl;dr I agree with you, the concept of /this shouldn't be a class/ being labled as ""oh it only has 2 methods"" is a limited worldview "
"Rust is switching from Ruby-style iteration to C#-style iteration because (for our purposes) it's more performant, easier to optimize, more composable, and more flexible.","Let me paste some clarification.  Rust is in the middle of switching its dominant iteration idiom from internal iterators to external iterators. See also:  The  for  construct will be changing in the 0.8 cycle to reflect this change. Currently it looks like:  for [1,2,3].each |i| { /* a closure */ }  Where eventually it might look something like:  for i in [1,2,3] { /* just a regular block */ }  I'm actually surprised that the author praises the ""freedom and flexibility"" of our old  for  loops... the reason for making this switch at all is because we found our old semantics to be neither sufficiently composable nor sufficiently flexible! :)  Hopefully the author will revisit this space once the new implementation is finalized, perhaps with a comparison to D this time.  TL;DR:  Rust is switching from Ruby-style iteration to C#-style iteration because (for our purposes) it's more performant, easier to optimize, more composable, and more flexible. "
"If your boss [and the rest of the political structure] doesn't understand or care about your unique field of expertise, it's going to suck no matter what you wear.","""Captain of Titanic vows to let passengers rearrange deck-chairs?""  Restrictive (aesthetic) dress-codes are usually just an  indicator  of deeper organizational/cultural issues against developers/engineers.  The policy may indicate that the organization is strongly focused towards people whose job involves schmoozing and managing their own appearances (e.g. sales)... and implies that they will be  deficient  at managing/rewarding people whose workflow/priorities are different, such as those creating a long-term product which has to objectively work according to the laws of physics.    ""Well, Bob, I'm afraid I'm going to have to ding you with a 'Below Average' for this evaluation period... Everyone has to do their part to maintain our company's reputation for professionalism and high-quality work.""  ""I... uh... I thought my work  was  high-quality, sir.""  ""The crux of the matter is the dress-code, Bob. You've definitely made some interesting... uh, gizmos...  but professionalism also requires meeting certain other expectations.""  ""You want me to wear my hard-hat and safety vest  in the office ?""  ""Ugh, no! I said  professional , Bob. That always means shiny leather shoes, white starched collared shirt, a tie, that sort of thing. You have to make a good first impression. What if a client were to pop in here and see you like this?""""  ""I'd ask them how the hell they got past the security-door and escort them to the lobby, because they aren't supposed to be wandering around engineering, sir.""   TLDR:  If your boss [and the rest of the political structure] doesn't understand or care about your unique field of expertise, it's going to suck no matter what you wear. "
I have a crazy non-standard use-case that made using regular pool allocators impossible.,"I ran into some design problems with using the boost pool allocator directly. I certainly tried that first as I didn't want to have to build such a fundamental data structure from scratch. :)  My use case is that I'm going to have these little long-lived work packets that will be shuttled around between the available work threads (yes, there's a reason I'm doing that instead of prioritized threads), so each work packet needs to carry around its own instance of a memory pool to be used for all allocations it does (and there will be a  lot  of them -- dealing with stochastic evaluation of information fragments).  TL;DR: I have a crazy non-standard use-case that made using regular pool allocators impossible. "
there is a reason the keyword strict was abandoned in c,"Rust is trying to be c++ with type safety and pointer safety. I loved it and thought it was beautiful when I first saw it.  Then, I actually delved into the language and it still has almost all the problems c has they are just hidden under a layer of unsafe functions. You can still make all the same mistakes and in all but the simplest cases none of the compiler checks will help you at all.  It's just as easy to use unique, ref counted, hazard or other wise safe pointer libraries in c++, compile with -wall and check everything with valgrind.  Rust is a step in the right direction but really only puts a bandage over the problem without truly solving it. I for one don't think the problems with c/c++ will ever truly be solved because so many compromises have to safety have to be made to get the performance a c/c++ replacement needs.  TL;DR: there is a reason the keyword strict was abandoned in c "
"Writing software takes time and money, especially when done right.  If you plan on using someone's software extensively, show your appreciation by paying them for their work.","Disclaimer: I love free software as much as the next guy.  The product I write is undergirded by scores of free software solutions that I didn't write or test.  In my time, I've devoted hundreds of hours and lines of code to open source and appreciate the accomplishments and potential of free software.  Despite this, as a software developer for a similarly priced application (though sold to a totally different target audience), I have to say that when I see this sort of thing, it doesn't excite me.  Lately, and at least more so than usual, a popular business trend I'm seeing is to spend valuable time and resources on the development of a quality product only to give it all away for free (or for a very negligible price).  I think that Apple's App Store has a lot to do with this - the constant trend towards free or abysmally cheap software is lowering the general public's already unrealistic view of the costs of quality software.  I understand free software models and alternative revenue streams like in-app advertising.  Sometimes it works, sometimes it doesn't.  What I'm wanting to convey is that if you're willing to invest in bringing a quality commercial software product to market, have respect for yourself and sell it at a value that's fair to both you and your customers.  If you don't have enough respect for your own product, know that your actions are reinforcing the idea that quality software and the work that goes into it is expendable, and it's affecting those of us attempting to make a living selling their product for what it's worth.  tldr:  Writing software takes time and money, especially when done right.  If you plan on using someone's software extensively, show your appreciation by paying them for their work. "
"It might be theoretically possible for voting machines to be just as accurate and hack-proof as ATMs, but without the right feedback and incentives it will never happen.","The incentives and forms of feedback for ATMs are totally different. With ATMs, any errors or hacks could leave the bank out millions of dollars. At the end of the day or month, there will be two groups with competing interests reviewing the results - the bank, and the customers - and errors one way or the other will be eventually be caught. There's both a paper trail (receipts) and a feedback system that provides incentives for everyone to get the calculations right.  With electronic voting machines, the voters have no way to double-check that their votes were counted correctly, the politicians have no way to double-check that votes for them were counted correctly, and no one at all has an incentive to make sure the results come out right other than a moral sense to  do the right thing . I'm sure 95+% of precincts do their best to  do the right thing , but you really can't design a system around hope that people will  do the right thing  with little feedback or enforcement.  I'm a big fan of the hand-marked paper ballot with in-precinct electronic vote-counting scanners. Each precinct can provide near-instant vote totals, you have a paper trail and can recount all ballots or do spot checks, and unlike touch-screen systems elderly and computer-illiterate people have no problem putting their Xs in the boxes on the paper ballots. I see no reason for anyone to use the more expensive, more hackable, and less user-friendly all-electronic touch-screen systems.  tl;dr  It might be theoretically possible for voting machines to be just as accurate and hack-proof as ATMs, but without the right feedback and incentives it will never happen. "
In the vast majority of 32-bit programs these exceptions are still eaten because Microsoft hasn't educated developers about the available solutions.,"Update: I appreciate your comment, but I have to disagree.  I just tried the compatibility manifest (see the comments on my blog post) and it didn't work.  The functions for turning off the exception swallowing are still not documented.  There has been no significant drive to encourage developers to use either of these techniques -- just a few blog posts here and there, but no support from Visual Studio, which actually actively discourages use of the compatibility manifests.  If you crash in a callback then PCA will come up and tell you that  something  went wrong but it gives developers no way of finding out  what . As far as I can tell all that the compatibilty manifest does (regarding this) is suppress PCA when an exception is swallowed.  tl;dr:  In the vast majority of 32-bit programs these exceptions are still eaten because Microsoft hasn't educated developers about the available solutions. "
"these exceptions used to be eaten. Microsoft now wants to show them to you, if you're willing to have them.","i think understanding [the history is useful](  On 64-bit systems, if an exception happens in user mode, that was called from kernel mode, the stack cannot be unwound. The exception cannot be returned back to the user mode process. It just can't be done.  Because the stack cannot be unwound properly, and the program (and the stack) would be left in a non-sensical state if it did try to unwind:  > The kernel architects at the time decided to take the conservative AppCompat-friendly approach – hide the exception, and hope for the best.  This  swallowing the exception  compatibility behavior was the behavior on:   Windows XP 64-bit  Windows Server 2003 64-bit  Windows Vista 64-bit  Windows Server 2008 64-bit   Starting with Windows 7 64-bit, any  64-bit  processes will, by default, not have user-kernel-user exceptions swallowed.  This is the new  preferred  behavior. Code that used to never  ""appear""  to throw an exception now  will  throw an exception   but only on Windows 7 64-bit  and only if the process is also 64-bit (the assumption is that if you're starting fresh writing a 64-bit application then you're willing to opt out of 20 year old app compat)   It's also important to note that if your 64-bit application  does  experience a crash due to this kind of exception, the Program Compatilibty Assistant (PCA) will pop up, and apply an application compatibility  shim  - hiding these exceptions again.  Note : Your application will not be shimmed if:   you have a designed for Windows 7 manifest entry  you're running on Windows Server 2008 R2 (which doesn't have PCA)   tl;dr:  these exceptions used to be eaten. Microsoft now wants to show them to you, if you're willing to have them. "
"They say a little knowledge is a dangerous thing, and this is one of the reasons why.","> Well I'm finishing a computer science degree in May, have read 3 books on programming (Code Complete, Data Structure and Algorithms in Java, Definitive guide to JavaScript), and just received a job offer as a software developer. I would say I've learned a lot in the course of a year  Yeah... that's what  I  thought when I had yet to graduate, I could even  remember  how many programming books I'd read (let alone count them on the fingers of one hand), and had zero professional experience of working on  real  problems and  real  code, out there in the real world of industry.  There's no nice way to say this, but suffice it to say that I look back on my arrogance and naivety then and laugh pretty hard, albeit in a pretty ashamed way.  The problem here is that you sound like you've learned a lot  for a beginner with a year of programming under your belt , but that's a long way from  being an experienced developer .  You can be proud of your ability  for a beginner , but don't ever mistake that for being  objectively good , or for giving you the right to patronise anyone else, at least for a good long while yet.  You've learned a lot in one year, but you should never really  stop  learning... and if you think you're pretty hot shit now, think how much better you'll be in ten years, and  then  think how astonishingly naive and arrogant that would make your assumption of experience and competence now sound.  I really don't mean to be a dick, but this is the classic case of someone who learns martial arts, gets their first belt or two and decides they're hot shit because they can beat up a few people.   Real  mastery takes years (and multiple languages), and you shouldn't presume to judge others until you've actually achieved a good deal of experience, or you just look like an asshole to everyone who has.  In addition, I would hold off until you've actually been paid to  teach  programming,  yourself  before you start throwing around patronising judgements about who will ever make a good programmer.  TL;DR: They say a little knowledge is a dangerous thing, and this is one of the reasons why. "
"This tactic works OKAY if you're self employed, but in a real world tech company, this wouldn't fly. Oh ya, keep building apps!","The reason people are pessimist is because if you work in a company that makes > $50, you need to be. Mistakes cost money, so planning for the things that can potentially go wrong saves you money. It's great that you were able to get an Android app up on the google play store, keep it up.  Just remember that how your code looks DOES matter in a team setting. If it takes your coworker hours to try and figure out what you did simply because you wanted to get it ""working"" and didn't care how readable the code is, you are costing the company time and money. Also 6 months from now when you want to make a change to your own code, it may look foreign to you and you may end up wasting your own time trying to make sense of it.  If you don't have a CS degree, the jobs that are available are limiting at first, but with the right work experience one can get an entry level job and go from there (just remember you WILL be at a disadvantage at first, and most interviewers will choose the person with a degree over one who does not almost every single time).  Don't ignore too much of the ""negative"" crap, just learn to take it with a grain of salt. If someone has something to say after being in the industry for 20+ years, it may be worth listening to what that person has to say, then making a judgement call.  TLDR; This tactic works OKAY if you're self employed, but in a real world tech company, this wouldn't fly. Oh ya, keep building apps! "
"there is no right answer, so just pick what appears to be the best and be consistent. Respond to changes, and good luck.","At the moment, you are only responsible to collect and remit taxes in states where you have nexus (employees, inventory, buildings, or other physical presence).  Every state has different tax laws, so the complexity varies depending on the state.  Congress is sitting on a bill that changes this liability and standardize the calculation, but there hasn't been movement lately.  As for liability for proper tax calculation, it is always on the merchant. For my state, there are people suing the state because the state refuses to clarify its own laws and leaves multiple tax jurisdictions claiming they are owed tax on the same sales and leaving the merchants to fight the jurisdictions themselves.  You could use the free cloud tax API, but the problem is that you then have to agree to collect and remit tax to all of the simplified sales tax states instead of just ones in which you have nexus, which could put you at a competitive disadvantage.  In any event, this is not a php problem. It is a political problem, and neither the general public nor the government are sympathetic to the merchant. If you get big enough, you will get audited. If you get audited, they will find errors because their own rules are inconsistent. If they find errors, you will pay. It won't bankrupt you, but it will suck. To the revenue department at the states, you are a cow to be milked. They know dead cows can't be milked, so they won't kill you. Just make sure you can explain why you did things and how you attempt to follow the rules and you will be ok.  Tl;dr - there is no right answer, so just pick what appears to be the best and be consistent. Respond to changes, and good luck. "
"i too have been bitten by %.3f, life can be a bitch!","I think we have all done this.  I certainly have! did a height map / topographical map (fake voxels circa 1997) and thought the problem was in my math.  I used integer shifting and manipulation (penitum) vs float point vs MMX w/ qmov(P6) vs SSE coding (p4), SSE2 (P4). we used it for benchmarking and optimized the crap out of it, i rewrote memcopy for sakes... then i had to thread it :P  one column just kept popping up to the top, it was unusable. took me about 3+ weeks to find out the MMX version screwed was screwing the float input using %.3f ... whole time one period. it only popped up 1-2 times a week.  tl;dr; i too have been bitten by %.3f, life can be a bitch! "
there is little benefit and weird risks. Also [Cthulhu is scary](,"You open yourself to risk and lose some subtle benefits running a composer install gives you. The risks are edge cases I grant you but you are introducing them for no reason. Consider the following:   I commit vendor directory.  I make a custom modification to a vendor directory either purposefully or not and commit those changes.  Some other developer clones my project and at some point runs composer update. My custom changes are wiped out and the app breaks. I've left the project and no one saw in my 50,000 line diff that I was an idiot.  Curse the heavens and proceed to summon Cthulhu.   Running composer install also makes sure that your current environment setup can actually run the app code. It will tell you if the version of php you have is too old or too new. If you need additional extensions ect...  In addition to many of the comments already made I think it is important to also point out that keeping your exact dependencies for your project (if that is what you are worried about by commting the vendor directory) is accomplished by commiting the composer.lock file. So there really isn't a need to commit the entire vendor directory.  TL;DRthere is little benefit and weird risks. Also [Cthulhu is scary]( "
"rails-core should be more open about the development progress/status, the rest is just internet whining.","To me the issue is simple:  The Rails core team doesn't owe shit to anybody. That's true.  BUT  it's good manners to say ""hey guys, we know we promised, but there are some delays, hang on tight,"" or some other form of feedback.  It's not their fault that it got late, things happen. Bugs are discovered. But the name-calling and attitude of some people is just sad.  You could use beta4 or the git master without much trouble, so it's not like people couldn't play with it. I've personally done a few things with rails 3 (none in production yet) and they all worked reasonably fine (or at least didn't present any immediate problems that weren't easy to fix/work around).  Basically, Yehuda nailed it on one of the last posts on that thread: they need to establish a better communication outlet, and be more open about the development process. Right now they talk about it on twitter (but then, only if you follow the ""right"" people) or sometimes some things on the core mailing list, or then you can see the status on the bug tracker. But there's no centralized place where they are showing the development progress/status.  It's not that hard, and there's no need to be a dick about it. But people just like to complain :)  tl;dr:  rails-core should be more open about the development progress/status, the rest is just internet whining. "
I create a payment schedule up front and tell clients that non-payment results in non-service.,"In one way I understand the web dev and the likely frustration from finishing a job and seeing no cash.  I've been there, it's a hassle.  However, this service isn't much different than putting an ad in a magazine, on the radio or on TV.  You get your ad and you pay for the service.  If you don't pay, you go on their shit list and can't buy ad time any more.  With a website, the only thing that is different is that it's always there so it's a little like the electricity bill you pay every month.  In either case:  You don't pay, you don't play .  When dealing with clients, I tell them that hosting is part of the design package for the 1st year.  That way I control how it is being deployed (predictable os, applications and service), and I control their actual traffic.  This gives me the ability to shut them off if there is no payment.  This isn't generally a big deal because  I tell people up front  that I will be hosting the site, and I run the site for  paying  clients.  If your potential customer balks at the idea - then  buh-bye .  tl;dr I create a payment schedule up front and tell clients that non-payment results in non-service. "
"Absense of evidence doesn't disprove something and if you want to play the high and might scientist, you might as well remember that.","""Your personal experience, unfortunate as it may be, does not constitute evidence of any sort.""  You're bo better. Not a single one of your posts has provided any evidence that chiropracty is BS, for any application whatsoever.  If you were truly so scientifically minded and not just out to get on a high horse and blast every gray area in sight, you would either:  1) Provide studies that show the effects and results of chiropractic treatment versus conventional medical treatment vs no treatment.  2) Disprove the theories supporting chiropractic treatment by showing that the problems it is supposed to be treating are fundamentally unfixable by pushing vertebrae around.  3) If 1 and 2 are either not available or impossible, act like any good scientist would and say that you don't know if the therapy works, but have no reason to believe it does. That's different from claiming it's bullshit without any supporting evidence whatsoever.  TL;DR:Absense of evidence doesn't disprove something and if you want to play the high and might scientist, you might as well remember that. "
"ZeroBin is not about encrypting content, is about reducing liability of the owner of the server","I hope your post gets more visibility.  A lot of people here are concerned about the page maliciously sending the plaintext to the server, but that's not the point: the purpose of ZeroBin is to reduce the liability of whoever is running the server. Perfect for magnet links, for example.  Quoting a [comment from Hacker News](> It not about user security. You can encrypt your data with gpg and send it via pastebin, zerobin, or mailinator to your accomplice and secure. ZeroBin is about lifting the liability off the service provider.  >Think the tired old ""does linking constitute a copyright violation?"" question. If someone posts a list of magnet links or torrent URLs to ZeroBin, they can claim plausible deniability: they cannot know the content of the pastes they host.  > This reduces to deep questions about internet freedom. For example, if ZeroBin couldn't for some legal reason host ""infringing"" links, they couldn't host any links, and Tor, I2P, Freenet, and other onion/mix networks would be similarly illegal. Or can it be illegal to participate in an anonymous networks or not? Some parties would like to see the answer to be ""yes"". But we all know that there's copying bits and there's copying bits: at some point it doesn't matter, at some points bits can't be legal or illegal but just bits.  > Technology like ZeroBin can help drive the liability question down to the very, very basics where it can be eventually solved.  tl;dr: ZeroBin is not about encrypting content, is about reducing liability of the owner of the server "
"This company is worse than Microsoft when it comes to proprietary stuff, and Intellectual Property games.","This doesn't surprise me. The scumbag company pretty much doesn't touch anything if it doesn't have a Microsoft, or SAP label on it. Not to mention everything they make has a damn 'Proprietary information do not share' label  on it as well. Even their pie charts showing public information, which were quite obviously made with a program from the Office suite have those labels.  What is really lovely is they want you to share your ideas with them so they can help you get them patented. What you get out of it is a reward inside the company!...while they get to keep and do whatever with the patent.  tl;dr: This company is worse than Microsoft when it comes to proprietary stuff, and Intellectual Property games. "
"I want a screwdriver, this looks like a hammer.","I'd like to see getter/setters, but I always thought that this proposal was not consistent with the rest of the PHP language. It may not be so bad if they hadn't exaggerated that fact that they by using non-standard capital letters for the property names.  The syntax structure there builds off of nothing PHP already has. (Its mandatory nested structure, optional parenthesis, odd semi-colon usage, etc)  There's also a bigger question of how dynamic PHP should be. Should these really be fatal errors? Is there a way to define/change getters and setters at runtime? Nothing proposed for reflection? Would it be better to integrate this with something to tackle dynamic methods at the same time (if $x->f is a lambada, $x->f() didn't work, something not discussed in the ).  I think PHP gets more hate than it deserves, but I'd certainly appreciate a bit more thought put into the syntax and the behavior for some of its features, especially those that may add more wtfs than they remove.  tl;dr  I want a screwdriver, this looks like a hammer. "
I'm not sure the anti-documentation crowd is completely crazy.,"Hmm. Off-topic, but I think there's an interesting debate to be had here. My comment was more pointed at external, user-oriented documentation. But you're talking about  internal  documentation: stuff to acquaint future developers and maintainers with the inner workings of a project.  I happen to work at a company that doesn't do a lot of internal documentation. We have several million lines of assembly code with painfully non-descriptive 8 character symbols. The code has line-by-line comments, and subroutines have a short flowerbox describing inputs/outputs (but often not side effects). And that's pretty much it.  It was a bitch at first, trying to get anything accomplished. But no one else at the company seemed to mind. At first, I thought it was because they'd all been working with the system so long that they just  knew  it inside and out.  It was slow going, but after a year or so I started to be very productive. And it wasn't because I had learned the whole system; the codebase is big enough that learning it all would take a lifetime. Instead, I had acquired a strong set of skills for analyzing undocumented code.  So I think there's a tradeoff: obviously, my company had to pay for my time as I became competent working with the system. But the company didn't have to pay for anyone's time to write internal doc. Given the small size of this company, and the low turnover of employees, minimal documentation may be the right choice.  Anyway, tl;dr: I'm not sure the anti-documentation crowd is completely crazy. "
"Don't auto-opt out of sanity checking.  If you can, try to get more sanity checking on changes over time.","> Never log into production servers via a shell. Instead, write a script to do it. And test your script in development first.  Burned hand on stove?  Never cook again!  True, configuration should be moved to scripts and config management, but testing and development and troubleshooting and getting a configuration working all can take time in the shell.  > aptitude -y update; aptitude -y safe-upgrade  He doesn't mention this in the problem list, but  not  forcing all prompts to answer  yes  would have also saved him a lot of trouble.  You're in shell, the changes you make are immediate and often irrecoverable.  And yet, in package managers, they help you out with this by determining a full list of work to be done, and then letting you agree or opt out.  Agreeing before you see the list of work to perform is  always  a bad idea.  This is why  rm  is often aliased to  rm -i .  Don't auto-opt out of sanity checking, whenever you are doing anything with a non-throwaway machine.  The r0/r1 machines would also not be throw-aways, even though they weren't in production yet, they deserve to have their configurations checked over because they will soon be in production.  tl;dr  Don't auto-opt out of sanity checking.  If you can, try to get more sanity checking on changes over time. "
"Since it's origin, Haskell's been a platform for exploring new ideas, but it's design was originally very conservative.","FRP is at the library level in Haskell;  it's not 'baked in' to the language itself.  There's a few papers about doing FRP in C++, and you can do FRP in most languages.  You could even do FRP in Java, although it would probably be pretty painful.  I suppose I should amend my claim to ""AFAIK, the sole new idea in the Haskell Report 1.0 was typeclasses"", though.  Haskell wasn't originally designed to be innovative, it was designed to be a fairly conservative combination of the half dozen lazy functional languages that existed in '87.  Monads as a programming idea was discovered in '89, and monads in Haskell were researched in the early 90's.  I'm not sure that GADT's or other advanced type system extensions (higher kinded types, for example) were originally implemented in other languages.  tl;dr: Since it's origin, Haskell's been a platform for exploring new ideas, but it's design was originally very conservative. "
"Desktop OSes should embrace the ""app"" culture popularized by mobile OSes.","There's nothing stopping Linux or even Windows from doing this. Windows always searches the local directory for needed DLLs, so you could just maintain a separate copy of all of your libraries within each program folder.  To be fair though, the shared library system isn't necessarily flawed, its just horribly fragmented. Every distro maintainer and developer tries to do something different, store data in a different place or use different names. Installation wouldn't be that bad if we agreed on a standard format for application, library, and data storage. Having shared libraries wouldn't be so bad if there wasn't 10 different places to put them on the system.  Linux doesn't even really have a place to put applications. Some developers are catching on by putting everything in /opt, but then you have others like Mozilla that sadistically place files all over the system so you have no idea what actually goes where.  Forcing developers to put all of their applications in /apps, and forcing the library maintainers to put everything in /libs would allow package managers with auto dependency resolution to really shine. While were at it we might as well clean up how those applications store user data, so users' home directories don't get littered with trash. Assign each application in /apps and unique ID a la Android and create a ~/data directory containing each user's app data, so if you want to wipe or backup a particular app's data you don't have to do a fucking google search just to figure out where the app put it.  tl;dr: Desktop OSes should embrace the ""app"" culture popularized by mobile OSes. "
"Government IT procurement  is  broken, but it's not because the government is too  big . That's a naive read of the situation.","It's not a problem with  big  government, it's a problem with  outsourced  government. States could, if  big  government was popular, build their own competent IT shops. However, under Reagan, Bush, Clinton and Bush, internal federal roles were closed across the board and contractors began to be used wherever possible, and many states followed suit. This creates a shitty situation where the government staff who are awarding the contracts are not necessarily domain experts, and so tend to pick ""safe"" companies, even if the budgets are ridiculous. And of course the budgets are ridiculous because all of the big contractors have realized that there's no real upper limit to their bids.  There are some moves afoot to streamline the IT project procurement process at the federal level, and to get some in house expertise who can better award and oversee the projects, in the light of the healthcare.gov and similar debacles. I'm not particularly optimistic, but it might pay off.  TLDR Government IT procurement  is  broken, but it's not because the government is too  big . That's a naive read of the situation. "
Having the freedom to choose is almost never a bad thing :),"I don't quite agree with the post.  Most of the underlying issues in web development are due to complexity. While I agree with the author on this point, there honestly aren't many choices to do away with complexity since data is handled at so many different levels. A really good project which kind of did away with the complexity of connecting different layers was  Blerg!  but the freedom to choose Y if X doesn't work, that's pretty important when you can't foresee completely how your app will scale (eg. Facebook).  The sheer volume of data being thrown at a web app (and the rate that it grows..) requires this kind of pipe-lining and stitching together as is evident in today's web apps.  Sure, we should fix it somehow but it's quite difficult to start constructing a building without knowing it's full specs. We need to somehow be able to add to the foundation later so that as we stack floors the building doesn't collapse.  The above approach is definitely not as good as having a solid super crazy kick ass foundation from the get-go. But we don't want to build that either in-case the building only reaches 10 floors and not the 1000 that we expected..  TLDR; Having the freedom to choose is almost never a bad thing :) "
"Those who can't do, teach. Those who can do, try and repair the damage.","Scala was deficient - Break was added to Scala in 2.8. Continue wasn't and hasn't as yet. Given both break and continue are called out in the Scala FAQ and your comment, this is obviously contentious.  Unfortunately all I can say is that in my experience, poor programming practices originate at Universities and we have to re-teach the graduates as lecturers enforce arbitrary standards basd on the lecturer's own world view rather than from cited papers (or all the graduates I've worked with have been unable to provide the citations).  Where are the papers showing improved readability and maintainability when break/continue are not used? Or decreased readability and maintainability where they are used?  Given, for example, that the Linux kernel is edited by a large number of contributors who not only use break and continue but also use goto, where are the comparative surveys demonstrating that the Linux kernel is unmaintainable or suffers from poor programming practices?  TL;DR: Those who can't do, teach. Those who can do, try and repair the damage. "
the situation where you rely on the brakes the most is  their worst case scenario.,"This is true but hydraulic brakes come with their own set of issues.  First of all they're power assisted just like the steering is: without the engine running, your brakes will be much harder to actuate and their overall stopping power will be greatly reduced. (It's very unlikely your average driver will have the lower body strength equivalent to their power assisted brakes.)  In addition they're assisted  by engine vacuum , which is practically nil under wide open throttle.  Furthermore brakes rely entirely on friction: which means even modern braking systems will fade over time. So you're quite literally in a race against the clock to stop your vehicle.   tl;dr: the situation where you rely on the brakes the most is  their worst case scenario. "
"1 valid complaint about number types and math, and the rest was FUD.","> The only numeric type is IEEE float,  Your only valid complaint about the  language .  > It doesn't have goto to make it a better cross-compilation target language  Goto's are generally code smell. It's likely an  improvement  that they've forced people out of using it. And it's not going to prevent a compiler from being implemented for any particular architecture, as you're seeming to suggest.  I can't think of a single instance where having  goto  would help me. ...Actually in any language I develop in currently, not only JavaScript.  > It doesn't have native classical inheritance.  ""I don't want to learn new things, I want JavaScript to be like my favorite language I already know""  > Objects don't have destructors/finalizers;  ""I don't want to learn new things, I want JavaScript to be like my favorite language I already know""  >  there are no weak references so good luck building any sort of caching mechanisms.  There have been plenty of caching mechanisms written. GC handles the rest.  > No control over when the GC runs; and thus absolutely no performance guarantees.  Not a problem with the  language , but a problem with individual runtimes.  And you're wrong anyhow. NodeJS actually allows you to manually trigger garbage collection. Run the runtime with  --expose-gc  and then just use  gc();  or  global.gc(); .  > That whole misfeature about how Javascript handles this.  ""I don't want to learn new things, I want JavaScript to be like my favorite language I already know""  TL;DR: 1 valid complaint about number types and math, and the rest was FUD. "
"I'm disappointed with this course. I'd been looking forward to it, but the content just isn't working for me.","I'm having all of the same issues - the course doesn't seem as well prepared as the previous one, and is perhaps overly ambitious in its aims.  It also doesn't help that the lecturers are quite clearly academics - the lectures are long on (poorly presented) theory but short on practical information. Perhaps it's more a reflection of the way I learn, but I'd rather have a few examples to start with, then dip into an explanation of what we've just seen - the lecture on monads being a perfect example of this. Apparently there are 3 rules that define a monad, but no explanation as to why monads are useful, or why these rules matter!  The assignments, too, lack the necessary scaffolding to help the learning process. I don't expect them to be easy, that would be a waste of time, but too often I find myself searching the forums (thank god for the forums!) to find out just what it is we're being asked to do. A lack of test cases doesn't help, either...  TL;DR: I'm disappointed with this course. I'd been looking forward to it, but the content just isn't working for me. "
"it's necessary because of some other features, and when those matter it is fantastic","If it's specifically with numeric types, that's probably a result of the way Haskell deals with overloading for them - all numeric types are instances of the typeclass Num, which requires the basic numeric operations.  The problem that this leads to is that Haskell doesn't actually know what type you're trying to use - the compiler will happily accept that it's some instance of Num, and until it actually reaches a point where an instance is supplied (by explicit typing or by having something where the type is unique), it just keeps the typeclass around.  On top of this, Haskell handles literals the same way - it recognizes the literal 1 as being a Num, not as being an Int.  If you know a specific type for the literal where it's created, and there isn't enough context to force it to a specific type, you can annotate the literal directly, but annotating the function (or leaving it polymorphic) is generally preferred - one annotation will probably solve all the type errors in a function because of the way Haskell does type inference.  TL;DR it's necessary because of some other features, and when those matter it is fantastic "
"If you really want names period, OpenID is kind of cool - but it is not a trust system, do not use it as one.)","It sort of does and then it doesn't. I completely agree that for spam prevention purposes it is pretty much equal to just letting people post. (Unless you verify an OpenID with a captcha or somesuch when you see it for the first time. Eh.)  On the other hand, though, some people care about names. To me this is silly, but some users care and some people running websites care, so they want users to be able to have a unique ID that other people cannot easily fake.  The most hassle-free but kind of not-so-intuitive-to-users way to do this are 2channely [tripcodes]( The less hassle free, but still pretty painless way is OpenID.  ( tl;dr:  If you really want names period, OpenID is kind of cool - but it is not a trust system, do not use it as one.) "
I didn't know shit about computers in 6th grade.,"I remember back in 6th grade I thought I was pretty awesome because I could write QBasic programs. I always wanted to know how to ""make an exe file"", though. So I asked my uncle, who knew how to program in C. I must have misunderstood his response, because I got out of it that I could write shit in a text file through MS-DOS Edit like ""print hello"", save that as a .exe, and run it.  When it didn't work, I got pissed and went back to QBasic for another few months, until I figured out what a goddamn compiler was.  tl;dr  I didn't know shit about computers in 6th grade. "
installing mirrors made elevator wait time seem much more bearable.,"I've read this story, I think it was in ""The Design of Everyday Things"". It goes like this:  It was the 1930s. Engineers were asked to design faster elevators for the new skyscrapers towering Manhattan, as the wait times for elevator cars had become unbearably slow, and many of the occupants were complaining. The engineers could only design them to move so fast without risking safety, so it seemed like a really tough engineering problem to tackle.  Then one day a few interior designers installed mirrors at the elevator waiting lobby. Suddenly, all complaints about slow elevators had disappeared, seemingly overnight. Everyone thought that the engineers had somehow overcome their limitations and had designed faster elevators, when everyone was just too damned busy staring at themselves in the mirror to notice the wait.  tl;dr installing mirrors made elevator wait time seem much more bearable. "
"My personal view is that designing nearly anything complex without caching in mind is akin to [assuming a spherical, frictionless cow in a vacuum](","I'll start with my usual preface: I'm not the most experienced developer, I'm not the smartest, certainly not the fastest, and I don't write the best code. I'm mostly self-taught as well, a blessing and curse, but I've been able to land a few jobs doing what I love.  Anyway, I've noticed that caching is a concept that just exists on every facet of development. I've found that many problems with any nontrivial amount of data involved become easier for me to tackle when I imagine them as a cache hierarchy. Planning by this concept leads to predictable performance and fits in nicely with good practices such as immutability and decoupling.  The concept of an object or block of data existing in different localities with different read/write-ability and performance characteristics hits that sweet spot for me between how the systems actually work and how my brain is able to deal with them.  The connections between those layers can be a function scope, a database connection, an internet connection, a GPU/main ram barrier, memory<->disk, or of course cache levels in a CPU. My limited experience shows that the best algorithms are the ones that intelligently handle traversal of these links.  Edit: **TL;DR - My personal view is that designing nearly anything complex without caching in mind is akin to [assuming a spherical, frictionless cow in a vacuum]( "
Is java fast enough? yes. Is it always the right choice? no.,"I expect the startup time to be worse for small programs, the jvm binary and libraries alone should be much bigger than the counterparts. 2nd thing, I don't think java regex function are as optimized as in perl or in grep (having read an article about the algorithms used in grep).Java.nio was expected to be faster that the old java.io, not the fastest in absolute, but above all, is expected to be portable, well-documented and easy to use (at least ""expected""). What I'm trying to say is: Do you need a small and fast program? Then use whatever is needed to make it fast, c would be my first guess. Do you need something complex? If you want it fast use c++ (or even c if you're hardcore, or even java or python for the right tasks), but expect it to require way more time to develop. It's pointless to compare the speed of an helicopter vs a Ford Fiesta. TL;DR: Is java fast enough? yes. Is it always the right choice? no. "
that's true when you're talking about a reference to an array but not true when you're talking about allocating for a pointer vs allocating the actual array.,"There are a couple verbose replies but I can try to explain it in layman's terms.  ""Arrays are pointers"" is from the fact that an array reference is just a pointer with the promise it points to several instances of that type laid out next to each other.  The syntax array[n] is just sugar for <location pointed to by array> + sizeof(<type of array>)*n.  The confusion is from the fact that this analogy falls apart a bit when you're talking about actually allocating a pointer vs an array because instead of calling the reference the array, we're now talking about the chunk of memory with the actual data.  In the article the major 'trick' is that, in foo you're defining the struct as a length and a pointer to an array.  The data itself is not considered part of the struct.  Where as the bar notation specifies the struct as the length and the data itself.  You can drop the pointer because you know where the data is by virtue of the struct definition, (its at location of bar + the size of length).  tl;dr that's true when you're talking about a reference to an array but not true when you're talking about allocating for a pointer vs allocating the actual array. "
You'll always be looked down upon if you have no real paid experience and the usage base to prove your code's integrity.,"Even if you have what you consider real-world experience and the final products to prove it, it's unfortunate for many in the autodidact category that a lot of employers want to know how your software scales more than anything else. I don't even bother providing opinions on things I hear my superiors talk about, even when I have working experience on it, because I lack the experience of taking that application to 1k requests per second, and I know they'll just snuff my opinions. It's bullshit.  If you want 6 figures, you have to start at a job that will give you paid experience. Do not be afraid to ask what you want. Be reasonable at first. In your next position or job, ask for more. I'm currently biting my teeth on a job for this sole reason. Once I have the experience, I will not be so reticent to speak my mind, because I won't have the bullshit condescension from others who just think it's talk.  TL;DR You'll always be looked down upon if you have no real paid experience and the usage base to prove your code's integrity. "
Frameworks remove complexity and actually allow you to solve real problems.,"Frameworks are not only about keeping you from making mistakes, you can make just as many mistakes with a framework. Frameworks are about development speed and maintainability. I went through vanilla javascript => jquery => backbone => angular and now learning react. Each step allowed me to make bigger and still maintainable codebases. Jquery makes html manipulation and ajax easy, Backbone gives a structure for an app, Angular is backbone on steroids with DI, directives, data-binding and other goodies. React looks to be a next step with its Flux architecture and virtual dom. (Edit: Still, I could be completely wrong about React)  I have eaten enough spaghetti jquery and I refuse to move back. I just can't imagine working effectively without a framework.  tl;dr Frameworks remove complexity and actually allow you to solve real problems. "
I see no compelling argument to drastically change my implementation. Nor do I see any abuse of static here.,"Well I've never been a big fan of static methods either, but..  Using functions the user would have to constantly input the source array as one of the arguments (alternatively I could load it every time the function is called, or load it once and introduce a static variable inside the function but then we're back to square one...).  I can change the visibility of the source array in the class with static methods from protected to private and remove the overloading functionality from load() so users would never be able to provide an alternative source, thus making it immutable. But that seems kind of extreme.  I don't want to make this into a class either.  TL;DR -- I see no compelling argument to drastically change my implementation. Nor do I see any abuse of static here. "
with a finite number of states N you can only count up to N.,"The idea is actually quite simple. Regular expressions can be compiled to finite state automata. A finite state automaton is essentially a program that processes a string one character at a time, and it only has access to a finite amount of memory to store its state. To check if a regex is valid you have to match balanced parentheses. To check if parentheses are balanced you need to count them. For example, is  ((((((((((|))))))))  balanced? When the parser is at the  | , it needs to keep track of the number of parentheses that are currently open to determine if they are all closed by the end of the input. Since there can be arbitrarily many open parentheses, you cannot keep track of that with finite memory. For example if your finite memory is 10 bytes, you can only store a number up to  2^(8*10) , so if there are more open parentheses than that, the parser cannot possibly determine if they are all closed.  tl;dr: with a finite number of states N you can only count up to N. "
"really smart and paranoid people have thought of all this before and already know how to avoid it, and are also kind enough to share that information with you.","It's not hard to construct a point-by-point refutation on how careful Tor users cannot be affected by this.  This will only catch sloppy people.  It's even possible with virtual machines to browse with Javascript, Java, and Flash, with no risk of personal information or network leaks.  [Hey, look, it's a big red warning disclosing the dangers of Javascript, Java, Flash, etc.  Right there on the download page.](  His intentions may be good, but if this were a genuine or novel attack on Tor, it would affect the whole value of Tor, not just to pedophiles.  But here's the part that makes me shake my head and wonder why: if he wants to catch terrorists and pedophiles with his ""uber-hacker"" technology, why on earth is he disclosing  exactly how it works ?  And is about to release the code?  Now we all know how to avoid getting snared in his net.  Dumb ass.  tldr: really smart and paranoid people have thought of all this before and already know how to avoid it, and are also kind enough to share that information with you. "
Books on MS Office isn't as absurd as it sounds.,"Why's that? Microsoft Office isn't exactly the most intuitive of applications to use. You'd be surprised as to the number of people that abuse the WYSISWG interface by not properly using the provided templating system. Hell, I'm sure most people aren't even aware that there exists a templating system in word. Not to mention how little knowledge of excel functions and macro programming. Sure, the average user may not necessarily need to know but when you enter any corporation and get handed a spreadsheet created using voodoo magic numbers you sorta want to tear your hair out and wish they had taken a course in Office.  Sorry, that's just my rant.  tl;dr Books on MS Office isn't as absurd as it sounds. "
Get a degree that gives you a skill-set or proves you can actually think!,"I think your assessment of the MIS degree is fair. I started off in Computer Science in 2001 without a clue about programming except HTML (which isn't really coding). After taking C and Java and fearing the rest of my life would be locked away in a dungeon somewhere (seriously, why do engineering builds have to look like dungeons?) I jumped ship into MIS. After realizing MIS only taught be some business basics I felt woefully unprepared and felt I had not developed any skill set. I finished out a Computer Science minor since I only needed one more semester of classes. I landed a job in consulting but find myself enjoying programming assignments for the challenge, the chance to automate routine boring work, or feel like I actually produce something of value.  tl;dr Get a degree that gives you a skill-set or proves you can actually think! "
By imperial decree the German 20th century started 1900.,"Interesting historical side note:  Your post is a reference to the old discussion whether the 21st century starts with 2000 or 2001. Few people know, however, that they had exactly the same discussion at the turn of the 20th century.  Back then you had all kinds of arguments whether it starts 1900 or 1901.  Finally, at least in Germany, the thing was settled. By the emperor. Who decreed that the 20th century starts 1900.  Although (by the mere logic of adding 100 to the 1900) the question should've been settled for the 21st century as well, we Germans apparently forgot that decision - or we simply don't like emperors anymore.  TL;DR: By imperial decree the German 20th century started 1900. "
I don't know that I agree or disagree.  There's a lot more to programming than the language you happen to be using today.,"Speaking as someone who has been a professional C# developer for ten+ years, I can see both sides.  On the one hand, C# is a fabulous language for learning algorithms.  ""classic"" algos like Dijkstra's shortest path tend to be pretty elegantly expressible.  So, point for C#.  On the  other  hand C# does not live in a vacuum - it expects some kind of garbage-collecting assembly-loading CLR underneath it. These subsystems themselves are all ripe for study and the lessons there would translate well to other systems (understand why GC works the way it does in conjunction with a book like Jone's  Garbage Collection  and you're well positioned to examine malloc()).   And learning about reference versus value types (for example) are lessons that carry over well to java, or even C++.  I worry though, that a student could wander into CLR-related weeds to the detriment of focusing on the algorithmic stuff that's important for beginners.  Thing is - Any other environment has the same problems - to learn in Java you must learn class loaders.  To learn C++ you need to bang your head against the STL.  Probably my biggest reservation with jumping right into C# though would be that C# (like Java or C++) pretty much requires a mature understanding of inheritance and OO.  ""Smaller"" languages might make better testbeds for pure algorithmic understanding.  But then again my first languages were TI-99 BASIC and then Borland Turbo Pascal.  All that mattered was that they turned me on. The same is true today - someone who is wired to love programming will get the bug regardless of the environment they start in, even if it's Lego Mindstorms.  From there exposure to more environments would be important and then things progress organically.  The Lego programmer will quickly discover limitations to mindstorms and then might discover C# lets them do a lot more ""cool"" stuff.  At that point stand back and let 'em go.  TL-DR: I don't know that I agree or disagree.  There's a lot more to programming than the language you happen to be using today. "
The code isn't meant to be a learning resource.,"> Is it horribly  illegal  to download  Why yes. Yes it is. See the Half-Life 2 code leak.  Second, the game was written with releasing as exe in mind. They didn't write the game with teaching in mind.  Third, even if you were to read the code on how it works, you won't learn  why  they designed the code as such. Learning why you would use Data Structure C instead of Data Structure B after trying out that Data Structure A doesn't work at all, isn't written anywhere on code. It is discussed and tried before the code is released. As a coder, these design decisions are just as important as how the code works.  Tl;dr - The code isn't meant to be a learning resource. "
"Threading will help a single application become faster, but moore's law can still work on the process level.","While we are talking about application developers and multi threading, remember, most applications run on a OS. That by itself is going to harness a lot of the power from multi core CPUs, even in single-threaded applications.  Basically, what I mean is that two processes can run at the same time. I can browse the web and listen to music, the OS can schedule my browser to one core and the music to another. Where as previously, the OS would need to time-share the one core between both the browser and the music.  This also gets abstracted in things like Google chrome, where each tab is a process (or thereabouts). Multiple web pages can compute concurrently, simply because of the browser's sandbox.  Of course, with more cores this becomes less useful, since users generally don't actually use that many processes simultaneously (most are usually in some kind of waiting state, e.g. waiting for input).  TL;DR  Threading will help a single application become faster, but moore's law can still work on the process level. "
"to know if something is a dead bird, you have to give it to a lot of people.","Wow.  This goes to show you why the article was so wrong.  One person's use case is not enough to go on.  I, for one, find suggest to be one of the most useful features of Google.  I can type the start of a word I don't know how to spell and BAM, the word and often the rest of the phrase I'm looking for shows up.  I don't have to google the word first, then google for related information.  I can type 3 letters and it suggests the rest.  This happens often enough that it is one of my favorite features.  Preview I haven't quite found a use for.  TL;DR  to know if something is a dead bird, you have to give it to a lot of people. "
"yes, it's mental masturbation. When did that become a negative on /r/programming?","Monads are a design pattern that a lot of programmers have trouble with yet want to learn, so the author of the blog thought he'd try taking a different approach than the usual ""monads are <familiar object X>"" tutorial approach.  And I hate to break it to you, but many programmers actually enjoy ""mental masturbation"". The whole field of esoteric languages arose because people wanted to play with their minds and see if they were capable of something. People do ridiculous things like Functional Java or extreme cold golf for the same reason. I don't think anyone is suggesting monads in C++ are necessary to write all code (just as most serious Haskellers wouldn't suggest they're essential to writing Haskell, unlike the strawmen many redditors like to credit us with) but it is an interesting mental exercise, and some people may actually learn from it. It helps if you don't go jerking your knee or frothing at the mouth at ""ivory tower academic elitists"" (sorry for stereotyping your position, but it gets rather tiresome to see this reaction over and over again) the moment you see the mention of anything with two degrees of separation from Haskell though.  tl;dr: yes, it's mental masturbation. When did that become a negative on /r/programming? "
What is the best language/framework/etc. to use for web development?,"In my PHP experience, it all starts with a simple HTML page. Then I start to collect some data from a form. Then I want something to happen on the page, but it seems like I immediately need to turn to Javascript to have any type of activities on a page (button events etc.). So why don't I go directly to Javascript? Is there something that I'm missing here?  I have mainly created small applications in college (Java, C#, C++), with very little web design. I was a huge fan of ASP.Net in the few websites that I created for school, but it's a proprietary framework; I love that there are openly available alternatives, and I want to become incredibly good at one, but all of them seem to pale in comparison to the .Net framework (and the awesomeness of C#).  The other problem is that there are all sorts of new frameworks/etc. that are cropping up, all with their own complexities and possibilities; where the hell do I turn?  The answer always seem to be ""well what do you want to do?"" and that just avoids the question, unless there truly isn't a universally great web development language that can be recommended objectively.  TL;DR: What is the best language/framework/etc. to use for web development? "
it's the comparatively low adoption of frameworks that bugs me about the PHP ecosystem (which I should have been clearer about earlier).,"In my (limited) experience, Python  web  projects are far more likely to use a framework. It's rare to see someone roll their own WSGI  server   edit: application object  into an app, rather than using Django or Pylons for something big or Flask or Bottle for something small.  PHP projects, on the other hand, seem far more likely to structure things their own way, rather than use a widely-used framework like Symfony.  This is probably where Python's disadvantage - that some boilerplate is required to do stuff on the web - turns out to be actually doing it a favour. PHP makes it easier to jump on the web without third party code, because the entry point is the top-level script, and outputting HTML is an  echo  away. With Python, however, it's  easier  to have a framework do all the WSGI boilerplate for you, and you get all the benefits of using a framework for free.  tl;dr  it's the comparatively low adoption of frameworks that bugs me about the PHP ecosystem (which I should have been clearer about earlier). "
I asked a stupid question and got banned from Stack Overflow,"It was a question about game making. It was a really dumb question. It was about live mobile games, the possibility of making one. I wasn't surprised with the responses, mostly saying stuff about how ridiculous the idea was because of latency issues and such. I have only been programming for about 7-8 months, at the time I was just starting. So the mods closed the thread and told me it was a bad question. Fair enough. I start working on an app, figured I'd actually go and post a legit question after not finding answers on that site for similar questions to mine, was told I'm no longer able to post questions. I'd have to get my reputation up by commenting and helping others (which will not be happening until...probably years from now). It even left a note saying that my IP address has been banned and don't bother making another account. When I ask dumb questions on reddit, at least I get down voted after being told I'm dumb and that's that. The environment at Stack Overflow is very unforgiving. I've gotten lots of help from /r/learnprogramming and other subs anyway. No real loss.  Edit: I'm not saying what they did was wrong, I was really naive and probably should have looked at other programming communities before posting there. They did it for the integrity of the site.  TL;DR:  I asked a stupid question and got banned from Stack Overflow "
Easier syntax. Full power of Ruby. Built-in tools that cover common tasks. Sane and logical.,"One of the big advantages is that you have the full power of Ruby at your disposal. Rakefiles are essentially Ruby scripts that are interpreted in a certain way by Rake itself. This means you just use standard Ruby syntax. No XML, no Make syntax, etc.  A lot of the other advantages come from tools built into Rake. For example, ""clean"" tasks are created simply by importing rake/clean and setting which files should be removed. Another example is File lists, which are exactly what they sound like. If you want to grab all the *.c files in your  src  directory, you simply add the line:  SRC = FileList['src/*.c']  to your Rakefile. To make a list of all the object files generated from the *.c files, there is:  OBJ = SRC.ext('o')  which simply creates a new list that substitutes "".o"" with "".c"".  tl;dr : Easier syntax. Full power of Ruby. Built-in tools that cover common tasks. Sane and logical. "
"CMake's language seems simple, but not understanding how it treats quotes can leave you hunting down some weird bugs for hours.","Do you know what this does?  if ( ${mystring} STREQUALS ""foo"" )  It does something dangerous that I suspect most CMake users don't know about.  It  usually  checks if the variable  mystring  is equal to ""foo"".  But if ""foo"" is a variable name, then it evaluates to the value of that variable.  Same goes for the value of  mystring .  This is the only safe way to do this in CMake:  if ( mystring MATCHES ""^foo$"" )  Quotes are not what you think CMake.  They are only used for string interpolation and to prevent a parameter from being split by whitespace.  TLDR  CMake's language seems simple, but not understanding how it treats quotes can leave you hunting down some weird bugs for hours. "
"Possible, but you must find a way to list all interface files.","At first, I didn't understand what the problem with Go was. So you want to do the following:   Generate the interface files.  Compile the source files, which may depend on  any  interface file in the source tree.   If the filenames of the generated interfaces can be specified in the build commands, Tup can handle a build description like that, but it's somewhat ugly: you'd have to list  all  interface files (or generate that list) and place them as ""order-only inputs"" in the compilation rules. This works because tup allows over-specification of order-only inputs.  Note that a change in an interface file does not cause all source files to compile, just ones that  actually  depend on it, based on file system monitoring of the previous build.  However, tup can't handle javac or doxygen default behavior, because outputs must always be specified. Personally, I use tup as the ""safe subset"" of a build script, which usually looks like this:  (cd externalLib &amp;&amp; make)(cd haskellTool &amp;&amp; ghc --make)(cd mysrc &amp;&amp; tup upd)(cd mysrc &amp;&amp; doxygen)  I've also thought of using make as the unsafe top-level tool that calls tup, but shell scripts have been enough for now.  TL;DR: Possible, but you must find a way to list all interface files. "
"Make something really good and popular, and then we'll underpay you and make money off it.","From the wiki page:   Submission is eligible, Apache License, Version 2.0  Submission identifies pull requests that have been made against existing NetflixOSS projects by the Participant  Submission provides an original and useful contribution to the NetflixOSS platform  Submission follows good accepted code quality and structure practices  Submission contains documentation on how to build and run code provided as part of the Submission  Submission contains code that successfully builds and passes a test suite provided as part of the Submission  Submission provides evidence that code is in use by other projects, or is running in production at Netflix or elsewhere  Submission has a large number of watchers, stars and forks on github.com  Participants shall reasonably cooperate with the nominating committee in any verification activities   TL;DR: Make something really good and popular, and then we'll underpay you and make money off it. "
"phonegap is still in alpha, not even close to beta or production ready.","+1 the biggest problem with phonegap is all buggy plugins and the really buggy command line tool.  For example: you update the code in your plugin. You run ""phonegap plugin add my-plugin"" -> ""error: plugin already added"". Why cant phonegap just overrite it? But okay, you delete the plugin folder and try again. ""Error: plugin already exists"". What you just deleted the plugin folder? But no, the plugin still exists in the meta, in plugins.json. okay. So you run ""phonegap plugin rm my-plugin"" -> ""errrrrror: cant remove folder 'my-plugin' folder doesnt exists"". Wtf? Just ignore it then. Gaaaah.  It also exists problems with the build command and the install command doesnt works at all. So you need to use eclispe to compile and run your code.  Tl;dr:  phonegap is still in alpha, not even close to beta or production ready. "
His logic is fine. Comparing bad English to bad programming languages is not a valid comparison.,"It is possible to generate crap with substandard English, for example laughably bad translations, but the worst that leads to is mild infuriation and sources of humor: ""All your base are belong to us""  But languages can have misfeatures or features very easy to abuse leading to crappy code. Since code has behavior (unlike English prose), and that behavior has to be understood and maintainable, the consequences are a bit more dire.  PHP is /r/programming's favorite whipping boy in that regard. But C++ has a lot of complicated features which in the wrong hands can lead to horrible monstrosities. That's not a reason  not  to use C++ in general, but it is perfectly fine justification for Linus wanting to keep C++ out of  his  projects.  TL;DR: His logic is fine. Comparing bad English to bad programming languages is not a valid comparison. "
Linus doesn't like object models. It's a religion for him.,"> How much of this is true?  The real question you should be asking is ""how true is this"", and the answer is ""not very"".  Linus isn't dumb (obviously), but he's developed a particular kind of tunnel vision that develops from intellectual celebrity, wherein one tends to substitute vehemence for reason, precisely because one can get away with.  There's absolutely no one whom Linus Torvalds fears pissing off, consequently he can [set the bozo bit]( on anyone for any reason.  He's forgotten that setting the bozo bit is an antipattern, and simply because one is a position to indulge this natural human weakness is no reason to actually do it.  As for what he says, it's a lot of hubris wrapped around a kernel of truth. C++ does tempt one to creation of architectures. This is, in fact, its greatest strength, as well as its greatest weakness. Its greatest strength because one can actually DO it (as opposed to relying on C's options of untidy-mess-of-everything-in-a-pile-on-the-floor, or painfully-reinventing-objects-wtih-giant-structs), but its greatest weakness is that one can (frequently) do it disastrously, epically wrong, and if you don't have the self-discipline and mental strength to ""throw one away"" then BAD THINGS can ensue.  tl;dr: Linus doesn't like object models. It's a religion for him. "
They really didn't care at the time; and it [appears] they still don't.,"It was an early analog to digital record retention project at Bank of America 25 years ago.  Look making processes more complicated than  completely obvious  in a large and historical enviro is bad for knowledge transfer over generations.  The wheel gets invented over and over again...and that is okay for these orgs; otherwise things get very brittle and it is difficult to replace people at a whim (ie reorganize, decentralize or centralize when the new CEO shows up...which essentially covers up any trails of incompetence or inability) .  I didn't know this at the time and I still struggle badly with this type of company....but they do have jobs on a regular basis.  TL;DR They really didn't care at the time; and it [appears] they still don't. "
"Offer your employees challenges and opportunities, not a vague promise of working in the same cubicle for 15 years.","I'm a mid-20s programmer and my longest tenure anywhere has been 22 months. The past few jobs have practically fallen over themselves to offer me incentives to stay, but I'd rather find a new challenge. At least so far, it's worked... By always looking for the new thing I'm able to find the compensation I want, and have learned so many new languages, platforms and organizational skills, I feel I'm a better employee.  FTA: ""I'm seeing them being much more [transient.]"" 'Them' being the web programmers, the ""younglings"" that are leaving after 1.5 years.  But isn't this the case? In this economy, transience is king. It's only when you've locked yourself into the trap of home ownership that transience goes out the window. If you're hiring 22 year olds, you better expect unless they're saddled with a kid, house, and/or local extended family, tying yourself to a single job really doesn't function the same way it did in the past.  I didn't even read the second page at this point, and just turned to it to find:""Why are your Java developers leaving? Are they being recruited away?  It's a mix of reasons. The last two folks left after other people reached out to them with opportunities. Before that, we had people who were actively looking for opportunities with more flexibility and different responsibilities. Some left for more money.""  So like OP says... ""we pay less than other companies."" You had smart folks but they weren't engaged in  some  way, and took other offers. And finally (and the reason I've left TWO companies) ""more flexibility and different responsibilities.""  Responsibilities are king: I took a job as a C# .NET app dev and got asked one day to write a new VB6 module.I don't mind doing what my employer needs me to do for them to make money... but I guess my point is that sometimes job postings really do not align with daily responsibilities.  If you have an obsessive micromanaging boss, you better believe you'll wind up formatting an excel file or powerpoint more than the technical tasks they don't understand.  TL;DR : Offer your employees challenges and opportunities, not a vague promise of working in the same cubicle for 15 years. "
Cost shouldn't be a problem for many people here with the free services MS offers.,I would like to make a shoutout to [BizSpark](  Many people are complaining about cost of upgrading. In reality MS offers a ton of products for free to startups. If you are an indie then sign up! I did (and am an indie) and was approved in a couple weeks. Now I have access (for 3 years?) to VS Ultimate and even windows. It actually seems like I have full access to MSDN.  If you are a student then checkout [DreamSpark]( Its the same deal but I think you dont get everything. You get a ton though including VS pro. Again this is free for students.  If you are a larger company that makes plenty of money then I have no sympathy. Chances are you dont like upgrading to new tech anyways. :/  tl;dr - Cost shouldn't be a problem for many people here with the free services MS offers. 
"I agree with you in principle, I just don't think this is a case where that's applicable.","If you're setting up the repo to act as a sort of facade or adapter pattern for a highly complex subsystem, then yeah, totally. And I completely agree that abstracting out the various parts of the process to independent portions of the code base is a good idea for reuse.But, in this case I don't think he's relying on a specific route or controller is a ""bad thing"" here. More than likely this is his validation portion after submission or processing. If this is the ""ok, work's done, time to submit/validate the new/updated document"" then there's no problem. /shrugs.TL;DR - I agree with you in principle, I just don't think this is a case where that's applicable. "
"Reality has a habit of proving me to be permanently Jr; I appreciate that :-) 
 edit: typos & fix link","I think I can give a real life example:  Current product has ticket about sort order (of CRUD html ~table~ backed by MongoDB) being case sensitive (a-zA-Z) which really doesn't make sense (from a user pov).From a quick glance I did classify that as ""easy"" (and still do - kinda); but it seemed conceptually simple, if asked I would have felt reasonably certain to give a complexity estimate; possibly even ""in a meeting"".  Sadly the hope for a quick fix (""switch to another collation"") was shattered ( SERVER-90 case insensitive index ; even evaluating the possible solutions and concluding where/how it should be implemented took more time (than I expected). Now that the choice (do it on-the-fly using aggregation) is clear (down to an existing class that could be extended / copied) the time to implement it would still vary (between our available devs); it is on a library layer were one/few are far more experienced than the rest (a skewed distribution).So that poor ticket (having prio=normal) is doomed to fall off the end of the backlog (repeatedly) in the vain hope of being picked up by the ""right"" person (who might as well do ""that"" in a short moment of boredom) - until someone finally has pity (on our users).  tldr: Reality has a habit of proving me to be permanently Jr; I appreciate that :-)  edit: typos & fix link "
"Why would anyone sue the programmer for failing to do the system administrator's job? 
 It just doesn't make sense.","I think you missed my point.  People who work in professions that require vigorous testing (every seen a CPA exam? they aint easy, takes days to complete), and go through years of school before the government even ALLOWS them to work in the industry. They do this to protect consumers, ie so you don't take tax advice from someone who knows nothing about taxes. This is why it typically costs an arm, a leg, and your first born child whenever you seek services from these individuals.  Programmers? Don't have to do SHIT. Any yokel who can figure out how to sign up on godaddy.com can be a professional designer/developer/whatever. Hence the market is flooded with people willing to write code for under 5 dollars a day, the startup costs are non existant and any money they bring in is pure profit.  Now the argument that bad programming can lead to just as many if not worse losses than what lawyers/accountants/doctors can do is valid. (ie, the guy who writes software for military jets better know his shit). However malpractice lawsuits aren't applicable to this line of work.  It's like sueing XYZ speaker company because their speakers didn't make ABC music sound fucking awesome, which caused your club to lose money.  TL;DR:  Why would anyone sue the programmer for failing to do the system administrator's job?  It just doesn't make sense. "
"Don't sue the sysadmin, sue the company.  Company doesn't want risk, they'll improve security.","Security isn't just the sysadmin's job.  It is definetly the sysadmins job, but it's not just his. It's mine too.  If I write crappy code that allows SQL injection, resulting in credit cards being spewn all around the internet  my company  should be liable in court for this.  I've never sued a doctor for malpractice, but based on [what I can see]( you sue his practice.  Likewise, my company should be liable for my lapse in security.  The goal of this is that companies protecting valuable data (credit cards, bank accounts, or (in my case) proprietary financial data) shouldn't hire any yokel for $5/day and should hire people who they believe are competent and won't expose them to adverse risk.  They'll spend more money on security and will improve it. [Bruce Schneier probably says it better than me](  I want to make companies liable for their security breaches.  If they go off and demand a certification program before they'll hire people, I can deal with that.  And if I'm a software contractor, a hired gun, the situation definitely gets more complicated - who holds what amount of liabiliy... I easily forsee 3-way court cases where Joe Brown wants restitution, Discount Monty's Hangliding and Internet BBQ Warehouse blames Aviewanew & Confucius Televentures and ACT blames Monty's server configuration.  But I'd rather have that than what we have now.  tldr: Don't sue the sysadmin, sue the company.  Company doesn't want risk, they'll improve security. "
another fool wants to engage in rms-bashing to look big,"This critique is silly. rms is not arguing against .NET and C# because of patents in general, but because of patents that are owned by Microsoft.  Let us not forget that Microsoft is an active hostile user of software patents against free software, here? I'm referring to the VFAT patent(s) and the shakedown that Microsoft is engaging in against embedded computer (camera, etc) manufacturers that use Linux.  I didn't read any of the rest. If the author is going to misrepresent and then nitpick on a tangent, well... maybe I have some better use for my time, such as writing this scathing meta-critique. Commercialism is, in fact,  not  what rms is about -- so why the hell would the author bring it up? After all it's pretty obvious that statements made inside a specific context could very well be silly in another!  tl;dr : another fool wants to engage in rms-bashing to look big "
test each filter one by one. Your mileage may vary according to your existing configuration and development practices.,"> I downloaded and installed the mod_pagesepeed module per the installation instructions and ran eleven test with it on, and another eleven with it off  You'll probably have to turn off all the individual components and re-run your tests for each component, one at a time, in order to properly test this suite.  For example, if your pages don't really contain much inline javascript, a filter that finds inline javascript and moves it to an external resource will not yield a benefit worth the increased processing time.  If you're already filtering whitespace (or if your development practices minimize the generation of unnecessary whitespace), then this module's whitespace removal filter will also worsen your performance.  tl;dr: test each filter one by one. Your mileage may vary according to your existing configuration and development practices. "
"LOL, a proprietary sdk 
 EDIT:  Also, I was unaware the Android Market had a developer fee.  I'm very disappointed with you now, Google.","It's less the paid aspect and more the closed-source aspect that makes it bad.  People shouldn't be developing using tools the developers don't want them to fully understand.  I'm not saying you can't write good apps with this, I'm saying that the developers of Corona intentionally limited the usefulness of their SDK by making it closed-source, all so they could increase their profits.  Being able to see the source code might not be too important for a beginning programmer, but if Twunger sticks with programming, one day he'll have plenty of experience, and will likely want to understand the tools he's been using.  So I think it's just better to avoid closed source software to begin with, so there's never an awkward transition later on.  tl;dr LOL, a proprietary sdk  EDIT:  Also, I was unaware the Android Market had a developer fee.  I'm very disappointed with you now, Google. "
"More expressive names possible. Less classes possible. Operations are just Applications (""Operators are just functions""), why distinguish them?","Nice :)  I'd name expr2 parseTerm and expr3 parseFactor, because that are their usual names.  It's fine as is but it would be possible to be simpler:  Not sure why there is BinaryOperatorExpr at all (and why it's called BinaryOperatorExpr when it's just part of an expression, I'd say Application). The AST is  abstract , it doesn't matter whether it was an unary operation, binary operation, function call or mushroom it came from.  That said, a lot of people do it, I just don't see the point in complicating it that much.  It would be simpler if an AST node were one of the following:   Application (like the BinaryOperatorExpr, UnaryOperatorExpr etc you have)  Abstraction (you don't have that yet at all)  Variable Symbol (since you don't have Abstraction, you don't need parameters and arguments either)  Atom (stuff outside the language that you'll just keep as-is, like the IntExpr you have)    Then there would need to be predicates in order to allow the user to, given a Java object instance, find out which of those it is, in order not to have to make a dummy class Atom which every Java-only object would then need to be derived from.  I remember when I learned it, it took me forever to understand because people then too made it so complicated (why is IntExpr different from Integer etc - in the beginning it shouldn't be, really[*]).  On a positive note, I like how you write expr1, expr2 and expr3 bodies in a way that makes it obvious that they can be further generalized, nice hint to the learner :)  [*] Except when there's extra info like line numbers etc in it. Even then, I'd think twice whether I'd do it, it just makes it so unnatural.  TL;DR: More expressive names possible. Less classes possible. Operations are just Applications (""Operators are just functions""), why distinguish them? "
You're correct that this is a hugely inefficient way of dealing with 3D rendering: I disagree this makes it completely useless.,"You're not entirely incorrect. The reason 3D rendering(and most programs, really), are usually modelled imperatively is because  that is how processors work . And naturally, working in a way that's closer to the hardware creates much faster code(usually).  But I disagree that no 3D rendering technique that doesn't have the best performance possible has any value whatsoever. There's a reason 3D applications aren't written in pure assembly, in fact there's a reason all programs in general aren't written in pure assembly, and it's not just to compile on different platforms. It's because the ease of writing and maintaining code matters as well. And while hardware hasn't quite progressed to the point where 3D rendering is easy enough that we can abstract at this level and still get very usable rendering, I'm sure that day will come.  Sure, if you're working on a Pentium I, the idea of a scripting language like python being used for anything in practice seems insane! Performance is absolutely critical! Users don't want to wait five seconds for every press of a button to register! But of course, writing every script in C would be insane today.(Note that the Pentium I was a bit before my time as a programmer, I have no idea if I'm going full anachronism here)  Another thing is that, even on current-gen hardware when techniques like this will never be feasible for  demanding  rendering, it can still be useful for other things. I have some limited experience with OpenGL, and from a programming viewpoint I imagine it would be a lot more pleasing to program in this way. I don't imagine I'll be playing a FPS programmed in Haskell, anytime soon, nor watching a pixar movie made with their new revolutionary functionally programmed animating software, but if I just want to, say, describe some 3D structure mathematically in a way that doesn't necessarily translate easily to C and render that, then I can see this being pretty useful.  And also, theory can be beautilful on its own. Most of higher mathematics has no connection at all to the real world, and is only even remotely usable in computers once we devise crude approximations of it - that doesn't mean the original theorems are any less interesting or beautiful.  TL; DR: You're correct that this is a hugely inefficient way of dealing with 3D rendering: I disagree this makes it completely useless. "
"Yes I'd like change, no I don't see it happening, and I don't think it matters all that much if it doesn't.","You seem to be taking the view that PHP was at some point a superior language to its alternatives?  Just because it's easy, popular, well supported, well documented, etc, does not mean it has at any point been better than other languages.  The fact that PHP has been more popular, while being technically a messier, less consistent language is exactly why I don't see it changing. It's always been behind technically, but drastically ahead in numbers.  The kids who want the new hotness always run off to use the next language or framework that is v0.2.0 and get bored by the time it gets to v1.0. Some stick around for a bit, then its off to the ""new hotness"" for the rest. The rest of us keep using whichever language makes sense for the specific project we're working on.  None of this needs we need a specific language to win. You don't, I don't, the core dev teams don't.  Besides, [one feature]( which seemed like almost a shoe-in is in the process of being bounced because core devs felt it would be ""too much maintenance"". If they don't want to click merge on this, do you think they want to rename or alias every single function in the entire language?  tl;dr: Yes I'd like change, no I don't see it happening, and I don't think it matters all that much if it doesn't. "
"1.) Two standard libraries in D1. 
 2.) D1/D2 split. 
 3.) Reliance on garbage collection despite GC being optional. 
 4.) Non-free backend for reference compiler.",">I would be really interested in some meta analysis of why D has comprehensively failed to get adoption.  The first was issues over the standard library. The official standard library was Phobos. A lot of people didn't like it for various reasons. The community also felt like their input wasn't being considered. As a result, a project called Tango was developed and it had its own runtime. It became a sort of ""second standard library"". Tango took off in the D community. The result though was a split. Because they used different runtimes, they were incompatible. One either wrote a Phobos D program, or they wrote a Tango D program. Then came D 2.0 which was much like the Python 2 to 3 split. Thankfully, D2 resolved the issue with the standard library and there aren't two of them anymore. However, the tale doesn't end there. Although there isn't a split in the standard library anymore, there are still issues that people have with it regarding garbage collection. D has optional GC, but the standard library was written with the assumption that it is enabled. This meant that you couldn't use most of it if you were trying to write a program without the GC. Some language features even relied on the GC, such as the append operator (~=). D was sold as a ""better C++"" and for a lot of people that meant the same kinds of applications that C++ could do, which sometimes involves programs that must be pause-less. GC is optional in D, but because there were so many caveats, it wasn't a simple thing to avoid. The fourth issue involved DMD (Digital Mars D) compiler. It is the reference implementation for D and has all the latest features. The problem is that the compiler back-end is not considered free software. Some distributions of GNU/Linux wouldn't include it as a result.  TL;DR  1.) Two standard libraries in D1.  2.) D1/D2 split.  3.) Reliance on garbage collection despite GC being optional.  4.) Non-free backend for reference compiler. "
"some academics use it because it's easy to learn and use without any CS education, not because it has good math libraries or runs amazingly fast","This is going to sound bad, but....  I work in academia, coming up on my 15th year, as a programmer.  Here, Python became super popular with the researchers (MD/PhDs) simply because it was simple.  Java was too difficult to learn for these already busy people, and once they saw how easy and adequate Python was, they stuck with it.  Horrificly horrible code flies out of the fingers of these researchers with 0 CS education.  Things got particularly bad when we purchased an 16 core server with 32 Gigs of memory to run one of these algorithms (thanks for that $$$ American taxpayers).  I was asked to take a look at the code when even with this beefy machine, things still wouldn't work.  I looked at it, asked a few questions about what it did, and it took me 5 minutes to explain how they didn't need to load the entire dataset into memory if they are only working on a small bit of it at a time.  This information fell on deaf ears of course, as they had no education/experience in memory management.  I explained to senior PI that had a C.S. Degree what was going on and never asked about that project again.  Now  that  researcher who made this horrific program now has students of his own, and he forces them to use Python (these aren't CS students so I doubt they mind at all), but at least here, that's how Python got so popular.  TLDR: some academics use it because it's easy to learn and use without any CS education, not because it has good math libraries or runs amazingly fast "
I'm inferring way too much about free markets based on a kids' movie.,">feedback from users is more statistical and less emotional  Consider the plight of Radiator Springs in Pixar's  Cars : the town's entire economy is based on supporting cars traveling along Route 66, but now that a newer and shorter path bypasses the town, the traffic and money have dried up. It's tempting to lament the town's demise, but the cold, hard reality of things is that people are nearly unanimously choosing to travel the newer road -- it's got more lanes, probably has superior construction, and is a faster route to begin with.  Sure, people are ""sorry"" that the old ways are changing, but what happens when it's time to speak with their wallets? Nearly everyone skips right over the town. The pure honesty of their actions belies their words and sympathies, and reveals the deeper truth of the situation.  Like you say, paying attention to what your users actually do is just as important as listening to what they say.  tl;dr: I'm inferring way too much about free markets based on a kids' movie. "
You could fake it if you really wanted to...,"Many would argue that ""Free Will"" is an illusion. Humans are not capable of acting outside of their ""programming"" any more than machines are.  Our programming is simply more complex.  Our decision making process is based on what you could call our personality.  That which we find most important helps determine what we do. You could simulate human personality in an AI machine by using a weighted random function to determine the machines' actions.  For example...  An AIs' priorities can be defined by the following weights:  1.  Not harming another being - 800  2.  Defending a female AIs' honor - 100  3.  Looking like a badass - 100  Now, if the AI is in a bar and some big piece of machinery comes up and gropes his girlfriend, the AI must make some decision. Choosing a random number he decides that this time he's going to let it go.  But 2 out of 10 times he's going to get in a fight.  That weighting is his personality, the random number is what we would describe as ""Free Will"" the ability to function outside of what you would normally do.  TLDR;  You could fake it if you really wanted to... "
"You could fake it if you really wanted to..."" 
 Turing would say you're not faking it if people are convinced.  To what other standard would you appeal?","I don't know that humans could ever accept free will as not existing although what I was referring to was the depressing light our ability to program an AI that indistinguishably mimicked our own would cast on such questions.  I think that no matter what the question of whether we have free will is undecidable, and the gravity so essential that one ought always to assume they have it, simply because to do otherwise would be the least sensible wager.  We won't part with that idea but we may very well devalue it once creating comparable AI.  I agree it's certainly conceptually possible, but I am also saying there's a lot of work to be done and right now we don't even understand the human mind that well.  It may very well be that recreating that in a functionalist approach  may not be possible because of some unique structure of the human mind.  ""TLDR; You could fake it if you really wanted to...""  Turing would say you're not faking it if people are convinced.  To what other standard would you appeal? "
"Don't use this in production code, or shit will get more [confusing]( than you can possibly imagine. Somehow.","Turn aside from this course, ere you bring ruin upon us all!  I know how it is. You see the abomination looming on the horizon, and you feel like you just have to get involved with it. Perhaps you can turn it to your advantage. Perhaps you can minimize the damage, or use it to stop someone else from using it worse. This is the beginning of your folly, your hubris! Yet, even knowing this, some will embark on this doomed course, heedless of what is to come.  And then, years later, the monstrous things with which you have involved yourself come at last to their long-prophesied apotheosis, and in that fated hour, as seventeen angels lie dead by your hand and nine beasts impale themselves upon their own fell spears, all of humanity will dissolve into liquid and then the apocalypse will run out of production budget and you'll have to fill the rest of the fated hour with confusing and angsty navel-gazing.  tldr: Don't use this in production code, or shit will get more [confusing]( than you can possibly imagine. Somehow. "
I don't think South Park would be a problem.,"Here's a screenshot of an episode. . I estimate that it would take about 30-50 polygons to render each object; however, I've often been surprised by these things, so let's call it 1000 polygons each. That gives a total of 22,000 polygons per frame (and TV goes at 10-30 frames per second depending on where you live and how ambitious the animators are).  For comparison, my three-year-old graphics card can render 15 million polygons per second, and that's for 3d stuff -- so it's doing occlusion, texturing, and probably half a dozen other things I don't even know the name of that don't need to be done for 2d graphics.  tl;dr I don't think South Park would be a problem. "
"predictable structure is good, but using classical OO or not depends entirely on how well it fits the problem.","I would disagree with 'need', and your teacher might be emphasizing that simply to put off students writing code ad-hoc code with no structure. Big applications do need structure.  I would say that you need your code well laid out, in a predictable model. Personally I find classical Java-style OO and inheritance is a pretty dumb and straight forward paradigm, and I mean that there is rarely any mind boggling magic going. It ends up working ok for most stuff. This model also works fine in JavaScript.  If a naive programmers decides not to bother with OO in JavaScript, what people can end up with is a large number of global variables which are changed within functions. When you have say 100 functions all reading and writing to different global variables within the functions themselves, it becomes very difficult to track how data flows in the application.  Classical OO improves on this by being able to better localize data (by storing it in fields on objects rather than globally), along side the functions that will operate on them (the methods of that object). Personally I find OO is rarely the best way to approach a problem, but is also rarely the worst. It's a solid middle of the road paradigm.  However there are a lot of other approaches to structuring projects in JS, because it is a very flexible language. I personally use a mix of OO, event driven, functional, logical programming, and old-school global variables + functions too. It ultimately depends on what you are building. I have projects which are heavily OO (such as a compiler I wrote), where it works brilliantly. I have some projects where OO is used for modularizing just a handful of major components. I have some projects where the OO bits sucked and have since been replaced with better alternatives.  Classical OO approaches can especially be very verbose and lead to a lot of boilerplate. Alternatives can end up more straight forward, shorter, and be easier to understand. Classical object orientation is also very stateful in nature, which can lead to the same headaches as what I explained above with the naive programmer.  Finally I would also say that bad programmers will still write bad code, regardless of paradigm.  tl;dr; predictable structure is good, but using classical OO or not depends entirely on how well it fits the problem. "
"If  CloudFlare had ill intentions, they could probably do some very very scary shit.","The original intention of SSL is to have a completely encrypted path between the web browser and the web server hosting the web site.  This prevents anybody with access to the data stream between the client and the server from eavesdropping on the data being exchanged between the 2.  If you are not familiar with CloudFlare to begin with, they are basically a DDoS mitigation company, they act as a proxy between the web browser and the web server.  The idea is you keep the IP addresses of the web server a secret that only you & CloudFlare knows.  You then setup DNS to point your domains to CloudFlare, so anybody trying to reach your website reaches CloudFlare instead, CloudFlare then brokers the connection to your web server on a secret address without revealing that address to the person connecting to your website (so they can't DDoS it directly).  The idea being, CloudFlare has huge amounts of bandwidth in data centers all over the world, to overload them with a DDoS and take them out globally is nearly impossible.  So back to the SSL part.  Now that CloudFlare will do SSL for free (previously only available for paid accounts with them).  Its important to realize that the entire data path between the web server hosting the site and the web browser is actually NOT encrypted for the entire path now.  Its encrytped up to the point of CloudFlare's servers, which then decrypts the traffic and then forwards it to your server, which could be in either an encrypted or unencrypted state.  Even if it is encrypted though, you need to realize that CloudFlare has access to all the data, as they brokered the original SSL connection between browser and their server, and they are now establishing a new encrypted (or unencrypted) connection between their server and yours.  In effect, CloudFlare is unintentionally pulling off a huge man in the middle attack as they have access to all the unencrypted data between the web browser and your web server.  This is true even when the web browser displays the lock / secure connection / whatever.  Instead of the unencrypted data being available only to the server & client, its now server, client, & CloudFlare.  tl;dr  If  CloudFlare had ill intentions, they could probably do some very very scary shit. "
"wrapper"" junk.  Seems like over-engineering/second system syndrome.  Isn't small, simple code more valuable than  everything  being (superficially) OO?","I don't really understand what a singleton buys you over a global variable(s) in this case.  Unix signals are inherently not object oriented, so trying to force them into that model is as much of an impedance mismatch as any it solves.  And a singleton, particularly in this case, basically IS a global variable.  This is just superficial window dressing.  If you like that, fine, but personally I think it's better to face the reality that programming is a diverse task and not everything fits into the same paradigm.  Furthermore, the article takes a short, simple, easy to understand code example at the beginning and proceeds to wrap it in a bunch of tl;dr ""wrapper"" junk.  Seems like over-engineering/second system syndrome.  Isn't small, simple code more valuable than  everything  being (superficially) OO? "
pyglet & PyOpenGL are different tools for different jobs.  Consider your application's needs when choosing.,"PyOpenGL is nothing but a wrapper for OpenGL itself - no sound, no window management, just 3D graphics.  pyglet does a lot more than PyOpenGL; it includes window management, sound, video, etc etc.  Choosing between the two depends mostly on your application's needs.  PyOpenGL's API is more pythonic, so it's easier to learn with nicer, clearer syntax.  pyglet's graphics API is closer to OpenGL's, and requires you to be at least passingly familiar with ctypes.  But, because pyglet's API is 'closer to the metal', it can perform significantly (1.5x, 2x) better than PyOpenGL.  PyOpenGL plays very well indeed with PyQt via PyQt's QGLContext.  I don't recommend combining pyglet with PyQt, since there's a ton of functional overlap between the two.  Pick one UI framework & use it.  Both PyOpenGL & pyglet should work fine with 2.7.  Both do indeed suffer maintenance & upkeep issues, but I'd believe pyglet is better & more actively maintained than PyOpenGL.  Also note that you can actually use PyOpenGL along with pyglet, as a drop in replacement for pyglet's own graphics API.  tldr: pyglet & PyOpenGL are different tools for different jobs.  Consider your application's needs when choosing. "
"reverse-dependency comments"" are a good idea that you probably already use in moderation; excessive use becomes a maintenance burden.","Whenever you look at a line of code, you can see the external things that line of code depends on (or at least figure them out in principle -- overloading, namespaces etc. can make them hard to ""see"" in C++!). From looking at the code samples, I think the essence of the idea here is that in addition to this  what-things-do-I-depend-on  information, which often doesn't need a comment because it's implicit in the code itself, it would be useful to have knowledge of  what-things-depend-on-me .  That latter information is what you would write in a ""negative comment"" at the site of the depended-on thing.  (Though I would call it a ""reverse-dependency comment"" -- I think the OP is mistaken in requiring that all these comments must be worded ""negatively"".)  Of course,  people do this already  :)  The only problem I see with this is just the usual problem of having lots of comments in general: in a changing codebase, the more comments you have, the more likely they are to get out of sync with what the code actually does.  If you essentially double the number of comments by adding in ""negative"" comments at the ""other end"" of every dependency, you now have 2 sets of comments describing the same thing -- so it's that much easier for 1 or both comments to get unstuck.  It's the ""extreme"" aspect that I take issue with.  TL;DR:  ""reverse-dependency comments"" are a good idea that you probably already use in moderation; excessive use becomes a maintenance burden. "
"Bad label, needless stuff to write and read, wrong level of detail.","I do not like ""Negative Commenting"".  The foremost reason I do not like it (besides the critique I've already given) is that the ""negative"" label hides the real value of the comments which is the  reason  the line has to be that way. So, Reason commenting would IMHO be a better label, and we could cut out all the ""This has to be there because"" stuff.  Also finding the right level of detail to comment (module/class, method, single line) is an art. And using the highest level of detail by default is in most cases wasting programmer and reader time. Also, commenting on this level will in many cases miss out on the bigger picture.  TL;DR: Bad label, needless stuff to write and read, wrong level of detail. "
this could be solved by composing LaTeX on your tablet and having it rendered on a remote server faster than the iPad could manage,"for one thing, an iPad (or any tablet for that matter) starts to feel a lot more crowded with ~4GB of LaTeX onboard.  For another, I've never been happy with the keyboard options for the iPad - either make do with a chiclet keyboard (not my personal preference) or lug around a larger bluetooth keyboard.  I don't take the idea of using the onscreen keyboard with LaTeX seriously, because come on, there's NO WAY you're comfortably touch typing on that thing.  OK, you say, get one of those keyboard cases and a larger-capacity iPad.  Well, that works, but now I'm approaching carrying around as much volume as a regular laptop and approaching the cost as well.  Here's a compromise: use a VNC, remote desktop, or X-server app on the iPad to connect to a standard computer running LaTeX.  You could even have it connect to an EC2 or colocated instance if you don't want to run your own machine all the time.  Just get something that has much more memory and processing power.  TL;DR - this could be solved by composing LaTeX on your tablet and having it rendered on a remote server faster than the iPad could manage "
"break your project into tiny pieces, give yourself more than 3 months.","In response to ""it looks like an insurmountable task from so far away"": Sometimes coding is no different than any other big project. I like to break things down from a really high level first, including (and ignoring deployment, version control, testing, communicating with art studios etc. All of which will take a frustrating amount of time.)   User management.   Facebook API interface.  Notifications.  Cron jobs for processing content when users aren't logged in.  Item management.  Battle engine.   I've never built a mafia game, but that's where i'd start.  Then I'd try to break each of those down into smaller components, and further down into byte sized, 1-2 hour tasks, and put them on your calendar. Todo lists stay full or are empty quickly, a calendar will tell you how far off your predicted completion times you are. As others commented, a ""1-2 hour task"" is usually 4-5 hours at best. Just because you can ""see"" how you're going to it, doesn't mean that you won't be tripped up by some bug, language quirk, or sometimes your code just gets away from you.  TL;DR - break your project into tiny pieces, give yourself more than 3 months. "
"Frameworks, like Laravel, are considered best practice by everyone everywhere. Roll your own at your own peril :)","Frameworks for building web applications are pretty mature these days, it doesn't even matter what language you use, they all essentially use the MVC model stolen from Rails. There's a very good reason for this: the request/response pattern is well understood and the MVC architecture is an extremely good implementation to deal with that pattern.  I just want to stress this: these frameworks (I haven't used Laravel, but it's MVC, based on Symfony components, etc) are the result of many years of experience in solving a common problem across multiple languages and platforms. If you choose not to use one (you can use a microframework for smaller projects) then  you are re-inventing the wheel .  If you're asking specifically about templating, then it probably doesn't matter much how you implement it other than that you use some type of template based system for the View layer. Symfony uses Twig for the default templating engine, but you can easily use PHP instead. From experience, after using Twig, I hate looking at old code with raw PHP in the templates. This may be just my personal opinion, however goodies like [autoescape]( aren't.  TL;DR Frameworks, like Laravel, are considered best practice by everyone everywhere. Roll your own at your own peril :) "
"this isn't enterprise related, and it's not really a case of missing documentation either.","This has nothing to do with enterprise-level chips; the exact same instructions and complexities are in any consumer-level x86 chip too.  Yes, it's complicated; and sure, intel might document more of this stuff - but what the article describes isn't really even a clear-cut case of a bug or weird feature - it's just an unexpected interaction between the semantics of the profiler and how the chip executes codes.  The profiler isn't  wrong , it's just giving you data that isn't trivial to interpret.  A profiler tells you where time was spent, not the information you really want - what to change to make things faster.  It's actually pretty common for profilers to reveal hotspots that, when ""fixed"" simply move elsewhere.  For example, if you fix slow code that's slow due to a bunch of cache misses by avoiding accesses of a datastructure that's cold, it's perfectly conceivable that you actually perceive no performance boost as a result if that dataset is actually necessary in the next steps your program takes.  You may even make the code  slower  if your initial access didn't cause direct sequentially dependencies, and the new initial accesses do, resulting in the same cache misses now additionally causing pipeline stalls.  TL;DR: this isn't enterprise related, and it's not really a case of missing documentation either. "
Serious fucking flashbacks to Windows MFC...](  -- Also: [Second System Effect](,"Jesus Christ, these bay area kids either have way too much time and money to throw around or they are even more pretentious and arrogant in their actions than initially believed.  They're making you look stupid as hell and it looks like it's going to take a few more years for these frameworks to figure themselves out (I've used the majority).  When you start saying shit like, ""But two pages isn't much... blah blah blah bullshit bullshit bullshit"" you're fucking up.  And you're absolutely forgetting the absolute fundamentals of computing at its core level.  Hello World in C++  #include &lt;iostream&gt;int main(){  std::cout &lt;&lt; ""Hello World!"";}  Seriously, Hello World has one fucking job.  I understand there's other components involved in the example given, but look at it.  Look at that code - what components do you see in that code at first glance that handle output?    Yeah.  When you fuck that up...  TL;DR  [Serious fucking flashbacks to Windows MFC...](  -- Also: [Second System Effect]( "
"Most of the time you can just think of it as an anonymous function - but it can be much more, particularly in lisp where it was popularized.","Lambda came from lambda calculus, developed by Alonzo Church.  In lambda calculus if you wanted define a function that was x+2 you'd write ""lambdax.x+2"" (using the lambda symbol rather than the word).  John McCarthy, inventor of lisp and student of Church's incorporated that sort of expression into lisp as a means of creating an anonymous function.  (lambda (x) (+ 2 x)).  In lisp lambda (anonymous functions) in conjunction with some other functionality form the basis of defining functions, producing a lexical environment and some other nifty tricks (mostly done for you by higher-level functions).  They are also convenient where you need a function once, but it's just extra overhead and page space to create a separate function - you can just create a function in-line where you need it.  This latter use has been adopted by newer languages (such as C#) and offers a kind of watered down version of the original functionality.  Summary / TL;DR: Most of the time you can just think of it as an anonymous function - but it can be much more, particularly in lisp where it was popularized. "
"You're an idiot. :)  ""Yes, and,"" is only sacrificing being right when it's done wrong.","Well, the article is pretty clear that telling the truth about the value of things is still critical.  That's why the suggested response to the suggestion of doing X in language Y is not ""Yes, that's a great idea,"" but ""X is a good idea, but I'm concerned about doing it in language Y.""  That tells exactly the same truth as ""language Y sucks"", albeit in less strong language and without phrasing an opinion as inarguable fact, but it's also a ""yes"" despite clearly stating that the idea as given is bad.  Even if you have inarguable factual reasons for language Y being a bad idea, rather than just an opinion, ""language Y sucks"" is useless -- worse than, because it doesn't convey any facts.  Better communication is ""X may be a good idea, but doing it in Y will cause these problems.""  And, of course, even better is ""X may be a good idea, doing it in Y will cause these problems; what are the reasons that you want to do it in Y (so perhaps we can find an option that does what you want while avoiding those problems)?""  There are, of course, the occasional completely horribly unredeemable ideas.  Those, you can respond to with an implicit ""yes, let's talk about that"" and an explicit, ""Why do you want to do that and why do you want to do it that way?""  And then they not only learn that it's a horrible idea, but where the good idea is that means they don't need to do the horrible idea.  tl;dr:  You're an idiot. :)  ""Yes, and,"" is only sacrificing being right when it's done wrong. "
"If there is an easy way to do things, use the easy way (and ""use strict;"")","Agreed, if Op only wants to manipulate the DOM - jQuery is the right tool to do so. Of course pure JS could be used, hell he/she could also use Flash, or chopsticks, or a cat - whatever floats his/her boat. Sure, jQuery gets very PHP'ish lately, but that also doesn't matter - because we all learned to code at one point, and you surely don't want to read the code from back then. Additionally, what I really don't get about the JS community, it seems like everybody wants to invent the wheel yet again, no problem with diversity, but why do you want to use pure JS to manipulate the DOM when you can have a tool that does the heavy work for you? It's not like clients peek into your code and congratulate you for not using jQuery.  tl;dr  If there is an easy way to do things, use the easy way (and ""use strict;"") "
Use reCaptch and avoid content from this site in the future.,"First of all, if the image doesn't display properly, comment out the header(.. part and open it directly in the browser. PHP is probably printing a Warning or Notice and this messes the image up.  Sorry to say that, but that isn't code you should put on a public website. It's barely acceptable for playing around with the gd library (i.e. dynamically manipulating and generating images with PHP) and learning new stuff, but the Captcha part is horrible.  There are a few basic problems like not having a ""give me a new code"" button and accessibility concerns. The way it uses sessions is suboptimal too, only the most recently generated captcha can be validated (opening two tabs or pressing the back button will mess it up).  It's very weak too. The code always uses the same font, color, position and length and there isn't any rotation or other additional distortion (except for three fixed position gray lines) either. I haven't tried it, but I guess a good OCR engine (like [tesseract]( can break that without any preprocessing at all. If not, removing the grey lines is about 3-5 lines of code and after that it's definitely going to work.  Additionally the design of it is quite poor. I especially like this part of the code:  //Let's generate a totally random string using md5 $md5_hash = md5(rand(0,999)); //We don't need a 32 character long string so we trim it down to 5 $security_code = substr($md5_hash, 15, 5);   That's a very ""creative"" way of generating a seemingly random string. Unfortunately it isn't at all. It's only capable of producing 1000 different strings (obviously, maybe less), which isn't all that much. Relying on internal seeding is also problematic, since the generator will keep it's state during the lifetime of one PHP process (which might be longer than one request). The problems with this are further explained here:  tl;dr Use reCaptch and avoid content from this site in the future. "
not a worry in practice; pretty neat in theory.,"Is there a mirror anywhere? This article is not loading for me and I think I might die if RSA Key Extraction were possible.  EDIT: Found [one](  EDIT2: While an interesting approach, this is tantamount more than spying.  Essentially, in order for an attack of this type to work, someone will have had to gain access to your computer, and ""bug"" the vicinity so as to capture the noise generated during decrypting specifically chosen ciphertexts.  This means that the attacker needs the following to successfully extract your private key:   Access to your laptop (I did not see whether this works on desktops or server-grade hardware)  Ability to place monitoring hardware on/near your laptop  Ability to know when you are decrypting their ciphertexts (as opposed to playing a game or browsing reddit)  Ability to get the acoustic data back (presumably, you would notice someone put a 3g-capable device next to your laptop, so the attacker would probably have to physically retrieve the device)   TL;DR - not a worry in practice; pretty neat in theory. "
Asia has way too many characters and people wanted more glpyhs than were originally anticipated.,"The number of ""reasonable"" characters was severely underestimated, and the ""reasonable"" definition of character was expanded.  In the original 1.0 specification, without Asian characters, there were only 7000 characters defined.  Version 1.0.1, which added Chinese and Japanese characters, had 28000 characters.  When version 2.0 added Korean characters the count hit 39000, and the additional 16 planes were added.  Then version 3.0 started adding symbols like music notes, and 3.1 added 42000 extra Asian characters (mostly ""historical"" Chinese characters) which bumped the character count to 94000, which exceeded the original 16-bit bounds.  These extra characters were important for the adoption of Unicode, because it's a lot easier to get people to adopt Unicode if it's backwards compatible with their current system.  Without adding all these historical and compatibility characters, Unicode would be just another set of character encodings among dozens.  In addition, their definition of historical is much too narrow; many ""historical"" characters were in common use less than 100 years ago, and some are still used in peoples names.  TL;DR Asia has way too many characters and people wanted more glpyhs than were originally anticipated. "
Development isn't simply about how much a developer costs per hour.,The Verizon developer was also relaying he received in meetings and likely from conversations. Outsourcing works when you want something that is standalone but integrating with an existing project or requiring resources from within your organization requires communication.  We had an offshore team at my previous job. The developers weren't bad or anything but it was extremely frustrating to communicate with them because of the time difference. Considering that company had 99% of its development done in the US the 1% that was offshore just couldn't communicate effectively. I'm sure they are still working on that product because the whole team probably cost less than a single developer here in the states but they probably also take at least twice as long as any local team to get anything done from communication lag alone.  TLDR: Development isn't simply about how much a developer costs per hour. 
Your post is like giving women a collective pat on the head and telling them it's silly to be afraid of programming.,"> Don't think that computers are just for men to program and debug, but come place your fingers on a keyboard and share with me the joy of making.  It's like you have absolutely no idea at all why women aren't more involved in IT. Your whole post summarises as ""Women. Program more!"" which is not helpful at all and the suggestion that this is somehow a useful contribution (you published it after all) is slightly insulting.  Do you think women just haven't noticed that computers exist or something? There are actual reasons that women don't feel welcome in the industry and you address none of them.  If you want to increase the number of women's CVs you see for development roles and generally hire more of them but couldn't figure out how to do that maybe you should have written something about how you don't know how to go about doing that and you'd like some suggestions?  TLDR: Your post is like giving women a collective pat on the head and telling them it's silly to be afraid of programming. "
"stfu noob.
>
>P.S. Feel free to keep making yourself look stupid; I've got all night.","To quote timeshifter_:  >Do I really need to go into detail as to why you're wrong?>>Yes?>>Ok.>>From your post:>>>Any great SC2 player reacts to the situation, this solution doesn't do that.>>Once again: the strategy at hand doesn't give an afterthought to anything beyond the goal. The notion that a good player reacts to the situation is fucking universal. You'd be a moron to think you can get away with any build without having to react to an opponent. That still has absolutely nothing to do with one specific build designed for one specific goal.>>tl;dr: stfu noob.>>P.S. Feel free to keep making yourself look stupid; I've got all night. "
"I like this build, but it's not a ""win button"". It's a great start to a game, I think.","I used this build twice tonight, both to good effect. It wasn't just a ""build seven roaches and magically win"" in either case, but it was a solid foundation in both games. First was against zerg, and he did a largely zergling army that I was able to overwhelm with my roach/hydra army.  The second game went a bit longer. It was against protoss. The initial push with seven roaches was rebuffed (same as in the first game), but I think it put him a tad on the defensive. I was able to three-base to his two. At 110 food (I waited too long) he attacked with a dozen stalkers. I beat those down, then at 150 food ( very  soon after 110, it was flowing at this point) I sent mutas, hydras and roaches at him. His army was stalkers, sentries, 3 colossi and a couple phoenix. He tried to wall me off and eat my mutas, but I was able to pull back and take out his army with plenty to spare.  TLDR: I like this build, but it's not a ""win button"". It's a great start to a game, I think. "
Car can be as reliable as a car if user didn't expect it to be so interconnected in functionality and if people weren't so cheap.,"On the first paragraph, none of that system is connected in anyway. If you buy a single software you would expect each of its submodule to be able to interact and affect each other. Yes, it is more complex to make A and B interact with each other than just put together a system that can do A and B independently.  And none of them can affect how engine accelerate or break. Will you buy a software that all its extension can't actually affect the core engine?  Proximity sensor probably use separate sensors from crash sensors, and many of them probably have lots of backup chip.  Oh, and are you willing pay for a software the same price as a car?  tldr; Car can be as reliable as a car if user didn't expect it to be so interconnected in functionality and if people weren't so cheap. "
Use the right tool for the job at hand.,"I wouldn't call Eclipse bloatware, though others might concure. It's plugin based so you can make it as slim as you want really. Of course, that's kind of a religious discussion, so i'd rather stop here :) I think what you mean by bloatware is more the initial learning curve. And i agree that Eclipse can be very intimidating at first sight. Once you understand what a perspective is and what a view is all gets really really simple.  Vi/Emacs are valid alternatives but the initial learning curve is quite a bit steeper than with something like Eclipse or Visual Studio. At least that's my opinion so take it with a grain of salt. I only used Emacs for a few months so i can't comment on its pros and cons. I'm spoiled by shiny GUIs so i went back to the usual crop of IDEs.  Notepad++ is something i use for editing text files and occassionally viewing 3rd party source code. I'd never use it for actual coding work. Auto completion, refactoring and so on are just such wonderful tools that can really increase your productivity a ton. So it's not just about big projects but about how you approach writting your code. While handtyping every single bit is a good excercise you get tired of that really soon.  Renaming your binaries in a cygwin install might mess up future updates of your installation.  tl;dr: Use the right tool for the job at hand. "
"CI's system is actually pretty safe.  If you can just verify off an IP address so someone isn't stealing the cookie info, if you're that worried.","Well, you're currently going through what I went through, a few years ago, and let me talk you back from that cliff you're looking to jump off.  Sessions, PHP's home built system, really work just fine.  However, it relies off of cookies to remember what the user's session variable is (you might be able to switch it to URL mode, I just never did).  You could put the session in the URL directly, but that's not really any safer.  I presume CI's prolly lays right on top of PHP's (I have no idea, I've wrote my own framework 5 years ago and do most programming in that).  The major issue becomes how far do you expect someone malicious to be willing to go.  In the session data I set up, I use a combination of mCrypt in cookies, a hash that points back to a session (just like PHP's), and then a control cookie (it's format changes on my mood, admittedly).  The control cookie has the basic data about the user's session, but namely the IP address (hashed for the love of god).  The session is for storing big sets of data that I might want to keep around, and the other mycrypt cookies are for anything else.  Now I know you say you don't like CI's cookies being encrypted, but you have to realize how hard that is to actually pull apart.  Can you do it?  Do you know anyone that easily can?  Then odds are you're pretty safe implementing the feature.  So what I propose is, just piggy back off of CI's, since you're used to that anyway, but just put an extra cookie out there with the IP (encoded of course, also prolly one way hashed).  Of course, if you can't lock onto an IP then you're just going to need to accept that the best you can do is CI's method, but make sure that the cookies are secure.  By that, I mean you're running your site with  and not just  TL;DR : CI's system is actually pretty safe.  If you can just verify off an IP address so someone isn't stealing the cookie info, if you're that worried. "
"I'm fucking tired of all these ""I've never had to use this, it must be worthless"" blog posts.","I don't think the author really grasps the full extent of the Builder pattern and why it is useful.  The Builder pattern isn't just about configuring an object's construction. It's also used  when you need to enforce complex validity constraints on the object under construction or when there is complicated internal state that needs to be maintained during the object's construction. An instance, that I've actually implemented and found worked really well, is creating an arbitrary 2D path made of several different curve types where you want to enforce some constraints on the path.  For example, in Python:  builder = PathBuilder(required_continuity=G1, require_closed=True, allow_selfintersections=False)builder.start(p0)builder.line_to(p1)builder.arc_to(p3, center=cp, direction=CLOCKWISE)builder.bspline_to(p4, control_points)builder.tangent_arc_to(p5, apex = pv, radius=r) builder.close_line()path = builder.create()  The object under construction is fairly complex and there are lots of things that can go wrong. Continuity constraints could be violated, self-intersections could be introduced, the path might not be closed correctly, etc. Creating the path by adding each curve directly and checking all these constraints is incredibly error-prone and hard to get right. The builder also automatically keeps track of relevant state, like the endpoint of the last added curve.  The extra indirection of the builder allows you to do helpful fix-ups prior to creation like optionally removing redundant collinear segments or discarding very small segments that are below the precision tolerance which could introduce numerical instabilities. It can also provide support for failing gracefully either at path creation or as soon as some required constraint is violated.  Sure, ""Builder"" is probably one of the more commonly abused/misused patterns. But there are times (like this one) where you really do need some sort of Builder facility to enforce constructed objects' validity that is inherently distinct from the object being constructed. Language features don't make these problems go away.  TLDR; I'm fucking tired of all these ""I've never had to use this, it must be worthless"" blog posts. "
help me out guys. Developping on an Atom netbook isn't all that grand. :|,"My reply.  >I'm an Electrical & Software Engineering student in McGill University. I have ADHD, meaning that I have it a bit harder than most people in my classes, but that's okay because I love what I do! Programming is amazing, especially embedded, systems and functional programming.  >I'm currently writing from my tiny weeny Acer Aspire One, which is my dev machine (I can't afford anything bigger or better.) It's served me well, cost $200 brand new a year ago and still kicking. Even then, when I went to my first coding competition this year, one of the organizers was puzzled by the fact that I was developping on a 10.1 inch screen. Our two-freshmen-on-a-dinky-netbook team still managed to beat a whole lot of seniors and TAs!  >Anyway, I'd do a lot of things if I had a MacBook Pro. First I'd get it to triple boot into OS X, Windows 7 and a custom linux kernel for developping kernel modules (I've always wanted to triple boot.) It would be really cool to have a more powerful machine, because the Atom processor takes AGES to compile anything - and when you program in C, compiling is half the fight.  >If I had a MacBook, I could start DJing again. I used to DJ on my desktop back when I was 16, but it was SUCH A PAIN to lug around to parties! One of my current projects is developping a digital DJ instrument (hardware and embedded software), but ironically my current computer won't run the DJ software I'm developping for.  >Oh, if I had a MacBook, I'd probably pirate a bunch of games. Then I could play BioShock and Dead Space and Crysis and Condemned and all the things I've heard people raving about for years.  >I'm moving in with my girlfriend  next week. Bitches love MacBooks. If you want to be at my wedding, get me a MacBook to develop on so I can look serious as fuck. (She studies Biopharmaceutical Science, she's more serious than I'll ever be but at least I can pretend!)  >Anyway, thank you for giving me the opportunity to dream a little. Have a good one.  TL;DR: help me out guys. Developping on an Atom netbook isn't all that grand. :| "
"Thanks for the setup, but your thinking is about 60 years old and incorrect.","I knew there was going to be someone that tried this response.  You're 100% incorrect.  What you're describing is pure hubris.  It is impossible for a person to focus on all of these things at one.  Sure, an expert can intemperate any one of these things at any give time, but can't do everything all at one.  This is why HCI is important, even for MORE SO for expert audit systems (nuclear power plants, airplanes, F1 cars, etc.).  You take the knowledge the expert has and you codify it so that a machine can pay attention at 100% effort all the time.  That is not to say that people aren't important, but machines should always be leverages to do the monotonous concentration that people can't do.  Good HCI is always important, there aren't cases where it's ok to have a bad UI, there just isn't.  Especially for things that at this high stakes where milliseconds make a difference.  TL;DR;  Thanks for the setup, but your thinking is about 60 years old and incorrect. "
"Reddit, our new non-profit needs a name--please help us, or live the rest of your life in a miserable guilt ridden existence.","From our Original Post on Ask Reddit:  > As you have more than likely deduced from the title(for the love of God I hope you did at least), we are starting a new non-profit in South FL. Our goal is to provide middle-school-aged kids from our area a place they can come after school a few days a week(we will have schedules for ages/classes) and learn the basics of web/mobile design and programming(and intermediate classes as well).  > Our number one goal is to provide this for free to kids in low-income households, heck even families making 60k a year in South FL still cannot afford more than a small 2 bedroom apartment with the cost of living down here, forget about the cost of sending your kid to get programming lessons. We will have a small fee for kids whose families can afford it (our initial thought is around $100 a month).  > We already have a location central to the schools in our area, and easy to get to from anywhere. We are working on getting the necessary licenses and completing the needed paperwork... and here is where we need our name.  > The foreseeable future will be digital and children who can get a head start in learning the basics of programming and web/mobile design will have a distinct advantage as they continue to grow into productive members of our society. We want to provide an avenue that would not be available to these kids otherwise--many of which do not even have a computer or the internet at home. If they can get a head start, they can improve their situation in life, and maybe they will have an idea they bring to fruition that improves the lives of people everywhere... that's our dream at least.  > TL;DR Reddit, our new non-profit needs a name--please help us, or live the rest of your life in a miserable guilt ridden existence. "
dealing with encodings with out a real string object makes me sad.,"PHP doesn't really have a concept of a string object that knows about encodings. A string is just a byte array. So if you get offsets back (from wherever) if a character takes up two bytes the offset will be adjusted accordingly. You can see this in action:  &lt;?php$str = 'ø';echo $str[0], PHP_EOL;$str_utf8 = mb_convert_encoding($str, 'UTF-8');// php doesn't give a shit that this is a UTF8 string when you index itecho $str_utf8[0], PHP_EOL;  You're probably better off to just use  preg_match  or  preg_match_all  with the [ u  flag]( There would be no need for finding string offsets.  &lt;?php$strings = array(    ""abc_o_pq XXXNoge{!n!} ad dio a"",    ""abc_ø_pq XXXNoge{!n!} ad dio a"",);foreach ($strings as $str) {    if (!preg_match_all('/XXX/u', $str, $matches)) {        continue;    }    foreach ($matches as $match) {        echo $match[0], PHP_EOL; // print XXX in both cases    }}  You can get offsets back from  preg_match_all  if you really need them:  &lt;?phpmb_internal_encoding('UTF-8');$strings = array(    ""abc_o_pq XXXNoge{!n!} ad dio a"",    ""abc_ø_pq XXXNoge{!n!} ad dio a"",);foreach ($strings as $str) {    if (!preg_match_all('/XXX/u', $str, $matches, PREG_OFFSET_CAPTURE)) {        continue;    }    foreach ($matches as $match) {        list($substr, $offset) = $match[0]; // $offset is a byte count        // this won't work because mb_substr is based on number of         // characters as defined by the encoding.        echo mb_substr($str, $offset), PHP_EOL;        // but you can use plain old substr which doesn't give a shit        // about encoding or character counts        echo substr($str, $offset), PHP_EOL;    }}  tl;dr  dealing with encodings with out a real string object makes me sad. "
writing your own special purpose CSV parser is often by far the best course of action..,"I think the author is missing a couple of elephant-size details though.  In many (most?) real-world usages of CSV, the consumer does NOT care about any of those special cases (or deals with one or two) and then writing your own is often much simpler than bringing in an external dependency.  And,  chances are very non-trivial, that the quirk you have to handle will be rare enough that your external library will choke on it.  And, in many cases you might want to parse that CSV into  meaningful (app specific) data structures rather than plain arrays, and then this external dependency will provide very litte value..   TLDR, writing your own special purpose CSV parser is often by far the best course of action.. "
"I've used it, it does the job for not-so-big data.","I have used Weka for a number of data mining classes at the university level, as well as for some work-related and just-for-fun projects. It's quite easy to use and does a fine job for whatever I have used it for. I believe its Achilles heal is the fact that it hasn't historically scaled well for very large data, although I  believe  much of this has been rectified, but perhaps too little too late to be viewed as formidable in this arena at this point (feel free to correct me if I am wrong).  The good thing is that the ML concepts are obviously transferable to other platforms (though I suspect if you have experience in other packages this will be less of an issue for you anyhow). I would say that if you worked in research in the future, or did grad research, there would be a better chance of you ever seeing it again. Also, if you were to get work with an organization that maybe got on the data mining wagon before it became trendy and never scaled up, you may find Weka still chugging along.  TL;DR - I've used it, it does the job for not-so-big data. "
"This has a few good use-cases, but it should be used sparingly.","I'm actually using this in an app I'm developing right now. I have a popup window which shows an interactive graph. The graph can get  very  large (100 million square pixels is the largest I've seen so far, but not all the data is imported yet), or it could be very small (a thousands square pixels or so). So I let the user use googlemaps-style dragging or arrow keys to navigate around.  Anyway, there's a control panel that lets you perform various graph-morphing operations. Since the canvas is much larger than the viewport, I made the control panel position: fixed to the top-left, but made it only 20% opaque (which is, IMO, just enough to tell it's there). When you hover over the panel, it becomes 80% opaque (which is just enough to see what's behind it).  tl;dr: This has a few good use-cases, but it should be used sparingly. "
"Cocoa, with its long, expressive method names (plus use of categories and protocols), is designed for a language that can handle them elegantly, and that language is Objective-C.","Much of Cocoa  can  be used without directly using Objective-C, using PyObjC, RubyCocoa, or MacRuby instead, but two of those suffer severe impedance-mismatch problems with method names.  [[[NSString alloc] initWithData:data encoding:NSUTF8StringEncoding] autorelease]  becomes, in Python,  NSString.alloc().initWithData_encoding_(data, NSUTF8StringEncoding).autorelease() .  Why not use keyword arguments? A couple of reasons. In Objective-C, the order and number of segments in the selector matters; in Python, the order and number of keyword arguments does not matter. Thus, you can easily have method collisions; for example,  -[MyObject foo:]  and  -[MyObject foo:bar:]  are different methods, but Python would put them both under  MyObject.foo  (with optional keyword argument  bar ).  That sounds fine (the bridge could do the work), but consider  foo:bar:baz:  vs.  foo:baz:bar: . In Objective-C, these are two different methods, but Python can't distinguish them with keyword arguments; there is no way the bridge can tell which  foo  you are calling.  Therefore, Python must require the  foo_bar_baz_ / foo_baz_bar_  form, which is uglier and keeps the whole selector separate from all the arguments, unlike in Objective-C, where they're mixed ( [myObj foo:foo bar:myBar baz:nil] ).  Even if you resolve that, as MacRuby does, you still have other things such as categories and protocols that are only found in Objective-C (and possibly a few other Smalltalk derivatives). I honestly don't know how MacRuby addresses those.  So you  can  use Cocoa in other dynamic programming languages, but it often isn't nearly as elegant as it is in Objective-C. Conversely, if Cocoa had been designed to work well in other languages, it probably wouldn't have such niceties as that—and it is really nice, as it allows you to have numbers of arguments that would be unreadable in other languages but are perfectly readable in Obj-C. To wit:  [GrowlApplicationBridge notifyWithTitle:NSLocalizedString(@""Foo"", @""Notification title"")                            description:NSLocalizedString(@""Bar"", @""Notification description"")                       notificationName:@""Thing happened""                               iconData:nil                               priority:0                               isSticky:NO                           clickContext:nil];  Long? Inarguably. Too long? Maybe. Unreadable? Certainly not.  Now, consider the Python version:  GrowlApplicationBridge.notifyWithTitle_description_notificationName_iconData_priority_isSticky_clickContext_(NSLocalizedString(""Foo"", ""Notification title""),NSLocalizedString(""Bar"", ""Notification description""),""Thing happened"",None,0,False,None);  Starts out OK, thanks to NSLocalizedString, but quickly falls apart. Objective-C doesn't have this problem, and Cocoa is designed for a language that doesn't have this problem.  (To be clear, I'm not knocking on Python or Ruby. I like Python, and use it for things that aren't Cocoa. I've never used Ruby for anything, but I respect it.)  TL;DR:  Cocoa, with its long, expressive method names (plus use of categories and protocols), is designed for a language that can handle them elegantly, and that language is Objective-C. "
Web applications should be equally concerned about the issues brought up in the article.,"> web applications don't count  I generally agree with the article, but I found this particular comment naive and somewhat insulting. It seems like a popular attitude among ""real"" developers and I have a hard time understanding where it comes from. Maybe one of you can explain it to me. You could certainly argue that web development requires a broader range of skills, but that's beside the point. Try telling Google, Facebook, Microsoft, eBay, even Reddit and countless others who deploy huge amounts of hardware to support their business online that they shouldn't be concerned about machine cycles. When you're talking tens (or hundreds) of thousands of servers there's a real and significant cost associated with even small changes in performance -- that's exactly why Facebook wrote their own PHP runtime.  tl;dr  Web applications should be equally concerned about the issues brought up in the article. "
"Stop using the word ""models"" and start being more specific, you'll find your questions much easier to deal with than the abstract and loose world that is ""models"".","Zend doesn't provide models because it expects you to provide your own. Your application will have models regardless of whether you use a framework or not. Ever used a Flickr API class? That's a model. Ever created a class to open a text file and read and write data? That's a model as well.  What you're asking for is not ""how do I work without models"" it's ""what are some good DB access libraries"" or ""how do I work with Zend_Db.""  Zend takes the definition that a model is any class related to retrieving or storing data and specifically doesn't provide any integrated ""Zend_Model"" classes to force you, as the developer, to really sit down and think how you organize your data retrieval. It also does this because it knows that ""Model"" doesn't just refer to a RDBMS access library, but also refers to API classes etc. and doesn't want to confine you to a narrow worldview.  tl;dr  Stop using the word ""models"" and start being more specific, you'll find your questions much easier to deal with than the abstract and loose world that is ""models"". "
"Use codecs.open and explicitly specify an encoding (that the user should be able to change through locale settings or similar), never mix byte strings and unicode strings.","Another one who basically misses the point.   UTF-8 is not the only encoding. In the easiest case, you have file formats which declare the encoding themselves (e.g. XML), or where you have a reasonable expectation what the encoding will be (e.g. JSON, which is usually encoded either as ascii with \u00xx escapes or UTF-8). For everything else (i.e., any text file that could have been written using a different program), you need a way to guess or ask for the encoding.  For the non-XML case, codecs.open() is what you want. You specify the encoding name as the third argument (besides file name and mode), and there you have your stream of unicode strings.  For dealing with file formats that are internal to your application, either stick to byte strings in the user's default encoding (which will lead to problems when the files are moved between systems with different default encodings), or use unicode.   Subtle issues may arise in unexpected places if you're not consistent about byte strings and unicode strings: For example, the byte string ""\xc6sop"" (Æsop in latin1 encoding) will count as unequal to the unicode string u""\xc6sop"" (also Æsop), with Python printing a UnicodeEncodeWarning but otherwise doing business as usual.  TL/DR: Use codecs.open and explicitly specify an encoding (that the user should be able to change through locale settings or similar), never mix byte strings and unicode strings. "
I am extremely impressed with MIT's free online courses.,"I'm taking an electronics course from MITx right now. It kind of sucks because I've already taken electronics. To be honest I thought it was more advanced.  Aside from that, it is the best online course formatting I've ever seen. There is everything a normal course at MIT would have except the physical presence. You start off with a syllabus, class calendar, explaination of all the tools at your disposal including online forums for the students and Professors' hours available to answer questions. Each lecture video allows you to slow down or speed up the video and on the side bar they have the dialog (CC) in sync. Whenever you click on a certain sentence on the side bar it will jump to that point in the video. Also, they provide you with the book and lecture slides to look at during the videos. They have additional videos where they work out sample problems by hand. lastly, the class is scheduled like a real one with assigned homework, reading assignments, and labs(virtual) due weekly, a midterm test and a final exam; all weighted according to the syllabus  TL;DR  I am extremely impressed with MIT's free online courses. "
I have a hard time with people griping about buzzwords. Go out there and get some - that's what I say.,"With respect to the author, my opinion is that this is mental masturbation, and not a reasoned view on the state of affairs in webdev today. This came across as a complaint about the latest crop of buzzwords in our field, and it ignores the fact that we are  programmers  first, and to me that equates to being a  problem-solver , not to being an expert in one framework or another, in Backbone or Ember, or Mustache, etc.  As I see it, my skills as a programmer are in being flexible, being able to figure stuff out quickly, and being able to use whatever tools/languages are necessary and appropriate to get the job done. Granted, it's important to have areas of expertise, but I feel like this piece doesn't acknowledge that that there's a balance, where we're a ""Jack-of-all-code"" in some situations, and experts in others.  So in that regards, I felt like it was a pretty obtuse rant. I can appreciate that there's a lot of new-fangled toys and stuff out there, but it's senseless to just gripe about the buzzwords. Instead, hop in and make something happen, and write/blog it up. It's a beautifully open space right now, new projects/widgets/toys/etc are coming online every day, and all of these things should be exciting to us, not annoying.  TLDR: I have a hard time with people griping about buzzwords. Go out there and get some - that's what I say. "
doctypes are deprecated and only used for backwards compatibility.,"&lt;!DOCTYPE html&gt;  basically just signifies that you are using HTML (all versions) and want the browser to render your HTML without using quirks mode.  HTML5 doesn't actually  have  a doctype and the only reason this minimal one is used is to ensure compatibility with browsers, which will otherwise flip their shit if you don't include it.  If you wrote a browser that didn't have quirks mode then an HTML document would not only work correctly without a doctype, but would be 100% valid HTML5 without a doctype at all.  So yes it is HTML 5, it is XHTML, it is HTML 4, and it's also HTML 1.0  Besides, even if it  was  the HTML5 doctype, HTML5 has now been defined as all of the previous iterations of HTML  too , so it would be a perfectly acceptable doctype to use for any HTML document ever written, for any version of HTML.  tl;dr:  doctypes are deprecated and only used for backwards compatibility. "
"I forgot what my point was, but I have a lovely basement.","We finished our basement a few years ago, and I found a lot of parallels.  We knew water might be an issue, so we chose materials that would minimize water damage (special flooring, waterproof carpet barrier, moisture-resistant gypsum board).  It gets cold in winter, so we wanted it insulated.  We wanted to maximize room height, so we found an easy-to-install low-profile ceiling tile system.  We had no load-bearing issues, so we didn't have to worry too much about how the walls were framed, as long as standards were met for the electrician and the drywall installation.  I knew I wanted a big TV, so I made sure there was a reinforced wall the right size with easy access to outlets, and that the room would allow for a couch the proper distance.  Then we had to figure out the steps. The floor we chose used an air pocket system to allow evaporation, so it had to be installed before the walls.  The insulation we chose installed directly on the foundation walls. We wanted to coat the bare concrete in waterproof paint. We had some tricky pipes and stuff to work around. The inspector needed to see the electrical work before the drywall could be hung. We had to frame the walls before the electrician could come. etc. etc.  The process was:   figure out requirements  sketch the layout  choose the best-suited materials  determine the order things needed to be done  actual construction  carpet, furniture, and paint color, etc.   So if by UX the auther means ""I want a dry, warm basement with a big wall-mounted TV,"" then he has a point.  On the other hand, the flooring system we found was essentially ""the latest whiz-bang framework"" that just happened to be perfect for what we needed, and our decision to use it really determined the rest of the design.  We couldn't have added it in afterward.  TL;DR:  I forgot what my point was, but I have a lovely basement. "
Planning for the future shouldn't prohibit you from living in the now.,"So, work to the exclusion of all else while saving up a whole bunch of money and having no fun when you're young.  That way, when you're older and spending less time working and instead spending that time raising kids, you'll still have no time to spend all that hard earned cash you squirreled away.  Finally, when those kids have finally left the house and you finally  finally  have the time to engage in all those activities you and the spouse so desperately want to try, you won't be physically able to because old age has started to set in and those aging joints suddenly aren't taking to kindly to skiing and mountain climbing.  Boo-urns.  Working and saving your money when you're young is a very responsible, forward-looking thing to do, and no one should discourage that.  But, if you ask me, you should not do that to the exclusion of all else.  The way I look at it, if you aren't spending at least some of your time in your 20s traveling/recreating/whatever-turns-your-crank, you're making a mistake.  Well, unless working and doing nothing else is what turns your crank, in which case, carry on!  tl;dr Planning for the future shouldn't prohibit you from living in the now. "
he kept Adobe from stalling the process by cutting through the red-tape that Adobe was throwing up.,"The short answer is that Adobe tried to throw up roadblocks to the work that the HTML working group was putting forward w.r.t. the the canvas element, and other useful tags -- stuff that effectively steps into territory that Adobe has had pretty much monopoly control over until now.  Adobe did so by objecting that the work was out of scope of the original charter and by asking that it basically be thrown out or that everything they were doing be suspended until the charter could be amended to allow the working group the power to do the work they had already done.  Basically, they wanted to stall for time to keep canvas from continuing to erode the need for Flash.  Tim Berners-Lee basically said, yes, the scope has changed, but no we're not going back to the drawing board, and will change the charter as we go to reflect its current responsibilities.  He did so while nodding to the fact that those new features should be defined by separate documents in the name of modularity, but that the current working group was well within its rights to put forward such documents.  TL;DR he kept Adobe from stalling the process by cutting through the red-tape that Adobe was throwing up. "
"Using memorization as your main technique has terrible results, but so does refusing to memorize at all.","Well, I too have always depended highly on my ability to derive from relatively basic principles.  And this is obviously a strength, of sorts.  For one thing, it makes the ""memorizing"" much easier, since I'm memorizing less stuff.  And also, obviously, it's a strength for the reason you outline; reasoning is a key skill, and a scarce one, and you should practice, practice, practice your ability to reason.  But that brings me to a counter-point, which is that memorization has a key role.  What if you had to re-derive the key properties of arithmetic every time you wanted to understand some higher math?  Suppose, for example, you couldn't remember whether multiplication was commutative or not, and when doing proofs you were perpetually having to work that fact out, to see if a step was legal?  Or suppose, to reference a thread about operator precedence and tbray.org from a few days ago, you couldn't remember that function-calling had higher precedence than addition?  Memorization is an essential technique for building abstractions, to let you think about bigger things.  Another word for this type of memorization is ""practice"".  People also talk about ""muscle memory"".  I learned this lesson by refusing to memorize the heuristics in high school algebra, and then being unable to do first-year calculus problems fast enough to pass the exams, despite being able to do well in third-year proof-heavy math courses.  tl;dr: Using memorization as your main technique has terrible results, but so does refusing to memorize at all. "
"mootools is filled with ego, go with another library.","Too me it's pretty easy. Do you want to deal with people who want to help you, or people who want to thumb their noses at you while they fart into wineglasses and sniff them afterwards. I love mootools but use other libraries because I can't stand the attitude of the mootools community. It's actually pretty easy to use, but they throw that ""intermediate to advanced developer"" garbage in there because they don't want to help anyone and think that anyone starting out in javascript deserves to be put down. How many iterations did mootools have before they finally realized that they need to actually work with other libraries? Prototype has a great community, extjs does as well - but nothing comes close to the great jquery community. Sometimes we need help and mootools is NOT where you find any. Remember that we were all noobs at one time.tl;dr - mootools is filled with ego, go with another library. "
"On the server, I absolutely love it so far...","Ok, here's my take after trying it both locally on the desktop and running on a VM to simulate the server case:  On the desktop:  Personally, I've been very pleased running StatET (at first) and Emacs + ESS for work on the desktop. It's convenient in that it allows me to shuffle many many open buffers, do some quick data cleanup via Emacs' unparalleled text editing functions and keyboard macros and jump back into R to load the results as a data.frame or whatnot.  That said, this a very polished option and is certainly more newbie friendly as far as setting up goes. It's not going to replace Emacs + ESS for me personally, simply because I'm already very productive with Emacs and have gotten use to being able to very easily and quickly jump from R to raw text editing back to R and RStudio doesn't really allow me the same kind of flexibility.  On the server however:  I've only been playing around with a toy dataset for 20 minutes and I already love it. The web interface is fast, responsive and, best part, gladly plots and displays my complex ggplot2 creations without me having to output to pdf, scp back to my local machine and open it there. For quick data exploration on a server (e.g. when datasets are simply to large to handle locally) this is a great boon to me as visuals help me understand what is going within complex data in my own way and the traditional plot() and ggplot() functions don't play nice on a headless server accessed over SSH.  Moreover, the install itself was very easy on a Debian server (it was actually so quick I thought it had failed initially...) and works with existing credential, allowing any of your users to get going very quickly.  TL;DR On the server, I absolutely love it so far... "
"plagiarism in academia is about intellectual dishonesty, plagiarism in the industry is about copyright and patent law. The survey is too generic to generate any meaningful data.","That must have been one of the most pointless surveys I have ever taken.  There is a huge difference between academic plagiarism and professional plagiarism. The former is also not just about ""education"". Also, being a student and being a working professional is not mutually exclusive, nor does the latter imply having been the former at some point.  Academic plagiarism takes many forms, some of which are not recognized as such outside academia. It is intellectually dishonest to pass off other people's research as your own, even when it's not strictly illegal.  On the other hand, when doing business, it may often be impossible to provide proper attribution (at the same level as it is required in academia) because doing so would open yourself up to litigation. You can look at available products and take inspiration from them, but if you credit them, you may accidentally turn your own product into a derived work and risk being sued for damages. Further, corporate policies may actively prevent you from giving proper recognition, even in internal documentation.  Lastly, plagiarism in the software industry is a complicated topic by itself. Apparently there is a rather common problem with individual companies not respecting third party license terms, but that is more about contract law and not always about intellectual honesty (consider including a AGPL database driver in a non-AGPL high-level project where the database access is merely an implementation detail rather than a core feature -- it's obviously violating the terms, but intellectual dishonesty is the smaller problem here).  There's also software patents, which most developers (i.e. the kind of people who generally produce the patent matter) consider counter-productive and harmful to the industry with little benefit to the actual authors, but are a huge issue with violations often resulting in legal cases or extortion (cf. patent trolls). Most of them describe concepts that are either trivial or commonplace enough that not giving attribution to the ""original author"" would not sensibly be considered plagiarism in academia even if the patents are legally enforcible.  tl;dr: plagiarism in academia is about intellectual dishonesty, plagiarism in the industry is about copyright and patent law. The survey is too generic to generate any meaningful data. "
"View implements methods to display stuff. 
 
 Model implements methods to manage data. 
 
 Controller communicates between View & Model.","Most designers ""dream up"" their projects in the following order: View -> Model -> Controller.  1) We draft up an idea (physically or mentally) for what we want the end product to ""look like"". At this phase we probably do a mock-up version of what the end product will be (the View).  2) We decide upon the most appropriate way to store the data, which might by MySQL or SQLite, and then we start writing the necessary code to write to/read from the database (the Model).  3) Finally we connect the mock-up to the database and start populating our view with data from the database. The ""glue"" that holds everything together (the Controller).  These three aspects make up the foundation for how applications work, whether that's a website, a web app, or a system application. Simply separating the code into different files would be one step, but building together a set of classes (or ""objects"") and implementing Object-Oriented Programming (OOP) allows you to write code much more efficiently. This allows us to do things like:  // Let's pretend this code is in your Controller.$model = new DataClass();$userlist = $model-&gt;getAllUsers();/* We let our model handle all of the database pieces.    All we really want here in our controller is a list of users.    We don't want to do any SQL queries in here! */// And if we want to add data back to the database?$model-&gt;addUser(array(""first"" =&gt; ""Brent"", ""last"" =&gt; ""Spiner""));  So let's say in a few months you decide you want to switch from SQLite to MySQL. If you've separated your code into MVC you just need to update your Model. The View will still display things the same and the Controller still communicates between the Model and the View exactly the same way, no change of code is required there. All you need to do is update your Model and ensure it spits out the data in the same way and nothing changes.  This also allows you to be more efficient at collaboratively working on projects. It doesn't matter what methods are used by the Model to collect the data, so long as it returns it to the Controller in a predictable/consistent way.  TL;DR:   View implements methods to display stuff.   Model implements methods to manage data.   Controller communicates between View & Model.   "
"Failures in large homogeneous systems are correlated, so no matter what paradigm you choose you still can have a catastrophic failure, even from a single stupid bug.","Well, imagine a huge-ass server connected with a bunch of optic fibers. If you cut those fibers server becomes useless to people outside, no matter how large and tolerant it is.  Malformed date brought down large system because it was critical to its internal communication fibres. This date was critical because it was used in crypto parts. All code is critical in crypto, as lax checking rules usually mean vulnerability. You shouldn't be robust in crypto code, instead you should give it rigorous testing and code reviews. This is what Microsoft lacked. Crypto code should have gone through meticulous, paranoid review, and detecting such a blatant error is pretty much certain in this case.  Another problem was in 'automatic healing' procedure which just assumed that hardware was a problem even though there was no real evidence. It is incredibly sloppy and it is what usually makes problem a catastrophe.  I don't think it's possible to make homogeneous architecture fault tolerant to the point where bringing down whole system is impossible: software bug will affect whole system just because it homogeneous. If effect from this bug is delayed and obscure, you won't be able to test it and prevent failure.  However, it should be possible to mitigate the problem. Everything should be logged in such way that all unusual messages will be immediately discovered, so it wouldn't take hour to investigate a trivial problem. Automatic self-healing should be conservative, and when something unusual happens it should stop to prevent further damage.  But if you care that much about fault tolerance, you shouldn't rely on a single service provider: use both amazon and azure, for example. It's fairly unlikely that both would be hit with a same bug.  I've heard this approach is used in some redundant storage array: instead of buying a bunch of disk from single vendor they install different disks from different vendors. Failures of disks from a single vendor are correlated: maybe in a year it will hit some bug in firmware, or maybe some mechanical problem due to wear. Disks from different vendors have almost entirely uncorrelated failures, so you can replace them one by one.  tl;dr: Failures in large homogeneous systems are correlated, so no matter what paradigm you choose you still can have a catastrophic failure, even from a single stupid bug. "
Systems that are properly designed end-to-end will work better than systems designed with crutches in various places,"First, how do the server send a HTTP 503 Service unavailable when there are no sockets or file descriptors available?  Second, the number of simultaneous connections is limited by available memory if epoll is used, not by FD_SETSIZE as in select.  My argument for long polling/persistent connections does not mean that one blindly should implement it without proper testing or analysis, or for that sake, a proper design on the whole system.  But still, how can returning several empty responses to a frequent polling client be more efficient than returning the response when there is actually data to deliver?  Also, if the server is overloaded, then the server has full freedom to disconnect idle clients that uses persistent connections to allow new clients to connect in a round-robin fashion.  With proper client design (which obviously the server has control over since its serving the client code) the client can be designed so that it waits between retries on a heavily overloaded server.  TL;DR Systems that are properly designed end-to-end will work better than systems designed with crutches in various places "
CPython without GIL won't solve your CPU intensive tasks running on multicore problems.,"Python does not have a GIL problem, CPython has a single-core problem.  Actually, CPython does not have a single-core problem either, there are tons of python libs can support multi-core or even GPUs.  It's just lazy developers want a magic bullet to scale their crappy, single-threaded, inter-locking, blocking spaghetti code to multiple cores for free.  Anyway, if you are doing CPU-intensive task in pure python you are probably fucked sooner or later. GIL is just the final straw on camel's back, the most common excuse people usually find.  btw I've seen millions of games written in C++ which can only max out a single core, yet no one blames the C++ runtime.  tl;dr  CPython without GIL won't solve your CPU intensive tasks running on multicore problems. "
"You will have to pry the text headers and the #include directives out of my cold, dead hands! :-)","""The module import loads a binary representation of the std module and makes its API available to the application directly."" GREAT! Instead of text I can read and pre-process and generally, you know, FIGURE out, I get a compiler-specific binary blob that I have no goddam idea what it contains.  Rant:  Compile-time scalability: I compile once and run an eternity. If your dev process that involves compiling every 30"" is slowed down, get out of the kitchen.Fragility: bollocks. God forbid you get, you know, a compiler FAILURE, cause you're doomed. What are you, an infant?! Deal with it!Conventional workarounds: Ooohh, but I CAN work around it, can't I? Try that with your binary blob. And don't tell me you won't have to cause it will be perfect. I heard that argument back in the 70's, the 80's, the 90's, etc. Are the workarounds a barrier entry and Joe Millennial cannot get to play C++ with the cool kids? Fine!Tool confusion: there are no confused tools, just confused programmers.  Finally, and this is really funny: ""To actually see any benefits from modules, one first has to introduce module maps for the underlying C standard library and the libraries and headers on which it depends. The section Modularizing a Platform describes the steps one must take to write these module maps."".  Dude, you're killing me!  tl;dr: You will have to pry the text headers and the #include directives out of my cold, dead hands! :-) "
"I voted ""Yes, but it's not very urgent"", which, at that moment, was on 11%, 34 Votes. :-)","Usually, when you exaggerate, it means that you have a poor argument, and as far as I can see, [the argument]( is quite exaggerated:   compile time scalability issue is eliminated by include guards and precompiled headers   fragility can happen, but honestly, how often does one deal with it?   ""conventional workarounds"" applies largely to C, not C++, because in C++ one needs macros much less and can use namespace   tool confusion - TBH, this is the only valid complaint, first sentence of it. The rest really is about badly made headers.    I mean, honestly...  world  has been written with #include. So people obviously can do it. Sure, make something better, but get real first :-).  tl;dr: I voted ""Yes, but it's not very urgent"", which, at that moment, was on 11%, 34 Votes. :-) "
"I'd hire a guy with a few solid, useful, working projects over a degree any day.","When I interview cs grads I always figure they're starting at a disadvantage - I don't know about in the US, but in Canada the focus on 'make a function that shows one concept, doesn't matter how you do it, test it against X, move on' leads to programmers who don't understand deeper concepts or see the big picture. As well, lack of experience and little training mean that the process of getting a project, understanding its scope, delineating clear, reliable functionality, making sure that coincides with the project goals and actually executing it is seriously buggy. If you can prove that you can do those things, say by taking something you're interested in, making a program that works and that you can give to potential employers, and hey, maybe even get some people using it, you'll be much farther ahead. Nothing better than being able to say ""I make useful things, oh and 1000 people agree with me and use my software""  tldr; I'd hire a guy with a few solid, useful, working projects over a degree any day. "
"syntax is quick to learn, supports every style of programming (and thus teaching), and lets teachers focuss on programming rather than fucking around getting the thing to compile","Quite simply, because it is the last programming language.  I don't mean that in the ""mine's better than yours way"", but rather it's where tired programmers end up.  And oddly that makes it good for beginners too.  It can do literally everything every other programming language can do, and usually more than any other language.  It supports every paradigm.  It can bootstrap itself.  There are implementations you can boot on bare metal.  It can parse itself (this is more handy than you think).  And the syntax is nice for both old programmers and new because you never have to look up a table to figure out if the >> operator has a higher precedence than the ?: statement.  Having said that, scheme does need more syntax, but no-one agrees on what it should be, so it doesn't happen.  Finally, scheme is just pure programming.  If you ever have thoughts like 'wouldn't it be great to just, somehow, program the code behind all this code', you're pretty much talking scheme.  edit tl;dr - syntax is quick to learn, supports every style of programming (and thus teaching), and lets teachers focuss on programming rather than fucking around getting the thing to compile "
The Nokia N93 sucked. Then the Nokia N97 sucked too. I'm getting an Android.,"I'm done with Nokia too.  Back in 2006, I bought the Nokia N93 right when it came out - it was the flagship then. It was supposed to get N-Gage support. N-Gage was used in the promotion material for the N93. But support never appeared. The N93 owners were never compensated for this bait-and-switch.  Then, when the N95 appeared 6 months after the N93, Nokia forgot all about N93. From then on, it was all about the N95. N93 users were left hanging with a poor firmware (and far too little RAM).  Anyway, I thought I'd give them a second chance, so last summer I bought the Nokia N97. Turns out it didn't have the 3d chip that the N93 had, so it was actually a  downgrade  in that department. And the N97 also had way too little RAM, and much too small C: drive to boot (pun intended).  And then there's the issue with the N97's crappy GPS. When driving, it always seems to believe I'm about 100 meters behind where I actually am. ""Turn right here"", the GPS says. ""Oh great"", I say, ""you mean in the intersection I just passed?"" Sigh.  As soon as I can find a slide-out qwerty Android, I'm abandoning the sinking flagship that is Nokia N97.  tl;dr The Nokia N93 sucked. Then the Nokia N97 sucked too. I'm getting an Android. "
the right amount of abstraction is better than too much abstraction which is better than no abstraction at all,"Even if it's spaghetti code, it's still modular in a way that ""JDBC in JSP"" just can't be:  > All the code that does this is in this place.  rather than  > All the code that does that is scattered across each page that does that.  I suppose, from my perspective, what my argument boils down to is:   the right amount of abstraction is better than too much abstraction which is better than no abstraction at all,   Layered architecture enforces a degree of modularity that is helpful in maintainability   In the day-to-day grind of adding stuff on a deadline, a layered architecture is just easier. It's more of a safety net, and prevents you from making errors you might otherwise make at 7pm on some Wednesday before a release.    But again, in my mind I'm thinking of the inevitability of scope creep: If it's useful/successful, it'll get bigger and more bloated. If it's not useful/successful, it won't matter either way how you built it.  At the very small scale, your argument holds, and the time to market is more important than the (now small) overhead of maintaining/extending unstructured code - but overtime that overhead grows and grows until it far overshadows the effort it would've taken to do it right to begin with.  At the end of the day ""fun"" code isn't rough-and-ready implementations that work and get pushed out the door, it's good code that can be understood in equally sized chunks at 1000 and 50,000 lines of code.  tl;dr the right amount of abstraction is better than too much abstraction which is better than no abstraction at all "
Optimizing a system is a matter of profiling and low-hanging fruit.  A Lisp interpreter in Python is not the low-hanging fruit here.,"A more apt analogy would be that I've added bumpers and airbags to a go-kart to make it safer.  Which was the intent.  I went ahead and benchmarked the implementation using tak and fib.  Both of them seem to be 1/100th the speed of the Python equivalents, which is not surprising.  But when the vast majority of operations to be performed within the runtime are of the form ""move column X from row Y to row Z"", ""add two numbers"", etc., and the underlying system is limited to around 2500 row write operations/second; the Lisp interpreter  only  being able to perform roughly 30k call/math operations per second is more than sufficient.  TL;DR Optimizing a system is a matter of profiling and low-hanging fruit.  A Lisp interpreter in Python is not the low-hanging fruit here. "
It takes ages to write things on a cellphone.,"Im sorry but isn't the purpose of reddit to share information and knowledge? Why is it bad of me to want to make sure I can take part of that information? I dont have insightful comments about everything I read, I read it because it is new and interesting which in it self makes the chance of having an insightful comment quite slim. Especially when there are 200+ comments already. Should I make something uninteresting up just to avoid downvotes? Should I refrain from making note and future use of the insightful post simply because I have no additional details to contribute? Am I to be excluded from taking part of the information the OP gave to the world simply because I have nothing to contribute right now?  Besides, if my post gets downvotes it puts it at the bottom so that it doesnt bother anyone. I isn't that the whole purpose of votes? I for one couldn't care less about 'karma'. Let the downvotes do their job and let everyone who wants to make use of information on reddit do so.  TL;DR; It takes ages to write things on a cellphone. "
"estimates can be useful even with scope creep, but YMMV.","Okay, it's true that you might have no good reason on your project to do estimates. I'm currently working on a project where estimates are not used, and it's fine. You have to have good reasons to do things that cost time and effort.  On the other hand, new requirements don't strike me as a good reason to forgo estimates. In fact, just the opposite: estimates can be used to demonstrate the effect of scope creep. Those new requirements should be either pushing out the schedule, or pushing other requirements out of the release. Something has to give. (In a dysfunctional org, it's your sanity, your nights and weekends, your performance reviews...) With estimates in hand, in a non-dysfunctional org, it's usually the release date that will give, because deadlines are usually just lines in the sand. A good manager knows that if the team believes in the deadline, it can be motivating, but if the team knows the deadline is shit based on crap, it can be very demotivating. Occasionally, there is a real deadline, effectively imposed from some external reality (such as, the game must hit the shelves before Christmas, or the Facebook API we are using will be deprecated by Facebook on that date) and then features are what will give.  There are other reasons for estimating that are completely independent of scope creep, such as prioritization. Hopefully, whether something takes 2 hours, 2 days, or 2 weeks has some effect on whether it will be done, or how it will be done, independent of any other feature. Also, estimates sometimes improve the conversation: ""why will it take a week?"" ""Because I have to do x, y, and z first."" ""Oh, no, I don't want that. Why don't we just..."" Finally, estimates help to not overplan or underplan a sprint, both of which are forms of waste (assuming your planning meetings are well-run and not a total waste anyway).  TL;DR: estimates can be useful even with scope creep, but YMMV. "
"Google doesn't take a proactive stance in protecting users unless they are certain it will harm their business if they don't, I don't trust them.","The point is it doesn't matter if you do have a perfect cryptosystem between you and their servers (if you're doing something generating millions of records on a remote server I'd hope you are  at least  using a basic SSL connection just for sanity's sake).  I don't believe Google is trustworthy, they campaigned against SOPA because it would hurt their business, they did  nothing  to campaign against a bill that would indemnify them against legal action in the unlikely event that someone was able to prove they were giving data away to the government while simultaniously giving up their biggest chip in defending the data they collect from users that could be used to defend the privacy of those using their services in spite of the implications to user privacy.  TL;DR: Google doesn't take a proactive stance in protecting users unless they are certain it will harm their business if they don't, I don't trust them. "
"Don't invent your own way to secure data, other people have already solved these problems and -
if it involves sensitive personal data, it's not a good learning opportunity.","A good lesson to learn early on in computer security - if you're handling sensitive data, building something from scratch is almost never a good idea.  There are existing libraries, frameworks, etc which have already been put through the ringer.  Use them.  Using a new or novel approach to securing data is almost always a terrible idea.  Just ask Adobe...  That said, if you didn't already have enough reasons to not collect SSN here's another:  There are laws and regulations that come to play when SSN, Credit card, Bank, Drivers license or other similar personal data is involved that can invoke serious penalties/fines, bad enough that it put you in personal bankruptcy or can shut down your org if there's a breach.  Specifically, both Massachusetts and California have aggressive privacy laws.  It doesn't matter where you're located, if you receive and store sensitive data about residents of either state, and there's a breach, the find is based on the number of people in the DB.  If you have 50 names in your DB from one of these states, multiply the fine by 50.  If you have 1000, you can pretty much count on shutting down.  This isn't to discourage you, it's great to you're getting into development and PHP.  So, two things it's best you learn early:  TL;DR - Don't invent your own way to secure data, other people have already solved these problems and -if it involves sensitive personal data, it's not a good learning opportunity. "
"its not just a fad, its worth learning and using it, but the community does expend a lot of effort ignoring problems with the fundamentals.","I don't think it was all google duping us into it. When you first get into using angular, it's quite exciting.  A lot of webapp frameworks use object models like traditional desktop GUI toolkits. As a web developer, it can feel like you're throwing away the ability to clearly layout your UI in code, losing some of the benefits of a page of markup. Sure you can have HTML templates, but they're scattered and it takes some hunting to work out what goes where.  The you start with angular and it's like coming home. Except that your house has been decked out with the latest home automation systems. It just feels right. You can knock up demos and create a UI that relates directly to the business models you are serving up from your code-heavy Java server. It all feels like the state of the art has turned a corner and fe development will never be the same again.  Then one day you realise that the shiny new gadgets in your house don't want to play nice with the old gadgets you have relied on for so many years. Angular is largely incompatible with anything else and the community has taken to claiming this is a good thing, re-writing jquery plugins from scratch. There are some apps that angular just isn't a good match for - they make this clear on the home page - but it's easy to get carried away with it.  TL;DR - its not just a fad, its worth learning and using it, but the community does expend a lot of effort ignoring problems with the fundamentals. "
"There’s nothing wrong with it, but I can’t see the real benefit, so I wouldn’t do it.","We did some form of that, but only for type-hinting. Our IDE couldn’t tell what kind of object the container was returning. We stopped it as soon as we discovered a plugin for it.  So as long as it’s a method of the container I think there’s nothing bad with it. If you were to extract it in a controller or any other class, there would be a lot of noise in your code, and none of these methods would be really reusable.  And then, is  container-&gt;get('client.address')  more readable than  container-&gt;getClientAddress() ? I don’t think so.  Instead of commenting, use better service / parameter names.  TL;DR, There’s nothing wrong with it, but I can’t see the real benefit, so I wouldn’t do it. "
Avoid using recursion and tail recursion unless necessary because they will break things.,"Normal Recursion takes place in linear space (each additional function call eats more memory when it adds another frame to the stack). Unless you have a good reason (for example, checking every descendant element of a DOM node), you should avoid recursion. The issue here is that if you go over a couple thousand recursions, you will overflow the stack and crash.  Proper tail recursion is an exception because it takes place in constant space and doesn't need to add a new frame to the stack and it can do some stuff that you can't really do with loops (the continuous passing style of programming).  The issue with proper tail recursion is support. The current ECMAscript 6 draft requires JS engines to implement proper tail calls, but nobody has gotten around to doing it yet (and once they do, there's still legacy stuff that will take years to catch up). As such the only tail calls that are easily done are self tail-calls that use a trampoline.  tl;dr  Avoid using recursion and tail recursion unless necessary because they will break things. "
if you can't change anything you can't change anything.,"> Imagine if Google and Microsoft pushed a universal standard by building an entirely new rendering engine from scratch for both of their browsers while still supporting the old.  Imagine if no new standard was needed; allowing that all you need is a drawing surface and network connection, and you can do everything that the browser does, that allows web apps. The ""web"" doesn't have to be a huge complex beast. Cut to this level, doing this would be very easy and take very little time.  If you want to impose the requirement that the new/better web be fully compatible with the old web then stop. There's no reason to do what you're proposing if nothing changes. Want compatibility? There's no reason you can't run the two together, as separate programs/processes.  So there doesn't have to be a huge cost. Producing something simple and better is easy enough. How you get that installed is another matter.  tl;dr; if you can't change anything you can't change anything. "
"I would be tempted to try a single big array despite what this paper says. 
 Does anyone here know what data structures our favorite text editors actually use?","I'm just an amateur, so please be gentle, but I have a few comments on this paper. First, this paper is over 16 years old. If it had been a baby girl it would have a nice ass and a driver's license by now. The computing environment has also changed. Modern micro-processors have multiple cores and large caches. Modern programming should be optimized to feed multiple cores smoothly and prevent cache misses. The problem is, the more complicated we make data structures, the more we keep the CPU from doing what it's really good at. I didn't think this up myself. Check out [this guy](  Even inexpensive CPUs often have L3 caches of 5MB or more. For perspective, that's large enough to hold the entire uncompressed text of the King James Bible. What gigantic projects are your user's editing? Even if his name is George R. R. Martin, he probably edits one chapter at a time.  TL;DR - I would be tempted to try a single big array despite what this paper says.  Does anyone here know what data structures our favorite text editors actually use? "
"Good find, too many people think they can suddenly become programmers just because they did a few hours of it.","Oh god this is terrible;  > When it came time for a technical interview with the lead developer, I felt pretty confident. Except for JavaScript ""engineering"" and anything related to algorithms, my technical skills are sharp.  So you can use standard tools and basically operate a computer? Great, but that's not the kind of ""technical"" that a job asking for you to answer FizzBuzz wants.  > Like, what's the use case? When would this come up in the role?  An attempt to deflect responsibility for their lack of skill.  > HTML5, CSS3, JavaScript.  If JS is a requirement of the job, then you need to  know JavaScript . Being able to use jQuery's UI functions isn't ""knowing JavaScript"".  > In this role, you will collaborate in designing, building, and testing world-class web applications.  So, yes, a full understanding of JS is needed. Web apps aren't build by people that can only do $(""div.someclass"").show() or whatever aren't exactly qualified to write world-class webapps.  So it looks like FizzBuzz did the job and this person is whining that they got caught out. Breaking down a job application into smaller, contextless, pieces and attacking it isn't a successful argument, it's a waste of time to attempt to deflect the emotions of a job rejection.  tldr; Good find, too many people think they can suddenly become programmers just because they did a few hours of it. "
Author brings up why you always want to ask the interviewer to describe what a typical day/week/month might look like in the role.,"Was going to rant that as someone who probably couldn't answer many of the questions mentioned, but could program fizz buzz despite rarely touching javascript since about 1999, that people should be able to figure it out, but I actually read the article, and it sounds from the description that they were putting emphasis on the wrong aspects of the job.  While it did sound like they were looking for everything, it certainly seems as though they THINK they need more of a programmer than designer.  Having been involved in the process of documenting jobs previously, it is quite interesting to see the divergence between what the job is, and what the job is in HR.  I remember one department I worked for at <insert some former employer> the job description was basically:  Hit print once a year, and answer any questions.  Well when we had efficiency consultants come in, they were licking their lips, as we seemed ripe for gutting, especially given the complaints re: our error rate.  The 6 sigma black belt comes in and diagrams what we do, and realized that we were doing tax research that would otherwise require outside consultants/a lawyer, and responsible for a data warehouse for taxes and statements, and heavily involved in the project management each year for many tax changes that happen federally and across each state.  All that said, the job titles were updated again to say ""hit print yearly, answer questions"" essentially.  The one good thing was that the six sigma guy basically told the upper management to back off re: the error rate, cause apparently we had one of the better rates in the industry.  I think it was 1-2 documented errors per million or thereabouts.  Wow I digressed a lot...  tl;dr:   Author brings up why you always want to ask the interviewer to describe what a typical day/week/month might look like in the role. "
Humanity wins when we learn. Everyone should know something about all the tech they interact with daily. [Logo - for ages 2 - 102](,"I remember doing a few classes in  Logo  back in the early '90s. Then, in secondary school (high school to most of you) we got basic Microsoft Office training and the option to get an [ECDL]( cert.  I think I would have been about the age of 10 when Logo taught me the fundamentals of conditions, loops and variables in the most simplistic way, using shapes. Shapes made it stupidly easy, as a kid, to visually see how changing things in my code resulted in differing shapes and in obvious ways - length of lines, angles made, number of turns, infinite runs. I think I played with the tutorials and sandbox env on that [5 1/4"" floppy]( every chance I got. For whatever reason, that was the only year in that school we did something like that, something development oriented rather than just email or how to use a web browser. Thinking back I suppose the teacher we had that year might have had a passion for tech or, maybe it was just an easy way to fill the hours, but whatever the motivation that's the year I got into computers and we've been great partners ever since.  But back to your actual question, many schools offer introductory programming or computer science nowadays (at least around Europe, can't speak for elsewhere). Increasingly more are doing it too and I think it's a great thing. As time goes on and more of our interactions with society shift online or into the devices we carry with us or wear, the greater control and knowledge everyone has of what they use and the better the things we're going to build will be.  TL;DR : Humanity wins when we learn. Everyone should know something about all the tech they interact with daily. [Logo - for ages 2 - 102]( "
"thanks to spammers, running your own mail server is problematic. 
 Now I just pay for a hosted exchange account and I'm much much happier.",">would like to run my own email server  Be very careful, and do a lot of research. I ran my own exchange server for a long time, but then started getting burned on lost emails. First my ISP refused to register my IP in ARIN, so some blacklists picked me up. Then I went through a rough patch where my ISP was  actively  placing my IP in blacklists (they didn't want mail servers running on their accounts; except I was paying through the ass for a business account which was supposed to actively allow this). Of course through this time I couldn't get hold of admins who knew what I was talking about...  Then as a result of this came a long stretch where some blacklist maintainers just listed the whole IP block from my ISP. Then finally I found out that Google was blocking inbound email from my IP. And through all of this I always had to find out the hard way because blacklisted email is just dropped, not refused.  tl;dr: thanks to spammers, running your own mail server is problematic.  Now I just pay for a hosted exchange account and I'm much much happier. "
nlevel trees for parent/child structures are what you are looking at. using nleft and nright values to collate groups of parent/child trees.,"you need to look at nleft or nlevel trees.  in addition to the table as described above you would have an nlevel, nleft and nright fields.  These three fields allow you to have an endless parent/child structure throughout your table, you could literally go 50 levels deep if you so wanted to, its not practical but its possible.  From experience, even four levels deep can be problematic.  You would also be best having a category value as well so you can categorise your blog article.  You would create the categories with the same parent/child structure.  When you call your blog,  a simple select * from table where id = 1 is not enough  As you are also talking about comments, these need to be added to a table with a parent/child structure as well.  So now you have three parent/child tables using nlevel, nleft and nright.  How do you get it all together.  using joins.  inner join, left join, outer joins.  read up on them.  select * from table1, table2, table3inner join table1.cat_id = table2.catid inner join table1.comment_id = table3.comment_id WHERE  now in your where statement you feed in user search terms, WHERE keywords = 'keyword' and username = 'name'  You need to do some research on nlevel because you will need a function to write the nlevel, nleft and nright into the table to create a parent/child structure to read from.  to get your category id above you need to read from the nleft and nright before you can feed the id into the select query.  Hope that makes sense, it seems complicated at first, but once you understand the underlying principles youll pick it up pretty quickly.  TLDR nlevel trees for parent/child structures are what you are looking at. using nleft and nright values to collate groups of parent/child trees. "
"Face recognition is a complex, quite deficient and CPU heavy process at the current state of the art.","Face recognition is a two steps process:   Detect faces in the image ;   Recognize the detected face(s) by comparing the face(s) with a database of known faces.    The detection process is quite efficient at the current state of the art, says ~90% of faces correctly detected at 10~15FPS on VGA images on a modern CPU. See the Viola-Jones method, which is the most famous detection algorithm and is implemented in the OpenCV opensource library.  The recognition phase is a lot more complex. Current algorithms suffer from :   Images quality: you need pictures with a good illumination, says magazine-like pictures and not late-night-party-photographies taken with a mobile phone ;   Face orientation: algorithms are a lot more efficient with front pictures in comparison with side pictures ;   Limited number of faces compared in the database: the more the faces are on the databases, the more false positives and wrong detection you will get when you run the comparison process.    The best recognition algorithms give ~90% of correct detection on good quality pictures (add this to the 90% of the detection process) when comparing on a database of ~5000 faces. These methods probably runs at around 1FPS on modern computers.  This page gives some results of different recognition algorithms:  In comparaison, humans are able to recognize faces correctly in more than 99% of the images of the same tests.  Some methods use 3D cameras and are more efficient but require a specific hardware.  Opensource libraries implement simple algorithms which are not really efficient, so I'm currently writing my own implementation of a face recognition algorithm (as an amateur project). This is a quite complex and requires knowledge in computer vision and machine learning.  tl;dr : Face recognition is a complex, quite deficient and CPU heavy process at the current state of the art. "
you don't have to sell your soul to get paid well and love the software you write,">That's the only way to make that much  Not so. The upper ladders of companies where software is a strategic asset, and engineering results are valued in billions have pretty large payouts.  Generally it's not all cash, its usually around a 200-300K in cash and the rest as stock.  The upper bounds of stock payouts can go quite a lot higher for partner-level roles.  What sort of jobs do they do?  The gamut of activities, software leadership, individual contribution etc.  The folk that I work with on such brackets are usually  very  experienced 15+ years in a wide variety of subjects.  They're  vocal on their opinions, interested in a variety of problems, and as passionate about software in their spare time as at work.  The number of roles at this level are usually limited - a handful in a division of a couple of thousand people where I work.  They are often treated as a corporate resource, sent in as ""movers"" or ""fixers"" then bounced onto the next problem when done.  TL;DR you don't have to sell your soul to get paid well and love the software you write "
"I'm a developer, I like math, don't shit talk math."," High school calculus (2) -> theory of probability ...-> finance, insurance, data analysis  High school algebra -> discrete math ...-> repeat 1. + programming, electrical engineering   Math is probably the most important skill that any technical career is based on. Even if you never end up using that Laplace transform, or that complex root equation you learned in undergrad again, you never lose the ability to think in logical coherent steps to solve a broader problem. Look at any of the true innovators in computer science, and you will always some sort of a math background.  While learning discrete math and being able to write a program in C that will give you all permutations of a string would be more fun and rewarding, you skip the part of learning why that solution would be the more elegant or appropriate approach to the problem at hand. While I can see how it is hard to imagine where anyone uses there 'high school math skills', I assure you, without them you would not have the ability to reason through the things you can. Also quants.  TLDR: I'm a developer, I like math, don't shit talk math. "
It will be hugely welcome of MS goes back to sane DLL deployment with VS 2010.,"This has caused me massive headaches. Microsoft's official solution is to package your app's installer with merge modules for the runtime library dependencies. That only works if you're providing an .msi installer, with all the baroque hell that implies.  If you're using an exe installer like Inno Setup, you've got to use their ""Visual Studio redistributable"" package, which installs all the runtime libraries, whether you use them or not. It also installs them system wide, which completely defeats the purpose of dll ref counting. That is, if the user uninstalls your app which used the redistributable, there's no way for her to determine if the redistributable files can be safely uninstalled as well.  On top of this, a silent install of the redistributable required some serious black magic in the VS 2005 version; this is at least easier in the VS 2008 version, but now it's impossible to do a completely silent install.  If you look around on the internet, you'll find countless hobbyists who tried distributing programs that they made with VS Express, and gave up because the process was too complex. I often have suspicions that MS is making unmanaged C++ development as painful as possible in order to get developers onto .NET.  TL;DR: It will be hugely welcome of MS goes back to sane DLL deployment with VS 2010. "
"See spot. See spot run. Run Spot run."" Is probably a better password than anything you are ever going to actually remember involving random !@£$%^*","What's your point?  A a 25 character password of a-z0-9 contains more than 128-bits of data. That's heat-death of the universe long to brute-force.  Using a-zA-Z0-9 gets you the same amount of data at 21 characters. Personally I'm not sure this is worth the extra mental complexity in remembering it.  Obviously I, being an english speaking human being am quite likely to prefer certain combinations that look like english over others but this is still a huge search space.  If you have say a 8 word lowercase passphrase selected from the 2000 most frequently used words (this represents over 80% of english usage apparently) you are talking about 88-bits of data which is still huge.  More character variation becomes basically pointless for any reasonably long password.  TLDR; ""See spot. See spot run. Run Spot run."" Is probably a better password than anything you are ever going to actually remember involving random !@£$%^* "
many developers knew IE was a trap.Many managers/salesman didn't.,"we were talking about a closed environment, specifically intranets. I was one of the developers that developed IE only intranets, because the client wanted them. They wanted ajaxified intranets (before it was called ajax) and IE was the only browser having a decent xmlhttp component. XML/XSLT/Soap were the strongest buzzwods at the time, and you needed an xml parser, again IE had one, others didn't. IE could transform XML on the fly using XSLT, that was a big hit in many web applications. But I've been developing web sites since 1996 and I knew IE only websites, were, well... IE only. And like me, many other conscious developers knew about that. If a developer is lazy and doesn't know how to use the right tool, in the right way, who's to blame? It's like saying that PHP brought a lot of spaghetti code in web development. Yeah, PHP is a horrible language, from language design perspective, but it's developer's fault no to embrace good coding practices. EDIT: TL;DR many developers knew IE was a trap.Many managers/salesman didn't. "
"Most bioinformaticians write shell scripts hooking together fast C programs and custom Python/Perl code, and run it on a cluster. This is unlikely to change.","Bioinformatician here, former C/C++/Java programmer. I mostly work on experiment specific sequence analysis.  I've written and benchmarked multi-threaded code for my jobs, but have recently decided that the best way to get parallelism is the Unix way - ie write a single threaded program, break up the data set then run multiple copies. This is simpler and scales across clusters as well as cores.  I think bioinformatics works best as multi-paradigm: There are jobs you want really fast C programs for (BWA, Samtools etc) which are worth making efficient as they are used by many people. Then there are experiment specific jobs (plowing through 3rd party output, databases, CSV files, making graphs) where you want to optimise for programmer time and flexibility.  The last time I looked at D was when Tumiki Fighters came out, and I liked it. However, my thoughts today are that if I wanted to write code in that niche (a more dynamic C) I would use Go.  TLDR: Most bioinformaticians write shell scripts hooking together fast C programs and custom Python/Perl code, and run it on a cluster. This is unlikely to change. "
"Clarity and Minimalism, if you have these two things everything else will fall into place. 
 EDIT: Also check out  (specifically check out  if you want a laugh).","Going to be totally blunt.  I disagree with a lot of the comments so far, describing top down coding, scaffolding and ivory tower style application design.  Bringing up pen and paper as if it is magically hip.  Sounds like I have been teleported to the mid-90s  </rant>  The best programmers I have worked with (16 years in the industry) -- seem to follow a fairly narrow set of core principals  1. Clarity.  Know EXACTLY the problem you are trying to solve.  Different methodologies handle this different, I think the ""tests first"" is the most obvious.  So, write the tests you expect to pass before you write any code.  This forces an absolute and honest focus on what you are trying to accomplish.  If writing these tests seems to hard, you need to understand your problem domain better.  2. Minimalism. Do the absolute most minimal solution that does the job (if you went test first, that passes the tests), don't over-complicate, don't try to see into the future of upcoming needs, don't optimize.  Short, simple code is the easiest to write, read and reuse.  TL;DR: Clarity and Minimalism, if you have these two things everything else will fall into place.  EDIT: Also check out  (specifically check out  if you want a laugh). "
don't write something off because you don't see a benefit of it,"Scalability could mean different things from different perspectives. You are thinking of scalability from a non-engineering point of view; but that does not mean that it does not apply else where.  If you have a low demand of resources at run-time NOW() will function fine but as soon as your load increases you will be having issues with  implementation in question. With this interval class you will be successfully handling larger load without suffering from un-cached NOW() look ups.  As I noted, caching is the obvious solution but this is not possible or simply is not a requirement for certain applications.  And to drive the point further, even if you are caching your application when cache expiration hits you might have multiple NOW() look ups during the application run time and will benefit from this approach while rebuilding your cache.  You will also will benefit from this if your cache servers have issues and other cases of cache misses.  When I wrote this to optimize one of the largest web sites we saw saw faster cold cache rebuilds and stayed up longer when cache pools were saturated. So it does help and might help you even more if you have a very date intensive application.  TLDR; don't write something off because you don't see a benefit of it "
"Not sure why ""pretty easy to achieve"" follows from ""if you fuck it up, it will be fucked up."" 
 Sleep well!","It's not so much ""if you fuck it up, it will be fucked up."" It's a matter of whether there's any way to check if you fucked it up. Hence, it's not actually ""easy to achieve"" when you're talking about ensuring that (for example) something the size of a Linux distro is reliably compiled correctly when some fundamental file changes. Change one of the files that stdio.h includes - would you bet your business that every single file that might need that change will get compiled, in all of Ubuntu? Indeed, from experience, it's not  that  easy to achieve even when you  do  have a tool that checks that stuff for you.  tl;dr: Not sure why ""pretty easy to achieve"" follows from ""if you fuck it up, it will be fucked up.""  Sleep well! "
Is it expected to set SO_REUSEPORT when using multicast?,"Can anyone tell me how this interacts with multicast?  I've got an application where the program should listen to multicast UDP messages, and this program may be started multiple times on the same computer.  When a message comes it,  all  listening processes should get it.  I've noticed that on Linux, it works fine if I don't set SO_REUSEPORT, and if I understand correctly, setting SO_REUSEPORT may be the wrong thing to do -- I don't want UDP messages distributed between the processes, I want all processes to get a copy.  However, on OS X, the second execution of the program fails to find a free port unless SO_REUSEPORT is set.  tl;dr: Is it expected to set SO_REUSEPORT when using multicast? "
"Without  finally , PHP would at least require a builtin  rethrow_caught_exception_like_we_had_finally()  method.","> Regarding try/catch in particular: The reason this was introduced only in PHP 5.5 is the same why C++ still doesn't have finally: It's just not necessary  The  finally  keyword allows you to let PHP exceptions quietly bubble up unchanged. Without it, you have a bevy of problems. The exact mix depends on what's going on, but here's a menu of possibilities:   You find yourself forced to throw a completely different custom  MyLayerWrappedException  type with the real exception nestled within.  Maybe you can instantiate, wrap, and rethrow the same exception type,  but  you might not be able to get all the non-public innards to match (Maybe  $e-&gt;getFooDetails() == null  on your new object?)  The exception type you want to rethrow might not even support exception chaining. You can rethrow the same one, but now you've lost your stack trace information.  Maybe it does support chaining, but you've interrupted the stack-trace and now it fucks with some sort of trace-logging tool...   If you want to semi-reliably work-around the lack of  finally , you're forced to make a shit-ton of code that does magical monkeying with the system.  TLDR : Without  finally , PHP would at least require a builtin  rethrow_caught_exception_like_we_had_finally()  method. "
"Optimize your class-names for coders, rather than optimizing your file-names for file-managers.","> a folder on my computer  You OS' file-browser was never designed to be a PHP coding tool.  Why not use a tool  designed  for your use-case (like an IDE) and make the computer do the kind of dumb rote work  it's  good at, and save your precious human brain for the important problems?  After all, isn't that the core of programming?  >  Don't make me guess.  With PHP, you're  already  guessing one way or the other. The definitive data for whether the file contains a class, interface, or trait (or multiple things!) is always  inside . The chosen filename--which is used for many other purposes as well--isn't a replacement for that, it's a hint.  If you want to know which files follow an internal convention for a single similarly-named interface/class/trait, the computer can easily show you with nice little color--coded icons. Heck, how about an entire HTML site instantly generated and personalized for you, designed solely to help explain the API with hyperlinks and graphs and charts? It might even be online already. We can do that kind of thing here in the exciting future-world of 2015.  TLDR:  Optimize your class-names for coders, rather than optimizing your file-names for file-managers. "
"the original copyright holder is not bound by the terms of the license, only those they distribute to.","The copyright holder (novell) can license under as many licenses as they choose, all with potentially contradicting terms.  If you were to buy this product, even though mono is open source you would be bound by whatever license novell want you to use, because they are the copyright holder.  If the company producing this was not novell, they would not be able to change the license, as they do not hold the copyright for the code.  This is how Mozilla get away with distributing firefox primarily under a non-free license; they dual license with the GPL.  Mozilla does similar.  They distribute Firefox under the MPL (GPL-incompatible) and the GPL.  If they were to produce a derivitive of Firefox and release it as propriatary software, they would be perfectly within their rights.  tl;dr: the original copyright holder is not bound by the terms of the license, only those they distribute to. "
"love Erlang or hate Erlang, but don't ever ask Erlang to change. :-)","Erlang has staked a very firm position on immutability.  It's not whether Erlang can afford to allow mutable structures, it's that it philosophically won't.  It's part of the charm of the language.  It extends the middle finger to performance in numerous ways, the least of which is representing strings as linked lists of 32 bit integers.  If you (or the industry at large) want a language that cares about performance, you know where to find it.  Erlang gives precisely zero fucks about performance.  Erlang says eat your functional vegetables.  It's not some pretend wannabe language like F# which claims to be FP but if you want a while loop and program like you're programming in C# then go right on ahead.  Erlang says Java got it right, people bitched when String is immutable and why don't we pass StringBuffer all around, well, it's the wrong default.  Wrong for strings, wrong for everything.  Now is Erlang too hard-core about immutability to be a relevant language?  Could it make some targeted compromises where immutability is still the default but you can cheat in ways that don't matter designwise?  Maybe.  Thinking of alternate, more OO syntaxes for Erlang, or perhaps some other language that could run on BEAM, is one of my favorite pasttimes ...  tl;dr: love Erlang or hate Erlang, but don't ever ask Erlang to change. :-) "
PHP is easier for newbies who want to do web work only.,"PHP is sloppy, you can dump it anywhere mix it up in markup or design your own classes.  Python is tight. The white space makes it horrible for inlining. That enforces an MVC model…so some would say good from a design perspective.  The end result is a guy with a few hours on a PHP tutorial can actually start cranking out dynamic pages that work.  The same time spent on Python by a newbie is almost completely consumed by beating their head on the desk and firing off emails and chats asking why they use or edit any of these python files without getting white space errors. And then, once a kind person instructs them on how to set up their editor and explaining white space they find out they have to learn this ""MVC"" thing or ""templating"" and ""modules"" and ""imports"" and python is  practically unusable for web without a ""framework"".  TLDR;  PHP is easier for newbies who want to do web work only. "
"If I wanted to waste my time with coffeescript, i'd go read r/coffeescript.","Misleading article titles specifically worded to get upvotes is just not right. There is no way you can defend that without being a giant troll. It goes against the basic rules of reddit.  Also, articles about coffeescript really do need to be posted in r/coffeecript, not r/javascript, specifically to avoid arguments over whether a post really deserves to be in a particular subreddit. Coffeescript articles belong in r/coffeescript and if you can't grasp that simple idea, then you aren't worth talking to about this subject any longer.  I plan to keep reading r/javascript for javascript, and coffeescript raises the noise floor.. plain and simple, that is what it does if you come here to read about javascript and see any coffeescript articles.  If there is any common ground we share on reddit it is post etiquette, and posting articles about coffeescript in r/javacript goes against that. If we can't follow simple post etiquette, then there's not much point to calling it r/javascript - calling it r/postanythingnomatterhowtangentiallyrelatedtojavascript  seems more appropriate. Coffeescript is only tangentially related to javascript. I don't recall seeing any Kaffeine articles in r/javascript (maybe one or two only when it was first introduced) and that is at least as tangentially related to javascript as coffeescript is. If I were mod of r/javascript (and I am sure you feel glad that I'm not), about the only thing I would concern myself with is what gets posted here and how tangentially related to actual javascript it is.  tl;dr; If I wanted to waste my time with coffeescript, i'd go read r/coffeescript. "
They are willing to change the FAQ. Diplomacy rules. Input welcome.,"Dear angry mob/OP: Diplomacy goes along way.  I've reached out to the organizer and they are willing to change the terms. The person who wrote the terms is probably an overzealous lawyer on staff or as a third-party trying to protect their interests. Clearly, there's room for improvement.  The director of marketing is willing to change the FAQs for the event. Positive suggestions are all that were needed here. I'm recommending they make the IP claim attached only to the offer of $20k and a job offer. That should simplify things for everyone.  Sorry for my own negative tone. I found out about this mini-shitstorm at the end fo the day and the whole conflict could have been avoided with a little direct communication. I AM DISAPPOINT.  I'll update this post next week after we talk.  TL;DR They are willing to change the FAQ. Diplomacy rules. Input welcome. "
"when I put a FUSE frontend on  my  Forever Project, it will turn into  your  Forever Project.","Nice.  Path from your FE project up to mine.   Rethink your storage platform so to be able to store structured/compound data without needing schema for that.  Ask for full-scale deduplication and automatic indexation (as those features are linked) so that your resulting computer system looks like concentrate Google  Ask for orthogonal persistence (21st century means no more ""save"" to me)  Perform an abstraction inversion and consider rewriting most of the existing software stack on top of this platform   That's the path I followed. I concluded that all the system storage may be sum up by something like ""K = { R }"" and ""R = [R, R]"" meaning that knowledge are relations and relations are pairs of relations. By this way, atomic data (read raw files/blobs) turn out to be some sorts of R. In other words, there is no more the old ""edges versus nodes"" dichotomy of graph theory, as in existing File Systems or Data Bases.  So I went very far far far in this direction.  Imagine for example that you could directly put the abstract syntax tree of a piece of source code into stored structured data entities thanks to your API; or even that the machine state of the code interpreter (call stacks, environments, ...) is itself stored into your data matrix.  Then, you got  (or  )  tldr: when I put a FUSE frontend on  my  Forever Project, it will turn into  your  Forever Project. "
Isn't this also school and university systems forcing people to dislike what they used to like.,"I have always seen this as school killing the interest in a subject by ""forcing"" you to do things and give you timetables of when to have it done, so many people just end up disliking what they wanted to learn at school.  Ex. I have University friends with deep knowledge int of programming and advanced math but most of them will cringe if I bring up the subject. I do programming as a hobby++, and I'm self taught. (mostly because my questions are brushed off by these people.) Also conventional teaching focuses on what you NEED to know not always remember what you want to learn.  TL:DR; Isn't this also school and university systems forcing people to dislike what they used to like. "
i'm lazy and i get paid to do stuff that i normally would do anyway.,"because a programming job is easy to get and is pretty chill. take a look at me for example:  i make 80k a year(average is like 50-60 for most undergrads where i live ) and i had like 0 work experience and all i knew some php, java, and C # and dicked around with IOS apps throught my college career.  my job  has nothing to do with any of those things and as ""junior software dev"" , i mostly spend most of my days debugging existing code(more or less) which is literally all i did my undergrad career. to top it off i'm in a completely stress free environment where i can fuck around with my coworkers, play ping pong, hit the arcade machines in office, sleep,etc as long as i meet deadlines.  i honestly believe ANYONE can do my job if they put  a tiny little bit of effort  into it and i make more than my peers and i have little to no responsibility(debugging code is pretty chill imo).  i got a friend in NYC working in finance for JP morgan. the dude makes like 170k a year. but he spends like 6-7 days a week just straight up working and has little to no free time(hes new lol). althought my job makes much much much less i don't have to put up with nearly even half of the shit he puts up with. so i think i got the better end of the deal lol. plus he worked MUCH MUCH harder in school to get where he is than i did lol.  tldr: i'm lazy and i get paid to do stuff that i normally would do anyway. "
"People need a basic understanding of technology, but not everyone should be a programmer.","I think it's more that programming, or computing in general, or a general overview of how computer or networks function, are things everyone should at least have a basic understanding of, at least on a high level, but not necessarily are they things that everyone should know how to  do .  I really believe this whole ""programming for everyone"" push is a way to glut the industry with CS grads and drive wages down. It's not about actual need for people.  However, I think there is a  severe  lack of basic understanding of technology in the general populace. I imagined that people would generally have a better understanding of things as technology became more pervasive in our lives. But they haven't. People have no more understanding now than they did in the mid-90s. And that's terrifying.  TL;DR:  People need a basic understanding of technology, but not everyone should be a programmer. "
"is that you need to give a better working definition of ""the stuff that hasn't been invented yet, that I'm going to have to learn in the future.""","How deeply do you want to learn these frameworks, programming languages, libraries, whatever? And, again, to what end? What do you define as success? Do you want to write the web framework? If so, knowing the language you want to use better is a definite requisite. Do you want to develop the next  sendmail ? If so, a close look at how to write secure code would be useful (and I'd suggest reading the  OpenBSD .  You also need to figure out how you learn, by reading books, by copying code down into your editor, by answering questions, by listening to lectures, etc. and leverage that to make yourself a more effective student.  tl;dr is that you need to give a better working definition of ""the stuff that hasn't been invented yet, that I'm going to have to learn in the future."" "
"Just because the thumb evolved ""for"" gripping bananas doesn't mean we can't use it now to  send each other text messages","The difference is, if we're raised to believe that all the rational mind is for is to excuse and rationalise our baser urges, we end up with a nation of Sarah Palins and creationists.  If we're raised to believe that the rational mind's job is to restrain and inhibit the baser urges and instincts, we end up more with a nation of Al Gores.  Even if the ""rational"" mind did initially evolve ""to satisfy"" the limbic system's emotional urges, that doesn't mean that's all it's capable of - we all overrule out emotions and instincts  all the time , so we empirically prove all day every day that the limbic system  can  overrule our instincts and emotions.  All we have to do is  choose  to do so ([possibly related](  TL;DR: Just because the thumb evolved ""for"" gripping bananas doesn't mean we can't use it now to  send each other text messages "
"it's not as exciting as it sounds, the rewards suck and you're stuck.","Games developer for 8 years.  Now a freelance contractor.  I'll try and keep it brief but if people are interested I'll write this up somewhere in full whiny detail.  Most games developers  aren't  working on Halo.  They're working on licences, kids games, budget game and rather tired cash-ins on existing genres.  I have 5 published games.  I'd only be interested in playing one of them.  Overtime is endemic.  One company asked me to do overtime even though my area was comfortably running to schedule.  At the time I was only on a temporary contact.  I said ""no"".  There was a lot of pressure for me to work overtime.  Never a threat that I might lose my job.  In Europe, firing someone isn't that easy.  I was made to feel like I wasn't a team player though.  It's surprising how hard it it so say no when everyone else says gives in.  Next company - Overtime was common. They kept saying they'd do something about it in the next project, but never did.  I repeatedly refused to work overtime unless  I  deemed in necessary.  This didn't go down well.  My not doing overtime came up in performance reviews.  I mentioned the ""work life balance"" they were so keen on. They assured me they were looking into it.  I don't know anyone who got anything resembling a bonus.  Ever.  ""Getting out of the industry"" isn't that easy.  Not that much call for c++ programmers - especially C++ programmers who don't know UML. There's the finance industry which is just as bad.  If I applied for jobs I'd always find that the advertisement was from an agent.  The agent was never interested in putting me forward to the job I'd applied to but always had a few openings for games companies.  tl;dr it's not as exciting as it sounds, the rewards suck and you're stuck. "
"solution: Don't retry on Linux, FreeBSD, AIX; Do retry on HP-UX and Solaris. 
 Under Mac OS X, consult the  $COMMAND_MODE  for advice on which way to go.","Oddly enough, that suggestion works for HP-UX, where the manual reads:  [EINTR] An attempt to close a slow device or connection or file withpending aio requests was interrupted by a signal. The file descriptorstill points to an open device or connection or file.  However a quick look at the Linux kernel source code shows that it [destroys the file descriptor before it returns EINTR](  That means that the linked article is exactly right: Retrying  close()  in a multithreaded program  is broken  since another thread could call  accept()  or similar  at the same time  that your thread receives a signal, and therefore causes your  close()  retry to close that other fd  unexpectedly .  TL;DR solution: Don't retry on Linux, FreeBSD, AIX; Do retry on HP-UX and Solaris.  Under Mac OS X, consult the  $COMMAND_MODE  for advice on which way to go. "
close() is broken  because  it can fail. All else is auxiliary.,"We would have a better world if failed close was causing process to terminate right on the spot (I mean, better than close that can fail for reasons such as what TFA describes).  A cleanup operation that can fail, isn't. This realization is at the very base of any programming. Actually, there  should  be one, and one only, reason why close fails, and that is program error (e.g. wrong fd).  And no, network problems should not count, nor should operating system or hardware problems count. This is because code at close() level has no (or at least, conceptually, should not have) a say in lower-level problems such as these. If the problem is transient (e.g. network), then system should take care of it. If not, there's pretty much nothing the program can do, so there's no reason for it to know about the failure. Underlying system possibly can react in concordance with the operator, but code that called close()? No, that's dumb.  tl;dr: close() is broken  because  it can fail. All else is auxiliary. "
Post anyway and expect little baby bitches to rip you.  Just ignore it.,"Just post it anyway and don't be sensitive to a bully. I've coded for 14 years and ALWAYS see devs being bullies.  Just ignore the negative and focus on the positive.  HN is like any other commenting site - it's full of trolls and assholes and intelligent people all at the same time.  When advances are announced in other languages over conferences and meetups, the conversations are a ton of fun and productive.  On HN, I once tried to describe how I integrated some serializers to work in Spring.  All I got were comments about how bad Java is, even though the tech I made helped two very large scale websites that HN praises all the time.  tl;dr  - Post anyway and expect little baby bitches to rip you.  Just ignore it. "
"is that a good college is a strong compliment to self study, not an adequate substitution.","> guidance to help you differentiate the important from the mundane; in self-study, you have to explore the many different paths yourself and figure it out yourself.  Probably the best argument for college I've ever heard.  I'm self taught so I was prepared to explain how you were wrong, but your argument is quite sound.  However, while CS curriculums may help differentiate important concepts from fads, the school/college system as a whole (semesters or even quarter systems lolzwtf) encourage learning at a superficial level in order to pass exams, rather than spending the time to master particular concepts on a deeper level.  TL;DR is that a good college is a strong compliment to self study, not an adequate substitution. "
"You need to either network better or show people that you can do the job (blog, contests, volunteer).","Why aren't you doing data science work already?  Do you have a blog where you tackle data science related questions or enter contests?  A data scientist is a rockstar position, a single person who can arguably do the job of 2 or 3 people. You are a data engineer, a designer, and a data evangelist all in one. These few times I've known these sorts of jobs to open up its been word of mouth, friends of friends or other social networking that got people the interview.  If you want the top tier job you're going to have to start doing some top tier stuff, but you don't have a track record of success yet so you have to build one.  Also, you need programming skills.  tl;dr - You need to either network better or show people that you can do the job (blog, contests, volunteer). "
comparing toddlers to adults (speaking in years the project has existed) is just tarded.,"I care nothing about Emacs, but I have used Vim extensively -- and am a huge fan when I have to ssh into a server and do some stuff via terminal.  That being said, Atom is in its infancy and the Vi plugin has implemented a very small number of commands. Expect it to get better... much better with time. There isn't really an intrinsic incompatibility that makes composing commands impossible -- i.e. higher order functions are super easy to implement in javascript.  The composability argument is invalid given the fact that Sublime Vintage mode had those commands and movements that are in question. Expect this to be implemented at some point in the future. The coolest thing, to me at least, is that one can use vi commands and sublime commands in conjunction -- which, in many cases, leads to less keystrokes than would be possible in just vi.  Additionally, it is no secret that the learning curve of Vim is very high. If anything, having other text editors that allows people to get exposure to Vim-like text editing is in no way a bad thing.  tldr; comparing toddlers to adults (speaking in years the project has existed) is just tarded. "
"Take a break, find your passion, get back at it if it's still your passion.","I had this problem for a long time and I seriously had to reconsider my commitment to the profession. I took some time off and worked an ""average joe"" job at a retail place for a few months rather than focusing on a job in which I could use my skills as a developer.  The first month or so was amazing as I felt like I was in a complete different world of problems, but more importantly it wasn't problems related to being in front of a computer. I didn't really care about what I was doing, otherwise.  After a few months it became work and was no longer the sidestep for me. The important thing is that it gave me a chance to get a much needed vacation from the job itself and give me a new perspective on how things might be without the profession.  Ultimately it led me to follow my real passion and combine it with my professional skills. I've been working on that project for years now and I've never been happier.  tldr: Take a break, find your passion, get back at it if it's still your passion. "
come back when you have an actual app in Closure.,"It really pisses me off when someone says X in Y minutes and Z lines of code. It's always just thinly-veiled propaganda that fails to actually deliver. It's like saying ""Reddit in Python in 30 lines and 4 minutes"". Except it actually takes more like several months to build anything meaningful in any language.  Let me tell you: That is not Reddit. It doesn't have CSS, it doesn't even have the possibility of CSS. Not looking like shit is no minor feature. It does not have JS and don't you dare tell me ""well, that is only a small thing that is easily added"". It's huge and it can't be added without resorting to fugly hacks. And Even if it could - IT ISN'T.  Doesn't have users built-in, doesn't have user interactions . Do you even know what a social network is? Who are you trying to fool with this?  This isn't Reddit, it's a highschool homework. Take your propanganda somewhere else. Yeah, I know - ""It's only an example, it's only to show how powerful Clojure is"". This can probably be rendered by creating a Rails Scaffold and downloading some gems(i.e. 0 lines of code) but no-one is being a smartass about that, NOW IS IT, PERSON-WHO-REALLY-LIKES-MOTHERS?  edit:  I must have been really angry when writing this. Removed excessive cussing.  Tl;dr - come back when you have an actual app in Closure. "
"The author has some nice intuitions, but would benefit from more of a mathematical and philosophical background, to see that there isn't really anything new being said here.","Well...  For one thing, we have for a long time had 'languages' that precisely define how they should be 'executed' (sorry for all the scare quotes, but the terms are being used so loosely in his article and here, that I feel uncomfortable without them). One such language is math. Here is Fibonnacci in that language:  a_0 = 0  a_1 = 1  a_{n+1} = a_n + a_{n-1}  This also 'automatically expands' into the entire sequence, like the Ruby code he gives. So there is nothing new in that respect.  Second, does the Ruby really even expand into the entire infinite sequence? On any concrete machine, the answer is 'not necessarily' - the machine may have a hardware or software glitch, or it may run out of memory, etc. So we aren't talking about any concrete machine, but that 'in theory' a machine can run and expand the entire series. But if we are talking about theoretical machines, then we are right back where we were with mathematics, which said exactly that a long time ago.  Third, even so, the Ruby code doesn't fully specify how it should be executed, despite what the article says. How it will be executed depends on the Ruby language (so, on what kind of virtual machine the code will be run). You might think to say that the code + the Ruby language spec will be enough, but of course it isn't - it depends on something to run on (a compiler, a CPU, etc.). Eventually you get into the laws of nature (but that doesn't stop the problem). There is a fundamental issue here, and unsurprisingly it was remarked upon by philosophers a long time ago. For example, see what Wittgenstein says about expanding infinite series in mathematics and 'following a rule'.  tl;dr The author has some nice intuitions, but would benefit from more of a mathematical and philosophical background, to see that there isn't really anything new being said here. "
Building in selection conditions makes the sim simpler and faster!,">If you already know all of the undesirable traits then why do you need a genetic algorithm?  Ah, the thing with reality is - EVERYTHING is modelled. So a head isn't used as a method of locomotion because it's very bony to protect the brain, and would be damaged on hard rock surfaces. Also, the eyes are around there and so would be blinded... and so on...  The problem with simulators is that much of reality has to be missed out, for speed and simplicity sake.  Missing this stuff out causes ""unrealistic"" animals to form, like a worm that jumps on its head! They're not unrealistic in the sim - they make perfect sense because there's no selection pressure!  But the sims are expected to model reality, people see shapes they recognise and go ""Ooooooooo!""... (you could say this is Sim selection pressure!) so to implement more accurate selection  without  adding too much complexity and slowing the sim down too much ""Built in selection is added""  Instead of pain receptors / brain physics / Vision ability being built into the body to decrease fitness as you bang them against rock - the sim will have a rule written in... ""Banging head on ground= remove some fitness points per bang.""  This has the same effect as reality... sims that are head bangers don't do so well... and it keeps the sim speed up and complexity down.  TL;DR : Building in selection conditions makes the sim simpler and faster! "
"a non-hypothetical situation where ""hg push -f"" doesn't suck.","> Even mercurial would try to dissuade you from pushing an anonymous head; you need to do ‘hg push -f‘ to override those checks.  > Anonymous heads only create problems and solve none.  As an aside, I actually discovered a good reason for anonymous heads, even if they are a pain 99.999% of the time.  Backstory:  I was maintaining what we called a 'gate' repo. The gate repo was basically a clone of a much bigger project that a specific subset of developers had to push into so we could test that their changes were OK, before we told the main project it was safe to pull in the changes from our gate and merge.  One of the developers did a very foolish thing and dropped his new files in a completely new location in our feature gate (he didn't even look to find his own files in the repo), which would have caused conniptions upstream. I had to get him to push a new commit to fix this, and I had a few options to consider with regard to the bad commit:   I could backout his changes then get him to push the new commit. Due to the various ground rules, that would have looked a little too 'dirty' upstream.  I could break out MQ and start 'massaging' the history. Thankfully we only had 2 contributors, and very little churn commit-wise, so this was a very viable option.  Or, given his were the most recent changes, I could get him to apply his changes to a clone at the revision just before his borked one, and then tell him ""You know that push minus f thing that I told you never to use? Use it. You have my permission. THEN  NEVER  RUN IT AGAIN.""   I'd then have two heads, one with the broken commit, and one with the correct one, and then I could use MQ's ""hg strip"" to remove the bad head.  In the end, I just went with option 2 and massaging the bad commits out of history, because I didn't want to put bad habits on the developer; if he started putting extra heads on that repo, I would have had to have words with him... with a stick.  tl;dr - a non-hypothetical situation where ""hg push -f"" doesn't suck. "
"They're unhappy that their product is no longer a limited commodity, and they have to provide an actual service (continual new quality content) rather than playing commodities trader.","> Actually, it means the rights holders won't license it to Hulu at all and so there'll be nothing... which is the point. Where did you arrive at that conclusion from?  The reality that the companies are competing with pirates, who can broadcast their shows for free with relative impunity, and that the only way to actually stop that is horribly repressive DRM, which would have far worse effects on societies than letting the pirates ""win"".  They have to compete with people who will distribute their product for free in an unlimited, all-over-the-world-at-once fashion; the only winning strategy is to broadcast at the same time with the same ease of use, and rely on people wanting to contribute to see art produced.  Dropping DRM from Hulu (and the weird broadcasting delays - sometimes on the order of months) would be a major step towards getting ad revenue from Hulu for my viewership, rather than just letting the pirates distribute your program for you instead.  tl;dr: They're unhappy that their product is no longer a limited commodity, and they have to provide an actual service (continual new quality content) rather than playing commodities trader. "
The code linked at the bottom of this article incurs massive overhead and should not be used in production,"The idea of making Python functions strongly typed through decorators and annotations is  fundamentally flawed  and in the typeannotations library linked at the bottom, implemented with wanton disregard for performance. One of the central purposes of having a typed language is to allow for compile-time optimization and error-checking, but this code performs extremely expensive  runtime  type checking. Putting this into any production code on any function that gets called often can have catastrophic effects to performance. Take a look at this example that shows a  200x slowdown :  &gt;&gt;&gt; def sqr(x): return x*x&gt;&gt;&gt; @typechecked... def sqr2(x:union(int, float)): return x*x&gt;&gt;&gt; from timeit import timeit&gt;&gt;&gt; timeit(lambda:sqr(5), number=100000)0.02607336299843155&gt;&gt;&gt; timeit(lambda:sqr2(5), number=100000)4.091551588004222  In Python, function calls have considerable overhead and the decorator in this code uses a wrapper function, which means that it doubles the function call overhead. On top of that, the wrapper function calls two other functions, which each call additional functions, multiplying the problem,  even if the annotation is empty .  Now, all that being said, it is sometimes reasonable to check the types of your arguments, but if you want to do so, you should put it explicitly in the code, so you're not hiding the fact that you're performing a series of potentially costly checks. Annotations are meant to annotate code, not enforce rules.  tl;dr  - The code linked at the bottom of this article incurs massive overhead and should not be used in production "
"Bootcamps aren't baloney, they're just another very valid method of learning coding and getting a job.","I don't like the hubris exhibited by fc_s, but to say that they are ""pure marketing baloney"" is very wrong. There is an extremely high demand for devs right now and a short supply. So people are willing to hire people with less experience. 12 weeks isn't enough to become an expert, but 12 weeks at 60 hours a week means you are getting more classroom time than most people get in 2-3 college semesters. You don't need to be an expert, just in a situation to be able to learn quickly and easily on the job (from various materials, including more experienced devs).  It's not easy for everyone to learn on their own. Yes, you can definitely do it, but some people don't learn well that way, and having someone teach you in person can go a long way. It also gives you a huge network of other devs, businesses looking for devs, and shows that you actually have some training. It's really not all that different from going to college, you're just not doing it for as long. It's more like going to a technical school that is crammed into a 12 week period and you're learning the essentials.  I went to a coding bootcamp and have a job and my job is very happy with my performance. Most of the people I graduated with are in jobs that pay well and they're very happy, too. We're not experts, but we are confident that we can contribute and learn and become more and more valuable employees.  It would have been much, much harder for me to get to this position in such a short time had I not dropped 12k on the bootcamp I went to. Yes, that's a lot of money, but it was an incredible experience and got me a job that I love and pays plenty well that the money I invested was well worth it.  tl;dr: Bootcamps aren't baloney, they're just another very valid method of learning coding and getting a job. "
The start of any journey is realizing there is somewhere to go.,"I feel like I just found out I didn't have to wear all that clothing.  All that verbosity, convention, and stuffiness was just holding me back. I learned that there are ways to be much freer than I ever though possible.  Functional programming  is  like throwing off the clothes and realizing it was only holding me back.  Going from these languages of my infatuation back to the old ways is like getting a minor part on Broadway, being surrounded by amazing talent, and beginning to really test the limits of your abilities, but then still having a day job at Medieval Times as the court jester.  Sure, you can become a great jester, but sometimes there is only so much you can do with the space you are in.  I freely admit I am no master.  It is the possibility that excites me.  TL;DR:  The start of any journey is realizing there is somewhere to go. "
"Git, on Windows, turned me from a CLI user into a GUI user!","Darcs still has a  far  nicer command-line UI, but..,  I was already using darcs and HG for various projects and was happy enough with them, but I started using Git solely to try out GitHub. Then I noticed that  git gui  doesn't make you want to claw your eyes out on Windows due to the lack of Motif theme. Then I noticed that you can use  git gui  to cherry-pick lines for a commit by highlighting the additions and removals, which is much nicer than looking at things one hunk at a time and opening an editor to split hunks. While I'm there, I might as well enter the commit message and click the Commit button. While I'm there, I might as well click the Push button to push the changes.  TL;DR: Git, on Windows, turned me from a CLI user into a GUI user! "
Step into the end user's shoes for a second. This is nice for web-designers but horrible for the user experience.,"I really dislike the design of this and frankly feel it will be abused and disabled almost immediately (see popup windows for example).  You will have adverts which lock the mouse into the page area (i.e. not allowing users to click the cross), and similar abuses.  Yes, they offer the escape key as a default exit strategy, but that only works when:   Users know they're being mouse-locked   A web-page advertises how to escape       Many users are still very ignorant of how the internet works. I can imagine a scenario where I get telephone calls because of this and that makes me unhappy. It is not an obvious feature or user friendly, and it assumes pre-knowledge which 90%+ of users won't have.  To be frank I think flash got this right. A click-able area of the page that can only lock the mouse when it has focus. So the following methods exit mouse lock:   Escape   Windows Key  Alt-Tab   CTRL-ALT-Delete    It also means that a background tab cannot gain mouse-lock and a user has to somewhat actively click it in order to get captured.  I wouldn't mind seeing a popup box with a ""Do not show this again, for this site"" checkbox which asks you if you want to give the site exclusive control of your mouse too, but I'm not sure the spec' can control how browsers implement their features.  TL;DR: Step into the end user's shoes for a second. This is nice for web-designers but horrible for the user experience. "
Treat female coworkers like you would treat a male worker in that same spot.,"A bit harsh spoken, but there's truth in it.  May it be for statistics, or decoration purposes - one does get the feeling that equality does not match with '=='.She may have her week, or ""aww c'mon - you know,- she's a woman"" or some bloke fancies her. It drives me mental that everyone speaks for equality, but just fucks it up. Equal means same amount of work, and the same result in the end. Equal also means the amount of money for both, I still don't get why women earn less if they do the same workload, or work even more as they have to show the male employees that they're equal.  tl;dr Treat female coworkers like you would treat a male worker in that same spot. "
"unless you intend to write a graphic engine, learning these is akin to learning assembly for a text editor.","It's much simpler to extend a completed engine to do what you need than go ahead and build one for that rare uncovered feature. Also you must not be aware of what writing a graphic engine entails (hint: it involves writing other libraries that your engine will use, like math libraries and alike)  Opensource engines won't limit you in any way, in fact you can learn your OpenGL  or DirectX in there because the engines themselves make calls to these APIs (and you can derive that stuff to create your snowflake features).  This being said, OpenGl or DirectX is no useless knowledge, but the same applies to say assembly, which your logic tends towards (do things on your own without depending)...  TL;DR: unless you intend to write a graphic engine, learning these is akin to learning assembly for a text editor. "
don't make decisions based on this benchmark because they used a very suboptimal algorithm in the RDBMS.,"This benchmark is very fishy. Look at the results from the RDBMS. From depth 2 to depth 3 the number of records returned is 44x larger, but the time spent is almost 2000x larger. Similarly, from depth 3 to 4 the number of records returned is 6x larger but the time spent is 51x larger. If you plot those points you can clearly see that this is not an O(n) algorithm. In fact I made a log-log plot and found the slope to be almost exactly 2, indicating that the algorithm is O(n^(2)). Clearly an RDBMS can do an (almost) O(n) algorithm for this problem.  Edit:  here is the graph , whereas the RDBMS is almost perfectly quadratic (the linear term is small compared to quadratic term).  tl;dr; don't make decisions based on this benchmark because they used a very suboptimal algorithm in the RDBMS. "
"stop scaremongering, javascript is  not exactly the biggest hole in a browser.","This just seems to be scaremongering, javascript will still be able to be disabled, addons like noscript and flashblock will still work, and in my opinion firefox is one of the most secure browsers in daily use, if people wish to release an exploit there are a lot of simpler surfaces to attack than javascript, such as html5 (which is still quite open to attacks as it is still technically in development) or pdf's or in zips/word documents. If a malware developer was committed enough to want to create a exploit, the best plan for them would be to look at the source code, both chrome (in chromium) and firefox are open source, and a lot of issues are visible if someone had the time and knowhow to look for them. Whether javascript has a disable option in the main settings menu or not is of little consequence.  TLDR: stop scaremongering, javascript is  not exactly the biggest hole in a browser. "
"I don't think ""hand hacking"" is a lost art. I don't know if it's an art or a science or engineering but it is very much alive and thriving.","Disclaimer: high-level coder here.  > This kind of hand-hacking is a lost art among most programmers  I have no evidence or experience but to a laymen like me it seems not so. Perhaps mainstream private sector commercial purposes have lost much of the need for low-level stuff but certainly the ""couple of places"" you mentioned are huge, gorrillion dollar industries.  Hardware manufacturers (embedded world) in addition to handheld phone devices are also desktop peripherals, specialized instruments, household appliances. Conglomerates like Google, Apple, MS and IBM are all about JITs and operating system kernels. You wouldn't know it by their marketing but I think they are far more involved in ""opcode overflow bit"" problems than their marketing departments would ever care to admit (or understand). Then there is advanced users' offerings like Java. For all its hype about its offering to high-levels, looking at it purely as a piece of software engineering, Java is all about mapping that bare metal stuff into high-level abstractions but the JVM is not written in Java.  In addition (and I assume a lot but from my limited understanding) it seems a lot of security exploits leverage off of code injection and arbitrary code execution which, in turn, sounds to me like a skill-set which relies on mastery of ""hand hacking"". As far as I can tell (I stand to be corrected on any and all of this) security exploits are the bread and butter of government intelligence in certainly Russia, China and US and prolly Israel, UK and Europe, too, as well as crime syndicates around the globe.  I think the high-level industry is huge too but I don't think it dwarfs low-level demand in comparison. Perhaps in the amount of code written because multiplying code monkeys like me by lines of code we write per day would certainly yield more code characters but I don't think that is a true reflection of how widespread low level is.  tl;dr; I don't think ""hand hacking"" is a lost art. I don't know if it's an art or a science or engineering but it is very much alive and thriving. "
"Whoever you are, each bug is a new opportunity to get better.","One of the junior guys pulled me up after I goofed and misunderstood a critical design requirement.  It was pretty embarrassing, but I also realised this was a ""teachable moment""^TM.  Without skipping a beat, I said loudly so the whole team could hear:  ""I  like  being wrong.  That's how I know I'm learning!  If I'm not making mistakes, I'm not pushing myself hard enough.""  We then sat down together at the same computer and the new guy schooled the domain expert while the whole team listened in with stunned silence.  It was pretty humbling, but totally worth it to model the behavior for everyone.  TL;DR: Whoever you are, each bug is a new opportunity to get better. "
Buying a development laptop is like buying a pair of shoes.  Fit matters way more than specs.,"Honestly, development doesn't require as much in the way of hardware as most people think it does.  Get a laptop with a nice keyboard, lots of RAM and a decent CPU.  Unless you're programming games or CUDA, you won't need a killer graphics card, which is where most of the expense is when dealing with laptops.  The main concerns are durability and ergonomics.  Even if you're buying online, go to a store and check out similar laptops to the one you're buying.  Make sure you get to try the keyboard and the trackpad.  Pick up the laptop and see how well made the case looks.  Those two qualitative properties of the laptop will affect your development far more than any quantitative property, so you need to make sure that everything's okay where fit and durability are concerned.  TL;DR: Buying a development laptop is like buying a pair of shoes.  Fit matters way more than specs. "
OP is making assumptions based on a small sample.,"I don't buy it. The ""type S"" mind purported by Simon Baron-Cohen is what enables people to excel as programmers, and plenty of women have it while many men don't. It's more common in men and people with Asperger's (which is an extreme of the type S mind) but it's well distributed.  I also believe your association of homosexuality with computing greatness is a result of small sample size. Although high-IQ people are more likely, for reasons that may be entirely cultural, to be androgynous (high-IQ men more feminine, high-IQ women more masculine) and this probably interacts with homosexuality, you don't need to be male, homosexual, or autistic to be a good programmer.  TL;DR: OP is making assumptions based on a small sample. "
"We build a list of e-mails ahead of time for a campaign, but 4-6 weeks is ridiculous lead time.","I have the likely answer to this.  Many companies will use a e-mail campaign software.  This will generate future marketing materials, and use the subscriber database to build it's list of receivers.  Many times it will pre-determine a list of people to send to AHEAD of time.  4-6 weeks is quite ridiculous, but I could definitely see it being delayed by a few days.  Here's what happens, It's Monday, and I, ""marketing guy"" will create a campaign to launch on Friday.  I enter in my criteria for the types of users I want to hit, and it returns me a list of people that will receive the spam.  I have a fixed list of receivers now.  I then design my campaign, and save it.  It will launch on Friday to those specific subset of users.  If you unsubscribe on Wednesday, it won't matter because you are already queued up to get the spam.  Next campaign, you won't be included.  tl; dr;  We build a list of e-mails ahead of time for a campaign, but 4-6 weeks is ridiculous lead time. "
"Unless you are writing perfect code that will cleanly handle every possible failure, pconnect will bite you in the ass.","The same downsides as exist whenever you use any  pconnect : the server is going to be unaware when your client disconnects, and therefore can't do any cleanup after errors that your program does not handle with elegance.  In other words, if you break something and your script dies without, say, rolling back a transaction, that transaction is going to be held open attached to that connection.  The next script instance that picks up that connection is going to find itself in the middle of someone else's aborted transaction and chaos will ensue.  Worse, holding open a transaction can have other side effects, including but not limited to deadlocks and performance penalties.  tl;dr : Unless you are writing perfect code that will cleanly handle every possible failure, pconnect will bite you in the ass. "
"I'm not trying to be lazy, but can you give a ""young"" programmer ideas on stuff to go after and take apart? 
 Thanks in advance.","Can you give me some examples of anything in particular I might try taking a look at? I'm taking my second CS class (just now cracking open the OOP parts of C++ [I am also teaching myself web and graphic design on the side as it interests me and I figure it may some day be useful]) so I'm fairly new at all this. The difference between cracking open every mechanical and electronic piece of junk I can get my hands on is that at in the garage I only have access to dumbed down, mass produced, cheaply built junk; i.e easy to understand when you look at it for a minute. Where as on the net I have access to coding equivalent of CERN's hadron collider. I'm not saying that I can't take apart bits and pieces of the hadron collider, but I'm going to learn a hell of a lot less because I have no fucking clue what most of the thing does.  TL;DR  I'm not trying to be lazy, but can you give a ""young"" programmer ideas on stuff to go after and take apart?  Thanks in advance. "
"nice ideas, but no java ee. JSF 2 still better.","It does have some nice ideas, but one thing I didn't like was that the bigger a Play app, the slower it starts. Now this is true for other frameworks as well (JPA, Spring, JSF, etc), but Play gets really slow...  And doesn't only gets slow, it uses lots of memory too when the app gets bigger.  My personal favorite is JSF 2. It has a brilliant templating language (Facelets) and is really easy to use. JSF integrates perfectly with Java EE, so all familiar knowledge can be used. With the exception of JPA, Play tries to reinvent the wheel for everything and makes us learn new Apis for no good reason.  It also means they have to spend time on maintaining all those things they must implement themselves, which takes away time from focussing on what could make Play unique.  tl;dr nice ideas, but no java ee. JSF 2 still better. "
you can trade off time against space in bio-inspired systems to solve complex problems in linear time.,"There is a similar field of theoretical computing science called P Systems. Many of the results prove that such systems are Turing complete, and able to solve exponential problems in a linear amount of time. This is done by using the cell's ability to divide into two equal cells. Essentially using an exponential amount of cells to solve exponential problems in linear time.  In the k-SAT problem, starting with one cell compartment, this compartment will divide, then each of those will divide, until you have 2^k compartments, taking k time steps. Each of these compartments represents a possible solution to the k-SAT. If a solution satisfies the problem, then the system will send out a success token. Thus the time for  solving is O(k) and not O(2^k ) as it would be on a Turing machine.  There is a lot of theoretical computer science out there. I think P systems were shown to be at least as powerful as Probabilistic Turing machines, but this really isn't my field.  That went off on a tangent  TL;DR -  you can trade off time against space in bio-inspired systems to solve complex problems in linear time. "
"You can hard code it, but obviously you should only do that for testing. You can setup a secure handshake, though, with a solution that resembles OAuth.","> There are a couple of ways to load credentials. Here they are, in order of recommendation:> > 1. Using web identity federation to authenticate users> 1. Hard-coded in your application> > We recommend you not hard-code your AWS credentials in your application; however, it is reasonable to temporarily hard-code credential information in small personal scripts or for testing purposes. It is also sometimes necessary to hard-code read-only credentials in your application.>  >  Using Web Identity Federation to Authenticate Users > > The recommended way to authorize users of your application to access AWS resources is to set up federated login through a trusted third-party identity provider. This feature is known as Web Identity Federation. Amazon Web Services currently supports authenticating users using web identity federation through 3 identity providers:> > 1. Login with Amazon> 1. Facebook> 1. Google> > After you select an identity provider, you must register an application with the provider, create an IAM role, and setup permissions for this role. The IAM role you create will be used to grant the permissions you configured to the users that login through the respective identity provider. For example, you can setup a role that allows users who login through Facebook to get read access to a specific S3 bucket that you control.  >  tl;dr: You can hard code it, but obviously you should only do that for testing. You can setup a secure handshake, though, with a solution that resembles OAuth. "
This isn't a problem. It's JS and JS sometimes a lot different than other languages (specifically with inheritance and object creation).,"To me it seems like he is trying to apply Classical Inheritance to JS's prototypal inheritance and making problems out of nothing.  For example,> The trouble with instanceof is that it is unreliable whenever you use Object.create instead of the new keyword  From my understanding of Object.create, this is ""unreliable"" since it works entirely differently than the new keyword.  Object.create() returns the prototype of the passed Object.  The programmer must then set the constructor and the parent. Once this is done, it will work just like ""new"".  As for when to use ""new"", I agree that it's a bit confusing at times, but it also doesn't seem like a huge problem to me. Use ""new"" when you are creating an instance of an Object, don't use it when you are simply calling it as a function.  tl;dr This isn't a problem. It's JS and JS sometimes a lot different than other languages (specifically with inheritance and object creation). "
"smart folks thought they were training a computer to recognize tanks, but the accidentally trained it to recognize cloudy skies.","Because it's not a big set of rules created by people sitting down and writing out that ""red"" means ""pixels (R, G, B) where R > 150 and...."" It's a learning system that can take a large set of example image/description pairs, then when given a new image, tries to produce a matching description based on what it's seen before. Nobody ever went in and said ""this is what 'red' means."" It had to deduce it from the training set, and presumably its deductions aren't 100% accurate here.  Think about how we teach toddlers colors. We give them big obvious fields of pure colors and say ""this is red, this is green, etc."" and even then it takes them a while. Now imagine that nobody ever sat down and did this to you, and you just had to pick it up from context. You could easily misunderstand exactly what ""red"" refers to.  For a concrete example of this sort of thing happening in computer vision, see:  TL;DR: smart folks thought they were training a computer to recognize tanks, but the accidentally trained it to recognize cloudy skies. "
"It's a work in progress, that's why. Patience, grasshopper.","The W3C version  is  the ""common agreement."" The Mozilla and Webkit versions are their prototypes; they are prefixed, like   mozRequestFullScreen  and  webkitRequestFullScreen , to indicate that they are prototypes, and that they may differ between browsers. Once people have gotten to test the prototypes, and early adopters have gotten experience with their pitfalls, the W3C can figure out what is working and what is not, and propose the standard. The standard may have minor stylistic tweaks like this; that's OK, the old, prototype interfaces will still continue to work even after the standard is released and implemented (the advantage of namespacing is that you won't have conflicts with the prototype implementations).  So, the W3C version is what the interface will eventually be. But that won't be implemented in browsers until the spec is more mature; they want to work out most of the kinks, and make sure that it mostly works, before releasing browsers that support the non-namespaced APIs, so that they won't have backwards-compatibility issues that will prevent them from fixing problems in the spec.  tl;dr: It's a work in progress, that's why. Patience, grasshopper. "
your program can drive gnuplot via the command-line interface without becoming infected by the GPL,"IANAL, but here's my understanding, which I'm pretty sure is correct.  Since gnuplot is a command-line program, and you're simply driving the gnuplot binary via the command line (via PHP's system() or exec() or backticks), you're in the clear.  Since you're not modifying or linking to gnuplot in your program, you can simply distribute gnuplot (under the GPL) along with your product (under whatever license you want).  The only obligation you'll need to meet is that you must make the gnuplot source available to anyone you distribute binaries to.  That said, if your product is aimed at modern unix/linux only, you can probably get away with just instructing users to install gnuplot as a prerequisite to run your software.  On CentOS, they just need to 'yum install gnuplot', for example.  TL;DR: your program can drive gnuplot via the command-line interface without becoming infected by the GPL "
"a) Software developers actually do what you tell them to do. (b) The non-focus aspects suffer, sometimes greatly.","Yeah Right.  Plumbing should be about building, creating, and innovating.  Obstructionism would be bad, though, I agree.  I didn't downvote you, because your enthusiasm is admirable, but I have to tell you: you are wrong.  A large share of developer positions is in maintenance. Most software is just getting the gears together in time for very mundane ends. If you compare software development to other engineering, our track record is abysmal. We do more voodoo than science. Planning is a joke. Software is always one of the alrger uncertainty factor in projects. Companies shy away from software projects not because they are not affordable but because they are not plannable.  Quality virtually always skims along the ""just bearable"" line - and often enough, on the wrong side. We rewri9te the universe every week because ""that other guy's code is so much crap"".  It's a young technology, but that excuse is wearing thin.   As long as you deliver in time, at cost and at quality, do whatever you want. But not everyone is a rockstar, and most teams jsut don't.  Code Complete  has - among the heaps of true hard data - the results of a little test: Tell a handfull of groups of programmers to solve the same problem - but with different optimization goals. Shortest development time, fastest code, least bugs, least source code etc. Results tl;dr: (a) Software developers actually do what you tell them to do. (b) The non-focus aspects suffer, sometimes greatly. "
A stock NES isn't actually capable of a lot of the games you see. Something like 30-50% of all games use the MMC1 or MMC3 mapper chips!,"The article mentions that the total space addressable by the NES's CPU is 64KB, and only 32K of  that  points to the ROM. This means that a cart going above 32KB storage needs a method of actually  accessing  it. The primary function of a ""mapper"" is  memory mapping  - pointing the address space to different chunks of ROM so that they can be accessed. These divisions of memory are called banks, and switching between them is called bank switching, as another comment mentioned. (The Atari 2600 was even more limited and also used 'mappers')  But, additional features were also often included in these mappers. What features exactly varies between each one. Super Mario Bros 3, for example, is able to have larger levels (and scrolling backwards!) unlike SMB1 because the cart has on-board RAM.  tl;dr: A stock NES isn't actually capable of a lot of the games you see. Something like 30-50% of all games use the MMC1 or MMC3 mapper chips! "
A little planning can save a lot of money.,"As a counter-example: I did QA on an agile project where 2 week sprints were being used.  It was a small project, perhaps 6 person years of developer time.  The design process involved some vague requirements and some skeleton drawings of the user interface.  A few months into the project, I sat down and worked out exactly what data needed to be kept, how it related to each other, and how it related to the interface.  A half day of work and I had enough information to prove that the the approach would fail due to a fundamental inconsistency.  They pretty much ignored the proof, instead thinking they could agile their way out of it.  6 months later, it became apparent that the system was not working as intended and they couldn't figure out why exactly.  Some examination showed that there was (you guessed it) a fundamental inconsistency.  Of course, resolving it now would require discarding most of the work that had been done.  So, it being government, they announced completion with little fanfare, swept it under the rug, and got started on the next version.  tl;dr: A little planning can save a lot of money. "
A snowmobile is better than a motorcycle at going over water. But don't bother testing the boat),"He missed the (three) advantages of linked lists:  One: Although iteration is slower, deletion of an element  you already have the pointer to  is (much) faster. Well, assuming you either have the pointer to the previous element ( true if you're doing an iteration through and occasionally deleting things ), or it's a doubly linked list.  Two: A linked list has better worst case performance. Sure, an array with exponential growth has O(1)  amortized  cost, but the actual worst case is O(n) (note: not always. Some remalloc-based implementations aren't. But again: test, don't just say ""this is always better""). In particular, if you're doing an operation in a game loop a random 50ms pause once in a while is a  very  bad thing.  Three: You can append a batch of any number of elements to the list by changing one pointer. Useful in some cases.  He also picked something that is pretty much the worst case for a linked list, and pretty much the best case for an array (lots of really small elements). But even then neither of those are good data structures for what he's doing! (TL;DR: A snowmobile is better than a motorcycle at going over water. But don't bother testing the boat) "
"with a few exceptions, almost everything is fast enough for most needs","put things in perspective. the json serialization shows various perl solutions delivering anywhere from 9k to over 20k (trivial) json serializations per second...which is realistically far in excess of what any user would require. indeed, resource use is likely far more of a concern in the case of perl than performance  is anyone really considering doing website programming in C just because the first result achieves 2.2 mln json serializations? i can't imagine tossing a good tool based on results from a list like this. use common sense!  i would personally only discard-out-of-hand the real laggards, those than can't manage to get to around 2k/sec...but even then, most real sites distribute load such that its rare to see more than 1k simultaneously conns per web server anyway...after that, it seems typical that other bottlenecks arise (DB, etc). many moderately-high-traffic sites get by fine with stuff like rails.  tl;dr: with a few exceptions, almost everything is fast enough for most needs "
"I still prefer Windows, don't say that Windows owns me. It's my own choice.)","ArchLinux - interesting. I'll give it a try.  I'm usually an end user. I'm not interested in compiling the source code of a kernel. If I needed to do so, I would have opted to an open source operating system. To put it quite bluntly, I don't give 8 fucks of a shit about the underlying kernel as long as what I use works well for me. But really, I don't see why I should have to recompile a kernel just to fix a problem. My best interest in being a consumer is  not having to manually tweak the product for every little problem .  I don't think it's really fair to say that Windows owns me. I prefer Windows because it just seems easier to do ostensibly simple tasks, whereas Unix seems to still rely on something that I personally view as archaic and not-user-friendly.  (TL;DR: I still prefer Windows, don't say that Windows owns me. It's my own choice.) "
"Reporting tools have a purpose, and people always generalize their tools","I don't know if your point about reporting tools being worthless is correct. Sure if you are writing in house software it makes (some) sense, but what if you are not?  And besides, isn't writing a reporting toolkit for every application a bit of reinventing the wheel? Sure there are many problems that aren't appropriate for reporting, such as how many X satisfying Y and Z exist right now. However there are just as many problems that reporting tools are perfect for, such as what are our peek times and how are we handling them?  I think the core problem you are showing is that SQL is often viewed as a hammer and every project is a nail. That is a more widespread problem and honestly has very little to do with this debate.  tl;dr - Reporting tools have a purpose, and people always generalize their tools "
PHP is a slow monstrosity of a language that is easily surpassed by faster and easier languages like Python or Ruby.,"The creator of PHP has openly admitted many times that the language was just hacked together and was never meant to be anything more than some glue for small websites, like Perl was at the time he wrote it. He only added features because people asked for them, not because he cared about making the language better or easier to use.  As such you have a mess of an API, with lots of lacking documentation for some detailed portions of the language, and without any formal specs. The language is awful to use for anything beyond simple touch-ups. The reason its so popular is it has a very small learning curve and is present on nearly any web host you can find.  TL;DR: PHP is a slow monstrosity of a language that is easily surpassed by faster and easier languages like Python or Ruby. "
The Church-Turing thesis being a thesis instead of a proof is exactly what I'm talking about.,"I think the intro on the wikipedia page for that summarizes it elegantly.  In other words, there's no good evidence that there will  never  be a mechanism that can calculate something a TM (or the other formalisms) cannot calculate. Quantum computing might have been such a thing (but turned out not to be, as I understand it). I'm not sure there's any evidence that there is nothing a human brain can compute that a TM cannot, for example.  (Not that I think there is, but I don't think there's any evidence either, except that we haven't found it yet. I.e., no good arguments.)  It's also pretty easy to prove there are things that can be calculated by a Turing machine with an initialized infinite tape that can't be calculated by a Turing machine with only an unbounded tape. So it's not at all obvious that everything calculable is calculable by a standard TM.  (And obviously there are things calculable by a standard TM that cannot be calculated by any actual machine, such as something that requires 10^80000 distinct states to calculate.)  TL;DR - The Church-Turing thesis being a thesis instead of a proof is exactly what I'm talking about. "
Watch out when you let off steam - it may come back to burn you.,"Years ago when I was working at Tymshare and maintaining their PDP-10 linker, my manager asked me to fix a little annoyance: The linker was leaving garbage behind in the unused areas of memory between the areas it actually used. He wanted that memory zeroed out so the linked images would be cleaner. A reasonable request.  But not an easy one. The linker had lots of places where it fiddled with memory, and it was a mess of spaghetti assembly code.  It was not a fun project at all, and by the time I finished, I was a little annoyed with the whole thing.  The linker printed a message at the end of its run that included a memory summary something like this:  14271 words used, 403 free  So I changed it to:  14271 words used, 403 free, and it's pretty fucking clean  Now, my manager knew it was an unpleasant project and he appreciated my efforts on it. So I knew he would get the joke, and then I'd take the message out after he saw it.  Unfortunately, before my manager saw it,  his  manager gave a demo to a customer that included a link step.  The customer did not get the joke.  I didn't get fired or anything, but let's just say it led to a few unpleasant moments for all.  tl;dr Watch out when you let off steam - it may come back to burn you. "
Michael Arrington would sell your mother into slavery for bus change.,"It's an old expression for ""THEY GONNA STEAL YOUR SHIT"".  Pretty sure it was ""check your purse, then check your fingers"" given that back then it wasn't strange for a dude to carry a purse, because wallets weren't invented yet or something.  Anyway, the premise was that they'd rob you blind - if they hadn't taken the contents of your wallet, don't assume that they didn't steal things, just assume they stole something more valuable.  The meaning there is twofold - in many centuries, wealthy people wore extravagant rings to show off their wealth.  Losing one of those oftentimes would be more expensive than losing one's wallet, but they're attached to your fingers, so harder to steal.  In this context, the implication is that if they're not stealing the easy stuff, they're going for more expensive, more challenging stuff.  The second implied meaning is that they may in fact just steal your fingers.  Here, of course, is a dig at whoever this aphorism is aimed at - that they are  so  disreputable that they would, in fact, steal your fingers if they thought they could get away with it.  And that they're so skilled at stealing things that you'd actually not notice they'd stolen your fingers until you counted and came up base 9.  TL;DR: Michael Arrington would sell your mother into slavery for bus change. "
"I'm looking forward to it but feel there are much more important things which still suck. 
 EDIT: Incorrect chaining example.","I've only just now looked at it, but it just seems like another quirky style of inheritance.  Now we've got abstract and concrete class inheritance, abstract and concrete interface inheritance, and traits. Also, we normally extend classes and interfaces, but for ""extending"" a trait, we say  use SuperTrait;  within the SubTrait. We can also have abstract trait methods, which AFAICT is exactly the same thing as an interface except that it can be bundled with concrete methods in the same trait.  I can see where it might be useful, but the number of ways you can do inheritance is getting complex as fuck.  The RFC also discusses Grafts,  All the while, I think there are much bigger problems. The standard library is still a mess. I get the feeling that everyone is too focused on the shiny features to do the dirty cleanup work, but it's IMO PHP's worst problem at this point. For example, the short array syntax. It's very nice, but does nothing but save you from typing ""array"" every time. Another real problem, this time in the core, is the way some types of chaining work: I can't say things like  $hash_functions['sha1']($plaintext)   get_array('foo')['bar'] . Or how all functions land in the global symbol table, although this is much less important now that we have real anonymous functions.  TL;DR:  I'm looking forward to it but feel there are much more important things which still suck.  EDIT: Incorrect chaining example. "
All of these people said they didn't use a CMS and were doing just fine.  But it just hadn't caught up to them yet.,"I'm not accusing you of anything here, and believe me I know that in the end, it's the results that count.  But you're not the first person who's said that to me.  Some of those people have been throwing up bad arguments for flat file sites because they don't want to learn PHP.  I recently turned down some work for an agency because they wanted me to convert a webshop to paypal shopping cart over a weekend.  The kicker was that it was 1200 flat files.  Another client had fifty  sites  on a vague homemade framework, then realized all of them were feeding a search box into an sql query with no filtering and had to change every one, EVERY PAGE, by hand.  tl;dr:  All of these people said they didn't use a CMS and were doing just fine.  But it just hadn't caught up to them yet. "
"Monads really aren't very compilcated, and they are certainly not space suits filled with pink fluffy thing stuffed burritos. 
 edit: >>= -> return, as pointed out by Headspin3d","Basically, it's a nice combinator library, derived from maths.  For example, suppose you had the list [1..5], but for some reason you want the list [(1,1),(1,2), (1,3),(1,4),(1,5),(2,1),(2,2),...(5,5)].  Say, you're going to filter out a bunch them, but you need to generate them all first.  In imperative languages, you'd probably make a loop and add them all to the list.  In Haskell, you start out with small building blocks, and use combinators to glom them together.  A monad is anything that defines >>=, pronounced bind, and return, which, despite the name, is just a poorly named function.  For the List instance of monad, the types are:  &gt;&gt;= :: [a] -&gt; (a -&gt; [b]) -&gt; [b]return :: a -&gt; [a]  -- which just creates a singleton list  That is to say, >>= takes a list, then a function that takes a list element and returns a new list.  It applies that function to every element of the list and concatenates all of the resulting lists.  For example:  [1..3] &gt;&gt;= \ x -&gt; [x,x]   -- ""\ x -&gt;"" introduces an anonymous function of one variable, x  evaluates to  [1,1,2,2,3,3]  So we can solve the original problem by just saying   [1..5] &gt;&gt;= \x -&gt; [1..5] &gt;&gt;= \y -&gt; [(x,y)]  Which there is some syntactic sugar for:  do x &lt;- [1..5] y &lt;- [1..5] [(x,y)]  It turns out that this library isn't useful just for iterating through data structures, but also sequencing effects.  If we tag anything that does IO with the IO type:   putChar :: Char -&gt; IO () -- (), pronounced unit, is like void in C getChar :: IO Char  and define a monad instance, so we have  &gt;&gt;= :: IO a -&gt; (a -&gt; IO b) -&gt; IO breturn :: a -&gt; IO a -- wrap a pure value in the IO wrapper  then we can say things like  getChar &gt;&gt;= putChar :: IO ()  which echos as Char back to the screen when run, or  echoNewline = getChar &gt;&gt;= \x -&gt; if (x == '\n') then echoNewline else putChar x  which reads in Chars until it hits a newline, which it then prints back out.  tldr:  Monads really aren't very compilcated, and they are certainly not space suits filled with pink fluffy thing stuffed burritos.  edit: >>= -> return, as pointed out by Headspin3d "
"You gain nothing from reading this. No new information is introduced in this article, which seems to be a trend.","I have to agree with you on this point. I was reading the article and I could not just help but think ""He is just blurting out the key talking points that everyone else is talking about"". The article does not introduce anything new, it does not go in depth on any topic and is just a rehash of the current ""hot"" topics.  But hey, I can't blame him, that is the best way to gather views and ""upvotes"". He does not even state an opinion, just that php is getting back in the race (who does not know this already?) and that you should make your own choices/research.  TL;DR;You gain nothing from reading this. No new information is introduced in this article, which seems to be a trend. "
Discrete optimization and real-valued optimization are two very different things so it's better to solve them separately than together.,"I haven't read the code fully but here's my understanding. Hopefully the author can correct me.  They break up the search into two layers because the optimization target, essentially a formula, is a combination of both operators drawn from a finite, discrete set (e.g. bit shifts, add, multiply, etc.)  and  constants which are drawn from real values. Just think about what you would have to do if I asked you to manually find the best formula and constants for that formula to maximize or minimize something.  This heterogeneity presents some issues for standard evolutionary algorithms because those methods tend to work better for discrete optimization whereas other methods such as convex optimization are better for real valued optimization. However, the problem is intuitively decomposable into separate optimization problems because the intuition is that the combination of operations has a much more significant effect than modifying constants. Now solving this decomposed problem is not the same as the coupled problem but the suspicion is that it's close enough.  The first layer is the ""combinatoric layer"" where they use an evolutionary algorithm to search for combinations of operators. These combinations are discrete and are thus suitable to an evolutionary algorithm.  The second layer is the ""continuous layer"" where they use standard real-valued optimization techniques (as encapsulated by SciPy's optimization library) to determine ideal constants to maximize the fitness (minimize the error) of a particular operation combination (chosen by the first layer).  TL;DR: Discrete optimization and real-valued optimization are two very different things so it's better to solve them separately than together. "
don't worry about math.  Keep banging programming and your love for some of the tangential math will come naturally.,"OP, I'm going to help you.  I have a CS degree from a top 25 CS school.  I failed Calculus 4 times, and took it (the 1st calculus course out of 3 required, mind you) 6 times before i finally got through it.  That was the first math course out of 9 total that I had to take.  I still hate math, but I still love programming.  And i'm pretty damn good at it.  If you goal is to work at Google, you're going to have to learn to love the algorithmic side of programming, which often directly, but usually tangentially ties to a lot of math.  You'll constantly lean on linear algebra, combinatorics, and some calculus, but in reality those Google monkeys just want to see how many obscure bullshit formulas and o-notation you can regurgitate, so you don't REALLY have to learn the math side hardcore unless they hit you with something you wouldn't have anticipated.  TL;DR: don't worry about math.  Keep banging programming and your love for some of the tangential math will come naturally. "
writing good code in PHP is more about knowing what parts of the language + lib  not  to use.,"Not just possible, but almost inevitable if you try and just learn the entire language + default libs. If someone were to learn all their PHP from simply reading through the full php.net doc, they'd be left with knowing 4-5 equivalent expressions for doing the same thing, and not always with a clear idea which is the 'right' way to do it, and which is just leftover cruft. Conversely, if one were to learn Go from reading through golang.org, they'd have an advantage, because it's a very new language and very focused on limiting the number of equivalent expressions (eg they don't even support the ternary operator, which is one I like quite a bit in PHP).  It's only really in the past few years that we've seen a huge push towards writing eloquent PHP, with tutorial sites like  , or the PHP-FIG, who have made a lot of progress. You'll find plenty of (often overly pedantic, but generally well-meaning) knowledgeable people in ##php on freenode IRC too.  tldr: writing good code in PHP is more about knowing what parts of the language + lib  not  to use. "
"rebuttal: Yes, you're right, you don't  have  to pay, in theory in the MS platform. But Communism also works  in theory .","Fair enough...but...  A hobbyist developer trying to write native Win32 apps is going to want to a windows license. And better tools (like Visual Studio) over simply cross-platform ones. Such a hobbyist can either pony-up, or live with the consequences.  Likewise, a comparison no one else is commenting on because it is apparently so apt: The iPad is like the XBOX 360. Microsoft DOES charge a $99 fee to get access to their console's SDK and support. And they only (officially) support their Visual Studio IDE for development there. And the only way to distribute your games in the 360 environment is through XBOX Live. People applauded Microsoft for opening up the XBOX 360 for ""hobbyists and small time studios"" to actually have a way to ""make a living off the console doing what they love.""  Apple has done exactly that, for devices it never intended to open up to third party development in the first place.  So the TL;DR rebuttal: Yes, you're right, you don't  have  to pay, in theory in the MS platform. But Communism also works  in theory . "
"If you're using a language that lets you poke at memory, understand what memory requirements your chip has.","You're absolutely correct. Each member may have its own distinct alignment requirements, and padding will be inserted to make each member happy. I believe on some (many?) chips, most members merely need to be aligned by their size. This would mean that a struct like this:  struct node {    int data;  /* 4 bytes */    struct node *next; /* 8 bytes */};  will be 16 bytes, but this:  struct node {    struct node *next; /* 8 bytes */    int data;  /* 4 bytes */};  may only require 12 because  data  is conveniently already 4-byte aligned after  next . (But, confusingly, an array of these structs  would  pad out to 16 bytes per element to keep the  node s aligned to 8-byte boundaries.)  I got to figure out most of this stuff empirically, as well as learning how a C++ object  with inheritance!  was represented in memory when working on a DS game. The level files were simply the in-game representation of the objects stored straight as binary data. So I got to write a tool in C# that would both create C++ headers defining classes for the game entities, as well as a binary file that could be loaded and directly cast to an object of that type. Fun times!  TL;DR: If you're using a language that lets you poke at memory, understand what memory requirements your chip has. "
"Traditional American pay/management methods can ruin everything, institute systematic thinking.","Per Frederick Herzberg's work, money is not a motivator, but a hygiene factor. People leave when they're unhappy on that or any number of other factors, and take a lot of tacit knowledge with them.  The only way to gauge this properly is for top leadership to hold long and trusting interviews with their workers to find out their concerns. The 1x programmer will feel like there's no job security, because 10x person blows them away. The 10x programmer is just waiting to move to Google and work at a spa for $150k/year.  Both issues are systemically problematic.  Per Deming's theories, if bonuses and pay are given out, the entire workplace becomes a lottery where people game the system. Lines of code? Spam it. Code reviews? Play personal office politics.  The answer is to get rid of personal measurements, and implement systematic ones. The root causes also have to be addressed. A lack of very careful hiring will cause the variation between 1x and 10x.  To reduce the variation, managers also have to learn from the best and train the rest -- themselves. No, 10x coder should not be training the 1x. The well-paid manager should burn the oil to get up to 5x and teach the 1x's to be 5x's, and then keep improving that number from there.  TLDR  Traditional American pay/management methods can ruin everything, institute systematic thinking. "
"Foster synergy, even if your suits abuse the word no-end.","Absolutely no, that just breeds false incentives, and completely disregards the individual value of each coder to the whole-team synergy: You need a mix, there. 10 highly productive programmers aren't actually that productive of a team because none of them will ever monkey for the other, and stylistic and philosophical differences will tear them apart. What you can, and absolutely should do, is offer high-productivity coders more time to work on stuff  they , as opposed to the management, think is important.  It's like saying ""Hey let's hire 10 field medallists to do our accounting, they surely can calculate, can they?""  Those 10x developers are the natural alpha animals of the pack, it's just that they could usually not care less about actually leading anything, and they usually won't fight each other, as long as each one got an, at least half-way defined, territory. So give them the opportunity to lead by technology innovation. Chances are the young and talented will follow in their footsteps, and the not-so-talented will be perfectly fine with working for long  manes beards, seeing how they can amplify those 10x, to, say, 20x and partake in the joy of seeing the results.  tl;dr: Foster synergy, even if your suits abuse the word no-end. "
"you will often end up doing things the proper way, because modules usually dictate that.","> Really? Surely that can only be true if I'm skilled enough to correctly encode the semantics of my problem in the types.  Indeed, that's certainly true. But acquiring this skill is fundamental in learning Haskell, since semantics are so strongly encoded in existing packages/modules. To give one example, if you'd want to serialize your data structures to JSON. There is no obvious 'ad-hoc' manner to do it. For instance, if you look at the type signature of the 'encode' function in aeson:  encode :: ToJSON a =&gt; a -&gt; ByteString  It's fairly obvious that you can encode anything to JSON, but you exactly have to specify how this is done by making your data type an instance of the ToJSON type class.  tl;dr you will often end up doing things the proper way, because modules usually dictate that. "
"A lot of clients simply will not PAY for unit testing. While we can debate what constitutes best development trends, there is such a thing as a fiscal reality.","Alesha,  ""General testing"" is when a developer gives his code/program to the testers (who are not associated with the development) and they return with bugs that a developer fixes. Has nothing to do with the development time or unit testing.  The development time includes development itself and testing by the developer.  To others,  Generally doing unit testing is optional. While, yes, it is a good idea but it is not something a client would understand as needed and pay for. It does take time to create unit tests. It takes a lot of time to create tests that test every possible data combination with very numerous permutations of which with other parts of the program. Generally, larger the program, more numerous the tests in geometric proportion (when many parts of the program play differently depending on other data there and here).  TL;DR: A lot of clients simply will not PAY for unit testing. While we can debate what constitutes best development trends, there is such a thing as a fiscal reality. "
"Going beyond SQL, a deeper understanding of databases is valuable knowledge, whether working inside or outside of the database world.","And there's more you can learn from the database world than just the query language. I found that learning the rules of [database normalization]( really helped me think clearly about organizing data in general. I already knew data structures from CS courses, but it's a different thing to apply that to real-world data.  And that's just the beginning: once you understand how to reduce data to its fully-normalized form, you realize you might not always want to do so. Mainly for performance reasons. But at least you know the compromises you're making.  There just a lot of interesting things to learn from the database world: the trade-offs between a  natural key , transactions, consistency models, or how to squeeze object inheritance into database tables.  TL;DR: Going beyond SQL, a deeper understanding of databases is valuable knowledge, whether working inside or outside of the database world. "
"While a limited liability company may provide some protection, it is unlikely to afford any benefit when held by only one individual.  *Edit: formatting","IAmA Certified Accountant, but this is more on the legal side of things, so talk to a lawyer before determining your business structure.  Regarding LLCs, It all depends on how you define ""Limited Liability."" If we are talking about credit, then the person owning the LLC would likely have to sign the loan as a guarantor, still obligating them to repay the loan even if the business folded.  In respect to liability for damages, if the business entity was sued, the individual could only lose what they invested in the business.  However, it is hard to imagine a situation where a one person LLC would not also be considered  personally  liable for client damages.  TL,DR: While a limited liability company may provide some protection, it is unlikely to afford any benefit when held by only one individual.  *Edit: formatting "
School started a program to identify technologicaly gifted persons and used that to achieve a mutual advantage by making us tech support for credit / monies.,"This is so odd to me.  At my high school we had a program that identified technologically gifted kids, we called it STAR (Student Technology Aids and Resources), and we were given the job of helping our IT guy and faculty with any problems that arose.  The first few years weren't that great as teachers had a hard time trusting students to come in and fix their computers, but about 2 years into the program, they started understanding that we really  were  there to help.  By my junior year I had  full  admin access to all our systems (less a handful, like the security cameras or email).  Now I'm almost done with a degree in computer engineering, and several of my friends who were also in the program have done awesome things ( cough  MrGrim) and are some of the brightest people I know.  I feel like if we had just been identified and expelled, many of us wouldn't have gone into the fields we are now, but because we were identified and out talents used, it had a profound effect on at least my life and I'm sure others as well.  tl;dr: School started a program to identify technologicaly gifted persons and used that to achieve a mutual advantage by making us tech support for credit / monies. "
Use PyUNO/PyODConverter w/ LibreOffice/OpenOffice if you want software that understands the vagaries of the .doc format. Don't believe the claims that other libraries are as sophisticated.,"When my Grandpa died it was a time of sad reflection but what made matters worse is that my inheritance came with strings attached: to get my dues I had to spend a night in .Doc Manor, a spooky old house of weirdness and quirks (some intentional, some unintentional). Although the skinny neighbourhood kids who loitered around the library carpark bragged that they'd made it through the manor they couldn't tell me what colour the 1995 carpet was, or what the photos of Wilfred Matthew Frankel or Elanor Mildrid Frankel looked like. They even talked about snakes and a --headless horseman which I guess means that they walked through the building with their eyes closed. I arrived that evening with my sleeping bag to find that house was falling apart except for a single room. The office door was left ajar and inside was a clean wood-walled control room with levers and cranks and switches. The rumoured photos weren't photos at all, but pencil drawings. Security cameras let me observe the rotting or caved-in walls in the rest of the house. I didn't sleep at all that night and as soon as the sun came up I left the dilapidated manor, never to think of it again until today.  tl;dr: Use PyUNO/PyODConverter w/ LibreOffice/OpenOffice if you want software that understands the vagaries of the .doc format. Don't believe the claims that other libraries are as sophisticated. "
"query parameters completely sidestep the issue of injections, at least they bloody well better","> Not 100% impossible. DB drivers have been known to have flaws in the past with properly dealing with parameterized queries on edge cases.  That is a critical security flaw in the implementation of your DB interface, which should be reported immediately. If it's not fixed within 24 hours then I'd take that as a sign you need to find a different database/database interface/programming language, depending on who it is that implemented that. Although personally finding that bug in an interface  at all  would be enough for me to stop using it completely.  One time I wrote a wire-level interface to a database (over Berkeley Sockets), as there wasn't an interface on the platform I was using. They use what I think is called Out of Band communication, basically, the parameter is sent in a packet that first says how big the parameter is, so no matter what the parameter contains, there's no escaping involved at all.  tl;dr - query parameters completely sidestep the issue of injections, at least they bloody well better "
"If you always install software from 'trusted' sources, and don't share anything with any service that you aren't comfortable having leaked out this exploit can't bite you.","Maybe I'm too tired, but I don't see the big  issue here. OAuth2 access tokens are meant to be shared with 3rd parties. All that's being accessed here is the token which a third party application  would have had access to anyways  had this gone normally.  If a user wants to get their own access token, then big fucking deal. They could code up the OAuth2 handshake themselves and print out the access token once it's received.  Google has a good walkthrough of the process in their OAuth2 API documentation:  To turn this into a malicious attack you'd need to have a program or browser extension installed on a victim's computer which intercepted http connections and redirected OAuth2 access replies to a page which inherited from about:blank. I  hope  that anything available from Google Play would have to pass a security audit which ensured no such inheritance could take place.  If it was a program running silently on a victim's computer then they're probably screwed. But if someone is compromised to that extent then they already have a more serious issue to worry about.  tl;dr: If you always install software from 'trusted' sources, and don't share anything with any service that you aren't comfortable having leaked out this exploit can't bite you. "
The real problem is the NoSQL guys picked a really stupid name.,"> SQL nay-sayers and NoSQL nay-sayers are as bad as each other.  They're also comparing apples and oranges; and confusing 5 almost entirely orthogonal concepts.   SQL's just a another somewhat verbose vaguely-ADA-like language.  And yes, it's fair to call it a language, since  [SQL 2008 is now Turing Complete](   For a cool example, check out this [mandelbrot program written in pure Standard SQL](   NoSQL advocates seem to mainly be refering to storage systems with different  transaction isolation levels levels   Some other people use the same terms to argue the pros and cons of table/row-oriented storage vs column stores vs heirarchical storage vs key/value stores, etc.  (without realizing that any of those storage layouts could have front ends that speak SQL or not, or support transactions or not).   Yet others use the terms to debate shared nothing clusters of inexpensive independant machines for storage.   (which may speak SQL or not; may support transactions or not; may use column stores or not; etc)   And bizzarely the mongodb detractors and advocates both seem to focus on when data is sync()ed to persistant storage - which is a tuneable configuration for many of the systems, so shouldn't be a highly emotional argument at all.    In the real world, many databases with SQL dialects can be configured to non-standard transaction isolation levels(along with associated performance benefits); and some databases that don't speak SQL dialects can be configured to parallel some SQL standard transaction isolation levels (with all the performance&scaling implications, of course).  TL/DR: The real problem is the NoSQL guys picked a really stupid name. "
"According to the Holy Word of God, the ratio of the circumference to the diameter is 30:10 cubits. 
 π = 3. So it is written, amen.","This is as it should be.  ""He made the Sea of cast metal, circular in shape, measuring ten cubits from rim to rim and five cubits high. It took a line of thirty cubits to measure around it. Below the rim, gourds encircled it – ten to a cubit. The gourds were cast in two rows in one piece with the Sea. The Sea stood on twelve bulls, three facing north, three facing west, three facing south and three facing east. The Sea rested on top of them, and their hindquarters were toward the center. It was a handbreadth in thickness, and its rim was like the rim of a cup, like a lily blossom. It held two thousand baths."" (NIV)  tl;dr: According to the Holy Word of God, the ratio of the circumference to the diameter is 30:10 cubits.  π = 3. So it is written, amen. "
The purpose of this font is not to actually render text for display. It's essentially size 0/invisible.,"FTA:  > ## So... what did you do this for?  > In a nutshell: @font-face custom font loading detecting. Loading custom fonts on a webpage takes time, because the font has to be downloaded, and even when it has been downloaded, it has to be loaded into memory. So, between ""showing the page"" and ""showing the page with the right font"", there's a period where the text has been typeset, but with the wrong font. Best case, this period is imperceivably short. But the bigger the fonts, the bigger the problem. Even a 100Kb font can cause a website to ""flip"" between the initial fallback font, and the intended font. If that's the page's main font, the user-experience is ruined. For web animation using the canvas, the problem is even bigger, because you might see an animation that's started before all the fonts are done downloading, and the text will suddenly change typeface a few, or maybe even 100 frames into the animation.  > What this font lets you do is typeset a div with ""font-family: 'Desired Font', tinyfont"", and then poll its dimensions over a few milliseconds. You can detect whether a font is really available now, using two approaches: 1) if the div has a width that is ""essentially 0px"", the desired font's not loaded yet. Or, 2) if the div has the same width as a reference div typeset with only the tinyfont, the desired font's not loaded yet. this makes it possible to accurately tell whether a browser is ready to show what you want to show people, rather than allowing it to show users something you didn't want them to see at all.  tl;dr : The purpose of this font is not to actually render text for display. It's essentially size 0/invisible. "
"this proposal is far from being a ""trivial change"" imho.","Another problem, apart from what millstone already pointed out, is that the scope of the temporary assignment is not always well-defined. Consider this snippet:  obj.Bounds.X = 1;obj.Bounds.Y = 2;obj.ResetBounds();obj.Bounds.X = 5;  Suppose ResetBounds() replaces the internal member bounds with a new object; if the proposal is internally compiled to this:  var tmp = obj.Bounds;tmp.X = 1;tmp.Y = 2;obj.ResetBounds();tmp.X = 5;obj.Bounds = tmp;  You will almost definitely not end up with what you want.  The other issue is that for properties that have no accessible setter this won't work, so for a class that defines  public Rect Bounds{    get { return bounds; }    set { bounds = value; }}  it might work, whereas for  public Rect Bounds{    get { return bounds; }    protected set { bounds = value; }}  you can't use the described method any more. This exposes too much about the inner structure of the defining class, for my taste.  Tl;dr: this proposal is far from being a ""trivial change"" imho. "
"do they implement typesafe discriminated unions properly? Or do they just ""cowboy"" it and access union members without checks?","I'm not familiar with gcc source code. Do they access the unions directly without checking the node tag?  I suppose a sane way to access such unions would be the following: the node struct contains an anum member, which tells the node type (tag). It also contains a list of members (of union type) which contain the actual node data, one member for each node type. The names correspond to an enum member of the type tag. There should be nothing else in the node struct.  Something like:  enum NODE_TYPE {    NODE_TYPE_binary_op_node,    NODE_TYPE_int_node,    ...};struct ast_node {    enum NODE_TYPE tag;    union {        struct binary_op_node {            enum bin_op op;            struct ast_node *left, *right;        } binary_op_node;        struct int_node {            int value;        } int_node;        ...    };};  Then you have a macro, that is used to ""derefence"" the node struct. Something like:  struct ast_node *node = ...;struct int_const_node *int_node = GET_AST_NODE(node, int_const);  It takes the expected node tag (like ""int_const""), converts it into the corresponding node type union member (using simple preprocessor magic, like NODE_TYPE_ ## tag => NODE_TYPE_int_const). If the tag mismatches, it abort()s or something similar (yay minimal typesafety). Otherwise, the macro returns a pointer to the wanted node struct member. (&node->tag => &node->int_const.) This pointer will have the correct type at compile time. (Yay more minimal typesafety.)  tl;dr do they implement typesafe discriminated unions properly? Or do they just ""cowboy"" it and access union members without checks? "
That's a very OCaml-y way of thinking; Haskell doesn't work that way.,"-- Notice:  typeclass instances are not packaged with your list data type.  They could live in a different module.data List a = Nil | Cons a (List a)-- instead, we have the Ord constraint on the function-- if we don't have one global instance per type, we need to make sure that every module imports the same instance.sort :: Ord a =&gt; List a -&gt; List a  Notice that we can have a list of unorderable values, we just can't call sort on it.  We just have additional things we can do if we have the Ord instance.  tldr:  That's a very OCaml-y way of thinking; Haskell doesn't work that way. "
"Even if you don't believe in pointers, pointers believe in  you  .","The problem here is that, given enough time, all abstractions leak.  I do 99% of my coding in C# or Python, so whole months at a time go by when I never see or think about a pointer, but it's not like my happy land of high-level abstractions and references isn't sitting on top of a whole mess of pointers, et. al., and every so often, up comes a bug understanding which requires being able to figure out exactly what the garbage collector, etc., is doing to hide those low-level things from me.  And, IMO, part of being a good and/or professional programmer is  not  having to throw your hands up at that point and say, ""Enh, the runtime's acting weird, guess I'm screwed.""  tl;dr Even if you don't believe in pointers, pointers believe in  you  . "
Having an absurdly large but unchecked limit is just as valid as having a more realistic but checked limit.,"I don't think he's entirely right.  First off, there has to be a limit to how ""big"" a window can get - either because of pure memory constraints or because of how some part of the engine was written.  There are two ways you can safeguard against stuff like that:   Code in explicit checks   Make the limit so fantastically big that it's exceedingly unlikely to be hit.    It looks like they either intentionally or unintentionally chose the second option.  We had this exact same choice at $work a few years ago when developing our next generation communication protocol.  We had to choose how many bits to use in a node identifier variable (it's a line protocol, so bits and stuff matter).  The word size was 16 bit, so we could either pick a realistic maximum (and have space left over to encode something else) or just say screw it and go with 16 bits.  We went with the latter, knowing that there was a tiny possibility that we could overflow in the future.  However, if we did overflow, that means we sold a device with 65k unique nodes.  If that 16-bit limit does ever become a problem, it's going to be an  awesome  problem to have because it means our development for that and a dozen other things will have been funded.  TL;DR: Having an absurdly large but unchecked limit is just as valid as having a more realistic but checked limit. "
"Far too many ""impossibly large"" limits turn out to be not just possible but desirable to exceed. A responsible coder will  always  check and trap the limiting case.","In certain cases it is a marginally acceptable decision to make that kind of trade (making a limit ""unreasonably"" large and then not checking it), but there is now a very long history of that kind of decision biting people in the ass, as the notion of ""unreasonable"" changes -- so you should think very, very carefully before doing so.  It's almost never a  good  decision in the long run.  A trio of examples:   People used to joke about Bill Gates' decision to limit RAM size to the now-laughably-tiny 640kiB in early versions of Microsoft Windows, because it was 10x the size of the then-largest personal computers.   The first $1B Ariane V rocket exploded because of an unchecked limit on a number representing lateral speed in the navigation system - the number rolled over its maximum signed-integer value, which was ""unreasonably high"" for the Ariane IV but not the V,  and threw an exception and error code - which was not checked-for and therefore got interpreted as real data by the modules ""downstream"" of the problem  ( ESA flight report -- check p. 3, bullet 2 .   I (and others) lost a fair amount of data due to an unchecked 2GiB upper limit on presentation size in Apple's Keynote (they used 32 bit signed integers for memory offsets, causing memory corruption in large files; since most presentations are a few to a few hundred MiB in size, 32 bit integers must have seemed like a reasonable choice to make...).    TL;DR: Far too many ""impossibly large"" limits turn out to be not just possible but desirable to exceed. A responsible coder will  always  check and trap the limiting case. "
"environment makes a huge difference in terms of probabilities, even if the range of possibilities remains roughly the same.","Depends - I've run into situations where a single line if statement that, by all rights, shouldn't do a single thing to any case other than the one what I was looking at right then broke 10-20% of regression tests.  I don't know what kinds of code bases you've worked in, so I could be entirely off base here since I've mostly only worked with my own code, professionally maintained library code, stuff undergrads I was sharing a class with wrote, or stuff my current set of coworkers wrote, but there's a huge difference between working with a large code base that was meant to be modified, and working with a large code base that a group of people just kinda threw together over the years.  The former generally provides a smooth, predictable experience with relatively few wrinkles to the overall design and implementation process, while the latter will regularly throw piles of crap of varying levels of size and vigor back in your face depending on the alignment of the stars, and whether or not you said your pre-compile prayers vigorously enough to be pleasing to the computer fairies living inside your desktop.  That said, yes, trivial things can usually get done with little note regardless of the context. It's the big things, though, that are worthy of note. I've implemented entire feature sets taking up a few hundred lines of code in one go with no issues after compiler errors. I've also struggled for multiple days to get tiny bug fixes through that end up taking up a line or two of changed code when all is said and done.  TLDR: environment makes a huge difference in terms of probabilities, even if the range of possibilities remains roughly the same. "
You're getting the error Undefined variable: group because the variable $group is not defined.,"This is... not good.   You have an SQL query next to a select dropdown. Your concerns desperately need to be separated.  Seriously. This is really bad practise.   Sweet jesus that's the most unreadable use of a ternary I've ever seen. Never ever do that. There's a place for ternaries, but if they go over one line they make readability go bye bye.   End_Of_HTML  Is... is this a constant? edit: Oh, no, it's heredocs. That's common.. I hate it, but others like it.   if ($punchclock_select_groups == ""yes"") {  You need to look into booleans. If this is being stored in a DB, making a boolean inappropriate, it's best to be stored as a 0 or 1 tinyint. Using values like ""yes"" is asking for trouble.   Global functions should be avoided. OOP is powerful and useful. You should be using it.   how the hell are $select_options going in as option tags, rather than just values? Oh, right, wait, you're passing in the sql to the function doing the job? Why on earth wouldn't you have the sql in select_options? That makes no sense.   onchange is not a valid attribute and inline javascript is horrible and has no place in a modern app. Learn to do unobtrusive javascript using event watching.    tl;dr: You're getting the error Undefined variable: group because the variable $group is not defined. "
"OP has a reputation of being a bit of a PHP gutter tripe, and I'm pretty sure he's Janosz from Ghostbusters.","I just laughed out loud and woke up my wife while reading this.  All the other glaring inadequacies of this article aside, I pictured myself asking someone in an interview [""What are a port?""]( and lost it.  Although, judging from one of [OPs videos]( I'm fairly certain that he is [Janosz from Ghostbusters 2]( and english is not his first language. Give the guy a bit of a break.  However, every time I see something by OP here (either peddling PHP's only brony framework, or talking about how he's going to ""hack some Laravel sites"" because of a vuln he didn't understand), a little part of me that still loves the PHP community gets stabbed in the throat.  TL;DR OP has a reputation of being a bit of a PHP gutter tripe, and I'm pretty sure he's Janosz from Ghostbusters. "
"Even though computer science is making its way into every facet of the sciences, the current state of CS in schools suck. We're screwed.","It doesn't help that states like California have no credential for Computer Science. If you want to teach programming, you have to get a math credential, and good luck trying to persuade the administration that your school is better off with yet another elective instead of a core math class.  Then when the administration does want to open a programming course, they see that it falls under a math teacher's qualifications, so then they force it upon one of them, and you get the situation mentioned in the article.  Until computer science gets added into some form of officially recognized curriculum and gets its own credential, computer science education at the secondary level will continue to rot. Many of the teachers currently teaching programming are retiring, and with the crazy requirements mentioned earlier, this will only hasten the downfall of the computer science curriculum in those few schools lucky enough to even have compsci in the first place.  TL;DR: Even though computer science is making its way into every facet of the sciences, the current state of CS in schools suck. We're screwed. "
Extending collections with non generic wrappers is no longer necessary because of LINQ and extension methods.,"As someone who started with .NET 1.0, and created collections for everything due to the lack of generics, I at first wrapped generic lists as subclasses, mainly due to being used to working with collections in that way.  However, by C# 3.0, with LINQ and extension methods, I find that I go now to great lengths to avoid declaring an additional class.  For example, with .NET 1.0, I might want to extend EmployeeCollection to contain a method called ""GetMaxPay"" because without linq, this is a non trivial bit of code to write and it is nice to encapsulate it in one place.  However, with modern C#, I'd either do something like:   var employees = GetEmployees(); // returns List&lt;Employee&gt; var maxpay = employees.Max(e =&gt; e.Pay);  Or, if it is slightly more complicated, I'd just stick it in an extension method of List<Employee>.  TLDR:  Extending collections with non generic wrappers is no longer necessary because of LINQ and extension methods. "
"just because a framework calls your code, doesn't mean it has to be monolithic.",">  Frameworks work like that , automagically, its not a library that you call and make it do things, frameworks do things to your code, and so they expect that you have this and that in place.  This is  not  true.  A framework provides one differentiating fundamental feature from a library:  Inversion of control .  That the framework provides this functionality by being a set of interchangeable modules or a monolith is an  entirely separate concern  from the primary feature of all frameworks (IoC).  As an example, you have Pyramid (a Python framework), which is entirely modular, and whose parts can be arbitrarily swapped, but which  still provides  inversion of control.  TLDR: just because a framework calls your code, doesn't mean it has to be monolithic. "
Back in my day it didn't even occur to us that ten lines of code was worth the trouble of putting up on the Internet.,"On the contrary, for bits of code this simple it is better for businesses to write it themselves. The overhead of keeping track of all software being used in a project along with all of the licenses involved for legal is already a huge pain for large libraries.  As somebody who has been an engineering director, interviewed hundreds of engineers and hired dozens -a project this small and simple isn't even something you could include on a resume to impress an employer.  Code reuse doesn't really apply to things this small and simple. It's silly. Any employable engineer could write this code faster than it would take to Google and copy from the Internet.  TL;DR; Back in my day it didn't even occur to us that ten lines of code was worth the trouble of putting up on the Internet. "
doing jQuery will force you to learn real JavaScript.,"I've been doing some polymer.js programming lately and their whole architecture really sort of discourages or obsoletes using jQuery. They handle the core issues jQuery solves: Ajax and element selection. So, including jQuery really just let's you use convenience methods like  .style()  or  .animate() , making me drop jQuery really quickly. The second I dropped jQuery, I realized how much I loved and used convenience methods that did little for me. Like writing  node.style[""transition-property""] = ""width""  is really obnoxious and so are your own animations. Proved to me you really don't need jQuery on modern browsers, maybe just an Ajax wrapper at most.  It's also taught me tons in a short amount of time. JQuery abstracts SO much is insane. Learning with jQuery, you're really learning how to use jQuery more than you are JavaScript.  tl;dr doing jQuery will force you to learn real JavaScript. "
"Google's decision is petty and childish, and it leaves the company vulnerable to being blindsided.","I think it can.  Corporate ""culture"" is hard to quantify, but that's an indictment of our quantification systems, rather than of the validity of the concept.  In this case, I do think that the long standing rivalry between the corporations has poisoned the air to such an extent that it may actually be causing them harm.  If Google and Microsoft are fixated on ""beating"" each other, they're actually hurting their long term best interest, since they're taking their eyes off what the consumer is asking of them.  For a better example, look at Sun.  Larry Ellison's business decisions over the last ten years have been driven by his envy of Bill Gates.  Has that helped Sun's value as a company?  Of course not.  Sun these days is far from the workstation giant that it used to be.  Largely this is because Larry Ellison became so focused on ""beating"" Microsoft that he didn't recognize other opportunities to grow the company.  In the same way, its unhealthy for Google to become so focused on ""beating"" one competitor that it misses other, much larger market opportunities.  tl;dr: Google's decision is petty and childish, and it leaves the company vulnerable to being blindsided. "
We can't blame everything on the closed nature of industry - increasing complexity also plays a role.,"I feel like although things are moving towards the restricted end, as far as allowing owners to tinker with them, a far bigger reason is that a lot fewer people  can  tinker with what we have today.  Take software, for example. Thirty years ago, it was quite a bit simpler. I'm not saying that  everyone  knew how to tinker with their software, but quite a few more people. If something like Word, for example, were to become tinkerer-friendly (open source, etc.), sure - more people would tinker with it, but far fewer people would be able to than with earlier, simpler software.  As we ""advance"" with our technology (I sometimes don't like to say that we're advancing, in some areas) it generally becomes more complicated - the natural result of an increase in features. This complexity shuts out a good number of people from modifying software, opting instead to leave it to the people who  really  know what they're doing.  I personally find this to be sad, and maybe a more open software environment would make things better, but what it really comes down to is the fact that as things become more and more complex, further specialization is required to  understand  the technology. At some point, software will become  so  complex that no one will be able to tweak and tinker with it unless they have a degree in computer science.  Is this good? Yes and no. On one hand, we will have more features that will, presumably work better; good for the ""end-user"". On the other hand, only people with a lot of experience, training, and specialization will be able to create and edit it.  tl;dr: We can't blame everything on the closed nature of industry - increasing complexity also plays a role. "
"The world is more modern, so tinkering is more modern. Disregard the laws if you need to.","Just because the Mythbusters say ""Don't try this at home"", doesn't mean you have to listen to them. I did a lot of dangerous things when I was a kid (put metal in microwave oven, short circuited a wall socket, etc), and it is the reason I enjoy tinkering today (programming, electronics, etc). What doesn't kill you really does make you stronger (or smarter). The reason products today are safer (you can't get an  atomic energy toy  is for the idiots who can't use common sense. The rest of us know what is dangerous and what isn't dangerous. And we know how dangerous something is. Short circuiting something at home will at worst throw the circuit breaker. It won't kill you. Epoxy resin is poisonous, but as long as you don't eat it you will be fine.  Tinkering has moved on from wood/metal/chemicals to electronics/programming/computers. I'm not worried about changing settings on my computer, because I know that there is usually a way to change it back. And when there isn't, I will have learned. I use Firefox because if there is something missing, I'll make it myself. Same reason I'd rather own an Android over an iPhone. It's also why I love websites like shapeways.com, where I can design and print my own plastic objects, things I would never be able to make myself.  As others have said, tinkering today is a lot more hightech, but it's not gone. Just because you can't keep up/don't want to break any laws doesn't mean its dead. Instead of working on a car and buying custom parts to it, people are working on electronics and buying custom parts for their toasters (at least one toaster is using an 8051). As many have said, the DIY mentality is coming back. People realize that they can go to a hardware store, or order parts online, and put them together in their own home. This way they can solve problems many people have no choice but to live with. Why don't comments in reddit have a border? Hobby programmers have fixed that with greasemonkey! If you have a problem, don't complain about it, fix it yourself with the tools you have available and the infinite information available to you on the internet.  TL;DR: The world is more modern, so tinkering is more modern. Disregard the laws if you need to. "
I want to make this a commercial-like product without the commercial-fee ($$) :),"If I had to explain it in X words or less, it's the simplest way to monitor and create remediation to typical sysadmin issues - it makes time for us admins.  That said, it's definitely not puppet, but it's not trying to be.  What it is:The most straitforward and simplest way to have a centralized post (major) migration management solution, with the least effort.  If you are deploying 200 MySQL servers, Omnisys isn't the tool to do that (although it can), it is, however, the tool to come in behind that deployment and make sure the lights are green, and they stay that way, front to back.  I hope to create this sort of overlap in what you're talking about, but it's really about having a ridiculously simple and powerful way to monitor/manage  your  software.  TL;DR I want to make this a commercial-like product without the commercial-fee ($$) :) "
"I can relate to feeling like a fraud, but it keeps me grounded.","I felt like this through college and through the past 2 years as a 'professional' developer. In college I hung out with people who seemed to know what they were doing all the time and I felt like I was always struggling. All the way up to my graduation I felt like this even though I successfully graduated with a decent enough GPA and managed to find work right away.  At work it's even worse. I'm given more and more difficult problems and in my mind I keep saying to myself, ""I can't do this shit!"". And yet, I somehow manage to get it done.  Of course I've attributed my success to pure luck and like OP and the [Why I feel like a fraud]( blog states I'm just waiting for people to find out I don't know what I'm doing.  However, the light at the end of this tunnel is that it helps to keep me grounded and I feel like I'm always needing to learn and be better at what I do so that I can actually be as good as people think I am.  Kinda crazy how that works out, no?  TL;DR; I can relate to feeling like a fraud, but it keeps me grounded. "
The wikipedia entries are too confusing because there's a lot of marketing BS that isn't helpful. No excuse...,"WebSphere is actually a brand, not a specific product. But most people are talking about ""WebSphere Application Server"" or ""WAS"" so let's stick to that...  A JVM runs a Java application but typically the input/output is local to one computer. Now, what if we want to run multiple java applications... and have their input/output received through TCP ports? That big JVM would be called an application server - and IBM's is called WAS. That's the basic idea.  Why do people pay for this? Because while it's very easy to set up one server, it becomes very challenging to set up and manage many of these servers. Why do you need so many? Perhaps you need to be able to handle thousand of requests a second. You can either throw a bigger hardware at the problem (an IBM mainframe System Z) or ""cluster"" using many smaller systems. Each way has their advantages. There are open source technologies that can help handle such a large number of requests, but many large companies don't want to build this they just want to buy it off the shelf and have a company to help them architect it.  TL;DR - The wikipedia entries are too confusing because there's a lot of marketing BS that isn't helpful. No excuse... "
It actually adds more work without making you any more secure.,"The replies to you so far give part of the story, but to my mind, there's a bigger reason why they suck.  If you're developing an application to release to the world at large, you have no idea what the configuration will be of your end users. Therefore, instead of just doing something sane like manually escaping all inputs, you actually have to do more work. You need to detect if magic quotes is enabled. If it is, strip out the quotes they added, then pass the variable off to your own escaping technique which you would have had to write anyway, to cover the case where people  don't  have magic quotes enabled.  tl;dr: It actually adds more work without making you any more secure. "
The 1911 is designed and safest to carry cocked and locked.,"It is a M1911.  That's how it's designed to be carried.  It is quite safe, and as a matter of fact, is considered more dangerous to carry with the hammer down.  In order to put the hammer down, you'd be carrying the hammer down (under tension from the hammer spring) on to a live chamber.  If it slips... boom.  Furthermore, the hammer is resting on the firing pin, which is an inertial firing pin.  Sufficient energy against the hammer will transfer to the firing pin and could potentially detonate the firing pin, making it go boom.  When the safety on a 1911 is on (the thumb safety anyways), it blocks the sear such that the hammer can't drop.  The grip safety prohibits the trigger from being pulled unless it is depressed as is done when you grip it.  To avoid dropping the hammer on a live chamber would be to carry Condition 3 (no round in chamber)... in a life threatening situation, you don't want the added step of racking the slide -- more to fuck up, and it takes more time when fractions of seconds may count.  Condition 1 (cocked and locked on a hot chamber) is the designed carry method and ultimately the safest.  If you'd like any more details, I'd be happy to provide them.  tl;dr:  The 1911 is designed and safest to carry cocked and locked. "
"Torvalds needs to get off his pet project and get a ""real programming job""","Its the old ""C++ is a broken extension of C"" rant again ~yawn~ will this ever end? Unchecked pointers, memory allocation, unchecked strings and arrays are all features in C that make it inherently undecidable and requires a programmer to mentally manage its execution. The fact that Torvolds has been re-writing the same software for 20 years does not make his opinions of value in developing software. His kernel code is riddled with sloppy pre-processor macros and runs overall on the simplest solutions to an overly complex demands. Things like constructors and generics go a long way to create software that allows programmers to design software that manages things like string instantiation and pointer references. The fact that Torvalds has a thousand programmers all over the world groking over his code sort of mitigates his need for these features.  tl:dr Torvalds needs to get off his pet project and get a ""real programming job"" "
"The Common Lisp ""loop"" macro is a very poor example to use to demonstrate how powerful Lisp is, despite how clever the guy who wrote it was.","Exactly the same argument can be used to say that PHP is a dreadful language (because it's used mainly by mediocre programmers), or Visual Basic (because it's used mainly by  bad  programmers).  But the reason that PHP and Visual Basic are bad languages are that it's damn-near  impossible  to write good code in them.  By contrast, it is possible to write good code in C++--but you have to be a mega-genius to be able to do so.  I've seen some excellent code in C++--but it was damn-near opaque to me.  Try as I could, I couldn't understand the tricks the programmer used to make C++ work so well for him.  On the other hand, I've seen brilliant code in, say, Ruby or Python and it made perfect sense to me.  A bad language makes it hard (or impossible) to write good code.  A mediocre language makes it possible for geniuses to write good code--but a good language makes it easy for anyone to write good code.  TL;DR: The Common Lisp ""loop"" macro is a very poor example to use to demonstrate how powerful Lisp is, despite how clever the guy who wrote it was. "
"Bottle does more than you think. And it is still evolving. Next release comes soon, yay :)","> However it misses some nice features of other “bigger” framework. One of them is a nice way to handle and validate query parameters.  There are some improvements in 0.10 (coming next week):   The  FormsDict.get()  method now accepts a ""type"" parameter: e.g.  request.query.get('page', type=int) . If that triggers a  ValueError , you'll just get a default value instead. No need for try/catch anymore.   The FormsDict supports an attribute-like access (dot syntax), too: e.g.  request.forms.name  instead of  request.forms.get('name') . These always return a unicode string. Decoding or re-encoding (Python 3) of byte values happens automagically (utf8 by default). If anything goes wrong (decoding error, missing key) the string is empty, but it is still a string. This avoids all the nasty isinstance() checks. If you want to get the real, unmodified value, you can still use normal dict methods.   The route syntax got an upgrade: e.g.  /object/&lt;id:int&gt;  or  /api/&lt;action:re:save|load&gt; . This is a great replacement for the old  @validate()  decorator and works with custom ""types"", too. The old  :key  syntax is still available of cause.    tl;dr  Bottle does more than you think. And it is still evolving. Next release comes soon, yay :) "
"Tcl was probably the best choice  at the time . Timing isn't everything, but it counts for a lot in engineering work ( EDIT  and product development).","I don't work at F5, but I'm seeing the following:  Their v9 software was [released on Sep 7 2004]( and judging from the scope of the changes, I'm guessing actual development started at least a year earlier, probably closer to 18 months, so let's say the Tcl testing mentioned in the article happened around Mar 2003.  Lua [released v5.0 on Apr 11 2003](  IMO, this version is significant on  two  three counts:   what looks to be  massive  changes in the language and its runtime happened around this time, and  this was the first release under the more commercially-friendly MIT license, and  EDIT  this was (I think) the first release that actually gained significant attention globally.   So I'm guessing:   Lua simply wasn't on F5's radar at that time, or  Lua  was  on their radar, but they were caught between potential core instability (v5.0) and potential license issues (v4.x), or  they  did  test the available versions of Lua (4.0.1 and/or 5.0) and found them wanting.   And I'll put forth a near-certainty: After they decided on Tcl, and put in all that effort, and it seemed to work well enough, you'd need a damn good reason (and titanium bollocks) to even suggest putting Lua in.  TL;DR: Tcl was probably the best choice  at the time . Timing isn't everything, but it counts for a lot in engineering work ( EDIT  and product development). "
Any real programmer knows that it's possible to build (largely) anything in any Turing-complete language.,"No, I'm fully aware that I'm in the PHP subreddit. My point was that though you have encountered many people who thought it ""wasn't possible"" to develop complex systems in PHP, the fact that it  is  possible doesn't say anything about the language.  Pretty much any Turing-complete language can build whatever any other Turing-complete language. The more important question is what am I optimizing for? Development speed? Readability? Maintainability? Scalability? Memory use? CPU use? Once you've defined the requirements for your system, only then should you select a language to program in.  My engineers and I might work in PHP, AS3, Javascript, Java, Python, C, C++, C#, Lua or ASM depending on the specific problem we're trying to solve.  When people build systems in languages that are sub-optimal for that domain, it increases engineering costs. I can hardly imagine the millions of engineering dollars wasted at Facebook as people fight to get the PHP codebase there into a performant and extensible state.  When a community of programmers become convinced that ""their language"" solves all problems, they are also less likely to become better programmers by learning about other languages. They will grow insular, stagnate in their professional development, and churn out terrible code.  TL;DR; Any real programmer knows that it's possible to build (largely) anything in any Turing-complete language. "
continuous improvement beats re-invention 364 out of 365 days in a year.,"I don't want a wheel that's  perfect  (or much better for a pretty specific situation). I want a wheel that's  good enough , that I can use in various situations, and that I can use  better  in  new  situations because it's  versatile  and because I know how to use it, because it's everywhere.  Look at the current vehicle wheel (wheel, tire, suspensions), who has undergone improvements over the centuries. It will happily go over bumps showed in the article, albeit slower. It's a  massive  stretch implying that you'll be on a road like that enough that it will pay off producing, carrying around, and putting on a square wheel for such road alone.  tl;dr: continuous improvement beats re-invention 364 out of 365 days in a year. "
"Rant. Teach it with passion, you can still succeed.","> I teach my kids that they can learn anything they are interested in, but  I can’t make them interested in any specific thing .  Well, at least you should try to make them interested in what you teach, otherwise I see no point in doing it (and also get the idea that the teacher isn't really happy with it).  Many times I've heard my friends saying they enjoyed trigonometry back in school just because the prof did expose it in a way that was fun to follow.  The same thing I can say with myself, changing idea about a specific subject (be it latin or math), as I've found another person who could explain it in a different and more enjoyable way.  Sure, is to be said that it's hardly probable that everyone would just  enjoy  trigonomtry, but I'm fairly convinced that the one who teaches you has a great deal of power on how you'll likely going to approach the problem and has many levers to use to make you like it (especially in the early years).  tl;dr : Rant. Teach it with passion, you can still succeed. "
"Use a high iteration count. No, higher. Higher is safer (but also slower).","You may be misunderstanding my words: I'm not saying to use only a single iteration; that'd be the worst possible thing to do. I'm saying: don't  manually  chain your hashes. Proper password hashing functions are perform chaining, but they are well-studied (which is a strong indicator that they are secure).  The iteration count is also known as a ""load factor"", which I mention in the post: you want to use a large number. This number should be small enough that it doesn't add excessive load to your server, but as large as you're willing to use. As a rule of thumb, you want to take, at the very minimum, 100ms to generate a hash.  Consider: If it takes the user 100 extra ms to login, it's so small they won't notice (you could even go for a whole second; making a login (which is done much more rarely than the rest of the uses) a bit slow isn't particularly bothersome).  However, for an attacker, if they need 100ms to check each password (and, since you have a unique salt per user, you must hash each password against each user), it means they can only try 10 passwords per second. Sure, they'll probably catch a few of the dumb ones like ""123456"" and ""password1"" in the first day; however, more complex ones would take so long that even if they were just 6 characters a-z, it'd take one year to try every combination... against a SINGLE user.  TL;DR: Use a high iteration count. No, higher. Higher is safer (but also slower). "
is that if you're stuck with Java the builder pattern can be useful in certain situations.,"The point is that you get a way to create immutable objects in java with a sane way to specifying particular fields. Similar to doing something like:  Foo bar = new Foo();bar.param1(a);bar.param2(b);  The above requires a Foo class to have to presumably mutable fields param1, and param2. If you wanted an immutable version of Foo you could just create a constructor with the signature:  public Foo(T1 param1, T2 param2)  The idea of setting all values that should be immutable at instantiation time is nice but what if you only want to specify some of the parameters? You'd need to create potentially constructors that set some defaults:  public Foo(T1 param1) { this(param1, DEFAULT_PARAM2); }public Foo(T2 param2) { this(DEFAULT_PARAM1, param2); }public Foo() { this(DEFAULT_PARAM1, DEFAULT_PARAM2); }public Foo(T1 param1, T2 param2) { ...set fields... }  You could see how this scheme would get a little out of control. First you need a lot of extra constructors for each new fields. Secondly it becomes hard to remember what each value is doing (i.e. new Foo(a, b, c, d, e, f, g, h) what variable is doing what?). One solution to give you immutable objects in java is the idea talked about in the article -- the ""builder pattern"".  Essentially the idea is that you create a mutable builder object that is then used to construct an immutable object. You can kind of think of it like a save functionality. Essentially you're mutating state then you call save (in java that would be the same as ""return new Foo(fooBuilder)"") which locks all the fields into their immutable values. The other benefit is that instead of having constructors with a lot of parameters you get an easy way to read what values are set:  ""new Foo(a, b, c, d, e, f)"" vs. new ""Foo.Builder().setAParam(a).setBPAram(b).etc(...).build()""  You could argue that it's still a lot of code in order to create immutable objects in java. I tend to be in this camp but at the same time there are not a lot of other good options that fit the same criteria.  So the TL;DR is that if you're stuck with Java the builder pattern can be useful in certain situations. "
"Generating HTML does take time, but it's a job that can easily be shared across many different processors.","> Surely it's not putting together the HTML string that takes time and doesn't scale well.  Well yes and no.  The difference between a badly written CMS in PHP and a well written, highly optimised web app written in C could be several order of magnitude in execution speed.  So putting together the HTML string  does  take time and there are measurable differences between different languages, approaches, etc.  If you go looking for these differences then they're easy to find.  But the important thing is this: you can add as many application servers as you like (also known as ""banging the hardware hammer"").  It doesn't matter how slow your template code is (within reason) because you can always throw more servers at the problem.  However, all those application servers rendering templates still have to get the data from a single database.  You can't  easily  add more databases.  The Highlander Principle applies here: there can be only one.  (I'm over-simplifying things greatly - there  are  plenty of different approach to the single-database problem, but it's much, much easier to add more application servers).  TL;DR   Generating HTML does take time, but it's a job that can easily be shared across many different processors. "
"If you own a Mac and want to develop on it, buy OS and Xcode upgrades as soon as they come out or risk getting left in the dust.","One day after a mandatory security update, C++ compiling on Panther shit the bed.  See the mandatory secuirty update came with the dynamically-linked libstdc++ from Tiger which, I believe, had a slightly different ABI than the old statically linked libstdc++. When a C++ compile ran, it would now try to link against the dynamically linked libstdc++.  No version of the new GCC or linker was available for 10.3.  The message from Apple was pretty clear: ""If you are serious about developing on our platform, you would have stood in line to get your Tiger upgrade on release day with all the other fanboys."" While 10.3 was still supported from an end user perspective, new 10.3 development was expected to be done on a 10.4 machine with the 10.3 compatiblity profile.  Tl;DR: If you own a Mac and want to develop on it, buy OS and Xcode upgrades as soon as they come out or risk getting left in the dust. "
"All of this is probably premature optimization. Profile your code, then start worrying about whether memory is on the stack or heap.","> There is no move constructor that could avoid copying [rep] if it's allocated on the stack.  Technically true. There is no ""move constructor"" for an array. However, you can use std::swap to similar effect because std::swap works very well for array types. This is because modern processors and compilers are really good at this sort of operation. Moving arrays implies strong cache locality and opportunities for aggressive instruction pipelining. What's more, your compiler might inline all of your moves to the point that it just constructs your array in its ultimate destination.  At any rate, I would avoid the issue altogether by making Representation own a pointer to an array of doubles. If the heap is absolutely to be avoided in this case, I would use some sort of memory pool (pre-allocated heap, static, global, or stack memory) to allocate the contents of Representation.  tl;dr  All of this is probably premature optimization. Profile your code, then start worrying about whether memory is on the stack or heap. "
"your argument dos not work in real world. (And I'd add: the world would be worse off if ""real world"" somehow changed so that the argument becomes prevalent).","Meaning that you're putting people off with a poor argument. (Well, you have certainly put me off). The claim that a random person on the 'net is ""just as capable at reviewing the source code"" is patently false for these reasons:   very few of us know shit about code; if those who don't know it to are be excluded from using open source, it  will not get used  (not the case here, but very much in general);   even for those who do... A random, I dunno, embedded C guy, will understand pretty much nothing by reviewing, I dunno, some Ruby (or whatever else he wasn't exposed to, and there's  a lot  of stuff out there)   there's a huge gap to cross from knowing languages and tools used on a project, to knowing the project in itself.    tl;dr: your argument dos not work in real world. (And I'd add: the world would be worse off if ""real world"" somehow changed so that the argument becomes prevalent). "
"It's not  the  solution, but intends to be  a  solution. 
 Disclaimer: $ uname -s 
 Linux","I BS your BS ;-)  I see it as just a different design, not for speed and not to be magical, just a different approach.  The advantages are mentioned [here]( as well as the [critique]( The ""demo"" it's [here]( but don't expect it to be magical.  It's just the [Gnumach]( kernel with the Hurd servers developed in part-time for like 2 or 3 guys, and not continuously, for 20 years. Probably not even the same guys.  Myself I see it as chance to have diversity and the possibility to learn and understand how operating systems (may) work.  Remember when others appeared (e.g. linux, minix)?  It's not developed further? That's because no one [contributed]( more to it. That's why.  TL;DR: It's not  the  solution, but intends to be  a  solution.  Disclaimer: $ uname -s  Linux "
"Do whatever your framework wants you to do if you don't want pain. If no framework, there is no single answer to this.","First, understand that OO makes more sense the larger the app is. If OO seems silly, it's because this app seems so small.  Secondly, there is no 100% right way to do things. You will get different answers on this. If you're using a framework my best advice is to do whatever it recommends. Trying to do your own thing will eventually bite you. If you're writing your own framework, do whatever works for you (which will take time to figure out).  Finally, on to the real answer: Most frameworks I've seen split things into two classes.  One is the actual model that mirrors your database table. There are very few/no static methods in this class. It's up for debate how much business logic is in here.  The second seems to have a different name in every framework, but is a class that handles the logically static methods, things like fetching instances of your model based on criteria. The people that don't put the business logic in the actual model put it here, usually.  tl;dr: Do whatever your framework wants you to do if you don't want pain. If no framework, there is no single answer to this. "
"file modification timestamps are for your convenience, they are by no means proof of anything, modifying them to anything you like is trivial.","File modification times are set when the file is modified, based on the current system time. There is no encryption or anything like that going on, and the computer does not update the modification time when you change the system clock. At best, file modification times are stored as universal timestamps (UNIX timestamps or UTC or a similar timezone-agnosting format), and when you change the system  timezone , access times will be converted to the new timezone when displaying (but not actually in the file itself: the stored modification is still the same in UTC).  So yes, faking file modification times is very easy. You don't even have to change the computer's system time: because this information is not encrypted in any way, all you need to know is where the filesystem stores it, access the disk at the block level, and flip those bits manually. There's probably even an API for changing the modification times in you OS of choice, which means there  has  to be some sort of tool to do it more conveniently and without programming it all yourself. In fact, backup / restore programs are typically designed to override file modification times as appropriate to conserve them - you want the original timestamps in your backup, not the time you ran the backup.  TL;DR: file modification timestamps are for your convenience, they are by no means proof of anything, modifying them to anything you like is trivial. "
this  is a JavaScript keyword that refers to an object.  HTML Elements  are not JavaScript keywords.,"The element name is not a keyword and it is not a variable, so jQuery needs to know what you're trying to work with.  ""p""  isn't actually a <p> element; it is a selector that selects elements named ""p"". You can combine multiple selectors together to select a specific element. For example, if you have an element <p class=""myParagraph""> and you only want to select that element, you can do this:  $(""p.myParagraph"").doThis(); // call method doThis() on all p elements with myParagraph class  You can refer to the jQuery API on selectors:  this  refers to a JavaScript object, which is the context in which a function is called. Explaining  this  is a bit more than I'm really capable of - I have an intuitive understanding that works most of the time, but often I get tripped up due to closures and function rebinding.  Instead I will refer you to StackOverflow:  TL;DR:  this  is a JavaScript keyword that refers to an object.  HTML Elements  are not JavaScript keywords. "
"your Mac is fine, the OS isn't the problem.","The same issue will follow you to Ubuntu - with exactly the same solution.  Look, OS distributions (various Linux, BSD, Apple, etc) supply whatever they feel is best for them.  They can never be totally current all the time because QA takes time.  If you want to stay up-to-date you need to manage it yourself, and hopefully have some kind of package management like conda (focussed on python packaging, not general system packages).  i.e. you could roll your own .deb files in Ubuntu but it's probably more hassle than its worth over the normal Python process of virtual environments and conda+pip  Note that Python distributions like Anaconda also lag momentarily (not as much as an entire OS of course) for the same reasons; but at least you get the capability via conda to fix it yourself & even contribute back via binstar.org  TL;DR: your Mac is fine, the OS isn't the problem. "
People who don't know how to design programming languages.,"The Haskell standard library is organized according to mathematical principles and not according to what the programmer needs. Every larger module starts with a large heap of imports just to get all the combinators you need. Some of the are in Control.Monad, some are in Control.Applicative, others are in completely different places altogether. Different spellings for the same thing exist (map / fmap / <$>).  Parts of the standard library's design are straight out of the papers that first described the corresponding concept without really thinking about how to implement the concept in a way that fits Haskell best. Often the paper's author moves on (e.g. leaves academia) and nobody else wants to maintain the code. That's a shitty situation. Similarily, there are many different ways to do the same thing with the Haskell standard library. There is no governing idea, no consistent nomenclature and limited interoperability. A system designed by people who want to show that it is possible to do so without much consideration for the programmer's need.  Examples for this are the many different classes for numeric types that neither work together not allow for easy conversion between different numeric types in general and the completely absent error handling concept - every component of Haskell uses a different approach, often error handling consists of an exception which you can't catch in pure code. Pretending that error conditions don't exist doesn't make them go away.  Finally, the name space system of Haskell is broken. You have a hard time importing things as qualified (especially since qualified imports do not really work with operators) and name clashes regularily occur.  TL;DR People who don't know how to design programming languages. "
"it was a bad point. 
 Edit : lol bring on the downvotes","Why stop at 500,000, why not say 500 million?? You cannot prove that increasing supply is bad if the number you have hypothetically chosen to increase it to is so ludicrously large that it dilutes all meaning. Going from 64,000 to 500,000 in the present state of the industry might be bad or it might not be. I don't know. You don't know. However, going from 64,000 to 128,000, doubling it, now suddenly it is not as apparent if it is as harmful is it. The efficiency of your argument depends on the hypothetical number you have chosen. Also by your argument, ANY increase will be bad. So stop all immigration? Tl;Dr - it was a bad point.  Edit : lol bring on the downvotes "
profile your code before making 'optimizations' and be aware of algorithmic complexity of the [operations]( you're performing.,"if x in range(y)  has the following problem: suppose  y = 100000  and  x = 99999 . Python will compare  x  to every single value in the sequence on the other side of the  in  operator until it finds a match.  if 0 &lt;= x &lt; y  does at  most  two comparisons. It only does one is  x &lt; 0 .  This may seem like a micro-optimization, but what if the comparison of  x  and  y  takes half a second, or a whole second, or longer? It seems harmless enough when you're working with integers, but you have to be aware of the [algorithmic complexity]( of the things you're doing.  Edit: As a bonus, here's the proper way to do what your original example :  if x in set(range(y)): . If the members of the sequence are hashable, you will get O(1) comparisons, instead of O(n).  Edit: As pointed out by /u/sholic, using set() still iterates over the entire sequence, so you don't see a performance gain unless you're doing multiple membership checks or the collection of values you're doing a membership check in is 'large'.  Edit  tl;dr : profile your code before making 'optimizations' and be aware of algorithmic complexity of the [operations]( you're performing. "
"To break out an old playground adage, Safari is fat and OS X FF is ugly. Safari can exercise, but FF is ugly for life.","> You mean a subjectively true thing, then.  Nope. You may prefer it for your reasons, but it's limitations are evident and there, solely because it's not a native app. Liking it more has nothing to do with it. It can never be a real OS X application. That is true, and no amount of coding short of a complete ground up rewrite using Cocoa will solve that.  > I prefer Firefox to Safari on OSX.  I would suggest you switch to another operating system then. You aren't getting your moneys worth out of it if you prefer FF over Safari, simple as that.  FF is the antithesis of OS X, no attention to detail whatsoever. The OS X version is truly slapped together, and offers no unique OS X features at all.  > If I was forced to use Safari, it would be an inferior experience for me.  Once again, nothing to do with anything. The middleware limitations are there and always will be there.  Apple could fix Safari's limitations any time it wants. Mozilla can never turn OS X FF into a native OS X app without starting again.  TL;DR - To break out an old playground adage, Safari is fat and OS X FF is ugly. Safari can exercise, but FF is ugly for life. "
"MS Access is great depending on the problem domain. However, there are many cases where it was used to create a deformed horror from the deep.","My thoughts...  Access gives the non-technical to semi-technical user an incredibly power tool set that will allow them to create a fairly elegant solution for a specific class of problems.  Those problems range from small database storage needs to fairly complex but locally hosted applications that are in small to medium sized businesses or organizational units.  However, where I see Access being totally shoehorned into places it should not be are larger enterprises.  On more than one occasion I have gone to a client site where their biggest complaint is their enterprise level client-server multi tiered app isn't working well anymore.  What is that app written in? MS Access...  Some bright idea 15 years ago has morphed into this unsupportable and hacked solution that corrupts 3 times a month.  Worst case that I saw was a 100+ million a year company running their entire client tax form generation off of MS Access.  Every year the tax law changed so the forms had to be manipulated so that they produced the correct output for the reports.  They were always late and the fines were something like $15,000 a day.  TL;DR;  MS Access is great depending on the problem domain. However, there are many cases where it was used to create a deformed horror from the deep. "
Just because the meme was created before you were out of your pampers doesn't mean it's pretentious or harmful. It has a glorious history.,"I like the meme, it says ""I'm going to tear some chunk of common practice (that is a false wisdom) apart"". I think the first one was ""goto considered harmful"". While goto is not harmful  as such  its overuse is definitely evil, especially when you consider the history of programming and coders who didn't think much of structured programming. Functions and procedures, that is. Encapsulation, back then, was unheard of, and reducing the usage of goto made people  consider  the benefits of encapsulation.  It can be overdone, of course, and again the history of and debate about gotos is a good example of that. Just imagine Mr. Enterprise Java Programmer and J. Random Linux Hacker [arguing]( about readability. I happen to stick to ""use it where it makes sense and just know why you don't want spaghetti code"".  What's absolutely unacceptable is calling a new technique, or some research result, harmful. This series of papers and articles is about reflecting on industry practice, and not meant, at all, to bash random topic XYZ for your own personal hybris or to slander. Languages themselves, I think, are also immune, rather criticise some defining aspect. Like goto, or OOP, or impurity. Unless, of course, the language you want to criticise happens to be BASIC or PHP. Even Pascal is too venerable.  tl;dr: Just because the meme was created before you were out of your pampers doesn't mean it's pretentious or harmful. It has a glorious history. "
"Mixins just add methods to a class, and don't get involved specialization, constructors, private members, and all the other messy parts of inheritance and composition.","The simplest explanation is that Mixins are how you follow the advice ""Favor Composition over Inheritance"".  Under inheritance, functions from the base class are included in the derived class, however it also creates a taxonomy in that the derived class should be able to masquerade in any place where the base class is used. The is the ""Is A"" relationship.  Traditional composition typically involves establishing the ""Has A"" relationship by adding a number of members to the class and working with them via accessor methods, which leads to a lot of code bloat.  Mixins are a more direct composition, where the class establishes a ""Has A"" relationship with the methods of the mixed in class, but not any instances of the class. Also, depending on which language implements (for instance, ruby), the methods of the mixin module can refer directly to an instance of the class they've been mixed into (via the @ operator).  tl;dr: Mixins just add methods to a class, and don't get involved specialization, constructors, private members, and all the other messy parts of inheritance and composition. "
"Memory reporting is difficult, chrome is probably more memory friendly than you think.","It's difficult to measure Chrome's memory usage in a way that's actually meaningful, because Chrome is very ""polite"" regarding the way it actually uses that memory.  For example, suppose Chrome is using 1GB of memory.  You switch to some other app and start using it, and that app wants to use some memory.  Chrome has some magic in it that allows it to notice that other apps want to use memory, and then Chrome will voluntarily relinquish its memory to the other app.  You won't notice it in task manager, but it happens.  Really this is just an artifact of a larger problem that ""memory usage is hard to measure"".  It's not useful to know ""how much memory is app X using"".  What's useful to know is ""how much memory does the app I'm running how have available to it?""  And it's MUCH more complicated than simply ""(total system memory) - (memory used by other apps)"".  One easy to comprehend example of where chrome does this is with regards to long running tabs.  Generally you've only ever got one tab visible at any given time.  Whichever tab you're using.  The other ones you can't even see unless you click on them.  So in theory, Chrome could get away with relinquishing almost all of the memory in use by that tab until such time that you clicked on the tab again.  Not all of it, because some tabs actually run scripts etc even when you're not looking at them.  But a lot of it.  And that's exactly what Chrome does.  It's a little bit smart about it, like only paging out tabs which you haven't used in a long time, etc.  As well as some other heuristics.  But even when it does this, it's not obvious from looking at the processes memory usage in task manager.  So TL;DR - Memory reporting is difficult, chrome is probably more memory friendly than you think. "
"You suck at crypto. Ask for help. Your crypto will never be perfect, deal with it.","I enjoyed your article in a tongue-in-cheek kind of way.  I work for a major software house in Silicon Valley, and I routinely (maybe once or twice a year) encounter new situations where cryptography is required.  The following is directed at readers of this thread, not you, the author:  Of  course  you should use cryptography. Your code is never going to be bug-free, even in the non-crypto parts. Remember that the goal of cryptography is not  perfect  protection of your data. Your goal is to make your data  too expensive to steal .  If you think you know ""something"" about crypto,  you don't . There are hundreds of subtle issues that you must deal with that you're not even aware of.  Always,  always , consult with someone who knows at least two ways to break your crypto (you know these guys because they're grizzled and riddled with battle scars). Then, continue iterating to plug as many leaks as you can. Do this  early  in your process so you don't build your entire security model on a faulty premise. And make things  modular  so you can swap out your crypto when a better method comes along in the future.  And after you do all your homework, you will discover that your crypto is  still  not airtight, because at some point security will contradict usability, and you need a usable product in order to ship at all. So you weigh the potential risks, and go live with an imperfect system, just like the rest of your software.  tl;dr: You suck at crypto. Ask for help. Your crypto will never be perfect, deal with it. "
"it is only ""illegal"" to drive a company into the ground if your shareholders sue you over it, barring fraud, theft, and other criminal acts","The board and executive level of a corporation has a fiduciary duty to the shareholders. Boardmembers and executives may be held civilly, but not criminally, liable for failing to act in the shareholders' best interests, aka ""breach of fiduciary duty"". Criminal charges may enter if the executives and/or board are found to have committed fraud, theft, embezzlement, etc., but not simply for failing to make good choices.  In private companies such suits almost never happen because the company is usually controlled directly by the significant shareholders and because there's usually not much money to gain from it.  In SF's case, they probably realize they are going to die and the shareholders wish to make as much money as possible while they're still around. The corporation is being run in alignment with shareholder interests, so there is no potential liability here, even if non-shareholders are in control of SF's governance.  tl;dr it is only ""illegal"" to drive a company into the ground if your shareholders sue you over it, barring fraud, theft, and other criminal acts "
"Implementation details can help you to gain understanding, but don't rely on them in your actual code.","While the implementation details given in the article can be helpful for gaining an understanding of pointers, one thing to be wary of is that you shouldn't be relying too heavily on them in your actual code. Doing so is a great way to end up with an unportable or subtly broken program.  For example, the article is full of casts from pointer types to  unsigned int . Leaving aside the fact that they should be C++-style  reinterpret_cast s and that the result of such a cast is not necessarily guaranteed to be a meaningful memory location, there's also the issue that a pointer value isn't guaranteed to fit in an  unsigned int  at all. For that you need something like an  uintptr_t .  TL;DR:  Implementation details can help you to gain understanding, but don't rely on them in your actual code. "
"your mileage may vary, but in certain cases, storing a bitmask as a numeric value and extracting the flags on the client-side can make a ton of sense.","In my experience, storing a bitmask in the database has its advantages for large datasets or where performance is so critical that shaving ten milliseconds off retrieval of a resultset could save your job. But:   the whole bitmask should be fetched as one field, and all of the bitwise operations should be done on the client-side, not in the server. You especially see the performance impact if you're programming in a low-level language like C where these operations cost nothing to do with native types, but kill you if you have to retrieve each bitmask field as a separate boolean column in the recordset; and  the mask should be stored as an int, bigint, etc. There's simply no good excuse to store it as a string and it's usually just a sign that either the interface is constantly changing (i.e. the size of the string is fluctuating whenever the code changes, and thus you cannot make any assumptions about which fields mean what and how big the overall string is), which implies a premature design, or whoever wrote it has no grasp of simple arithmetic and how to do bitwise operations outside of base-2. I've encountered both cases and am not sure which is worse.   TL;DR - your mileage may vary, but in certain cases, storing a bitmask as a numeric value and extracting the flags on the client-side can make a ton of sense. "
"taking course in college, love it; averaging ~10+ hours a week in class, and personal.","I'm taking an introductory PHP class at my local college, currently.  3 hours a week for class and aim to be reading, and ""playing"" for another 8 or more over the rest of the week, depending on time.  I have had some experience in the past, but never too deeply. (installation/configuration and deploying php applications built by team members, but never an app I've actively participated in programming, or otherwise.  For the class, we're using "" Murach's PHP & MySQL  this to be ""the least  lazy  example of php design"" in all of the text books he's reviewed.  I didn't fully understand what he meant by that, until I started doing the assignments, where I've found/run into issues with uninitialized variables, which in turn causes errors in display in some cases.  That being said, the version of xampp the author recommends for use is 1.7.2 which comes w/ PHP 5.3.0. I've installed xampp 1.8.0 (at the suggestion of the instructor) which has PHP 5.4.4 and found the above mentioned issues. I have not tried the applications/examples, code whatever, in another version of PHP as of this point. I realize/imagine the initialization issue would likely not be resolved in a different PHP version... Uninitialized vars are still uninitialized after all, right?  But, I digress.  The book is descriptive, informative and so far, useful for a relative PHP Noob.  Aside from that, I have some Lynda.com videos/docs, and I have a copy of SAMS Teach Yourself PHP4 in 24 Hours (bought it years ago, never really touched it except for basic reference)  I think I'm kinda doing things ""backwards"" as it were with programming, since I'm starting w/ PHP, but it seems ""easier"" for some reason, than C, C++, C#... or Java (which I tried years ago, and still need to take to complete my program)  Either way, I love it, I'm learning, moving forward with my career choices, and having a blast getting lost in a sea of variables, and functions etc.  tl;dr taking course in college, love it; averaging ~10+ hours a week in class, and personal. "
"1) GitHub, why not!  2) BitBucket (private and free!) 3) DIY (you'll learn something, and honestly, what self-respecting programmer doesn't have at least a VPS anway?)","Another vote for GitHub.  If it's a ""personal"" project, then why not share your source?  Nobody is going to look at it anyway.  If it's your new super-secret business idea (which I would say is not a ""personal"" project), then, still, nobody is going to look at it.  GitHub is great.  If you really don't want anyone finding out what a terrible programmer you are, there's bitbucket.org, which is still free and very usable.  But when you run out of free ""slots"" in whatever loss-leader some vendor is offering you, just get a cheap VPS (<=$20/month), and install gitosis (at least), or if you want the whole web-based thing, something like gitlab, and be done with it.  So, TL:DR:  1) GitHub, why not!  2) BitBucket (private and free!) 3) DIY (you'll learn something, and honestly, what self-respecting programmer doesn't have at least a VPS anway?) "
Don't take yourselves so seriously.  Its just code and no matter how well you build it now some kid will say its crap in 10 years,"IMHO engineers tend to develop overly polarized opinions about different technologies and techniques they encounter.  Its not indicative of a mature understanding in many cases but rather groupthink or rigidity.   The flames are fanned by legitimate anxiety over keeping systems live and stable but it gets magnified way more than necessary frequently leading to overconfidence,  over-conformity,  etc.  Modern conventions like mvc separation and testing are real advances because they really do make software easier to maintain and to remain stable in the face of rapid codebase change.  However keep in mind that software is just a tool that people use.  WordPress may be coded inefficiently but it is and probably will remain for a long time the most popular cms.  Users do not care about technical elegance.  They just want their tasks made easier to accomplish.  WordPress delivers that.  TL;DR Don't take yourselves so seriously.  Its just code and no matter how well you build it now some kid will say its crap in 10 years "
"Pick out your code points, and encode them. 
 Disclaimer:  I don't actually know what I'm talking about.  Please correct any mistakes.","Speaking for myself, the biggest thing that confused me about unicode was the distinction between unicode and it's byte-form encoding.  It's really not that difficult, but nobody ever really made it explicit.  Maybe this will help clear things up for you too, /u/ichbinsisyphos.  unicode:   Don't think of unicode as a string.  Think of unicode as a huge table of symbols that represent written characters from around the world.  These are formally called ""code points"".  When you write a unicode string, you're really just lining up a bunch of code points.  For example, when you type  ""J'ai niqué ta mère"" into the python interpreter, your string actually  is   ""J'ai niqu\xc3\xa9 ta m\xc3\xa8re"" .  Now, for some reason that I don't quite know (but that also doesn't really matter) it has been deemed unacceptable to encode unicode strings as ASCII text files.  I suspect it has to do with disk space considerations.  For this reason unicode strings typically need to be  encoded  before being used...  encodings:  Encoding is the process of taking a bunch of  code points   (again, a unicode string) and turning it into some more compact byte-wise representation.  There are several methods of doing so, the most common of which is  UTF-8 .  TL;DR:   Pick out your code points, and encode them.  Disclaimer:  I don't actually know what I'm talking about.  Please correct any mistakes. "
"fill in the blank please? ""atom is better because it is modern, and that means it has  ___  better feature/property""","I appreciate the response.  I think there is more to what you mean by ""modern"" than you are explicitly stating though. To consider a further analogy, what about (for example, since it is another interest I am familiar with) knives? Their origins are prehistoric, and yet I doubt those interested would call Benchmade or another big name antique. Why not? Well, although they are making knives(and they have been sinces the 70s, and knives have been around for longer than that), they are using modern techniques and materials  that impart a benefit on the knives/knife users .  Back to your analogies: there is something similar implicitly stated in them. We don't ride horse and buggy, or use typewriters, because there are real tangible benefits to the modern alternatives. ""We use the modern alternatives to horse and buggy"" is correct, but incomplete. Lets say ""We use the modern alternatives to horse and buggy  because they are better ""(and not ""...just because they're modern"").  Finally to bring it back to atom vs vim/emacs/sublime: hopefully there is an advantage that atom has as a modern tool that vim don't. Can it leverage new software or hardware that has come out that vim/emacs can't? Is its new style faster/more efficient(a subjective, yet valid question)? etc.  tl;dr fill in the blank please? ""atom is better because it is modern, and that means it has  ___  better feature/property"" "
"though 
 P.S. -- [Fordiman's answer]( is concrete and goes beyond the scope of static analysis","Getting your code to run clean during static analysis is a noble goal, but often unrealistic for large codebases: the ones that spit out an endless sea of errors and warnings.  So if you can't fix everything, you've got two main strategies:   Triage the errors/warnings  Triage the code   A mix is probably best for big projects. So break it up into smaller projects, and for some subprojects you fix everything, others you drop the last triage level, and for the rest you drop two triage levels, basically only going for errors.  Now, Carmack is talking about separating internal code from to-be-shipped code. I personally think even the code being shipped might be too much, so I changed the scope to ""critical"" code. I'll elaborate the TLDR though  P.S. -- [Fordiman's answer]( is concrete and goes beyond the scope of static analysis "
I don't think attempting 100% native integration is worth it. Installing MSys is easier than going through the bother.,"To be honest though- if you are a programmer (and you are to some extent if any of this concerns you), you are probably well served to have a POSIX environment regardless. ""The Windows experience"" or ""The Linux experience"" or really the experience of participating in any environment that is highly constructed is only a matter of taking a couple days to learn your way around an environment you should probably bother to understand anyways.  The only exception I've found is in programming video games or highly-performant responsive interactive apps; going through a POSIX layer on an NTFS kernal has never been performant- but I'm not going to need to integrate a git interface for that =) If I want to use git, I think SVN has a git compatibility layer on it, actually, but IMO- fuck SVN. Ew.  I suppose I can see the draw of having a 100% native integrated experience, but all the reasons I would select for attempting that are reasons that I would be building a nix/BSD system- not Windows. If the reason to use Windows is because that's the limit to what you know or because you're not good at POSIX environments then I think the response should be to learn enough to be comfortable outside of windows. If your tools don't work with POSIX, writing a layer to work with an emulated POSIX environment is pretty simple and abstracted away to preconfigured installs (MSys, Cygwin) or low level API's; so you should be able to serve both worlds in a simple manner.  TLDR: I don't think attempting 100% native integration is worth it. Installing MSys is easier than going through the bother. "
"GUIs are hard to code, HTML/CSS is the best we've got so far.","As a developer that had to create UIs in multiple languages/platforms I have to say that programming UIs is a hard job, no matter the tools.  On one hand css/html have a quick turn around (i.e.: write code, see the result) browser inconsistencies tend to slow you down a lot.  Now compare that to something like Java/Swing UI development where you write code, compile, run, test and then repeat for each OS you want to support and you'll see that this is even worse.  Even in Android UI development where you are supposed to have only one platform you still have to go through the compile, build, test dance for all versions and screen resolutions.  Now think about having to do the above for multiple locales and you can see that it will get out of hand pretty fast. At least with HTML/CSS a lot of those stuff are handled by the browser leaving you to deal only with colors and stuff.  Now that CSS3/HTML5 is becoming widely supported I think HTML/CSS UIs are even easier to write and I find it hard to justify creating a UI with another technology even for non-web applications.  Also I have to say that after 7 years of web development I've developed a feel for cross browser coding and the cases where something does not work correctly on all browsers is pretty rare nowadays. I usually code with what has the best tools (firefox/firebug) and test on other browsers once I'm done.  Protip: Learn to use your tools. Read more about how browsers work. Learn the DOM and how inline vs block elements work. Minimize the need for CSS rules and use the cascade effect to you advantage. Set some baseline styles for all elements and work only on what's different.  There is a point where everything falls into place and you can write HTML in your sleep. Hang in there.  tl;dr: GUIs are hard to code, HTML/CSS is the best we've got so far. "
"bound checked arrays in C are not going to happen, and that's a good thing .","> I don't think that's such a bad thing  It is. First, you'd break compatibility with a huge amount of libraries. Second, often C code deals with variable length arrays (think of network packets and file headers); a common practice to deal with them is using zero length arrays (called [flexible arrays in GNU C]( or size 1 arrays: bound checking would make all that code invalid. Even if bound checking could be selectively turned on and off, bound checked arrays and non-bound checked arrays would be essentially different types, and mixing them would be impossible or, at very least, very difficult and extremely inefficient.  Things get even more complicated when looking at compilers architecture: the C standard allows compilers to run certain optimizations if they know that memory regions do or do not overlap. Bound checked arrays would not mix with normal arrays, making a lot of code much less efficient.  There are several more arguments against bound checked arrays in C, for example it adds complexity, would be hard to implement on severely resource constrained processors and microcontrollers.  C is all about getting as close as possible to bare metal without resorting to assembly, philosophically bound checked arrays are not the kind of feature expected from C.  tl;dr: bound checked arrays in C are not going to happen, and that's a good thing . "
"the university, the guy submitting the proposal, and the OP are all morons.","I'm not sure who is more stupid here.  Arguing that Vim is better than Eclipse for learning  programming , what the fuck? And I say it as someone with a 500+ line  .vimrc .  ""Right now we have a few Java guys who would be incapable of writing any working code if we took Eclipse away"" ? Really? Because you can tell Eclipse -- implement me this algorithm, -- and it does, diminishing your ability to  program  on your own? I mean, this is wrong both because ""you wouldn't be able to program if I gave you this punch card reader"", and because your preferred input method doesn't have  anything  to do with your ability as a programmer.  On the other hand, the original guy's expressing a grudge against Putty makes me think that he is a bit brain damaged as well.  On the third hand, wtf, give them XMing (for Windows, use the native X server on macs) and NEdit and you can teach them programming, forcing students to use Vim counts as a child abuse.  tl;dr: the university, the guy submitting the proposal, and the OP are all morons. "
"Very probably clever algorithms, followed by low level magic in critical code paths only","""All"" they do is perform vector-operations on a mesh (at least that's what I used ZBrush for). A dataset of 10M Vertices is not that much, given that probably not all are modified at once, but merely a subset.  I'm certain that they make use of clever algorithms to quickly access subsets of the mesh which are then modified by brushes and whatnot.  Low-level optimizations probably play a part in that program, however they are not the core of it's performance.  When working with big datasets and your ""goto"" algorithm for what you want to achieve (for example looking for a specific datum) is too slow for your requirements, then you use another one that has a different algorithmic complexity, for example a binary search or hashing instead of a linear search.  tldr: Very probably clever algorithms, followed by low level magic in critical code paths only "
"we all know HTML, and for most things, most HTML helpers are a huge waste of time IMO.","I do both, and I can say when I'm on the frontend it's really annoying to have to google how to do something as simple as adding a class to a generated element. Unless you always use that framework, it's a web search every time or a request to the backend guy to add a class used only for CSS. So very lame. And when I'm on the backend, I don't really see a huge amount of benefit from these ""try-to-catch-everything"" helpers vs e.g. just specifying that id prefix and validation with separate functions/variables.  I've also seen monstrosities like 60 lines of javascript passed as one string argument to a ""helper"" because it seemed like that's how Yii wanted it. I know there was a better way than that, but still, a developer with experience writing  html and js came across that helper and thought that was the answer.  Another example: is it easier to replace every img tag in an html email with the equivalent helper, and copy all the annoying attributes into the various string arguments, constantly referencing a cheat sheet to figure out what goes where, OR should you just put <?=$img_path?> before the file name in the src attribute?  To the people talking about reuse, what is more reusable than html+php? If your company is working on one mega-site then great, reuse your framework-specific html. If you work on more than one thing, you might find it easier to copy your address form from your last project than to copy it, and then replace every element with its helpful helper version. And then change it to the other helpers for your next project.  tl;dr  we all know HTML, and for most things, most HTML helpers are a huge waste of time IMO. "
"If you don't code in your free-time, you usually won't have the best time.","With Python being a language quite easy to learn thanks to both docs/amount of tutorials around and the language itself, you can just do start contributing to something open-sourced or play around with your own projects. Just make something on a regular basis, and it is by definition experience.  If you say you have used Python five days a week for the past three years, that is usually good enough for an interviewer to count as experience, plus you can directly show off what you made and that you are having fun with Python.  There are so many coders out there because everyone can just go online and start learning to code, so there are a lot of young coders around 20 who already coded for the past 5 years as a hobby, and perhaps already made something on their own others use, or have a respectable amount of commits in their favourite piece of software. Those are the guys who might not even need any kind of 'official' education/degree, because they have already proven they can code and have experience. Ideally, you can go directly through their commits on GitHub and see if they are a match coding-wise.  TL;DR:  If you don't code in your free-time, you usually won't have the best time. "
"The number of files/classes/interfaces  used  may be marginally useful, but just looking at the number of used and unused classes isn't really an indicator of anything.","> Look at Symfony's session manager. 26 classes and interfaces. T  I pretty much agree with you. Not everything needs to be that 99%. And yes, Symfony's session class does implement some things that not everyone need (e.g. not every user may need/want flash bag or attribute bag).  However, I'm don't agree with just looking at size/number of classes as an indicator of needs/complexity/bloat. The sheer size and number of classes in a library doesn't indicate usability/complexity. With modular libraries and applications, many of the classes are loaded on-demand (only when needed).  If the number was truly important, the big libraries can be broken up into smaller ones. Architecturally, breaking up the packages may be better. However, the developer overhead increases with the number of packages (e.g, the average developer will have to install 3 packages instead of 1). By trying to make the library  appear  smaller, its usability has decreased (as its become more complex to use). There's also other things to consider (more complicated process for contributors, more complicated documentation, more complication following the codebase), but in general, breaking up the package does more harm than good.  Is it worth it? Is it any faster? Is it any more efficient? Not really. Even in multiple packages, it'd still just load 3 interfaces and 3 classes for the average user. Was it worth the developer time to configure 3 packages vs 1? Probably not.  We shouldn't just look at the number of classes/functions/methods/files-- that's just a number. Can we stop saying ""look at how many [number] [project]"" has. Its becoming a nearly-useless marketing number at this point. Can we evolve past this? I'm starting to get tired of looking at so called minimal projects that are more complicated/bloated than their competitors.  tl;dr  The number of files/classes/interfaces  used  may be marginally useful, but just looking at the number of used and unused classes isn't really an indicator of anything. "
The syntax of C# and the runtime of Golang would make the perfect platform.,"You're missing the OPs point.  Golang is so great in many areas that other languages are terrible at.  Green threads/coroutines in Java/.NET/C++ are hard to implement and aren't native to the language.  The garbage collection is pretty neat, and the performance is closer to C than most other managed languages.  Plus with the backing of Google, there just aren't that many other languages that can offer that.  In 10 lines of code I can write a fully async web server than can support 10k concurrent users on 2008-era hardware.  Thinking about scale is a thing of the past with Golang.  However the basics that many are used to in Java/.NET just aren't there.  Collections (which require Generics), simplified syntax, etc. It's back to C programming in the early years and one of the industry's toughest dilemmas:  having to choose between programmer productivity or performance.  TLDR:  The syntax of C# and the runtime of Golang would make the perfect platform. "
Fuck the IRS they are blocking our money from Microsoft.,"This is probably a similar problem with the Xbox Live Indie Game area. I have made 3 games for that and haven’t received any of my money yet, however none of this is Microsoft’s fault. The reason I have not received any of my money (Around 6k for the 3 games) is because I live in Australia and there are some evil people in the IRS in America.  Basically for 9 months I have been trying to get a special number from the IRS and I am still waiting for them as we speak. Until I have this number I cannot be paid by Microsoft.  TL:DR = Fuck the IRS they are blocking our money from Microsoft. "
"Chapel is super cool, but not yet ready for real world heavy lifting.","Chapel's design documents were exciting enough for me to try writing a small project in an early-release version of Chapel.  Briefly, my experience was:   It's just as much fun to program in as it looks like. Maybe more. It's like when you see a motorcycle as a kid and think ""Wow! That looks fun."". Later, when you actually drive one for the first time, you realize it is somehow  even more fun  than it looked like when you were 8.   If you need to do binary I/O (and lots of other things)... you're on your own. I had to write my own C functions to link in.   Most of the core spec is actually there and working.   When the docs say that they haven't focussed on performance yet...  they are not kidding . It ran multiple orders of magnitude slower than the final ""real"" UPC implementation.   It was much faster to code than had I written straight C or Fortran with MPI, but not much faster than using UPC (or probably co-array Fortran).    tl;dr: Chapel is super cool, but not yet ready for real world heavy lifting. "
"64kb writes, Reads vary. 8kb baseline most of the time.","Well, its complicated. Especially on virtualized systems.  For reads, the memory page size can vary and you'll only be able to establish the 'best allocation unit size' of the file system via profiling io usage patterns. There are a few whitepapers concerning this, and I believe for reads the value was 8kb (page size in MSSQL). [Citation needed] of course. ;)  For writes, the system allocates new extents, 64kb. So that would be the optimal NTFS cluster/allocation unit size for write operations. PS: Make sure your filesystem is aligned properly such that this allocation unit does not straddle raid stripes/segments. Hint: Win2k8 pushes you 1024kb away from the beginning of the logical chunk, so you don't have to worry about it. But a virtualization file system could mess with you if its partition offset was wonky.  Then there is the matter of whether a virtualization layer honors the io requests of the guest -- whether a request for a 64 kb allocation unit to NTFS translates into 64kb request to VMFS.  And this mystery, I do not know the answer to...  TLDR: 64kb writes, Reads vary. 8kb baseline most of the time. "
"tabs for indentation, spaces for alignment.  Get a better IDE, spend some time to configure it and never EVER again manually indent / align code.","Awww sweet!  You guyz still WASTE YOUR LIVES manually indenting code..   Bless.  Get an IDE that does it for you and spend more time actually doing something valuable to humanity.  Pro-tip - get Resharper (or set up your IDE's equivalent), agree a style for the project and never EVER do this crap manually ever again.  Also stop arguing about it.  On the tabs vs. spaces debate...  I like my code to be indented by 3 spaces.  My colleague likes 4.  Others like 2 spaces.  We use tabs for indentation + spaces for alignment and configure our IDEs to display tabs with our preferred indentation width.  Everyone's happy.  Spaces would make us have to compromise.  Compromise is not needed.  TL;DR:  tabs for indentation, spaces for alignment.  Get a better IDE, spend some time to configure it and never EVER again manually indent / align code. "
"Having special sigils makes variable highlighting easier and helps to declare programmer intent, which I value.","The lack of symbols in python is one of the things I dislike most about python. Highlighting variables becomes more difficult, or perhaps highly improbable. Also, sigils declare developer intent. I know when something was meant to act like a scalar, because it has the scalar sigil next to it. In python, I have to trace all of the way up any possible call stack to figure out what a variable is supposed to be. While I understand this could be mitigated, perhaps, with more verbose variable naming schemes, I have also spent the better part of a day hunting down a variables origin through pdb.  tl;dr: Having special sigils makes variable highlighting easier and helps to declare programmer intent, which I value. "
"I like your project, keep up the good work!  Check out my notes if you don't mind (hopefully) constructive criticism.","This is a cute project.  I really love these kinds of things.  But I had a few questions and maybe a few suggestions (if you're open to 'em).   Shouldn't there be a Tile base class?  It seems peculiar that you would have a new class with the same name in the same module for every single tile.  Reusing commands seems a bit hard in this model.  Have you considered making an extensible Command class that allows the user to customize them without having to write new code?  Or maybe having a dictionary of available commands?  Does this have a concept of entrances/exits?  It looks like if you're somehow at a grid position, you've entered into it as if there was an entrance or exit.  Am I correct?  (This isn't bad or anything, I just wasn't too clear on that.)   --  Suggestions (if you're open to them):   Make commands easier to reuse.  A dictionary of lookups maybe?  Store the tiles in a different way that allows you to reduce the number of classes you have.  Description and Leave could both be attributes rather than methods too.  Since they're common to every single tile.  I would suggest a yaml file.  And just because prematurely optimizing is all the craze these days, you could even load these yamls into mongodb or redis when you first start up, so the entire file isn't sitting in memory.  You're going to need a dummy object class that can act as a container -- almost everything is a container on a fundamental level.  People contain loot, chests contain loot, books contain spells, etc.   I wrote up a gist to better explain what I mean --  Keep in mind, I think this project is really cool.  I don't think you need to change ANYTHING.  It's your project, so you should do what you want and take pride in it.  I just thought I'd pass along a few notes.  :D  Muds (as they exist now) typically have millions of rooms.  You're gonna want an easy to maintain project structure that lends itself to that kind of thing.  tl;dr: I like your project, keep up the good work!  Check out my notes if you don't mind (hopefully) constructive criticism. "
It provides a defined data structure for when things get too big,"For smaller scale simulations or games where you have a few basic entities it only really helps by being a defined model (i.e. Entities, Attributes and Actions). But for larger simulations where you may have many types that are handled differently it starts to not be so fun.  Using the entity type system you can make all entities that move MovingEntity and these could be Birds, Cows, Baddies, Cars but you can process them all the same (much like a polymorphic collection). This gives you a sense of multiple inheritance coupled with how you can add fields on the fly (but still has type erasure using AttributeTypes).  A lot of JALSE can be swapped out for your own components, like an AttributeContainer that reads from a DB or an RMI based ActionEngine but by default each component allows for concurrent manipulation of fields and entities (so you can move them as you work out who's hungry at the same time).  tldr: It provides a defined data structure for when things get too big "
apples to oranges. But it is interesting in itself.,"I have mixed feelings myself about the applicability of T1/T2 optimizations for many-core x86/x64.  One massive problem is that Intel and AMD don't make heavy use of hardware multithreading to a large degree (Intel's Pentium 4 and new Core i7 chips aside).  Also the on-chip multithreading for the T1/T2 is done through running the instructions through a barrel-shifting pipeline.  For Intel's chips duplicate  enough  hardware along the pipeline to track multiple threads. But often the ALUs and issuing+retiring hardware aren't always widened enough to be able to run multiple threads at 'full' speed afaik.  Side note: Out of order superscalar, speculative CPUS are a restricted form of dataflow. Operations are executed when their operands are ready, not by their order. The results are retired in the original control-flow order iff they are valid. It's a relatively small step to extend the surrounding hardware to account and distingush between the two states.  tl;dr: apples to oranges. But it is interesting in itself. "
"Web is easy to scale, storage is hard to scale.","You should know how web application work on a low level.  Like understand HTTP, encoding, cookies, etc.  Python has WSGI, sockets,  urllib, hashlib, etc.  Everything you need to create a web app.  A web server, is basically 'GET /\r\n\r\n'.  I'm not saying you should use only Python 2.6, just know enough that you could.  Frameworks are useful, but if you don't understand what they are doing, you are going to have trouble down the line.  You can put web servers in front of a load balancer and serve different requests to different servers; HTTP is stateless.  You can scale up to a ridiculous amount.  Where you run into scaling issues is with your storage.  You have concurrency, uptime and other issues.  Lets say you use a SQL server as your back end.  Will it scale?  Not really. You have to come up with complicated partitioning or master/slave schemes to scale beyond one server.  Document databases (Big Tables and Simple DB) can scale, but they have down sides.  Reddit recently upgraded to handle some scaling issues.  They had to make changes to the way they handle storage, not web.  Reddit, Amazon, Google, Facebook all had to come up with solutions to storage, and they are all non-trivial.  I made the mistake of starting a web application before I thought through storage issues.  Luckily, we coded in a way that we only had to change one file, but we changed our storage framework 4 times.  And we may still change it again.  ;TLDR Web is easy to scale, storage is hard to scale. "
"Your IT department is lazy and incompetent, or you are using a version prior to Notes 7.  Or both.","This is a configuration issue.  If you are using any version 8 client (I believe also version 7, but I don't have direct experience with 7), there is a feature you can select on install (I don't know if you can do it after install, it's never come up for me) called ""Client Single Logon.""  This basically creates a process that captures your Windows password when you sign in to Windows, and then automatically passes it to Notes when you launch Notes.  It detects if you have just changed your Windows password, and changes your Notes password to match if you allow it to.  Also, if it passes the Windows password and it fails, and you haven't just changed your Windows password, it will ask if you want to change your Windows password to match your Notes password.  It will change it automatically for you if you let it.  I work at a place where we are required to change our Windows domain account passwords every 90 days, and the ""Client Single Login"" has worked just fine every time that's come up.  Enabling this feature has made me the hero of my users.  As far as the web mail is concerned, it is bizarre your Notes admin has not set the web login to be the same as your Notes ID file password.  It is a simple option you can set on any user's ""Person"" document to make their web login the same as their ID file, and keep up with any changes to the ID file.  Unless your web mail is not using your Notes mail system, then all bets are off, of course.  tl;dr - Your IT department is lazy and incompetent, or you are using a version prior to Notes 7.  Or both. "
My advice would be to learn (even if it's only the basics) of some other programming languages such as Ruby/Obj-C/C# and learn how to be a better Kohana developer.,"I've been keeping half an eye on the Fuel development for a while, mainly via tweets from some of its more prominent developers. From what I gather, they've taken what they class as ""the best bits"" from KohanaPHP and CodeIgniter and then tried to add some of the magic from Rails after one of the developers tried to get into RoR and struggled.  I nearly tried Fuel, I probably should have done (I'm a Kohana developer) but instead I got into Ruby and played around with Rails and Padrino instead. I have found that Ruby is quite a steep learning curve although it'll probably be worth it in the long run. However, what I've found since beginning Ruby development as well as learning Objective-C is that I now write much tidier PHP and structure my code to a much better standard.  TL;DR My advice would be to learn (even if it's only the basics) of some other programming languages such as Ruby/Obj-C/C# and learn how to be a better Kohana developer. "
"Grow up, figure out your project requirements, use what you feel you master the best, consider what your team masters/will master best.","PHP just don't look cool. It has an old, but loved by many, java/javascript/c++/{}-like syntax. It has it quirks (that in my opinion is overstated), slow but steady development, and it's been around for some time.  I'd say that if you use PHP, you just want to get your shit done in a good, flexible, and timely manner. Not to brag about how cool your infrastructure is cuz you sprinkled it with random missplaced Javascript-snippets, never-to-be-released Railz forks, ultra fast asynchronous pidgins, and UDP-based deploy monkeys.  Grow up and realize that what ever tool you use, it will only suck as much as you do.  Ok, 'nuff with the non-constructive rant. If you are trying to figure out what to start your next project on, first figure out what requirements you have.   Lifespan of the project  Is it a simple one time project or huge multi-year project? If you will need to maintain it for a long time, make sure to use something tried and tested and well supported. Don't go for the experimental stuff. Before you know it DatLanguage(TM) will be extinct, along with all the developers.   Will you be working with a team?  Use something that YOU and most developers can master. If you got a start-up on your hands. Tying to find a skilled Ruby developer for a good price will be HARDer, compared to a skilled PHP developer.   Will you REALLY need that super experimental feature that makes you believe that the bleeding edge ""flying sharks with lasers on their heads""-interpreter is needed as a dependency?  It's very easy to over-design, and over-elaborate your ideas, because, well, lets face it, we're geek. We LOVE bleeding edge technology, and desperately rationalize reasons for us to incorporate odd technology.    Their is a lot more you can think about. But whatever other start-ups use as their environment of choice, is completely besides the point, and only reflects whats ""HOT RIGHT NOW"", and just like the new trendy handbag it will be out of fashion before you even get to use it.  TL;DR Grow up, figure out your project requirements, use what you feel you master the best, consider what your team masters/will master best. "
"For me, a ""top programmer"" is not only a good programmer, but also a good teamplayer who embraces critique and communicates to achieve the best outcome possible.","Nobody. There is no ""top php programmer"". There are some ""gurus"" who post and blog around on the net, but i pretty much met the best people during work and they don't blog anything. I don't look up to him, but i recognize that Sebastian Bergmann (the author of PHPUnit) has done some great deeds for the PHP Community with PHPUnit and his book about professional PHP 5 development. I respect people for their commitment, not necessarily for the skills. I also respect the authors of Symfony, Laravel, ZF and basically every other open source PHP project. Because they work for us, and i know how difficult it is to do open source work alongside your normal work.  What makes a top programmer for me? The best thing you can do as a programmer, is communicate with every other programmer, to gather different opinions, share experiences and simply work together.  However, to be a good programmer, being able to code very well is only 50% of the job. The other half, is communication, commitment, pragmatism, reliability  and creativity. I know many people who write good code, and even have good architecture skills, but totally lack the ability to communicate with other people (other developers included), amongst other things. Social skills are, in my opionion, the greatest problem in the development community. Because a lot of people don't like to receive critique, or / and are very self-indulgent and most important, are no real teamplayers who see the goal, rather than building up their own reputation.  TL;DR: For me, a ""top programmer"" is not only a good programmer, but also a good teamplayer who embraces critique and communicates to achieve the best outcome possible. "
"C++ can rock for web development, provided you know how and where a network-connected program is vulnerable.","IMO, C is too primitive for the task without adding a lot of infrastructure... but C++ can be amazing at this.  You just have to be really good at knowing where and how things can go wrong, and adopt stringent practices to avoid those mistakes.  C++11 with standard containers, RAII, try/catch, and move semantics, are a good start.  The problem is that interpreted languages (php, python, java, etc) make it nearly impossible to make dire mistakes by accident.  This is made worse by the fact that C doesn't have a lot of those niceties, and C++ isn't taught that way in school (yet).  Directly compiled languages like C or C++ make it very easy to create buffer overruns, resource leaks, and pointer mistakes.  This is where the developer needs to think like a systems engineer (stack, heap, memory locations, handles, etc) , and not a web developer (moving files back and forth over HTTP, data integrity, i18n, etc). So one could argue that such languages are not a good fit for the problem domain since you really want to abstract all that stuff out.  That said, interpreted languages either compile to machine code (JIT) and/or integrate with c/c++ system libraries at some point.  So technically speaking, they have all the same vulnerabilities too - they're just a lot more obscure and harder to reach since there's an extra layer of technology in the way. Plus you don't have amateur engineers creating brand new holes all the time.  tl;dr: C++ can rock for web development, provided you know how and where a network-connected program is vulnerable. "
Lines of code is a pretty useless metric for complexity.,"In my programming languages class we built the same project five times, each in a different language.  It took four lines of code in Prolog and about 200 in Cobol.  When you remove context information, like the language, and just plot ""lines of code"" you don't really end up with a very good measure of the complexity if the software.  Also, there's situations where some code over here generates more code over there, which number do you report?  For example, suppose you write something in PHP, it outputs to HTML.  It's not unreasonable to think that 100 lines of PHP could support say, ten parameters, each of which, when toggled, generates a different batch of HTML.  Maybe there's 100 lines of HTML there.  So even though you've written only 100 lines, there are really 2^10 * 100 ~= 100 thousand lines of code involved.  Now, you're under pressure from somebody to deliver a working product, and it kind of sucks still.  Do you tell them that you've written 100 lines, and there are a few bugs, or do you tell them that you've written 100,000 lines, and there are a few bugs?  tl;dr  Lines of code is a pretty useless metric for complexity. "
"You can't mine for shit, but you could crack someone's private key and steal their wallet contents.","No. The hash algorithm being used is fast to execute on optimized hardware, but still too mathematically complex for quantum computing to be helpful.  That said, if you want to use a quantum computer to get bitcoins, there is a strategy you can use. Remember, bitcoins are secured with  asymmetric keys , which have a public side and a private side. The reason you can hand out the public key without worrying about people using it to determine the private key, is because to do that, an attacker has to do a really hard prime number factorization. But quantum computers are good at cutting the hard steps out of prime number factorization.  tl;dr: You can't mine for shit, but you could crack someone's private key and steal their wallet contents. "
Install a Microsoft plugin into Chrome to support a format that isn't open? I think not. I think most other people think not too.,"I fail to see the point. Google removed support for H.264 from Chrome because it is a patent-encumbered format that Google is not interested in promoting as an ""open standard.""  Microsoft responds by developing a H.264 plugin for Google Chrome, enabling compatibility with the non-open format for those Chrome users who choose to install it.  I have a feeling this is going to turn out to be a bit of a damp squib for Microsoft. Yes, Chrome users can again view H.264 in Chrome by installing a Microsoft plugin, but I assume there are a large number of people who use Chrome for the same reason that I use Chrome: I prefer a non-quirky, stable browsing experience based on open standards that behaves the same way across multiple platforms. Therefore, I simply will not install a Microsoft plugin aimed at backing Google into the corner of accepting H.264 as an ""open standard."" I presume most other Chrome users would probably balk at installing it too.  TL;DR - Install a Microsoft plugin into Chrome to support a format that isn't open? I think not. I think most other people think not too. "
Other systems had the automatic install part down.  Linux added the central repo and automatic update part.,"> > so third parties were able to create  packages  device drivers  > FTFY.  Read the doc.  It's about packaging device driver binaries after you've built them.  It's not about creating a device driver.  Or, read  this old Sun doc , which shows how to use a System V package to modify /etc/inittab.  And to run ""init q"" to make init re-read the file afterward.  Note that this is the ""Application Packaging Developer’s Guide"".  It's not for system software.  It's for third-party applications.  (It also covers how great and convenient it is to distribute your software on CD-ROM.  And it covers CD caddies...)  That doc is from 1994.  Unfortunately, I can't find a copy of the older documentation, part number 800-6347, the ""SunOS 5.0 Application Packaging and Installation Guide"".  However, if you want, read the [this page]( which contains material from June/July 1992, including the Solaris 2.0 product announcement and a SunFlash write-up, which says:  > A new package interface defines a consistent mechanism for installing and managing all software, including third party products.  Sun delivered a working package system in mid-1992 which supported installing executable, device drivers, inittab entries, and really whatever you needed, and which was intended to be used by both the system and third-party application developers.  Yes, Linux certainly took that idea and ran with it.  The Linux community took the idea of ready-made binary packages and extended it to include a repo and an integrated and automated download/distribution/install mechanism.  That second part was new with Linux (and sunfreeware.com and similar tried to copy it), but the actual installation part had already been developed.  TL;DR:  Other systems had the automatic install part down.  Linux added the central repo and automatic update part. "
"Computer Science is not alone; Library Science and Poli Sci have the same terminology thing, and it makes sense because ""science"" means more than just the scientific method.","IMHO, Vint Cerf's article isn't really  about  the terminology; he's just using the terminology to introduce the question of whether scientific/quantitative things can be applied to the field in certain ways.  But, people are talking about the terminology a lot, so I'd like to point out [the definition of science](  Specficially, compare meaning 2a:  > a : a department of systematized knowledge as an object of study <the science of theology>  against meaning 3a:  > a : knowledge or a system of knowledge covering general truths or the operation of general laws especially as obtained and tested through scientific method  It is the first sense that the word is used in ""computer science"".  Compare the names of these fields:   Physics  Chemistry  Biology  Geology  Astronomy   with these fields:   Library Science  Political Science  Environmental Science  Computer Science   The fields in the first list are about the scientific method, and none of them include the word ""science"".  The fields in the second list are not about the scientific method, but all of them do include the word ""science"".  TL;DR:  Computer Science is not alone; Library Science and Poli Sci have the same terminology thing, and it makes sense because ""science"" means more than just the scientific method. "
Kotlin tries to fix as much Java design mistakes as possible while still having [interoperability]( Xtend tries to fix Java verbosity but does not touch semantics.,"From a glance, it seems that Kotlin does not fear making some big changes to Java, such as: no fields, no type erasure, a module system, no static members, nullable and non-null types, etc...  Xtend takes a more conservative approach. It doesn't try to fix Java. There is still type erasure, there are fields, static members, there are primitive types. Instead, it tries to make it less verbose (and does a fine job at it). Due to this, Xtend is 100% compatible with Java and compiles to Java source code (and the Eclipse plugin lets you examine the generated source to see what it's exactly doing).  So tl;dr: Kotlin tries to fix as much Java design mistakes as possible while still having [interoperability]( Xtend tries to fix Java verbosity but does not touch semantics. "
"Learn how Linux is distributed. Learn the difference between the Kernel project and Linux distributions, and Open SSL. Learn more about what a library is and what it does.","> Considering the warm welcome Theo always received from the Linux devs I don't think OpenBSD gives a flying fuck about sharing upstream and sorry to say it but I think they're right in ignoring upstream and let e.g. Linux figure it out themselves: if they want to use it, fork it and contribute, not the other way around.  OpenSSL is not the Linux kernel project.  > I mean: every Linux distro is affected by the heartbleed issue. Have you seen any corporate paid Linux kernel dev take responsibility and do something about it?  I repeat, OpenSSL is not the kernel project. And the kernel project is separate from any distribution which chooses to distribute OpenSSL.  > So please enlighten me, why would OpenBSD make sure the corporate paid devs in the Linux camp have a field day and reap the benefits of OpenBSD volunteers who have a hard time keeping their own servers running?  Why would they be spiteful about it? Also secondly, OpenSSL is not a project undertaken by any of the major Linux companies. In fact interface compatibility benefits OpenBSD because it means that their new library could act as a drop in replacement with no recoding necessary on other pieces of software.  TL;DR: Learn how Linux is distributed. Learn the difference between the Kernel project and Linux distributions, and Open SSL. Learn more about what a library is and what it does. "
"I don't like the word  battle  in this context. 
 Sorry for the (kinda off topic) rant. Your link is fine. Here, have an upvote :)","There is no battle. People have to maintain legacy code bases or use forgotten libraries no one else cares about. That are the only valid reasons for 2.x-only development and that reason will not go away any time soon. If you write new applications, use Python 3 only. If you write new libraries, try to support both but default to 3.2+ because that is available everywhere. It's that simple.  People that choose Python 2 for their projects with no valid reason just because they don't like Python 3 are lazy (the bad kind). People that brag about it are hipsters. People that write 2.x only libraries just don't want their libraries to be used. Leave them behind. There is no war, or battle. Just progress. Nice and slow and steady progress.  Progress needs time. And I am very happy that the Python development takes its time where it is important. Think about it. We currently have two very stable and usable branches of a great language, and an almost flawless history of backward compatibility on both branches (3.0 does not count). We support legacy code a LOT longer and better than other languages or frameworks. You can always just upgrade your Python (or operating system) and everything keeps running. You get new features for free. Try that with Ruby. You can almost throw away your big RoR application after a year because porting would be a nightmare (correct me if I'm wrong).  It's progress. No one gets stabbed or shot. We are nice people and there is no deep and frightening split in the community (hey Perl guys) or anything like that. Stop overdramatizeing, start porting. Just go on and enjoy.  tl;dr  I don't like the word  battle  in this context.  Sorry for the (kinda off topic) rant. Your link is fine. Here, have an upvote :) "
"Happy S60 smartphone user, and content AT&T customer ( shock, gasp, horror ).","A few months ago I replaced my ancient (six year old) Audiovox CDM-9500 (Verizon) for an unlocked Nokia  E63, and I am quite happy with it.  I know everyone here likes to hate on AT&T but where I live (in the sticks), AT&T gets better, and more consistent, coverage than Verizon, as well as coverage in places that used to be Verizon dead spots.  [/tangent]  It did take some trial and error to find applications that suited me, but the ones I did find were great.  The built-in SIP client is great to have when I'm home and don't want to use prepaid minutes.  Opera Mobile 10, Nimbuzz for IM, TweetS60 (for reading various twitter feeds; I don't twat anymore), plus the Google Search, GMail, and Maps applications, as well as PuTTY and tGUI (Transmission torrent client GUI) are invaluable for me.  The shortcomings of the S60 platform, or the phone itself (lack of GPS, or auto-focus camera as found in the phones big brother the E71) don't really faze me.  tl;dr Happy S60 smartphone user, and content AT&T customer ( shock, gasp, horror ). "
Don't change/hinder the OSS movement. It will have adverse effects.,"I can guarantee to anyone, that if it becomes difficult/impossible to get/develop Free Open Source Software, piracy of proprietary software will skyrocket. Many people who can't afford, or just don't want to buy software at some of the crazy prices these companies charge quite often make people turn to the open source community. Open Source software has it's place, and so does proprietary software. I will buy the software if I believe that it is worth it, and if there is no free replacement that will do what this software does. I will not buy it when the price that the vendor charges will not reflect the amount of use I get out of it. I won't pirate it either (I don't condone/encourage piracy). The software vendor believes that their software is worth the price that they charge for it. They have every right to charge that price. They should however have no right to hinder the open source movement. Be very careful about what you are trying to do (proprietary software companies). It could come back to haunt you. That is all.  tl;dr: Don't change/hinder the OSS movement. It will have adverse effects. "
After reading the post I'm pretty sure she's a lying bitch who has some sort of vendetta against the accused.,"If you're sexually assaulted, this isn't the way to go about it.  If she was sick of all of this supposed harassment why didn't she go to the authorities instead of outing someone on her blog only to start a large circlejerk among 'feminists' in the comments.  I don't know anything about either party, but I'm imagining this is her smearing this guy for professional reasons.  Oh and she's now apparently removed the post.  Looks like national attention is a little more than she wanted, and if she really wanted justification for a supposed assault she'd have left it there for the world to see, not take it down before shit gets more out of hand than it currently is.  tl;dr  After reading the post I'm pretty sure she's a lying bitch who has some sort of vendetta against the accused. "
I applaud the stand here - just question the timing.,"Wow an ugly end to a fun day. So sorry this happened to you (Norin).  It is accurate that it has nothing to do with the fact that you were flirting and wearing bike shorts... Someone who uses that excuse is just being disingenuous.  Really you should have gone to the police first, with witnesses and filed a complaint -  and published this after that. It sounds like you had a number of witnesses to corroborate your version. The fact that you went so public first might undercut any actual punishment.  One good thing is that if this happens again to someone else then there is a precedent. A bad outcome is if this turns into a libel situation and he is able to sue for damages.  tl:dr I applaud the stand here - just question the timing. "
"the fact that individuals are criminals or sociopaths should not be projected onto their entire gender and/or race. crime is bad, criminals deserve punishment.","I really wish people wouldn't lump half the world's population in with a relatively small group of criminals (sexual assaulters/sociopaths/rapists).  Maybe I am reading her intention in her blog post incorrectly. But this is viscerally how it comes off to me, speaking as someone who has never assaulted another human being (man or woman). Here is an analogous blog entry.  ""I saw a black person steal my bike yesterday. Due to the circumstances that I live, this has happened before but I am going to speak out about it this time. It is completely unacceptable for a black person to break the law and steal from me, I shouldn't feel like I need to specifically protect myself from having things stolen from me by black people. I know a lot of black people who don't steal from me, many black people didn't steal from me yesterday, but one did. It is the responsibility of black people not to steal from me, not my responsibility to protect myself from crimes being committed against me by black people.""  I'm not black and even I would be offended by someone lumping my entire group in with criminals in a public address (which is my visceral reaction to this blog post) which seems to target all 'black people' (people in the group who the offender belongs).  Again, I may be reading her intention in the blog incorrectly (though writing a blog entry to try to convince rapists and sociopaths that what they enjoy doing is wrong seems a little far fetched, for that reason it comes off as an 'open letter to all men'). Being assaulted, having crimes committed against you, really sucks and is horrible, but individual criminals do not represent a whole group of people, and that group should never be confronted in a way that implies they are all 'potential criminals'.  tl;dr : the fact that individuals are criminals or sociopaths should not be projected onto their entire gender and/or race. crime is bad, criminals deserve punishment. "
"There are benefits of ""everything is an expression"". Just because you don't know what they are yet doesn't mean they don't exist or aren't important.","There is a gain, you just don't happen to see it. For example:  x = if (y &gt; 10) then    dosomthing(y)else    yend  The fact that the ""if"" construct returns the value of the last expression run means we can do things like the above. If the ""y"" in the block had been ""return x"" like your way would require, it would return from the context and the code wouldn't work. Instead, we'd need to write  x = null;if (y &gt; 10) then    x = dosomthing(y)else    x = yend  The first version is more functional which, to many people, is a good thing.  TL;DR: There are benefits of ""everything is an expression"". Just because you don't know what they are yet doesn't mean they don't exist or aren't important. "
"We spend  too much time  a necessary amount of time logging time. 
 EDIT: readability","In theory, I fill it out several times during the course of the day so it's 1-5 minutes every hour, which comes to about half an hour a day. These are pretty detailed logs, mentioning specific tasks I did.  In theory, I fill it out at the end of the day, so it's 15 min/day while I review my ManicTime logs. These are moderately detailed logs, listing generally what I did during the day.  In practice, it actually gets filled out about twice a month (usually the 1st and the 10th).  That takes 2-3 hours each time and is mostly me scanning ManicTime and looking at folder names, so I can mark huge swaths of time as ""4 hr: this project. 1.5 hours: standing around in the hallway discussing implementations. etc."" These are sufficient for Accounting ... but not actually that great for reporting.  This task is allegedly worth the time, for estimating future project timelines.  They should be able to write gain/loss reports on it -- after all, we even have a time code for when we're logging time!  tl;dr: We spend  too much time  a necessary amount of time logging time.  EDIT: readability "
"ML allows nonalphanumeric characters in identifiers for user-defined operators, which is the most convincing use case for nonalphanumeric identifiers in the first place","Whoever wrote this doesn't have adequate experience with ML to judge it. You can use nonalphanumeric identifiers, but only in a special case where it is highly preferable (defining operators), e.g.,  let (++) (a: vector) (b: vector) : vector =    (* Body of function *)  This defines a function that operates on vectors, presumably to add them together. However, it works as an infix function (e.g., it would be used like  vec1 ++ vec2 ), which allows the user to define new operators. This also makes operators acceptable functions for higher-order function, e.g.,  List.foldr (++) base_vec vec_list  This is a real use case for nonalphanumeric characters (better than a ? or ! at the end of a function name) in identifiers where ML does allow nonalphanumeric characters.  BTW, this is all in OCaml, but the same applies (albeit with slightly different syntax) in SML.  tl;dr  ML allows nonalphanumeric characters in identifiers for user-defined operators, which is the most convincing use case for nonalphanumeric identifiers in the first place "
high dimensional objects are both really unintuitive and really awesome.,"A lot of strange things happen in high dimensions. For instance, imagine a cube 1 meter across. If you pick a completely random point in that cube, how likely is it that the point will fall at a distance of at most 1 mm from one of the sides? Not very likely; that's a very small volume and the probability is something like 0.6%.  But in high dimensions that's not true anymore. In 3000 dimensions, 99.8% of all points fall within 1 mm of one side. It's as if a package's wrapping, no matter how thin, eventually got more voluminous than its contents. It's not difficult to understand why, though: all it takes is for  one  coordinate (out of a whopping 3000) to be low or high enough.  It does tell us something interesting about hyperspheres, though: while a sphere may touch all sides of its enclosing cube, most of its surface won't be close to these sides. Indeed, the more dimensions a hypercube has, the further away from the center its corners get. Thus an increasing percentage of the volume gets stuck in places that a sphere, which has a constant radius, simply cannot reach. Therefore, the ratio of hypersphere to hypercube plummets (of course, you can also see that from their respective formulas).  As mentioned in the article, the ""caps"" of the sphere also get progressively smaller as a ratio. You can somewhat picture why if you consider the subset of caps that touch the sides of the enclosing cube: as dimensionality increases, there are more of them and  the most distant caps are progressively further away  (okay, actually they're not, the distance between unit vectors doesn't scale up with dimensionality).  Tl;dr: high dimensional objects are both really unintuitive and really awesome. "
"Computer Science is such a broad subject, I really don't think something this specific can be considered ""mandatory knowledge""","We did deal with MIPS and had a mandatory architecture class in second year, but as stated by other people who have taken computer science, this one example would likely have just been one or two slide in a list of topics covered in one lecture.  But at this point it would be 4... maybe 5 years since the last time I dealt with topics that low level.  Personally I was much more interested in Functional Programming languages, lambda calculus, and things like Discrete Mathematics.  I really think that a lot of people are really up playing the importance of this one tiny little topic. I also would hazard a guess that people who took more compiler and machine arch courses in the upper levels may have dealt with this topic again reinforcing the learning they did in the early years as opposed to those of us who remember writing a compiler for lambda expressions in second year while writing three pages of calculus trying to create a Hermite interpolation spline for a set of points by hand.  tl;dr: Computer Science is such a broad subject, I really don't think something this specific can be considered ""mandatory knowledge"" "
"I hope the idea behind it will succeed, I don't care if Firefox OS itself will succeed.","I think that Firefox OS itself(!) has its market: One market are curious (web) developer, as well as countries in South America and Africa:   Because it's cheap   Instead of Android newer versions of the OS will (hopefully) run on cheap cellphone.   The idea behind Firefox OS is to use one technology stack for your app for all platforms. Today on cellphones we are back in time where Desktops doesn't had browsers:  Instead of surfing to facebook, twitter, google maps or whatever, you install a programm on your PC for every one of these sides.Today on cellphones you have your facebook app, your twitter app etc. .  Even worse you have to implement the App for every OS (which means atm Android & iOS but someday maybe more).Switching from iOS to Android and vice versa means you have to buy your apps again. Afaik not just because the app is a different one, but instead - at least on iOS - it is prohibited to make your app for free and after you install the app you buy an account. That's of course on the one hand because apple wants the 30%(?) of the revenues, but it also blocks to have 1 purchase for Android & iOS Devices.If your app is running in the browser this is not a problem: You can buy the app at the developer website and use it on Firefox OS, Android, iOS and your Desktop: Every system which has a current browser.  That means I don't care if firefox OS will get an market share <1%, as like Chrome OS has probably a market share below 1%. As long as it (it = the mobile browser) has the critical mass of developers who developer there apps for (mobile) browsers.  Firefox OS is (just) a good promotion for the mobile web. As 100%  e-cars today aren't supposed to be mainstream, but to show its possible and to push hybrid cars.  On the other hand I understand people who complain about Firefox OS. The Firefox browser lose grounds towards Chrome and now Mozilla is splitting its resources apart.As well as the question if the critical mass of developers are willing to use HTML/CSS/Javascript to achieve platform indepence, instead of using Java/Android SDK or Objective C & iOS SDK.  tl;dr  I hope the idea behind it will succeed, I don't care if Firefox OS itself will succeed. "
Backbone is an excellent un-opinionated library but I found it necessary to provide convenience features to bring developer efficiency up to the level of some newer frameworks.,"I've been working on a medium sized SPA for a few months (30k loc of js/600+ modules) and I'm very happy with Backbone. To allow sane nesting and to keep things organized I added a few features on top of Backbone's core. I can share APIs but unfortunately my bosses aren't fond of the idea of releasing OSS.  First, models and view models are packaged into components wrapped around a constructor function. The view model is automatically chained (more on this next) to the view so it gets destroyed when the view is removed. Components can be inherited by calling  .extendModel()  and  .extendView()  var myFoo = utils.component()  .extendModel({/* new properties for model prototype */})  .extendView({/* ... */})myFoo({/* model attrs */}).render().$el.appendTo('body')  Destructing is handled similarly to QT using chaining. New objects are chained to their container. This attaches a handler to the destroy/remove event that invokes the child object's destroy/remove method. By doing this, destroying a container recursively unloads all of its descendants.  render: function() {  // render template in this.el...  myFoo().chainTo(this).render().$el.appendTo(this.$('.target'));  return this}  It's often the case that API calls will return data used in multiple screens. For example a list of countries. In order to minimize coupling while providing shared access to these resources they are treated as quasi-singletons with reference counting garbage collection. Components capture and release the resources and fully disused resources are destroyed. Capturing works like chaining to automatically release resources .  initialize: function() {  Countries.capture(this).ready.then(_.bind(this.initCountries, this));}  There are a few other helpers sprinkled in sparingly like getter/setter methods for promises, configuration loading, i18n, etc...  tldr;  Backbone is an excellent un-opinionated library but I found it necessary to provide convenience features to bring developer efficiency up to the level of some newer frameworks. "
"What you're doing has little relevance, your  experience  is what counts. And any recruiter worth having knows that.","I wrote a crapload of C say 10-15 year ago. Nowadays I do JS/PHP/C#.  Am I a worse programmer now than I was 15 years ago? Fuck no. I just use different tools for other purposes, I'm a much better programmer now than I was then, and a lot of that has to do with the very differing environments I've had to work in since I've used so many different tools (over the years, it's not that many if I just list 4 of them).  I've seen some hardcore, to-the-bone embedded C programmers who are, after years and years in the game, doing nothing but copypasta with overflows and holes all over the place. But hey, it's C, so it's lower level than C#, so it must mean that he's a better programmer..  Right?  tl;dr  What you're doing has little relevance, your  experience  is what counts. And any recruiter worth having knows that. "
"if you've got an imagination, this game is like crack.","The free version you can play in the browser is Minecraft Classic's Creative mode, an earlier iteration of the game that's essentially Legos on crack -- only really good for building things.  It lacks the features of the alpha (which you have to pay for); Alpha adds survival horror to Creative's original sandbox.  You don't start off with any bricks or other items; you have to gather them all yourself.  There's a day/night cycle; during the day, you have to scramble to get resources and build shelter before night falls and the enemy mobs come out -- there are zombies, skeletons, spiders, skeletons  riding  spiders, and kamikaze exploding creepers.  Once the first day has passed, you generally go exploring the terrain; the terrain generator creates everything from underground caves to sky-high cliffs, all hiding resources for you to find and harvest.  Gold and diamond glitter in the bowels of the world.  Cows, sheep, chicken and pigs provide leather, wool, feathers and ham.  You can plant wheat, reeds and cacti for food, decoration or defence.  Lava and water can kill you (digging yourself a 1x1 hole all the way down is tantamount to suicide), but you can also harness them to build some neat stuff.  [Here's a picture thread showing what people have found or built.](  Beyond that, the goal of the game is entirely up to you; there's no set objective.  Be a spelunker, living entirely underground.  Build a city with the stone you've mined from the side of a mountain.  Construct a floating fortress on an island in the sky.  If you're in Alpha's multiplayer (it's new and buggy), people sometimes cooperate on large building projects.  I'm probably forgetting a ton of stuff and completely failing to do Minecraft justice, but:  TL;DR: if you've got an imagination, this game is like crack. "
May you call a thousand methods if it saves touching the disk.,"Very interesting, thanks.  Related anecdote: I was writing some Perl code a few years back and was getting a bit concerned about the efficiency of a particular bit of code.  There was one method right at the heart of my library which got called loads of times.  To keep my code elegantly simple and easy to maintain, the parameter validation was handled by a separate schema-driven validation system. That resulted in a whole bunch of calls to other bits of code, which called other bits of code and so on.  It seemed rather wasteful to me that I should be calling so many different validation methods before I could even start executing the main body of code.  So I stopped for a coffee break and pondered the problem for a while.  To cut a long story short, my facepalm moment was when I remembered that this code ultimately resulted in rows being read from a database.  A quick benchmarking script showed me that I could easily validate a few  thousand  parameters in the time it took to read a single byte from disk (although it should be noted that the actual timings varied wildly due to various caching effects).  Of course we all know about the dangers of premature optimisation, but this one really brought home to me just how pointless optimising code can be when there's the elephant of disk access in the room.  TL;DR:  May you call a thousand methods if it saves touching the disk. "
"learn to speak the language of business, and learn to be a confident professional. You'll make more money.","Programmers are (typically) bad at two somewhat related things: 1) communicating their value, and 2) presenting value in usable terms for their audience. My experience is all anecdotal, but fits trends other people report.  Allow me to explain. 1) covers the usual fact that the programmer stereotype isn't the most socially adept. 2) covers the fact that programmers usually want to speak in technical terms.  Combined, you end up with someone who isn't particularly good at selling themselves, and when they try to demonstrate value, they're presenting ""my changes saved, on average, x percent load during peak processing times."" Unfortunately, this does not tell the people who sign the checks what the real value is - we saved $y on capital expenditure for new servers, or we avoided z amount of downtime.  tl;dr: learn to speak the language of business, and learn to be a confident professional. You'll make more money. "
Good developers are a dime a dozen compared to truly good PM's.,"I worked at a company where one of the two PM's was pretty much terrible. No matter how long she was there (even after a few years) she was never able to learn even the basic outline of our web development process. When she told clients wrong information (which happened often) or give promises for work outside the document scope and without consulting the developers (""yes Wordpress can easily be used to manage a social network"") she would go out of her way to cover her ass and blame the developers. The one thing she WAS good at was bullshitting. Her clients loved her no matter what she did to screw them over, and she had the owners of the company fooled for a looooong time until I and a co-worker started bringing up all kinds of stuff to the owners. The owners listened, but didn't immediately move to do anything.  I finally decided to leave that company because I was sick of working 60-70 hour weeks to make up for her idiocy. Turns out I was able to get a new position relatively quickly. Turns out right after I had a new job offer, she announced she would also be working elsewhere (FFFFFFUUUUUUUUUUUU).  When the company started hiring for our positions, we had several decent development prospects with good sets of skills. For project management, there were also several applicants, but only one was even remotely right for the job, who had experience managing development projects, and even had a little web development experience. Enough to know how the process worked, but not enough to think he could outsmart the developers.  So, for the market I was working in at the time, that's my anecdotal evidence to say a good PM is hard to find, because there are sooooo many bad ones, and if they don't have the requisite skills out of the box, they usually can't be trained. But a bad developer can usually be trained to be a good developer.  TL;DR Good developers are a dime a dozen compared to truly good PM's. "
"our governor has a profit motive in drug testing people; drug testing is seen as a way for (usually white) well-to-do to stick it to (usually non-white) ""lazy people"".","Forgetting any constitutional notions on when you may search one's person (which by any real job, we seem to give up on these days); it has been expected that the people applying for benefits, might also pay for said tests. And here in FL, its been an issue because #1, attempts to test have been tossed out in court before; and #2, the governor's interest in his Solantic health clinics, which do drug testing as a paid service, creates a potential conflict of interest. On a side note, the governor has been opposing ""Obamacare"", and holding out on implementing rules here in Florida: I am one to think that since Solantic and other clinics serve the uninsured, he wants people to remain uninsured as a profit source.  Its also a given that employers kick into said unemployment benefits; you have to have been working for most of the past several years to get them;  and people are not making their old salaries with them; so costs to the state are already mitigated to some degree. Having the state, or person, pay for another means test for assistance; is either meant to discourage their use, or a backhand on the (usually non-white) ""lazy"" poor (all the potheads and pain pill users I know have been white).  TLDR : our governor has a profit motive in drug testing people; drug testing is seen as a way for (usually white) well-to-do to stick it to (usually non-white) ""lazy people"". "
"Hey. I can write poor analogies of code writing to extraneous parts of my life, can I have excessive karma, too?","Come on man. Sure, there are people who just ""don't learn"" beyond a certain point. And there are people who, as the author cites, ""just don't interact with other people in the community"", despite their desire to do so. You're really going to lump the two sets of people into the same group? My experience at university? If you weren't ""smart enough"", ""just didn't get it"", etc. you were ostracized. Seriously, I went into studying Zermelo-Frankel set theory and transfinite numbers because it was easier to learn in a group setting than a lot of the algorithmic courses I was studying as part of computer science. To this day I'm still stunned by how readily software engineers simply clam up and shut down at the sight of someone who earnestly wants to learn but in many ways doesn't even know where to begin. It's almost as if they're saying  amateurs. thank goodness I don't need to work with or educate these unwashed heathens . It reminds me of the old joke that the best way to get an answer to a thorny technical problem is to post in the right usenix group that thing A simply cannot be done; you'd get at least four or five solutions from gurus who just want to prove you wrong.  ultimately, posts like this are not helpful. Because, as other people here have posted about the impostor syndrome, a lot of people read this and think ""hey, that's me!"" But aren't  really  sure it is. So they want to know if they can identify themselves as one of these ""expert beginners"", but more importantly, how they can untrack themselves and move into this author's elite, revered space of ""truly competent""; because obviously that's what we're all striving for. But there's none of that; just a sniggering ""hey, all these d-bags think they're  really  coding, when in fact they're lower life forms"" type of blog post that offers no way to fix the problem.  tl;dr: Hey. I can write poor analogies of code writing to extraneous parts of my life, can I have excessive karma, too? "
"When you're just starting out, people would rather see you know  programming .","> I don't know what it was that they liked, but I am thankful to be at a job where I am constantly challenged and can expand my knowledge base every day. I still have a long way to go, but I am loving the ride now.  Perhaps I can answer that, having interviewed people myself :-). If you are fresh out of school, what people are really looking for is to ensure you are a quick and able learner and that you have your concepts and theoretical basics in place.  Everyone (well, any  smart  employer, and they're a minority nowadays) realizes that, given your relative lack of experience, you won't be able to just sit at your desk and churn out a new, beautiful kernel in two weeks. But if they hired you, they also realize that at this point it's not your lack of skill that keeps you from doing that, but rather natural issues like that maximum amount of code you could efficiently organize and work with and other things that come with experience and practice. Languages, frameworks, tools and even  some  concepts are things they can help you learn at your new workplace, and chances are you won't even wait for them to do so and you'll just hack at some of them in your spare time, for the fun of it (and, in fact, you'll probably do that for the rest of your life anyway). It's the supporting knowledge behind it that they would rather not teach, and for good reasons.  tl ; dr When you're just starting out, people would rather see you know  programming . "
"this legislation sounds good,  but it seems to me the downsides outweigh the good.  Maybe that's why companies that have actually fought cases to reform patents opposed it.","Was this actually good law? It seems to me that more challenges the patent office can't handle won't fix the problem.  How do you work out which patents are bad and should be reviewed? Can anyone challenge any patent this way? How many times? Who pays? What if the big companies challenge the little guys patent?  Patent reform is very necessary, but is this good reform? Wouldn't getting more money and better rules to the patent office for the initial review process be better? Or making it easier to work out if something is patented or not? Maybe if software patents are the problem we need some legislation limiting them or shorter patent durations.  TL;DR this legislation sounds good,  but it seems to me the downsides outweigh the good.  Maybe that's why companies that have actually fought cases to reform patents opposed it. "
"I dunno. Maybe ""don't hate the player, hate the game"".",">We had this discussion at work.  Halfway through, the following phrase lept from my mouth:  >> Because no good thing ever came from the thought: ""Hey, I bet we can write a  better  memory management scheme than the one we've been using for decades.""  Sigh . I wrote a custom allocator for a fairly trivial event query system once.  I built the smallest thing I could that solved the problem. I tried to keep it simple. We cache the most recent N allocations for a number of size buckets. It's a bucket lookaside list, that's it. The idea was clever enough; the implementation didn't need to be, and it was about 20% comments.  This ultimately let to a 2x speedup in end-to-end query execution. Not 10%. Not 50%. 100% more queries per second, sustained. This took us from being allocation bound to not.  This gave me a lot of respect for the ""terrible"" code I sometimes see in terrible systems. I know that at least one or two ""terrible"" programs were just good programmers trying to do the best they could with what they had at hand, when doing nothing just wasn't cutting it. Could be all of them, for all I know.  tl;dr? I dunno. Maybe ""don't hate the player, hate the game"". "
it's not just about equal numbers for the sake of it.,"Ah come on now, you must agree that it isn't just a massive coincidence that 1/20th as many woman as men are interested in coding?  It's societal inertia. Coding is seen as a man's thing so girls don't cultivate an interest in it as children. There's no women in software firms so they feel out of place. Many software developers have never met a competent female developer (because there's so few female developers in general!) so they develop this ridiculous, gendered idea of what a 'serious developer' looks like....so it's not just a genetic predisposition to be disinterested in coding.  it's not people doing what they want to do - it's them doing what they want to do while being constantly pressured by society to do a some things and not other things since childhood.  Now, that would be all fine and dandy if this trend wasn't so harmful. Hell, I spend more time with my team than I do with either my family or friends and I can't remember the last time I worked with a woman. That can't be good for my professional development let alone my interpersonal skills.  Studies show that if you're single leaving college, 2 in 5 people meet their partner through work - that's a big hit for IT folks given how singularly gendered the whole industry is.  And software engineer became the most common profession in many places (namely many US states) over the past few years - trumping stuff like 'factory worker'.  If coding is the most common well-paid, middle-class job, and women are almost entirely excluded from it, that has a profound effect on our society and the equality of the sexes.  tl;dr it's not just about equal numbers for the sake of it. "
"Clifford Stoll was a hero of mine, and dispite this article, will always be.  Check out [this link](","Clifford Stoll was one of my minor-heroes growing up after I saw the NOVA show on his tracking of the East German cracker/spy.  It's somewhat perspective changing for me to read this article because in 1995, when I first attempted college, I had a 14.4 modem and dial in to the unversity as an ISP, and I found things online that I never could have imagined on CompuServe or AOL.  Access to the Internet given the simple set of tools on 4 3.5 floppy disks literally changed how I looked at information and media, in 1995.  Of course it was another 5 years until the work was done to realise this sort of vision, but I imagined what could happen, while Dr. Stoll, from his NSF-Net background couldn't relate how the Internet was traditionally used to how it could be used as a public and commercial entity.  My understanding as of late is that this is a generational norm found across many demographics, where the younger generations just 'get' some form of technology they are familiar with, and thus 'just use' the technology, where the previous generation tends to either want to learn the new stuff, or are indifferent to it since they have gotten along fine in life without it thus far.  Then I saw this: [this TED talk]( and realised he is as human as anyone else, and extremely introspective and honest.  So now, he's an even bigger hero of mine.  Thanks reddit for making me revisit my past and making me better for it.  TLDR  Clifford Stoll was a hero of mine, and dispite this article, will always be.  Check out [this link]( "
I can use both emacs and vi but want to use neither.,"I guess I am not made for vim.  I briefly used both emacs and then vi about 5 years back (before that, kwrite or nano). Then undergrad programming courses more or less required eclipse so I used that. After learning to hate Java and everything remotely related with a passion, my attention was again drawn to emacs. Over the next one or two years, I started to really get used to it until the point where the scrolling and saving keystrokes became hardwired in my brain. Even switching keyboard layouts from querty to [colemak]( on a short notice didn't really hurt my productivity, because the zxcv-row is the same in both layouts. So I guess you could have call me a happy emacs user.  Except I wasn't happy. I totally buy into the [suckless]( philosophy and emacs is pretty much hated there and for all the right reasons, too!  So in somewhat of a masochistic move, I decided to shun emacs and see if I can wrap my head around vi(m) instead. (Should be easy, right, I already switched my fucking keyboard layout to something no one has ever even  heard  of, I should manage to switch editors...)  Well, about two years later, I can't say my productivity in vi even remotely approaches the level I once gave up in emacs. The link above helped a bit (when it was first posted on reddit), but not as much as I would have liked. I am even thinking of switching back from time to time but am always discouraged by the idea of having to again put up with the Soviet Union equivalent of an editor.  Is there any better keyboard centered editor out there, which has a comparable set of features but maybe less bloat than emacs and a bit less awkward than vi?  TL;DR  I can use both emacs and vi but want to use neither. "
"look into TDD, it is worth it and isn't a trend.","It isn't a trend. PHP is just behind on picking this up as a development practice because, well, most people don't learn to test their code using unit tests when learning PHP.  TDD isn't that hard to do. The basic idea is you write your test for some small functionality, watch it fail, write code to satisfy the test, and watch it pass. This helps you determine what your code should be doing, helps isolate your code, and determine dependencies. You will increase your dev time slightly, but you will make up for it with less bugs, better documentation, and be able to more quickly identify and fix bugs.  Check out  by Chris Hartjes, one of the more vocal testing proponents in PHP. His current book shows you how to craft testable code, and he's working on a PHPUnit cookbook. He is also starting to offer classes about testing. I've got his book and it is a great read.  Keep in mind that TDD isn't always the answer. Check out BDD (Behavior Driven Development) as well, and look at the difference between unit tests and integration tests. If you have lots of legacy code don't try and force TDD into your project as you will most likely need lots of refactoring, which will discourage you. By all means make new code as testable as possible, but don't feel like you have to go back and reactor everything at once to make TDD worth it. One project I as working on has about 10 years of legacy code, so only the new functionality was written under TDD. I'll fix the rest as we get bug requests and I need to refactor.  TL;DR - look into TDD, it is worth it and isn't a trend. "
"great tool, but article showcases it in a wrong light.","> Several mutations might be done for that code:int foo(int x) return x+x;int foo(int x) return x-x;int foo(int x) return x/x;The first one will still pass, while the second and third should be successfully killed by the unit test. You would then have to write at least one other unit test to kill the first.  And that is a problem. Now we've got two passing tests, 100% coverage and implementation that violates the contract for most possible inputs. I mean whole idea of writing tests to maximize coverage metric seems wrong to me, you'll get a lot of tests, that you have to maintain, but little information about implementation.Actually, this approach gives zero information about implementation, so it's kind of wrong to declare that it helps to catch bugs, it doesn't. What it really does well is assessing quality of your unit tests, which is important.  tl;dr great tool, but article showcases it in a wrong light. "
"I had extract a ""sub-message"" from a multipart message and didn't feel like applying all of the standards and whatnot.","A use case I came across this week:  I had to parse an MHTML file (in other words a multipart message) and extract one particular ""sub-file"" from it. It was guaranteed to always be in the file I was searching, the sub-filename never changes. It is for internal use by two employees tops. There is nothing for this in .Net (I was using C# as per my boss's request). So I had to write a module myself for this. I  could  have made it cache other sub-files while it's a-searching, so that if I ever need a different sub-file, it doesn't have to search this  huge  MHTML file again. I could have it account for boundaries of multipart ""sub-messages"". I don't need all of that. While that would be perfect and awesome to have, I just need to do a quick scan. That's it. No need to write it properly and acknowledge the relevant RFCs and whatnot. That's just too much.  TL;DR: I had extract a ""sub-message"" from a multipart message and didn't feel like applying all of the standards and whatnot. "
"ADHD basically just makes me feel like saying ""fuck it"" to shit I don't want to do.","As an ADHD diagnosed person, I can tell you that having ADHD only seems to affect the shit you DON'T want to do.  If I'm into it, I get super focused, it could be anything from painting, to programming, to a good book.  The real problem comes in when you're focusing on something that doesn't feel ""rewarding"" much like mentioned in the OP's article.  I've been diagnosed since I was a kid, I've been unmedicated since about 8th grade.  I'm a full time self taught programmer.  I enjoyed computers, figuring them out, learning new stuff, etc. so focusing on them was ""easy"".  I however have been doing it for over 10 years at this point and the honeymoon would be over if not for the depth and spread of computer technologies.  I mean... at work doing my 9-5, it's admittedly very difficult to focus.  It's not fun, it's not challenging, and it's not entertaining... it's a struggle.  Oddly on the weekends or after work when I'm am doing self study on security, gray hat security, penetration testing, and all of the other stuff that now interests me but I don't get to do much at work... I can sit for hours on end reading articles and books without issue.  tl;dr ADHD basically just makes me feel like saying ""fuck it"" to shit I don't want to do. "
"Dollars to donuts you don't have OCD, you're just a pain in the ass.","There are two possibilities. One is that you have OCD. The other is that you don't.  This pisses me off. It's like saying you have ebola when you've just got a stomach ache. OCD is a crippling and debilitating mental condition. People who say ""Oh, I like my socks lined up neatly because I'm so OCD, lol"" are trivialising what is a horrible condition to live with. It's a life of routine and constant anxiety, ritual, psychosis, discomfort and deep psychological hurt. OCD is not a personality trait, it's a disease.  > I just always assumed and was told by friends/colleagues that it was OCD.  Unless you were told by a doctor, and are in intensive therapy and probably medicated  you do not have OCD .  What you are is fussy.  You like things to be a certain way. Some people are fussier than others. Some are very fussy. In conclusion: get over it.  You don't get to decide. You need to understand the purpose of code formatting. It's not to make things ""right"". It's to make them consistent. If your company or team uses a syntax, you  must  use that. If you're rewriting code other people have written to make it conform to your standards, you need to stop it, because you're wasting everyone's time.  If you genuinely do have OCD, get help. Seriously. Go to a doctor, go to a psychologist, and work on it. Mental health problems are no different to any other health problems, see someone, get it diagnosed and get it dealt with.  There are things you can do with IDEs now that will allow you to format code as you need it without wrecking it for the rest of your team. Some of these options have been suggested by other people. You may need to speak to your team leader about some sort of exemptions, or someone else working on your code to reformat after you're done. But the first step is to get diagnosed so that it's a health issue your team can work with you to resolve, instead of you just being fussy.  tl;dr Dollars to donuts you don't have OCD, you're just a pain in the ass. "
Do small side projects to gain experience. look at frameworks and realize how they solved problems with vanilla.,"Hi,javascript is amazing, you'll notice soon. Learning isn't sth done in steps, it's a really long progress. I'ld recommend you to do some side Projects. Small things you can finish within a day.  Try:  make a chat, todolist with vanilla js or frameworks you consider using for your job. So you gain in this framework and how others tried to solve a common problem. E.g. i struggeled with backbone one time, so I reinvented the wheel. Not for production, but to get a really deep knowledge how they did it and especially why.  Make small games. html5 and js is powerful enough to make small short games. It'll be fun for you and people you might show (coworkers family, friends). if they like it extend it with other technologys to make more content an online highscore.  Experience with new technologys. html5 is way more powerful than we are allowed to use by the force to support IE. This shouldn you prefenting to know whats could be done, only because it's not buissines relevant. Websockets, canvases, ecmascript6  tldr.: Do small side projects to gain experience. look at frameworks and realize how they solved problems with vanilla. "
"Write good code that follows DRY, SOLID, and the Law of Demeter while also being mindful of YAGNI.","This is exactly why I always say that the best architecture and design for code is an emergent property of solving the problem by writing good code.  I almost never know what I'm building when I code something (unless I've coded something very similar before) so often I'll either start with a very small piece and do something similar to TDD and just do some hand-waving in terms of dependencies, or I'll just brute-force-hard-code the whole thing and then go back and refactor to improve the code (which is precisely what the author of this blog post did).  This post and most of the other ""you should actually be coding like this instead of object-oriented"" brand of posts can all be summed up in one tldr:  Write good code that follows DRY, SOLID, and the Law of Demeter while also being mindful of YAGNI. "
I bet any academic in programming language research would answer this question with a firm  no,"Came here to say this. The very question being posed shows that the judge knows nothing about copyright  > Each side shall take a firm yes or no position on whether computer programming languages are copyrightable.  Ugh. What does that even mean if a programming language is ""copyrightable""? If I invent a programming language that uses accent marks as syntax, does that mean I can sue future programming languages that use accents? If I use an ""if"" construct in my language, am I violating Java's ""copyright"" because I ""copied"" a feature of the language?  What about having  functions  in my language? That surely violates  someone's  copyright, because there  must  have been exactly  one  company that thought very carefully about this problem and were the  first  to put functions into their language, and so they  deserve  to be paid royalties for the next 100 years because everyone else is  copying  them and benefiting from their  unprecedented  research. /sarcasm  tl;dr I bet any academic in programming language research would answer this question with a firm  no "
If you choose to use a module loader you should be combining all the modules to reduce http requests in production.,"For production you should be bundling your Javascript into 1 (or few) files.  The advantage of loading 1 JS file is not so much the added compression of having 1 file, the advantage is from reducing http requests.  That said there is a point where combining all your Javascript into 1 file is too large of a download.  At that point you can use RequireJS (or similar) to make separate ""bundles"" of Javascript where you have a bundle for initial load and bundles for each sub page or menu that can be navigated to later. (or however you see fit to bundle things)  RequireJS has r.js which makes it very easy to make and manage these bundles.  TLDR: If you choose to use a module loader you should be combining all the modules to reduce http requests in production. "
"the post is not about how jQuery implements promises, but rather how jQuery implements a deferred object based on the promises/a spec.","And you're missing the point of my post.  Please note two things:   That I mentioned jQuery's deferred implementation is BASED on promises/a -- not that it implements the promises/a specification  That I carefully refrain from calling anything returned by jQuery's implementation as a promise, instead called it a deferred object.   The title of the post might be a little misleading  (actually, the original title was, but I changed it before publishing to specifically not call it promises) but a careful read of the post will show that the point is specifically to talk about how the  $.Deferred  object in jQuery provides just what the piece you linked to says: a callback aggregator.  I do use the jQuery  $.when  and  .then  methods to demonstrate how simple it is to queue actions up to fire on a specific set of states; this is how jQuery's system is designed to work.  TL;DR the post is not about how jQuery implements promises, but rather how jQuery implements a deferred object based on the promises/a spec. "
People insist on the limitation to programming because people like you want it to be drawing and cello playing.,"> ""how to use Adobe Illustrator to create better web graphics.""  How are web graphics less specialized than OS features? Or embedded devices? Or anything you named?  They're not even related to being a programmer - at all. They're related to the assets that a program uses. You might as well say that people should learn to play the cello, so their theme music is better when loading.  More to the point, there's a lot to be said about programming and networking as they pertain to each other; this did none of that. Instead, it was just a pop culture joke that happened to happen at the networking layer. Nothing that could even remotely be applied to programming about the network layer.  If you want to see pop culture jokes about networking, how about going to a subreddit about networking?  tl;dr: People insist on the limitation to programming because people like you want it to be drawing and cello playing. "
"we need more IP addresses; you're right that pointing the finger at one group doesn't actually solve anything as ""we need more IP addresses"" is the bottom line.","This is a ""B"" class network. The MIT one is an ""A"" class network.  You are right that using up part of your A class network and getting in shit, you'd probably point to buddy's B class and how many IPs he's using.  The real long-and-short of the issue is that we don't have room. It's not a thing where we can make room. It's a thing where families now have an Xbox, PS3, Wii, 4 phones, 2 laptops, a PC, a wi-fi enabled TV, etc., and we quite simply are not going to have space for them all on the Internet.  Right now we solve that issue with subnetting, but eventually people will want all their devices, on the internet, all the time. And they won't want a home router slowing them down. I mean, really, why troubleshoot trying to get 3 Xboxes to work when you can just give them each a separate IP address and there we go.  tl;dr - we need more IP addresses; you're right that pointing the finger at one group doesn't actually solve anything as ""we need more IP addresses"" is the bottom line. "
No one in their right mind relies on GPL libraries.,"There are various good reasons for why people might be opposed to the GPL. That being said in any code that ends up being used in a networking context and web frameworks certainly fit the bill, using the GPL is a problem because the chance is fairly high that someone might want to use OpenSSL in some form. The OpenSSL license is incompatible with the GPL though so a project legally can't depend on both OpenSSL and some GPL library.  In fact pretty much any Python project that does HTTPS and has GPL dependencies is probably in violation of the GPL because the stdlib relies on OpenSSL.  TL;DR: No one in their right mind relies on GPL libraries. "
We all just eat free food while the robots do everything.,"Googler here:  One of the things that was really exciting to me coming to Google was seeing how they take these  massive  problems like web search running on giant numbers of computers and break them down into a bunch of separate projects that can be understood and managed by humans.  For every Google product you know of, there are hundreds of separate projects, each with its own codename, engineers, managers, etc. They are basically mini-products whose customers are in turn other teams.  The stack is unbelievably deep and it's really cool seeing it work.  The downside is that for a large fraction of Googlers there's no answer to ""what do you work on"" that would make sense to anyone outside of the company. You could say (these names all made up), ""I work on Polyphemus."" But what's that? ""Oh, it's used by Ichiban and Zygote to help interface with Hummus.""  After traversing that graph a few more levels deep, you eventually give up before reaching anything an outside user would recognize.  TL;DR: We all just eat free food while the robots do everything. "
"made little project in C, way too much work. stopped.","Yesterday (before seeing this article) I thought it would be fun to write a simple tool in C. I had been playing around with scripting languages, functional languages, higher-level C-based languages (like Objective-C) and wanted to go back to basics for an hour or so.  Boy, what a bummer. Procedural programming is like a completely different world again. And so much to keep in mind while coding. Refreshing, but writing C code is still a lot more work than I remember and had expected. I gave up when I had to think about how big a buffer to use for reading a file – not because I couldn't handle that, but because I felt like I was wasting my time.  tl;dr:  made little project in C, way too much work. stopped. "
"Some phones suck, some don't, but especially retarded are those that suck and have a lot of memory.","> For games, on natural transitions that require loading files, there's a good time to do a GC as well.  Those natural transitions may very well not occur at all. What we did on j2me phones is preallocating everything and then calling the GC every frame in the hope that it'd do a collection of the nursery and reclaim all that memory platform library routines allocated during that frame. Usually that worked, but not every phone has a generational GC and, worse, some of those would just ignore the call when they still had enough free memory. On those, the only way to prevent framedrops was to always use enough memory, so we'd allocate a strategically-sized byte array to fill it up.  tl;dr: Some phones suck, some don't, but especially retarded are those that suck and have a lot of memory. "
if Prezi doesn't actually have this patent they can get fined $500.,"If there really is no such patent, then they are in [violation of US patent law.](  >  Whoever marks upon, or affixes to, or uses in advertising in connection with any unpatented article the word ""patent"" or any word or number importing the same is patented, for the purpose of deceiving the public; or Whoever marks upon, or affixes to, or uses in advertising in connection with any article the words ""patent applied for,"" ""patent pending,"" or any word importing that an application for patent has been made, when no application for patent has been made, or if made, is not pending, for the purpose of deceiving the public [...] Shall be fined not more than $500 for every such offense.  tl;dr if Prezi doesn't actually have this patent they can get fined $500. "
My company thinks a senior software developer is really a project manager and that you should be happy you get to work for such a fantastic organization.,"I am in this exact situation now and I'll be in it for about another six weeks before I bail.  The interview was tough and I was selected out of a pool of six precisely because of my experience with emerging technologies and my experience as a team lead.  In the entire time I've been there I've been put on one software project and it lasted three weeks.  The extent of my work after that has been analyzing bugs.  Not even fixing them.  Just finding out where they're happening and writing them up for the H1B's to fix.  When I confronted my bosses about this strange setup, they looked perplexed.  One of them even said, ""Well, you are a senior developer.  What did you expect?""  They then started pounding the ""This is the greatest company in the world with the most upside potential you can find"" drum as if derailing my professional development was worth it because, gosh darn it, this is just a fantastic company to martyr yourself for.  Well, bosses, I expected to design and implement software projects.  Not write instructions for our halfwit, offshore, slave-laborers.  tl;dr  My company thinks a senior software developer is really a project manager and that you should be happy you get to work for such a fantastic organization. "
You are the sane man attempting to justify the actions of the crowd. This argument is better served against those who genuinely intend and aim to abuse IT workers.,"THere are MANY Exploitative firms out there, and if you wanted to take a measure of them PA would NOT be amongst them.  This post has EVERYTHING up front, and its not your typical game oriented firm either.  If you on the other went to join something like EA? Well thats abuse, and  everything you are saying would be valid  Don't you think - objectively - this entire issue is basically running a hate train on PA, and the point you are making is ancillary to vast majority of posters.  In this case the point is the pretext to be able to launch at a target.  And programmers, as sucky as it may be to get a bad job - still are in demand in this economy.  Woe be you if you are some field that got nuked in the past few years.  Look, you are obviously trying to be rational and you have a valid point.  Take an objective look at the context of this discussion, and see if the point makes any sense.  PA - is not your typical dev firm: as a dev you are not going to be helping create a software product to be sold.  PA doesn't operate like a typical conglomerate or programming house - and it does operate like a fantasy geekery team that somehow managed to survive.  Are the PA guys exploitative?  I mean fuck - if you think PA is exploiting you, then why the heck are all these people quiet with the 100s of other firms screwing people over on a daily basis?  TLDR:  You are the sane man attempting to justify the actions of the crowd. This argument is better served against those who genuinely intend and aim to abuse IT workers. "
"Open source is life on hard mode. 
 Nothing wrong with that. Just don't be surprised when participation is low.","In commercial projects, the onus is:  ""I pay you to deliver a quality product.""  But as many wannabe mobile start up business men have learned the hard way from hiring overseas contractors at the cheapest rate,  you get what you pay for .  There is no such onus in open source.  We like to think the pride of a project's members will ensure they always make sure they release quality.  That's a rather weak entitlement on the part of the consumer.  I'm just saying contributing to open source has people expecting to high quality code that  they didn't pay for .  All the entitlement from proprietary land, with no guarantee on the support to make it easy.  TL;DR:Open source is life on hard mode.  Nothing wrong with that. Just don't be surprised when participation is low. "
"In Guido's opinion, as I understand it, the benefits of tail call optimization are far outweighed by the cost in loss of debugging information.","I don't know why you were downvoted, you're pretty much correct. GvR doesn't like tail-call optimization:   it is only applicable to a  tiny  subset of recursive problems  it destroys backtraces as a diagnostic  most cases where a problem is better written with recursion, you only recurse a few times and tail call optimization is not a great help (I'm stating GvR's opinion, as I understand it, not mine).   Don't think of those cases where the traceback looks like this:  Traceback (most recent call last):  File ""&lt;stdin&gt;"", line 3, in factorial  File ""&lt;stdin&gt;"", line 3, in factorial  File ""&lt;stdin&gt;"", line 3, in factorial  [repeated 1000 times]  File ""&lt;stdin&gt;"", line 3, in factorialSomeException  Instead, you should think of the hard-to-debug cases where there are multiple functions, and some of them are mutually recursive:  Traceback (most recent call last):  File ""&lt;stdin&gt;"", line 7, in spam  File ""&lt;stdin&gt;"", line 12, in eggs  File ""&lt;stdin&gt;"", line 35, in cheese  File ""&lt;stdin&gt;"", line 11, in spam  File ""&lt;stdin&gt;"", line 11, in truffles  File ""&lt;stdin&gt;"", line 12, in eggsSomeException  In this case, collapsing the stacktrace (as tail call optimization will do) throws away useful debugging information.  TL;DR In Guido's opinion, as I understand it, the benefits of tail call optimization are far outweighed by the cost in loss of debugging information. "
"There is always risk, App Store policy is just another risk you have to manage.","> You shouldn't base your business on the iPhone because due to the agreement and app review process, you are not in control of the destiny of your business. This applies to everyone who makes iPhone apps, good or bad.  Wake up call.  You're never 100% in control of the destiny of your business: anything from dissatisfied customers spreading the word on the Internet, to being sued by various parties, to your staff walking out on you, to a large competitor creating an app faster and better than yours, to running out of money etc. etc. (not that any of that actually happened to me...).  The most you can do is manage the risk. Your linked article (""Note to Entrepreneur: It's Your Fault"") should really say: if shit happens to your company, and it will, it's your job to clean it up and not bleat on about it.  Yes, sometimes that means not getting into an area at all if it's too risky e.g. quitting the App Store.  But sometimes that just means putting in measures to reduce the risk. You could port to multiple mobile platforms, or make multiple apps for the iPhone, stay away from potentially troublesome areas in the App Store, leverage your iPhone expertise to do consult work or teaching, move to Mac OS X shareware since they share Cocoa fundamentals etc. There are so many creative ways to thrive in the App Store ecosystem.  TL;DR. There is always risk, App Store policy is just another risk you have to manage. "
a desktop/server OS multiprocess system is not suitable for a mobile phone. A mobile phone requires a different system. Android provided this. Apple did not.,Yes the iOS had multitasking but the point was it didn't have Android style multitasking which is still the only thing that works consistently in the mobile phone department.  The point is that the iOS had a system exactly like OSX. It did not have the carefully envisioned system that Android has which is designed for mobile devices. Apple were quite right to tell people not to use it. The system was not suitable. You need to be able to serialise and deserialise a process on demand to do it properly.  That people cannot see the distinction suggests they are ignorant of what these platforms actually do.  TL;DR a desktop/server OS multiprocess system is not suitable for a mobile phone. A mobile phone requires a different system. Android provided this. Apple did not. 
"Managed languages are slower than c++ in benchmarks, but they make developing fun, at least for me","All those answers forget the singular fact that a JIT compiler (be it the .Net one or the JVM) can produce code for multiple platforms.  Measuring c++ performance against a managed language on one platform only is pointless because one  big  strength of managed languages is that they strip away the underlying platform for good reasons.  Other answers are just as arrogant because they fail to account for structs that are present in .Net and therefor it's really easy to create a compact array of stuff that's packed together. Iterating over it shouldn't introduce any more cache misses than iterating over a c++ vector of structs.  The one person is correct that c++ templates have the benefit of introducing compile time polymorphism, ie. (for once) allowing polymorphism without virtual calls, he fails to mention that the c++ standard doesn't say anything about the ABI. If you want to stay compliant to the standard, then you can only use templates internally, relying on a good old c interface for the outside world.  My personal opinion is that managed languages are not slower because there's a JIT present, it's that they are way better defined than c++. There's no magic wonderland of undefined behavior that causes the entire program to fail randomly because one person screwed up at one place. However this does come at a price. Looking at some benchmarks done by Microsoft, the .Net runtime is outperformed by a pure c++ application in a lot of benchmarks. However it also shows that each version of the .Net runtime is improving it's performance.  tldr: Managed languages are slower than c++ in benchmarks, but they make developing fun, at least for me "
I was just trying to activate the reddit mind-hive.  :-(,"I know how VoIP works.  I sorta run one.   MOST  people who use/provide VoIP, do not pay by the ""unlimited channel"", and if they do, they do not have a few hundred.  So, a majority are per minute.  So, again, bogging down a couple hundred of their channels, even if unlimited, is still going to cost them either directly (per-minute), or indirectly (per-channel) in the long run when they have to expand capacity, and  pay  for more.  I am well aware of the ""normal residential use"" garbage as well, and doing this for a few hours at a time, does not fall outside of ""normal residential use.""  TL;DR: I was just trying to activate the reddit mind-hive.  :-( "
Most flash is done poorly and lazily by jamming all the content into the library instead of making it much faster with Actionscript,"I have to disagree with flash ""sucking"". Most people abuse the library in flash and bog down the loading with all sorts of shapes and objects that could have been drawn with code instead. Flash can be awesome if you can take the time to write it out with Actionscript instead of jamming everything into the library.  Don't get me wrong, there are things that need to be put into the library that would be too complicated to draw with code but most flash I see can have the loading times dramatically cut down by using code instead of abusing the library.  I worked on a site, that sadly never got finished and isn't up, that was completely done in flash and the loading time was only a few seconds because we handled everything we could in the code. Only the complicated logos and design pieces were in the library. We also only loaded the essentials at the beginning and loaded everything else we needed as we needed it.  tl;dr - Most flash is done poorly and lazily by jamming all the content into the library instead of making it much faster with Actionscript "
"They had to do something, so they did it in the least painful way possible.","C'mon guys don't be so difficult.  Here's the original PEP for Python 3:  I'll be honest I thought there was more of a rationale as part of this... so I did some more searching. The PEP related to the moratorium offers some insight into the migration thinking:  You're right that there isn't an obvious ""this is why we did it"" article, but I have two (personal) thoughts about it:1) This wasn't a spur of the moment decision, I think the core developers reached a point where they had to make very hard choices about the evolution of the language. Either bloat the language to attempt complete backwards compatibility and limit the ability to cast off long standing problems (looking at you Java) or make a clean break with no plan and court disaster (PHP 6, Perl 6). I think they made the choice to keep the language small at the cost of introducing incompatibilities.2) They went about it in the best way possible. I don't think anyone expected Python 3 to become the de facto standard overnight (or even in a year). I expect that Python three will become the standard at about the same level of maturity that Python 2 really took off (3.5?).  tl;dr: They had to do something, so they did it in the least painful way possible. "
"if we take the definition that being in risk of losing your income is a stress factor, most freelance work is fucking stressful.","A job I would consider not stressful is one where I could come in at exactly 9am, leave at exactly 5pm and know that that shit was done with. I doubt an audiologist ponders how to fix a problem overnight. He executes techniques that have been established by industry... deviating from these is not only shunned, but a potential liability.  There is no such industry standard in programming (nor can there be). I can guarantee you that while the PM has a shitty job of being between a rock and a hard place, a programmer who's actually in charge of a critical component that just mysteriously lost some data is not a stressless position.  tl;dr : if we take the definition that being in risk of losing your income is a stress factor, most freelance work is fucking stressful. "
"If it doesn't break anything, and some people prefer it, what exactly are we arguing about?","When I first learned of CoffeeScript—rather, when my boss told me I would be writing in a language that compiles into a script—I was nervous. I imagined sprawling, unmaintainable output that would boggle the minds of mere mortals, if not user agents themselves. But this didn't happen. So, what confuses me most about this debate is why it's a debate in the first place. I have never encountered a situation where using CoffeeScript cost me time, produced code that I didn't understand, or worse yet, code that was wrong (unless it was my own fault). So who cares? Who does this hurt?  That learning CoffeeScript costs time that would be better spent learning better JavaScript is the only reasonable argument I've seen. But this completely misses the fact that real people have preferences, and when you are doing something you prefer you will be more motivated to learn and more likely to do a better job. I find JavaScript ugly, so using it can feel like a chore. That's the last thing you want a developer feeling. Others I'm sure prefer having ""function"" or ""this"" spelled out and dealing with CoffeeScript would feel just as bad. But this is not a debate where one side gets to be right. Unless, of course, someone can provide a convincing example of CoffeeScript breaking something that wasn't already broken in JS. If someone can—and I mean this as an honest request, not some snooty challenge—I would very much appreciate learning about that.  Or, if this whole knowledge gap thing is about programmers having a hard time reading StackOverflow...well...we should probably be used to that by now.  EDIT: TL;DR - If it doesn't break anything, and some people prefer it, what exactly are we arguing about? "
"This is made for large-scale applications, not ""average joes."" 
 *","Well, in the documentation for HipHop (I know this thread is about HHVM, not HipHop, but it's relevant):  > HipHop is not the right solution for everyone deploying PHP. We think it will be useful to companies running very large PHP infrastructures who do not wish to rewrite complex logic within C or C++.  Namely because for most sites, the speed of the PHP interpreter is  not  the bottleneck. Any sort of disk access (especially database) is probably going to be taking up most of a page's time. For such sites, attempting to speed up the PHP runtime won't have much impact, whether from a native C++ application via HipHop or from the JIT compiling that HHVM provides. Most pages aren't particularly CPU-intensive. Of course, I'm speaking in generalities here.  The reason it works for Facebook is scale. They have a *huge** memcached setup to prevent accessing disk whenever possible, etc. So, for them, PHP execution time starts to take up a larger percentage of the overall time.  As it stands, there's not much a ""normal joe"" could use this for. It isn't integrated into the standard PHP runtime and doesn't work with Apache in any way. In fact, unless you manage your own server and are willing to completely abandon Apache, I don't believe there's much this could do for you at all, nor should you really want to use it for small applications.  tl;dr: This is made for large-scale applications, not ""average joes.""  * "
Commentators love to bitch and moan about how there is a sex imbalance in tech but never propose solutions.,"You know what the truth is? Even the most egalitarian, politically correct, and sensitive company culture won't attract appreciably more women. There are simply very few women with the background to attract.  So you know what? Companies are recruiting like this because it works better than the old, sterile Silicon Valley image.  Yes, there need to be more women in tech. But you're not going to fix it by yelling at companies for working with the talent pool they have rather than handicapping themselves in pursuit of 10% of the talent pool.  But hey. Everyone knows that the best solution is to yell at the nerds, right? That always fixes things, doesn't it?  tl;dr: Commentators love to bitch and moan about how there is a sex imbalance in tech but never propose solutions. "
this is caused by interaction between GC and generic runtime structures and can be avoided if you use less generic structures with meta data at runtime.,"> his is what GC buys you, hacked intrinsic datatypes?  Depends on your GC, the OCaml GC apparently uses a flag bit to see if a value is a pointer or not, others might store meta data about the object structure to see if a value at a specific location is a pointer.  Since the OCaml list uses the same structure to store both primitive values and pointers having static metadata alone would not be enough (unless you decide to leak memory like a sieve - simple c++ GCs do this)  Java uses a metadata based approach since lists only store objects and AFAIK there was a paper  about using flag bits to store small integer values directly in  lists to reduce the memory/performance overhead of handling Integer objects.(sadly not used on any jvm)  I do not know what C# uses, however it creates a separate implementation of List for each type so a meta data based approach seems likely.  TL;DR; this is caused by interaction between GC and generic runtime structures and can be avoided if you use less generic structures with meta data at runtime. "
Suffixing interfaces helps keep consistency over time-- as code is refactored.,"When it comes to refactoring code, and you find you want to create an interface, it becomes a bit less clear what the code is expecting. I'd rather update code to use the new interface than break code that used or extended the class.  Say, we have a Parser interface, with two classes, XmlParser and  JsonParser. Lets say we use SimpleXML in the XmlParser-- but run into issues with huge XML files. So we implement a libxml-based parser that's much better with memory, but slower overall. Because they have different use-cases, we make both available-- and decide to make XmlParser an interface. We end up with one new interface and two more implementations (SimpleXmlParser and LibXmlParser).  Without Suffix  Interface Name: XmlParser (the same as the old class name)  Advantages:   Existing automatically uses the interface-- for better or worse (type hints, extending, method calls)  Easier to read   Disadvantages:   Broken code will probably give unhelpful 'undefined method' fatal errors on implementation-specific methods.  No migration path available.   The disadvantages may be fine with an internal project, but when it comes to Libraries and Frameworks, you do not want classes to suddenly become interfaces. Doing so breaks any code that extends yours-- the users of the software will probably lose a bit of trust when their code breaks. Not fun.  You could work-around the problem by making some inconsistent naming pattern-- but that pretty much leads to developers not knowing whether to implement XmlParser or XmlParserInterface.  With Suffix  Interface Name: XmlParserInterface (unused, but consistent with rest of project)  Advantages:   Backword Comparability can be broken when desired, instead of immediately.  Naming consistent over time (You know to always use Interface suffix, doesn't change based on history)  Code that should use the interface is easily identifiable (searchable)  Errors from non-updated code is clearer: ""Expected type XmlParser but got LibXmlParser instead""   Disadvantages:   6 characters longer-- this is a severe waste of space, developer time, and Disk IO.  Existing code doesn't automatically use the interface-- for better or worse   tl;dr  Suffixing interfaces helps keep consistency over time-- as code is refactored. "
"not all indian programmers are bad, but the situation as a whole is sad.","first of all India is a very big country, using the term 'India coder' is wrong generalization.  I am from India, and have been writing code since i was 13-14 and this was and still is very uncommon. Not all Indians or even most Indians don't know much about computers or start making programs at a young age.  i am also currently studying in CS in an Indian college and the educations is nowhere near the quality of the courses I've seen online (Open Courseware).  although it depends from college to college but from my experience the professors are not very good, this is maybe due to the high demand of CS professors.  in my first year i was very excited to be learning CS, but to my disappointment in the first year we were only taught general engineering (chemistry, mechanics, basics of computer, physics, etc.)the professors were not aware of modern or even old concepts like functional programming concepts, no one who i've talked to knew about lisp, haskell, etc. C++, Java are the main focus and PHP is good for getting jobs.  most of my classmates aren't very good with computers and neither are they interested. Most of them will join these companies which pay low and do contracts for firms abroad.  I know some extremely good programmer who i go to for questions but they are shadowed by the huge number average or below average programmers.  tl;dr  not all indian programmers are bad, but the situation as a whole is sad. "
Eastern Europe has had decades upon decades more to build out it's educational and societal infrastructure than India.,">Many Russians and Eastern Europeans (Ukraine, Poland, Estonia, etc) come from somewhat similar educational environments (many learning institutions still use Pentium 1's and Borland Delphi), but their code is much, much better than code from India. Much friendlier and customer oriented. And their rates are largely comparable to those in India across all IT areas.  To compare the former soviet block and India is laughable. The Soviets, everything else aside, had a proper modern education system and generally a developed nation standard of infrastructure. They generally had stable societies without massive shifts or growth. Eastern Europe was actually filled with modern developed nations at the start of WW2. The USSR was quickly, and bloodily, being converted into a modern nation at that time as well. India in contrast was a bunch of peasant farmers at the time and for some time afterwards.  The late soviet era and the 90s weren't exactly kind to those soviet block nations but they didn't reset everything back to the 1800s either. All those trained soviet era engineers and professors didn't die off or disappear (and quiet a few who did emigrate eventually came back). There wasn't a giant influx of new workers compared to the old guard and in general society stayed stable. The culture and intellectual infrastructure survived. India however is starting from almost nothing and what they do have is likely overwhelmed by the sheer influx of new workers.  tldr: Eastern Europe has had decades upon decades more to build out it's educational and societal infrastructure than India. "
For  getting shit done  I use Haskell. For academic  wankery I use Agda.,"To expand on Don's answer: The startup I work for uses it for our back-end, Google have used it in the past for a variety of projects, Facebook have used it to transform PHP code, and a whole variety of theorem provers and formal verification tools  are written in Haskell - these don't seem esoteric to me.  Add to that all the finance companies with large closed-source Haskell codebases and companies like Galois which use entirely Haskell, there is certainly a big place for haskell in the ""Real-world applications"" space.  Most of those people that enjoy esoteric concepts (like me) have moved on from Haskell for those purposes because:   Its type system is not as expressive as it could be  It's a fairly old language, and the space of Cool Stuff to do in it is fairly well explored.   That said, there are a number of cool extensions to GHC Haskell that make it still worth the while for those people.  tl;dr: For  getting shit done  I use Haskell. For academic  wankery I use Agda. "
Stop confusing scalability (how much work you can handle) with asynchronicity (how soon the user gets a response).,"> I could have saved hours of frustration – and worrying about scalability – if someone had told me how easy this would be instead.  There is nothing special about queues that suddenly make your app scalable. Queuing drops your latency to approximately zero at the cost of having no clue about when or if the work gets done.  The ""scalability"" benefit you claim is just because you're aggregating work at a single place and distributing it to many workers. A load balancer does exactly the same thing, and lets you immediately know when you're over capacity at the cost of latency. With a load balancer, shit gets unresponsive and your browser spins its wheels. With a queue, everything appears to work but exactly nothing gets done and hopefully your monitoring system lets you know.  ""When you must fail, fail noisily and as soon as possible.""  tl;dr: Stop confusing scalability (how much work you can handle) with asynchronicity (how soon the user gets a response). "
"find what works for you. Don't worry, lots of ways to succeed in this industry.","I wouldn't worry to much about it. My general view on this is between the blog link and you. I see the value in having a wide array of knowledge but am fulling willing to consume my self into one aspect if necessary and/or directly benefits me.  The thing to remember is to, at the very least, pay attention to what is going on in development community as a whole. Watch for shifts. If you like C# or ruby, dive in. Have fun. Right now both are sought after. But pay attention, is the demand for your interest dropping? Maybe be time to jump to something that is rising.  A prime example of this is more or less what I lived through. For the last 6 years, I did fat client internal development. I dove deep into my work. I more or less became an expert in .net WinForms. However the last few years, I noticed the web was becoming more of a force. And what could be done on the web was becoming practical because of jQuery and ajax. I started to focus my learning more into thin client work. 8 months ago, I switch from a fat client dev to a web developer.  tl;dr find what works for you. Don't worry, lots of ways to succeed in this industry. "
"you use the word zealot, but it does not mean what you think it means.","> GPL zealots are happy to use their GPL licensed free code.  That is not a zealot. That is a reasonable user. No one is complaining about  them . A zealot condemns others for their choices. For example, if a person Microsoft development tools to produce software for Microsoft, telling that person that he contributes to an immoral system. Or telling a user of Microsoft or Apple software that they are contributing to an immoral system. This is a zealot. There are plenty of people in the free software camp who do this, which caused the schism between free software and open source in the first place.  Microsoft people and Apple people can also be tech-centric, they just don't adhere to the ""free software"" philosophy.  TL;DR - you use the word zealot, but it does not mean what you think it means. "
"This is unreadable, unusable piece of monkey-code, nothing to be proud of.","Okay guys, as an developer from Estonia here are my few bits.   It's no way an open-source project. You can fork it on github, but you cannot change it - current license does not allow that.  If you take a look at the source-code you'd see how poorly taxpayers money were used - the code itself is not readable, cause it's written in ... Estonian! Function name, variable names, e.t.c, e.t.c ... It's a shame.  The code released is a backend code, which has no documentation and no tests. There is no easy way to get it working on a local machine. Therefore there is no easy way to see it working and no easy way to test if it's written correctly.   TL;DRThis is unreadable, unusable piece of monkey-code, nothing to be proud of. "
Python community has a real finger pointing problem. That's why Python 3 is failing.,"> This kind of moral argument may work on those who are active members of the Python community, but it won't work those who are using Python as a tool for a job. I'll get back to them, as they're important.  I would say the most important, and I think the author doesn't give enough lip service to that. In fact I don't think that anyone in the Python community is. I'm a person that uses Python as a tool for a job as I'm sure many, if not most, readers of this sub are. My problem is that this post, and several others like it along with Twitter, can easily be read as 'yeah Python really screwed up (and yeah it effected these people over here) and dependencies or this or that are the real problem.' That sort of finger pointing that is the real problem, and it's almost a side note that real users are being effected. Internal quibbling, silos, ignoring your end users (customers), and focusing entirely on your internal operations are not good ways of delivering and selling a solid product. Just ask Kodak and every other company that self-destructed. I get that Py3 was immature at release, and that OSes haven't done a good job of pushing it in their software repos, and that many libraries are slow to adopt. I'm willing to overlook all of that and install it where I need it and develop with it where it makes sense. The Python community needs to get a plan, put it in action, and stop bickering.  TL;DR: Python community has a real finger pointing problem. That's why Python 3 is failing. "
denying commits if static analysis fails will be the best thing you've ever done.,"I can really get behind this and I understand the reasoning. Part of the lint process is that it forces the person who wrote the code to do a code review and re-analyse it from a different perspective.  I recently ran the codebase for a new project through  splint , oh the surprises - it picked out many things, it made me re-structure things a little and completely re-write a module. Now the code is much cleaner and I fixed a few previously unknown bugs in the process.  The same thing applies with  pylint , the first project I used it on I made sure that before every commit it passed  pylint  with a 8+ score and got comments from fellow developers that it was some of the cleanest Python they've seen in a while.  It's one of those things like continuous builds/testing and source control, once you start using them and more importantly when the system starts bitching because you broke the build or fucked up a merge  before  other people notice it's a massive productivity and code quality boost.  TL;DR: denying commits if static analysis fails will be the best thing you've ever done. "
there are plenty of other ways to get there but they may be hazardous to your health.,"Ah, yes, video games. For some reason I tend to lump that in but they are two separate things.  And now that you mention this I realize that I spoke too quickly and that I can think of two other occasions where I have felt this way.  One is competitive sports, especially when it's 1v1 or 2v2;  you get no breaks, the game is over when you screw up.  This feels pretty good, almost godlike, especially if you get the sensation that time is moving too slowly (all that oxygen and adrenaline hitting the brain).   The other doesn't feel so good: fight or flight situations.  In my prime I avoided what should have been a number of crippling accidents by zoning in and ""planning"" my way out of a bad situation already in progress.  For one hot second it feels glorious, then the buzz wears off, the self recriminations and the shakes follow close behind.  tl;dr: there are plenty of other ways to get there but they may be hazardous to your health. "
You'll have to set up a server on the target machines.  There are plenty of javascript libraries to make the GUI.,"Funnily enough I was thinking about the same thing a couple days ago, and it's definitely possible, though I'm not sure it's the best idea.  I looked into hacking together a small websocket server that would push stats, then using [this library]( to display nice graphs.  Things to note:   That (and other) chart libraries have non-trivial CPU usage — if you're running it on the same machine you're monitoring, the results will be polluted.   You'd have to have a server running on the target.  I don't know if that's practical for your situation   If you're allowing any commands to be sent back to the target (kill process, etc) then yeah, security will be an issue. Setting up authentication would be (IMHO) more trouble than it's worth.    You say ""small systems in the field.""  If these systems are already running a server, it should be pretty easy to add an endpoint that returns cpu stats.  If that's not the case, then you'll have to evaluate whether or not setting that up is worth it.  TLDR; You'll have to set up a server on the target machines.  There are plenty of javascript libraries to make the GUI. "
Increasing constant factors to make crypto harder is pointless. Go for a better complexity class instead.,"No, that's still too easy.  Think about it, what if I get a video of the hash changing? To get the first character, I have to 'bruteforce' only one hash per each possible character. Once I have that, to get the second char, I again only need one try for each possible characters. So, without collisions, the attacker only needs to do the algorithm (number of chars in password) * (number of possible chars) times more than the user. You cannot stop bruteforcing that by making the hash function more complicated. You can, however, stop bruteforcing by increasing the amount of possible collisions, because each collision doubles the work to be done.  tl;dr: Increasing constant factors to make crypto harder is pointless. Go for a better complexity class instead. "
The physical structure of DNA matters. It is not simply abstract data.,"I studied genetics at uni with a programming background. I tried to look at DNA as binary data like the author here. But it's not.  The big difference is that the physical structure of DNA matters. A's and T's are physically more rigid than C's and G's (which are more floppy). So, for example, genes often start with a TATA sequence which acts like that end bit on a zipper so enzymes can attach (e.g. to transcribe the gene). [tata box]  DNA is not abstract data. Evolution doesn't work with pure abstractions because evolution doesn't have the foresight or need to manage complexity in that way. Evolution is one hack built on another.  TL;DR: The physical structure of DNA matters. It is not simply abstract data. "
"my opinion: never just ""fix"" the bug, recreate it in testing environments. Debugger becomes useless.","There is something missing from all this debate - the context of the process; The kind of a bug. The work process I've applied in our team simply does not require a debugger.  I for example apply full blown XP at my job. So the first ""bugs"" to be caught are failing unit tests. Since in my environment hitting a single key runs the tests of this specific unit and the results are almost instant, i use print statements. Why bother integrating another tool for this, if it works really well.  But lets say the system ""fails"" in one of the tests the QA is running. Before fixing the bug, we first make sure it's visible through the logging system. One of the tasks assigned to the QA is to be the playground for our debugging infrastructure, so it will be mature once we deploy. But making the bug visible is not a phase in the debugging, but just a ""training session"". We then write a test that recreates the bug: either a unit, whitebox or blackbox test. But before fixing the bug, the programmer must show he knows how to reproduce it. If a new utility for recreating the bug is needed, we write one, and give instructions to the QA of how to use it, and which new tests are possible. But only after this the programmer is allowed to fix the bug. Of course verification of the tests then follow to make sure no other tests failed due to this fix. Think this is slow? Usually takes minutes. The quality of the software, and happy faces of the team when coming to work can tell you it makes all the difference.  TL;DR my opinion: never just ""fix"" the bug, recreate it in testing environments. Debugger becomes useless. "
Fat consciously chose not to pass JSLint. His response should have been an explanation why.,"I think a lot of people here are getting a bit detached from the subject at hand. Maybe because this is  /r/programming  and not one of the webdev specific subs.  JSMin is designed not just to minify javascript, but to do it  fast . Fast enough that in a situation where somebody doesn't have full control over the server environment, or there are reasons it's not practical for them to minify beforehand, it can be run on the fly (as in during a request to the webserver) and still provide a performance improvement over sending the code unminified.  This is an impressive achievement and to accomplish it Crockford has made some assumptions about the input code. In fact, just one assumption, that it passes JSLint, a bigger, more complicated,  slower  parser.  Fat's response should have been,  ""You shouldn't use JSMin. It assumes that the code passes JSLint and we deliberately do not do that because we think we are cooler/smarter/whatever than that.""  Personally, given what they are trying to do with bootstrap and how widely it has been taken up, I think it's a massive mistake for them to choose not to pass JSLint. It would have made things easier for their users if they had.  TLDR ; Fat consciously chose not to pass JSLint. His response should have been an explanation why. "
DRM isn't going away. But DRM standards could make it easier to consume.,"26,000 people are trying to persuade W3C over whether DRM should exist or not. That isn't the W3C's job. W3C makes standards so there can be agreement on how to best communicate with current technologies. DRM will continue to exist because of the legal control it gives content creators over their content. It has nothing to do with user preference.  The W3C is trying to define a standard that would allow DRM to be easier to implement and deliver over the web.  If successful, the user experience while consuming DRM content would be improved. I imagine instead of needing a Netflix app, a HBOgo app, an Amazon app and ITunes app, etc.. that there could be just one app. Hopefully the spec defines a search API, so one app could find the content you wish to see across all of your subscriptions and display it in your favorite compliant agent.  TL;DR: DRM isn't going away. But DRM standards could make it easier to consume. "
keep the HTML standard purely platform-agnostic. Deliver DRM-ed content in other ways. I don't mind how.,"I understand your reasoning here, and it's a valid argument. Personally I believe that, if content cannot be supplied in a way that is compatible with the principles of HTML, it should not be supplied via HTML. Content delivered by proprietary means  should  be siloed from normal HTML.  I don't mind companies coming up with their own applications to deliver content 'securely'. If they want to control their content, and consumers are happy with that, it will work. I don't think they should be using HTML as the vehicle to do this. I think the suggestion that EME will encourage content providers to expand to more platforms/be more open is highly dubious. I don't think they will change their positions on Linux/mobile etc.  tl;dr:  keep the HTML standard purely platform-agnostic. Deliver DRM-ed content in other ways. I don't mind how. "
"Some optimizations can be made, but the prime trade off is one of security over speed.","Some Javascript can be compiled to machine code -- From a security perspective: Ugh.  VMs with JIT are equally as horrid from a security standpoint.  What we need is new hardware.  ARM typically only has one execution ring for isolating kernel vs userland code.  Recent Intel x86 had execution ring levels.  We need at least 3 rings and OS participation to create proper sandboxes for JIT machine code.   (bug free) Interpretors can execute interpreted code and enforce an unlimited number of security rings, vs the current One Ring to Run Them All, present on some ARM chipsets.  AMD is also guilty of taking away privilege execution rings.  So, although optimization and execution time is key, some parts of Dalvik can not be optimized properly for security reasons.  It's difficult to truly enforce application security policies with only one ring -- Sure you can protect kernel level stuff from userland, and userland processes can be protected from other user processes via virtual memory addressing, but JIT is hard to protect from its host environment.  This is why browsers are better if they're inbuilt with the OS.  Thus allowing the browser to be native and the barrier to be used to isolate JIT code...  However, this makes exploits in such ""root level"" applications more likely.  We need more rings.  As a hobby OS dev who has experimented with new secure paradigms of separating code and data -- Separate call stack from data stack, code pointer modification protection enforced via execution ring levels, etc.: The available hardware is severely lacking, even on more capable desktops.  We lost a segment register when we went to paged virtual memory.  We need better hardware that wasn't designed with single stack for both code and data in mind, software can only do so much.  TL;DR: Some optimizations can be made, but the prime trade off is one of security over speed. "
Open sourcing government funded things can be a pain and is sometimes disallowed by the government.,"Source for the following: I work at a government funded research and development center (Software Engineering Institute @ Carnegie Mellon) and have open sourced code funded by the Department of Defense.  In many cases the government contract limits open sourcing. The way it works, at the simplest level, is you cannot give out anything to anyone except to your government sponsor. The big exception to this, is if you get permission from them you can put it out there (this is how research papers actually get published). A good deal of research is classified as well, which the government would never allow to be opened.  In my project, I had to contact multiple internal lawyers who then talked to the sponsor and got us permission. It might sound simple but trying to explain software dependancies to lawyers is a daunting task at times, and the process took weeks. Luckily, we were not working on anything that was classified/ITAR.  I think a lot of stuff is not open sourced because the software is more of a proof of concept than any real deliverable. The end goal for most research places is simply publications, not distribution of products. Due to this, most people don't want to bother with the headache and/or do not see the point of open sourcing their software.  In the case of the originally linked software, I think what they are doing is perfectly fine. Not giving out their code is their choice, but they are at least giving out the software which is more than many places do. Also, I have worked personally with the Smile team before (they are < 1 block away from the SEI) and in their case they are a great bunch of people. Sometime last year Smile was not available for Android, which is when I first contacted them. I explained what we wanted to do with it, had some in person meetings with them, and two days later they had ported it to Android for me. I feel like this kind of team is much more valuable than open sourcing something and just forgetting about it.  Sorry for rambling, I'm not the best of writers.  tldr: Open sourcing government funded things can be a pain and is sometimes disallowed by the government. "
It shows that the applicant was more concerned with putting fancy things on their résumé than the actual use case of reviewing a résumé.,"Most of the time, over-designed résumés are tossed almost immediately. We are looking for people who understand the purpose of what they’re making, and design it accordingly. Being in graphic design, we see a lot of résumés that are so over-designed that they almost look like posters, which is the opposite of what a résumé should look like, i.e., a résumé should be immediately recognizable as a résumé. Applying this thinking to the use of a QR code, what purpose would it possibly serve? If I’m going over a résumé, I’m most likely sitting at my desk, and want to view the applicant’s website on a full size screen, not a phone or tablet. The bottom line on QR codes is that no one actually uses them in the real world. Don’t believe me? Go ask 10 random people on the street if they have ever scanned a QR code. You can even narrow this down to 10 random smartphone owners, if you’d like, so long as you realize that a group of programmers in an office is not a random set of smartphone owners.  TL;DR: It shows that the applicant was more concerned with putting fancy things on their résumé than the actual use case of reviewing a résumé. "
"Found myself saying: ""I pray that I never become a software architect."" 
 Edit: Whoa, reddit gold!  Thank you kind stranger!","Very early in my career, I worked with an ""architect"" whose job was quite obviously little more than ""Rational Rose Jockey.""  That is, the man slung UML all day, every day, and attempted to design a system in a vacuum, with minimal customer or developer input.  Not to say that he didn't  try  to get developer input. It's just that the lines of communication were incredibly noisy.  The first month of that project was little more than daily meetings watching this UML diagram  grow   metastasize  into a colossal clusterf*ck of a design.  It was so incomprehensibly complex that you needed to pack a lunch just to get through the top-to-bottom explanation of it all; and of course there was only ever one man who could do that.  Yup, the ""architect.""  After the first few days, the team was utterly demoralized by this tapestry of madness.  UML was conceived as a visual communication tool.  And to be fair the architect had done just that - communicate - only in the same way that you could say a forest fire is an example of sending up a smoke signal.  And just like that, one day, it was thrown over the cubicle wall for us developers to turn into software.  He was then off to orchestrate some other object-oriented nightmare.  Our DBA was dumbfounded since he was going to have to turn this thing (somehow) into a relational schema without slowing the team down.  There were other institutional problems there too.  Like the micromanging style of the leadership, and the 4:1 employee to manager ratio.  We were ""Rational Unified Process"" compliant too!  Oh, and then there was the guy from the main office that wrote his own prototype of our project,  while  we were in the middle of writing it.  Fortunately, the happy-hour parties were always fun.  TL;DR; Found myself saying: ""I pray that I never become a software architect.""  Edit: Whoa, reddit gold!  Thank you kind stranger! "
"Use Vim. The learning curve is steep, but the payoff is great.","Hear hear! I made the switch myself and I must say that it's one of the best decisions I've had in a while.  I was a Textmate user too—that's actually one of the main reasons why I decided to switch to OS X in the first place. I remember seeing all those cool videos using Textmate and I just wanted to know what exactly they saw in the editor.  Textmate is actually pretty great, especially because of its extensibility. But the development of the next version has gone on for too long and the problems haven't been addressed in the interim. Things like the absence of splits, the lower than expected searching capability and the horrible handling of large files.  I was waiting for Textmate 2 to arrive, but I have work to do, so I finally decided to take the plunge and try Vim. It took me quite a while to get used to it (I tried playing with it 5 times before I really dove in), but the payoff is great. Almost all of the features that I needed came out of the box, and whatever I miss from Textmate I could just integrate via plugins.  TL;DR: Use Vim. The learning curve is steep, but the payoff is great. "
On a bad project with little ability to fix the core problems due to bad management and politics.,"I am in Agile hell on my current project. The project is to build an in-house replacement for our company's manufacturing software. We're a fairly small equipment manufacturer, but we utilize technology everywhere we can. This project is big (multi-site), but there are only two coders on the job, me and a contractor. The contractor is also doing PM and convinced our management to use Agile. Every cycle we have is a mess as our meetings with the management team end up pulling out new requirements that are not even remotely fleshed out. Their physical processes are not defined well at all either. No documentation is being made and there is no QA/QC to speak of. The code is suffering from ""patch-itis"" where we just put band aids over fundamental flaws in the code since there was no architecture done. The contract developer and I cannot seem to stay on the same page though I have tried repeatedly to fix that problem. The company's software development practices are being ignored by the contractor and making my life hell because I can't integrate my code since his is not using the agreed upon styles and practices. Code check-ins are nightmarish and there is no continuous integration. Ugh.  I've been through Waterfall, RAD/JAD/Spiral, Extreme Programming, Rational and CMMI methodologies in my twenty years of programming. None of these methods has ever been without their share of problems, but nothing comes close to this level of fucked up. I'm just about ready to say fuck you and hit the door. This is obviously a case where Agile is not implemented properly or even understood. This experience will leave me with a complete distrust of the Agile methodology, but I do recognize it as a very wrong implementation in this case.  Anyone have any words of encouragement for me to keep from snapping?  Edit-   This is more a badly run project than an Agile failure  IT Manager is company owner's brother. Contractor is owner's old friend. I have no decision rights in the project.  Already discussing this with my IT Manager. He wants to fix the team work problems instead of the project or technical problems.  I'm not in a good position to change jobs right now. I may be forced out, but I can't leave of my own will at the moment.   TL;DR -  On a bad project with little ability to fix the core problems due to bad management and politics. "
there is nothing specific that ties you to Metro.,"It ultimately depends.  IE 10 renders pretty well, most of the differences are due to bugs in browsers (like translate3d can cause artifacts in Chrome and especially iOS). If your HTML works ok in Chrome and Firefox, it'll almost certainly work fine in IE 10.  The WinJS specific bits are pretty easy to isolate, and for my own application I only have code for startup, and for changing pages. Even then it's copied from an example, and could probably be skipped entirely. It's all isolated away into literally 2 or 3 functions, and I just use jQuery and my own code for most of the work.  There is some data binding it offerrs, but it all uses the HTML5 friendly 'data' attribute prefix. However again, you don't have to use it.  The HTML/CSS can be similar; it just depends on if you want a typical Metro layout, and want to use their specific layouts or not. If you don't, then your HTML/CSS will work the same in browsers.  The only truly Metro specific bits are with items which don't have any web standards, like webcam. If you currently want a webcam working in FF and Chrome, you'll have to fake it using Flash. So it's not surprising that MS offerred their own API.  However you could detect if there is Metro support, and fall over to Flash if it's not there.  tl;dr there is nothing specific that ties you to Metro. "
"Some properties needed to be bound to the object, others tossed into a grab bag.","It's not so much a grab bag of properties as all the properties are related to the object.  But that's why I was so torn between the 40 properties and the Dictionary.  Because both extremes could be used to accomplish the same goals, but both extremes would leave me feeling dirty.  The issue was that some of the fields are important.  Take for example a Person, things such as firstName, lastName, date of birth all belong on the object.  But other fields have lower value, such as hobbies.  Hobbies can change and aren't anywhere near as important as other information, so I stuffed that into the dictionary.  Sure, it's probably wrong, but it's the best I can do at this stage of my toy project.  TL;DR Some properties needed to be bound to the object, others tossed into a grab bag. "
"you are in fact disingenuous because you have ignored reality to make your little hypothetical situation completely unrelated to your previous statements about ""in this situation""","He was in fact being choosy about how and which copyright law should be enforced. So the fuck what? Do you not see a little bit of difference between an author saying ""you can't do that with my work, stop"" and ""go to jail for downloading?"". The difference in degree is apparent to me. Maybe you are just one of those vapid people who things that not doing something is the same as being forced to live in a small cell surrounded by people who actively want to rape and hurt you. If you are, there is no hope for you ever being a worthwhile human being.  Going on the completeley baseless assumption that you are not a worthless vapid piece of trash: Not only is the method of enforcement different, but the actual rights being enforced. In one case, only the integrety of the work is being enforced, aka ""dont change what I did"". In the other, integrity, distribution, redistribution, the ability to pass on ownership, the ability to use the medium in a way you see fit and the rights granted by fair use are all taken away.  To be clear, even in your game of ""i don't get it"" you have to admit that the lists are very much different in non-trivial ways. Further, you must, despite your vapidity, admit that minor changes to a whole book are not covered under fair-use.  Now, in an abstract sense about DRM, where we ignore 100% of the actual, and 75% of the theoretical implementations of DRM, it could be done as you suggest.  However you chose to go to the most abstract definition of DRM possible, and ignore the equally likely meaning of his statement (""DRM as currently exists""). This is why I called you, and still choose to call you disingenuous.  TL;DR -- you are in fact disingenuous because you have ignored reality to make your little hypothetical situation completely unrelated to your previous statements about ""in this situation"" "
"You're either a troll and don't know it, for which I pity you, or you're just trolling. Drive people away? Please do be driven. I shan't weep for you.","Implication != inference. There's no imagining, I was being kind calling it an implication, on the assumption that it was unintentional. In fact it's pretty explicit.  >you imagine I said.  Do the accompanying quotes not fully substantiate my claims?  > lambast me  no, your tactics. You seem to have trouble separating criticism of your approach and actions from criticism of you as a person. I've never said anything bad about you, just about the way your comments sound. Unlike you, who's repeatedly insulted me as a person.  > Nitpick ... has not occurred.  This tangent is the result of you nitpicking the phrasing. You seem to forget that there's an entire comment history so we can each look at exactly what the other said. You nitpicked. Now you deny it. Troll.  > Peace, not piece.  Rather an inconsequential detail don't you think? Oh that's right, you don't nitpick. Also, please google ""said my piece"" or ""said my peace"" and note the general theme of the questions relating to that phrase.  > Oh Bullshit.  You swear rather a lot, and yet call me unpleasant. You also are dismissing an apology, which is rude.  > For example, I... blah... followed by some kind of argument from analogy, that because I'm apparently engaging in dong-waving here that I therefore couldn't possibly offer anything of value elsewhere. Or something. I'm not really following your reasoning there.  tl;dr You're either a troll and don't know it, for which I pity you, or you're just trolling. Drive people away? Please do be driven. I shan't weep for you. "
never do anything that has undefined behavior under any circumstances.  Ask your compiler provider about unspecified behavior.,"What he's saying is that the specific C++ implementation is allowed to do whatever they like - but it's unspecified behavior, which is different from undefined behavior.  XCode, for example, has a mode where it fills memory with ""bad"" values and then automatically dies when you read that memory.  That is a Good Thing - nearly all the time I want to know when I read memory that I haven't written.  However, you can be pretty darned sure that no compiler is going to do stuff like that in an ""optimized build"" mode, because setting every byte you allocate is going to seriously slow down any program - and AFAIK in all compilers which have modes that let you do this, there is a way to turn that off selectively.  ""Unspecified"" behavior is somewhat different from ""undefined"" behavior.  With ""undefined"" behavior, you can't rely on the behavior being the same between invocations of the same program, or different compilations from the same compiler, or different versions of the same compiler, forget about different compilers!  You should simply never allow undefined behavior to occur under any circumstances.  The optimizer is allowed to assume that you never ever do anything which results in undefined behavior, and will aggressively re-order and even remove code as a result, which can result in hard-to-diagnose problems.  Unspecified behavior is a different kettle of fish.  It merely says that the spec does not identify what happens, and leaves the actual behavior open to vendors, who are free to have very specific implementations.  The optimizer  cannot  assume that you won't do things that have unspecified behavior!  An unfortunate truth of programming is that sometimes you need to rely on behavior that you observe from other packages but that is not in the spec.  It is never, ever safe to rely on observed behavior for undefined conditions - your only correct to choice to is make sure that undefined behavior never occurs.  It is however quite possibly safe to rely on the observed behavior of unspecified functionality - people have done this for years for Microsoft's products!  Indeed, you have a reasonable case to be able to go to the vendor and say, ""We are relying on this unspecified behavior, please attempt to maintain this in future versions"" and the vendor will often attempt to comply - again, I point to Windows as an example.  tl; dr:  never do anything that has undefined behavior under any circumstances.  Ask your compiler provider about unspecified behavior. "
"There is no significant difference between PHP's and JS's typing systems, at least not enough to cause you to compare the two in any meaningful way.","If you're using PHP, you probably don't have a very big need for strong typing. At least not to the point where you wouldn't chose/dislike javascript because of its typing system. Literally the only difference between (well, for the purpose of this conversation) javascript's typing system and PHP's is that JS technically does not have 'integers' (at least, what you mean to say is they don't have a type for it). They do have integers. You can do 1 + 1 and so on. Furthermore as previously stated, most browser implementations of JS will treat integers as int32's for optimization reasons. In the case that they don't, javascript still has integers, they're just not in 32 bit form.  TL;DR There is no significant difference between PHP's and JS's typing systems, at least not enough to cause you to compare the two in any meaningful way. "
"if you have an MBA you are likely the problem, not the solution.","We work off of story points, even when we are forced to follow a waterfall like workflow. The basic gist is  stories are a themed group of features that are all highly correlated. We quote the time we think we could complete a story in a perfect uninterrupted environment, and then multiply it by our averaged velocity of completing story points.  Currently my velocity is about .3 (anything I say will take 1 day ends up taking 3 on average) but that's mainly because I spend a lot of time doing non-coding jobs (meetings, training/mentoring, and supporting of other teams). As such, we can compare how long it would take someone thinks it would take to complete a story, with the reality of how they have completed stories in the past.  This doesn't mean that any specific quote/velocity is going to be accurate however, sometimes (almost all the time) the quote is fundamentally wrong because nobody ever has perfect world environments to actually gauge this.  Of course, I've fully trained our PM to not use SP/Velocity as any kind of bible. Just because the burndown chart says we're going to be finished on November 16th doesn't mean anything. We mostly use it to see if our team is suffering too many interruptions, and as a loose guideline for what features are plausible in the time allotted.  I imagine that in other environments some fuckwit with an MBA would come in and scrawl those dates onto a stone tablet and start making ill-informed prophecies of what ""will"" happen in the future.  tl;dr if you have an MBA you are likely the problem, not the solution. "
"You're not going to find a ""solution"" to Godel.","> Don’t you mean that all of them can be written as recursion? (Like in Haskell, for example.)  No. General recursion (like in Haskell) is equivalent to a while loop.  sum [] = 0sum (x : xs) = x + sum (x : xs)   See the bug? It's the same as the bug in this one.  sum := 0i := 0while (i &lt; length(list)){    sum += list[i];  }  The equivalent of a  for  loop is structural recursion or guarded recursion. However, most people who've gone through GEB probably aren't familiar with that term.  > I will further look into finding a solution for Gödel’s incompleteness theorem in the realm of simple observation of reality and neurology.  There is no solution to it. Unrestricted self-reference leads to contradiction. Godel is a feature, not a bug. It is the solution to the classic paradoxes in logic (Russel's paradox, Girard's paradox, and all their cousins).  There is a ton of research going on at the moment over  restricted  forms of self-reference. In dependent type theory, the theory on which Coq is based, there is an important topic called ""reflection"". The term is used in the same way as in mainstream programming languages. The goal is to allow languages like Coq to generate code dynamically. (It also helps alleviate some issues writing Coq code. Dependently typed languages tend to be much more nuanced than a Java or Haskell program).  tl;dr - You're not going to find a ""solution"" to Godel. "
"Yes, but there are guards against this sorta thing.","The short answer is that you can write all sorts of malicious code in C.  The long answer is that the I simplified the instruction pointer issue, it's a bit more nuanced. There is no programmatic way in C to directly modify the instruction pointer register (EIP/RIP) for security reasons.  Each function call creates a new stack frame. The stack frame is initialized by copying the address of the instruction following the function call into the beginning of the frame. When the function returns, that address is then copied onto the EIP/RIP. This is what I was talking about. If you want, you can access/modify the value on the stack frame where you know will be copied onto the eip, but it has to be the correct location relative to your virtual address space. There is no way to access anything outside of the address space allocated to you. So the malicious code you've written has to be somewhere within the source code of the program.  And I believe its standard for the kernel to randomize the beginning of the virtual address space. Program code is somewhere at the bottom, so there is no way to hard-code in the specific address of some instruction.  tl;dr Yes, but there are guards against this sorta thing. "
Explain the problem that you are solving with pointers before explaining pointers.,"My university's main teaching programming language was C++ the year I was there (originally C five years prior, and my class was the last class to get C++ before a switch to Java).  I did not understand pointers until my third semester.  The reason for my confusion was the  way  in which pointers are presented.  We were taught ""Here, this is  int , this is  int* , and the  int*  is a pointer to an  int .  This is functionally useless to understanding, because the student doesn't see any benefit whatsoever!  Don't start off with the base case of  int* .  Start by teaching what structs are.  Then explain how passing these giant structures to and from functions would take up waaaay to much stack space.  A-ha!  Every data structure has an address in memory, right?  So, instead of passing the entire structure, copying it out part by part, why don't we just pass the memory address to the function?  Well, it turns out that there's this syntax,  &amp; , which will give us the memory address of a variable.  But the receiving end needs to know what kind of memory address we sent to it.  We have a special declarer,  type* , which says ""This variable is just a memory address,"" and syntax  * , which means ""get me the thing at the memory address in this variable.""  TL;DR:  Explain the problem that you are solving with pointers before explaining pointers. "
Bottom-up reuse doesn't have to be naive if done early.  There is the added benefit of lower overall system complexity too.,"In your application code/database structure example, I think 'emergent reuse' would look like this:  Your first piece of functionality in your application code that needed to communicate with the database may be 'glued' to it.  Now for your next piece of functionality you may find it pragmatic to extract out the duplicated database parts into a shared component and thus achieve emergent reuse.  Now, of course, there driving force behind what his argument is cost.  If it costs the same (or less) to not glue the two together upfront, then, by all means don't.  Specifically there may be a framework you're already familiar with that enables you to avoid this gluing up front.  The past several teams I've been on have used the approach of reuse when needed with much success.  Of course, it's not black and white.  There are definitely times when bottom-up reuse is more painful and costly than top-down reuse would have been.  However, if your team is diligent about refactoring to remove duplication (bottom-up reuse) then that approach has the added benefit of keeping system complexity down since you avoid the frequently unnecessary abstraction of always designing for reuse.  TLDR   Bottom-up reuse doesn't have to be naive if done early.  There is the added benefit of lower overall system complexity too. "
types are worth the tax when an extra layer of bug prevention is priority #1,"Types have their place, but as extra information in the code, they impose a tax. Types are a burden on development speed and they make modifications more expensive than in dynamic systems where type information does not have to be refitted to match the new design (compare java interfaces).  Types are a speculative investment; pay a tax now in the hope that they'll help find bugs later (sometimes an involuntary investment if your language is type-heavy). This investment often isn't worth it if you're optimizing for code adaptability and development speed. For projects that are more top-down, with full specifications, you've done them before (or you're writing for the [space shuttle]( the type investment increases in value.  For me, the golden balance is Lisp where you can start adding types as needed when the system has firmed up. For other languages, I use no types and no tests in R&D, add tests as things firm up (no new changes without tests). When someone asks for a space shuttle, you don't just need types, you need a whole new way of writing systems.  tl;dr: types are worth the tax when an extra layer of bug prevention is priority #1 "
I'm not a javascript programmer I'm a several-other-languages programmer pissed off that they have to use JS,"The thing is, I don't want to get into JS. it's completely uninteresting to me as a language. The problem is, I don't get to make that choice, like with every other language I use.  I can write my websites in python, ruby, java, PHP, or perl. I can write my desktop apps in C++, C#, Python, Java, or VB. I can write my quick hackjob scripts in perl, python, shellscript, or ruby. I can write my client-side browser scripts in JAVASCRIPT AND JAVASCRIPT ALONE.  It's really the only case (aside from plugins on Windows) where I'm forced to use one language. Add in the fact that I'm not primarily a web coder (so even if I have to, I don't end up doing much JS) and you have a great recipe for annoyance. It's a language I rarely use that I have no choice in using. This is not conductive to remembering the stupid quirks of the language.  tl;dr I'm not a javascript programmer I'm a several-other-languages programmer pissed off that they have to use JS "
BTRFS is about as stable as a whale on a block of ice.,"Here's one:  And quoting from     Is btrfs stable?  Short answer: No, it's still considered experimental. ...Pragmatic, personal and anecdotal answer: (HugoMills, 2011-08-21) In the last few months, the vast majority of the problems with broken and unmountable filesystems I've seen on IRC and the mailing list have been caused by power outages in the middle of a write to the FS, and have been fixable by use of the btrfs-zero-log tool. We also have more filesystem-fixing tools coming along soon, which may make you happier about stability.   I dunno about you but ANY filesystem that can't handle power outages doesn't strike me as ""enterprise"" grade. It will happen eventually its only a matter of time or hardware failure for things to go tits up. Having more filesystem-fixing tools doesn't make me happier about stability actually, i'd rather have a filesystem that doesn't need them. ZFS is MUCH better than BTRFS on so many levels.  But for linux, its the extN filesystems I use. Ok, so PROVE that BTRFS isn't buggy, its really easy to prove it is. Nice try with the deflection though, but this is one deflection I'd shy away from trying.  TLDR: BTRFS is about as stable as a whale on a block of ice. "
"If you're good enough at disassembly, everything is Open Source, or at least Open Ended / Open Outcome.","From the Conclusion: [It's not hard to crack programs] ""With the difference that this process in an actual application will be more time-consuming. Do you know a single popular stand-alone application that has not been cracked ? That is why you need to think of better ways of protecting your software.""  I'd ask this: why should developers spend extra time ""protecting"" ""their"" software?  Should writers spend more time making their writing harder to read / making their plots less accessible, as a way to keep people from understanding their ideas? Do readers not have the rights to modify books they've bought, and even change the plot any way they choose (in  their  copy. Snape  kills  kissed Dumbledore: don't need JK Rowling to sign off on it or assign me a new ISBN.)?  Just because you've created a system and a user has acquired it, you shouldn't assume authority over  their  system, let alone attempting to enforce the [inner sanctity] of your system despite what the user determines the rights of your system should be  within theirs .  Your program won't - no program will - dictate how my system will behave, even as I use it. The option to modify, add, or remove instructions is universal and ultimately unrestricted, despite the arrogant illusion that software authors and publishers control and  dominate  ""protect"" all copies of ""their"" products.  Black box software -- Inaccessible, ""Protected,"" and Restricted -- is an insult to users everywhere, and deserves the hooks Hackers and Crackers of many shades sink into it over time.  Software protection is an opinion ""enforced"" through obfuscation, misdirection, kludges, and sometimes remote-control, which bank on the ignorance and complacency of users despite the increasing ubiquity of technology and increasing dissemination of systems knowledge. Become proficient and begin asserting yourself within systems you own. Or don't, but it won't stop me from learning.  Users are not ""the enemy."" They are the lifeblood: without them, your system runs nowhere, does nothing, and earns nothing. Users needn't wait for the FSF to evangelize every software creator and publisher out there - that may never happen: users could act  now  to increase their proficiency, increase understanding, and [assert their control]( of systems they already own. Unwrap all the boxes! :D  tl;dr: If you're good enough at disassembly, everything is Open Source, or at least Open Ended / Open Outcome. "
"Allows you to reference a set of things as their self, instead of a representation of the thing.","Creating sets of pure logical constructs.  For their example they use weekdays.  If you made a weekday class, and then made the 7 weekday objects, it wouldn't make sense.  Would it ever make sense for a client to make a new instance of the class?  It's not really a full fledged object since you're never modifying anything, it's not storing any data.  It is only itself and that's all it is.As for dictionaries, a weekday isn't a string, so why represent Thursday as ""Thursday""?  Instead represent it as an enum and then when you reference Thursday, you're actually referencing the idea of the weekday Thursday.  Tl;dr: Allows you to reference a set of things as their self, instead of a representation of the thing. "
"That hash has been seen in 2005 on the same site, the shit expands back in time","[Internet Archive link](  Now that's wonky... that page in 2001-2004 had some unrelated stuff (GUPX - some exe compressor) and a spooky  > What about my anonymity? It's worse than you could imagine ! > Move the mouse over the sad smile :( and know all the truth...  and had a [binary background pattern](  Then, in 5th february 2005 it contained this page, containing the hash [link]( Another interesting fact is that this page has been scanned on jan 29th, feb 3rd, 4th and 5th and on march 3rd, the strange page disappeared, revealing the first page, but usually the Internet Archive doesn't scan so often.  Then, on the scan from dec 18th 2005 the page contained the text:  > Esquadrão Netuno 15 Anos> 20.000 horas de vôo  What does it mean ?  TL;DR : That hash has been seen in 2005 on the same site, the shit expands back in time "
"If you want to program in Java, then program in Java.","Opening a file is not a feature of the C language. Neither are finding out the length of an opened file, heap allocation of a buffer of that size, file I/O, or closing of a file. Rather, these are library functions with defined behaviour in failure circumstances.  Why you would want a function to do something besides return an error code is beyond me. libpng handles errors with a longjmp(3) to a specified  jmp_buf , and because of the extra  volatile s that approach is no better than libjpeg's return value idiom in terms of being tricky; on the contrary it is worse since the programmer is used to reading  if(n != 0) goto fail; , expecting an absence of such only in the case of functions that don't have a failure condition that breaks the program's subsequent operation.  tl;dr: If you want to program in Java, then program in Java. "
Just ask them questions that have something to do with their actual job because you are just freaking these people out!,After reading the responses to this post I do have something to add. I think you are asking the wrong type of question. I do admit I have only been developing for 3 years but this type of question has nothing to do with with web development. If you are really trying to be fair with the programmers coming in then ask them questions that actually relate to the portion of the language you are actually going to use on a day to day basis.  Plus if someone really can't figure this type of problem out then they can just google it at work and learn that way.  tl;dr; Just ask them questions that have something to do with their actual job because you are just freaking these people out! 
"If you are considering doing something like this for optimization, please, please don't.","A few things here. I don't know jQuery, but your x86 example is not representative of modern compilers, at least not how something like gcc or icc would optimize C code.  First, most back-end optimizers work on an intermediate format which abstract away control flow to ease analysis (RTLs or an SSA format, typically). So most optimization that rely on fixed-iteration loops use data-flow analysis to determine if the optimization can be performed. Incidentally, the ""optimization"" you describe is akin to induction variable elimination, which requires data flow analysis regardless, because (as shown in the example) a for loop is not required to have an induction variable.  I don't know what makes you think that jnz costs ""far less"" than a test / jne pair. A test is literally just a bitwise-and (you would probably actually use a cmp, which is just a subtract). And on a modern micro-architecture the performance difference would be inconsequential. The branch predictor is getting that jump right 99% of the time in either case so the test will be masked by the next iteration.  tl;dr If you are considering doing something like this for optimization, please, please don't. "
Microsoft has certain companies locked in their yard. Opening the gate would let those developers out.,"I cant quite see it happening. Unlike developing MS office for OSX, porting directx to Apple would cost Microsoft in terms of windows licence revenue, at a time where they have trouble growing their market share.  Suddenly a huge reason for game enthusiasts to buy windows machines would evaporate, and Apple would surge in market share. Its currently just under 10% as per  In that case they would have to figure out some way to get game makers to cut them a share of the game revenues. That would be per use licence on directX or a developer cost or per sale... opengl would just get more attractive.  If the game developers put up with that extortion they might pass it on to the gamers in the form of higher priced games. Then you might see directX games for 70 dollars while opengl ones are perhaps 50.  TL;DR Microsoft has certain companies locked in their yard. Opening the gate would let those developers out. "
Not your lawyer. Rendering orphans into soap might be considered evil in your jurisdiction.,">A lawyer sees that clause and they have to take it very seriously. You see, lawyers usually don’t have a sense of humor when it comes to the law, and they can’t ignore something in a license. A license is just that, a legal document, everything in it must be taken at face value.  Intellectual property attorney here.  I personally would be fine letting a client use this because honestly, the clause is meaningless. The only possibility of a problem would be if the copyright owner decided to sue you for ""using it for evil"", which in my mind at least would a damn hard thing to prove in a court of law. I suppose if you used it to control the machine you keep in your basement that renders orphans into bar soap it might be a breach, but if that's the case copyright infringement is the least of your worries.  TL;DR  Not your lawyer. Rendering orphans into soap might be considered evil in your jurisdiction. "
"Your system needs on-the-fly input validation for the sake of the user, and there is no better way to validate complex strings than RegEx.","This suggestion is really dumb. And just because you consider regular expressions ""complicated"", doesn't mean the rest of us do. Your alternate solution of sending users an email misses the point  entirely .  You don't prescreen email addresses for the sake of you or your backend, you prescreen them for the sake of the user. So you can say ""hey, user, did you really mean to type that percent sign in your email address or is that just a typo?"" Which would be 10 times more common than someone who actually has a percent in their email address.  And so what happens with the invalid email address you send a confirmation email to? User never gets it and now he's just frustrated. He might not even know he entered it wrong. And then he tries to re-register, but now perhaps that username would be taken albeit not activated, and now you gotta waste your time writing in some failsafe in your code for that.  Or you might tell me, well have the user put in their email address twice. But first of all that can still easily fail if they are lazy and copy/paste their error, and for two they are again frustrated because you are making them jump through more hoops to register.  TL;DR: Your system needs on-the-fly input validation for the sake of the user, and there is no better way to validate complex strings than RegEx. "
Haskell successfully converts most logic errors to type errors.,"Most logical bugs, when using a rich type system such as Haskell in a good way, manifest themselves as type errors.  For example, I have serializable key/value-store references. They are represented by a simple key value (a string). However, their type is tagged with the type of the value, and an identification of which table they refer to.  This means that if you try to use a reference to the wrong table or wrong row type (definitely a ""logic error"" by some standards), you will get a compile-time type error.  Similarly, polymorphic types make sure you don't accidentally use the wrong logical operations on the wrong values (the general types won't match).  Discouraging the imperative style, and using arguments and return-values instead, also means much more of your logic is being type-checked. If you forget a statement in an imperative language, that will not be a type error. But if you forget to return a value or pass an argument, or use the wrong one, it will.  tl;dr Haskell successfully converts most logic errors to type errors. "
"1) Get a job
2) Set goal as learning about bits/mem allocation
3) Learn shit
4) ???
5) Profit","Sorry, I tend to ramble a bit. After thinking it over, I have some better advice. SPRINT with making those webpages. Once they're done, have someone on reddit or some other programming site review your code and give you pointers. Clean that shit up, put it on Github. Get a job.  Most employers these days have a review process for determining your bonus. In all the coding shops I've worked at, you, more or less, get to set your own goals for the review. I just took all the stuff I wanted to learn and the boring, tedious, yet necessary stuff I needed to learn, and set them as goals. Perfect motivation because not only do you get to learn new stuff, you get paid for learning new stuff (on company time), AND, if you learn it satisfactorily, you get paid AGAIN at the end of the review in the form of a bonus and/or raise. And if you're not quite an auto-didact, a lot of companies will pay for you to take a class. Though, YMMV depending on where you work, anyplace worth working at as a coder(or any profession for that matter) will have a manager that encourages your professional development.  TL;DR:  1) Get a job2) Set goal as learning about bits/mem allocation3) Learn shit4) ???5) Profit "
A language without macros cannot be the best general purpose language. ),"> As far as the language itself goes, Python isn't really that great.  Well put, really. You make some very thought-provoking points...  Almost every language is ""general purpose"" so I don't see the point from the beginning. Haskell is a ""general purpose"" language and it's obviously better designed and more thought out than C# both syntactically and feature-wise.  If by ""general purpose"" people mean it's used a lot that's a bit like saying all the most popular movies and books are well made. We all know the most used and bought things aren't always the best and more often than not they're actually the opposite.  To offer a different view of ""general purpose"":  Any language that allows you to rebuild it to fit your current use case has to be the best general purpose language, as it's obviously the most dynamic and flexible one. A language that cannot adapt and be rewritten to suit the problem at hand is not as general as one that can.  (Last paragraph TL;DR:  A language without macros cannot be the best general purpose language. ) "
"this isn't about language technicalities, it's about communities and ecosystems","i've recently gotten into clojure, and have hence been exploring the java standard libraries and ecosystem. the discoverability could use work (there are a lot of very useful libraries scattered around the place, and you could spend ages solving a problem without ever realising that someone has already written and opensourced a solution), but on the whole i've been favourably impressed.  i've not done much c++ in the last decade (and what little i have has been qt based), but referring to the ""standard libraries"" is misleading. the commercial and open source ecosystem around c++ is  huge , and in excellent shape. it really makes no real difference what the language blesses as a standard library; the community is active and thriving and can make its own standards (and has. several of them, and not in the oxymoronic sense.).  the d libraries seem like they're in bad shape, i'll admit, but i'm watching hopefully. unlike .net, there will hopefully not develop a culture of sitting back and waiting for the powers-that-be to deliver an ""official"" library for any given task.  tl;dr: this isn't about language technicalities, it's about communities and ecosystems "
"Your ""hackers or script kiddies"" idea is a false dichotomy and can safely be ignored.","You don't seem to be thinking this through properly. If the access to the PSN network wasn't available until a certain release by PS3 firmware hacky type people, then all of the 'hacking' done to date was completely impossible until that point.  After its release - a few weeks ago, AFAIK - all of this shit went down. And it went down fast.  To be honest, it was probably done by both groups. A vulnerability was exposed. People with the knowledge and knowhow immediately went for the throat and stole things like CCs, while ""script kiddies"" learned how to get free DLC, started hitting the servers too hard and in crazy ways, and made Sony super suspicious.  I don't know if this  was  the case. It's entirely possible, though. That and many other possible combinations of events that you have completely dismissed.  tl;dr: Your ""hackers or script kiddies"" idea is a false dichotomy and can safely be ignored. "
"Good ideas backed up by good notation are used, good ideas without good notation are not.","Fundamentally, all programming languages and conventions beyond 'throw enough machine code at the chip to get the specific result you want at the moment' are just syntax sugar.  This is a bad way of looking at it, though: Notation is powerful. Notation opens your mind to new ways of thinking. Basic arithmetic is hobbled until you hit upon things like place-value notation; without it, you can't develop the same intuitive grasp of numbers an average elementary school child has. With place-value notation, the concept of 'orders of magnitude' begins to make sense, and you can see that the numbers 1,000 and 3 are connected in an intuitive fashion. This leads directly into logarithms and exponents and, therefore, compound interest and the entire economic system as we know it.  Bringing this back to Java, well, you think encapsulation and hidden state (private variables) are good, right? It's entirely possible to do that without any help from the language, but it's harder and a lot more prone to simply breaking down entirely. What's more difficult is even getting the idea of private variables without a notation to guide your thought.  You are almost certainly unaware of what 'spaghetti code' in its oldest, unreconstructed form was. Imagine a language with no control structures beyond 'goto' and 'if this then goto'; essentially, imagine assembly language and FORTRAN IV. Now imagine people who have never seen a while loop or even an if-then-else block writing code in such a language. They have gotos going all over the place. There is no high-level structure, no convenient blocks of code you can analyze without having to look at the whole program at once. They  could have  organized their code better, but, in practice, they  didn't  until they saw languages like Algol and Lisp which had support for such structure built-in. It simply never occurred to them.  tl;dr : Good ideas backed up by good notation are used, good ideas without good notation are not. "
I've given a variety of Java alternatives a chance but every time I keep coming crawling back to Java+Eclipse.  Perhaps Java 8 will be my salvation.,"Very interesting, and pertinent to me as I've been thinking about migrating our company's (Java) codebase over to Scala.  Unfortunately, every time I fire up the Scala Eclipse Plugin and start to hack, it is just a matter of minutes before I run into frustrating problems that have little to do with the problem I'm trying to solve, and everything to do with the complexity of Scala, or the immaturity of the Scala Eclipse plugin.  Bear in mind that I've been playing with various iterations of the Scala Eclipse plugin for about two years.  Every now and then I'd hear about how they've rewritten it, or made significant improvements, but every time I conclude within minutes that it isn't ready for me to bet my company on.  I've also tried Idea's Scala support based on numerous suggestions, but here too I run into various headaches.  Our stack is based on Maven, but trying to get Maven to play nice with Idea to play nice with Scala was also an exercise in frustration, I gave up after an hour or two.  I am therefore interested in languages like [XTend]( which are far less ambitious than Scala, but which attempt to address some of Java's shortcomings such as lack of closures and limited type inference.  Unfortunately getting XTend set up with Eclipse was  also  a frustrating experience (even though it's created by the Eclipse people!).  The setup process for it is very poorly documented (I think about 2 sentences on their website last time I checked), and I gave up after about 45 minutes of frustration.  tl;dr I've given a variety of Java alternatives a chance but every time I keep coming crawling back to Java+Eclipse.  Perhaps Java 8 will be my salvation. "
"OP needs a UPS for his  next  computer.  His current system is mortally wounded thanks to brownouts and blackouts, however short.","I for one, didn't see the video since I can't view it at the moment.  > His IDE crashes several times, has power failures and internet connection problems.  As someone who used to code in an environment with power issues, I can say with confidence: this is why his IDE crashes and his internet goes down.  He may well have some filesystem corruption, or even some accute hardware damage.  To give you an idea of how terrible power drops and brownouts can be to a PC, I had a system that developed random instabilities because of this.  It would blue-screen at odd times, and I could never reproduce the fault on demand.  So I ran memtestx86 on the thing over a weekend.  For the first 36 hours or so, nothing.  Then, it started getting 1 bit of error on every byte read... a few hours after that, 2 bits, then 3 and so on.  Basically the only thing keeping memtest running was that it was hiding in the CPU cache.  The system lost total memory stability in under two days of uptime.  tl;dr;  OP needs a UPS for his  next  computer.  His current system is mortally wounded thanks to brownouts and blackouts, however short. "
"I want to make games, not spend days wondering why my parent's ATI card is rendering a black screen.","I started using unityscript with it's hideous Javascript bastard child and then changed to C#. I had never used any of those languages.  I've had some fun making my own engines on java and C++ but honestly if you just want to make a game use unity.  Sure it has issues but it takes care of most of the heavy work for you. My Java OpenGL engine gave me a lot more freedom in what I could do but in the 3 months it took me to make a decent rendering pipeline I could have cranked out several games in unity.  TLDR: I want to make games, not spend days wondering why my parent's ATI card is rendering a black screen. "
"Just because something shouldn't be outlawed, that doesn't mean it should be encouraged.","To extend your analogy, DRM is like a manipulative boyfriend or girlfriend. Sure, you are technically free to leave the relationship at any time, but there's something about them that that you can't find anywhere else (content providers have monopolies on their content).  If you want them, you have to do whatever they tell you to.  You want to leave, but every other potential mate (other content provider) are just as bad.  The manipulators know they're being assholes, but as long as everyone is doing it, they have all the incentive to keep manipulating and no incentive to stop. In an ideal market, they'd be competing in this regard, but it's in their benefit to stand together on this, and even when new players enter the game (like netflix's new exclusive content) they have more incentive to join the manipulators then stand against them.  (un)Fortunately, the web is all about freedom.  Netflix is free to create a DRM laden app.  And send encrypted packets to and from it over the web.  Similarly, Google chrome or any other browser is free to implement a nonstandard DRM mechanism and other companies are free to make use of it.  What's not OK is making DRM a WC3 standard.  The WC3 isn't in the business of telling people what they can and can't do.  It only tells people what they should do.  To finish up the analogy, making DRM a WC3 standard is like the federal government officially endorsing manipulative relationships and telling its states (web browsers) that they should too.  Instead they should be officially denouncing manipulative behavior.  Tldr; Just because something shouldn't be outlawed, that doesn't mean it should be encouraged. "
"I think it's too little information to start calling PA jerks 
 EDIT: I didn't proofread before I clicked post","I don't like the posting itself but I think we're making a lot of assumptions about the job and PA itself based on the posting.  Not to mention we're saying the pay is horrible without really seeing numbers.  Being able to perform all these tasks isn't same as expected to perform all these tasks at once.  Might be the person would have to switch between roles with already existing staff.  We're assuming the pay is going to really low but we don't really have numbers here.  As far as we know, they could be considering 200k annual salary normal and want to pay 20k less to spice up your work life.  What if, when they say they want to make your work life really good, they really really meant it.  Really good and awesome office, sign you up whatever fitness centre you want and lunch/dinner/breakfast included, etc.  TL;DR I think it's too little information to start calling PA jerks  EDIT: I didn't proofread before I clicked post "
"he and programmers just  don't  have a lot in common, no wonder skill set is different.","Bear in mind that you're a programmer, and the guy over there is showing  cracking  skills.  The two overlap, but the latter is really something else.  For example, one of the skills is patching ptrace C function (or some other, doesn't matter). A programmer needs to do this only when   he's calling into native code  that code gives some result that causes a bug  that result is too hard to produce in normal operation   I have a decade or so of experience, most of it with reasonably low-level stuff, and I've done it a handful of times.  A cracker, however, needs to do this kind of manipulation as a matter of fact on a daily basis.  tl;dr: he and programmers just  don't  have a lot in common, no wonder skill set is different. "
you weed out the bad ones quickly so you can spend more time with the good ones. Fail fast.,"You are correct, but there are other things interviewers are probing for, especially at the beginning of an interview.  The applicants resume says he knows <lang>. Is this candidate familiar with <lang>'s stdlib?  A surprising number of candidates can't even pass fizzbuzz type questions. Process questions are useful. My only problem with this question is that it spells out all the details of the problem. The point is to see if they ask you to specify what the problem is.  This should take a few minutes. If it takes longer you already know this person is a bad hire, don't bother asking them them anything harder. Asking one or two ""small"" questions can give almost as much insight as a ""big"" one, but in a fraction of the time. Engineer time has a cost.  Tldr: you weed out the bad ones quickly so you can spend more time with the good ones. Fail fast. "
People become professors 'coz they can't land a job even in the low-caliber IT Services firms.,"Conversation between me and one of my 'professors':  > Me: ""Why exactly did you join here anyway?""  >   > Him: ""I finished my B.E (Undergrad) in 2006. Height of recession, and I wasn't able to get a job. So I did an M.E (Postgrad). Finished in 2008 - still in recession, so no job. So I applied wherever possible, and this college took me in. After spending two years here, I've decided to stop applying for jobs everywhere else and stay here""  People like this set our curriculum and 'guide' us. This guy will probably be here for a decade, then switch to another college and eventually become ""Head of the Department"" due to his 'experience'. Sigh.  TL;DR: People become professors 'coz they can't land a job even in the low-caliber IT Services firms. "
"boss treats me like a human being, I work my nuts off in return.","My boss gave me an unexpected bonus (not saying how much but it covered nearly all of my new decent spec gaming PC) because I worked some long days without been asked (it was required to make a deadline) which I thought no-one knew about (I don't draw attention if I work over my hours, it is my choice) which translates to about 83 12"" stuffed crust pepperoni's from the local place.  If we stay late for anything I get the time back and he buys pizza or take out, I can pretty much work whatever hours I want.  I pay half the rent on my new place that I should since the boss owns the company that converted the building.  If I go out for lunch and fetch him a sandwich he usually pays for my lunch.  I get carte blanche on any hardware or software I need to do my job as long as I can make a reasonable case for it (dual head 23 inch TFT's no problem, Cray XD he might query).  He took me aside one time (I'm salaried) and told me ""You're working too hard, you need to cut back on the hours, if something breaks it breaks we can fix it, if it takes a few days longer that is no problem, I'll just tell everyone I screwed up the schedule, no point making yourself ill"".  My job is hectic and can be stressful (mostly because we move  fast  on everything) but he looks after his staff like no-one I've ever worked for and at the end of the day I walk out looking forwards to work the next day.  As a boss he has set the standard for me in the event I ever run a team again since he gets the maximum out of everyone, has happy staff and makes it look fucking effortless.  Throwaway in case anyone from work see's this.  TLDR: boss treats me like a human being, I work my nuts off in return. "
Linus Torvalds opinion on XML is basically worthless unless you're working on the Linux kernel.,"> The point is that Linus has very good reason to dislike SVN - because it's simply not of any use to him  I'm not sure how this is any different from what I said. He can't use it, doesn't need it, so he trashed it. Again and again you have said as much in your comments. It didn't work for  him . It didn't have the features  he  needed, etc.  This would make for  good advice  in a world where everyone was Linus Torvalds. In the real world, where professional programmers deal with a myriad of different use cases, this is  bad  advice. There is a huge difference between bad software, and software that is bad for  you . Some software is bad for it's intended purpose, and some simply doesn't align with your needs. Git is shitty on Windows, Mercurial is much much better. That doesn't mean Git is shit. If Linus were a Windows programmer, we both know he'd be claiming Node.js is crappy  because it doesn't work on Windows .  If you're doing C development on a large open source linux project, Linus advice is  great . Otherwise, his opinions are indistinguishable from the average Internet blowhard. It's a shame, because he's undoubtedly brilliant. He simply doesn't give a rat's ass about anything that doesn't directly affect him. Contrast that with, say, John Carmack - whose opinions are insightful, instructive and drawn from a broad perspective.  TL;DR: Linus Torvalds opinion on XML is basically worthless unless you're working on the Linux kernel. "
"Security on the Internet for the mass population basically isn't so good right now, and will probably get worse before it gets better.","Every web browser has a special list of ""root certificates"", these are cryptographic signatures that are trusted by the browser for verifying the authenticity of other ""certificates"", which are cryptographic signatures used to verify that the server you are talking to really is authorized to talk on behalf of the domain. If you see an address in your browser beginning with ' then this mechanism is active. This mechanism is  crucial  to the security of any private transactions on the web: reading e-mail, Internet banking, shopping, ....  Without it, anyone on your local network, Internet provider's network, or potentially elsewhere ""lower down"" in the Internet infrastructure, can read everything you type.  The thing is, getting a root CA (certificate authority) to provide you with a certificate is supposed to be really hard, and require some kind of evidence of ownership for the domain in the address bar. Unfortunately over time, some CAs have become increasingly relaxed with their requirements for evidence, or potentially in this case, lax with their computer security.  The end result is that, somehow, either through collusion on behalf of the certificate authority and the Iranian government, or more likely, through some kind of hacking incident, someone has managed to get a hold of a signed certificate covering all of google.com , signed by a root CA recognized by all major browsers.  In laymans terms, your browser will tell you that you have a secure connection to Google (green address bar, etc.), whereas in reality that connection is being routed by a third party (Iranian secret service, supposedly) who is logging everything being sent.  This isn't the first incident of its kind. It's not even the first incident of its kind  this year . So far, there are few mitigations available. You can manually remove CAs from your browser, but this is almost impossible to do in any kind of sensible way, since you might want to visit a secure site whose certificate was provided by one of the CAs you removed. Google Chrome has a new feature called ""pinning"", which helps mitigate the problem on a per-domain basis (as I understand it), however this is far from a universal solution and requires manual maintenance for each site to implement (so far, I think pinning is only used for Google.com itself in Chrome).  tl;dr: Security on the Internet for the mass population basically isn't so good right now, and will probably get worse before it gets better. "
"viper begat vimpulse, and it begat evil, which is a good and righteous mode","Evil is the most recent iteration in a series of incremental improvements on the good old viper-mode, essentially adding more advanced stuff like Visual Mode. If you stick only to the basics of vim you won't probably notice the difference.  From  Evil supercedes Vimpulse. Vimpulse prehistory started with extended-viper, by Brad Beveridge. Alessandro Piras took extended-viper and enhanced it into a codebase called vimper, which isn’t available online. Vimpulse history began when Jason Spiro split vimper into two parts to make two projects: viper-in-more-modes 0.1 and Vimpulse 0.1. Jason was maintainer of Vimpulse from 0.1 (May 2007) until he passed the maintainership on. Alessandro inherited the maintainership from him in June 2009. Vegard Øye inherited it from Alessandro in April 2010. In February 2011, Frank Fischer proposed to merge the Vimpulse and vim-mode projects. Frank’s initiative culminated in a new project, Evil.  tl;dr  viper begat vimpulse, and it begat evil, which is a good and righteous mode "
You're better off doing it the right way. People who know far more about security than either of us wrote and recommend bound parameters.,"Eh, that's even worse. You've now included a side effect in just connecting to the database?  Every time you connect to the database, you're changing the super globals without warning.  What if you need to connect to two different databases? You've sanitized the strings twice then.  &lt;?phperror_log(""Received last name: "" . $_POST['last_name']);$mysqli = new mysqli('localhost','user','pass','db');// your code snippet here$Smarty = new Smarty();$Smarty-&gt;assign(""last_name"", $_POST['last_name']);  Poor Tommy O'Leary. When he submits his last name, it's written to the developers' log file as ""O'Leary"" and inserted into the database correctly, but when the page renders to confirm his changes, it now says ""We saved your new last name, O\'Leary"". And then, because it saved in the database correctly, if he refreshes the page, it'll then say ""O'Leary"".  TL;DR: You're better off doing it the right way. People who know far more about security than either of us wrote and recommend bound parameters. "
"Standup meetings, even the 30 second ones are worth it.","I think this is a great idea with limited use. I worked on a team this past year(student game project) that consisted of four members. Some mornings were exactly like he described, we stood up and someone asked ""Is there anything that's giving you trouble?"" or ""Hey, can you come look over my code/art/bug log"". However, I think the most effective method for our team was to utilize something that was right in between the two. We set a timer for 5 minutes and we went in a circle and answered each question one at a time. We all took turns answering what we did the previous day(s), and then we went back around and told each other what our plan for the day was.  The biggest change we experienced was when two of our developers left for a week to go to GDC in San Fransisco. At that point the meetings turned into the 30 second stand ups that were always ""Anything giving you problems? No? me either. Let's work on X today"" and we sat down and worked on it.  The quick stand up meetings worked for us because we had a small team of focused people; there were multiple teams at my College and I know that many of them did not use their time wisely, I think that stand up meetings are what put my(and one other) development team light years above the rest in terms of how much of our game we got completed. We were one of two teams that released a full game this year, and I honestly attribute that to the standup meetings we had every day.  tl;dr: Standup meetings, even the 30 second ones are worth it. "
"don't dismiss the seemingly hard problems, try solving them anyways step by step;profit from experience gained along the way","Forget this thought. Seriously, this could be one reason for your ""creative gap"". I am overwhelmed by the majority of problems when I see them at first and I do have a decent amount of programming experience in multiple languages and paradigms. And so what, I still need to learn sooo much everytime I tackle some idea or problem. Try to break down a complicated seeming problem like a Reddit bot into basic thoughts, e.g. ""What does a bot do?"" --> connect to reddit, make a post, read some posts etc This gives you already a small task list, you have to find out how it connects and what you do need to set up a simple connection. After that, you can try adding another feature, then another and before you know, you are writing a reddit bot. A feat you deemed to be well above your level. Simply by not caring about levels and just tackling the problem ;) Of course, there are indeed projects you will not finish or where you will be stuck a long long time or that you won't understand for a long time until hopefully you encounter that ""Aha!"" moment. But at least in my experience, in those cases the problem could be something else than the actual programming at hand, constraints like time (you can't do this alone in a reasonable amount of time) or wrong choice of tools ( e.g. that point your though-process towards an unhelpful direction) or something else. And even not finishing something still teaches you a whole lot and you will do better at the next task. You can then always come back to the unfinished stuff and trust me, it will look different suddenly.  tl;dr; don't dismiss the seemingly hard problems, try solving them anyways step by step;profit from experience gained along the way "
you bought into lies. be skeptical about what you read online.,"[](  There is a link to the messenger app, android permissions. It requests these permissions because without them you would not be able to utilize those features on Android at the press of a button. It's a communications app that allows you to record messages. For example, the checkers app I wrote, had similar eerie worded permissions such as ""access your contacts"".  [  That is facebook's terms of service, which also applies to their mobile app. It's one of the most human readable ToS I've ever read. Yet in both of these your quote does not exist.  Again, this is a kneejerk reaction, with people like yourself ignorantly repeating misquoted information in an attempt to rile people up against a big corporate entity who  MUST  be bad. But really? All apps have these creepy sounding permisisons. It's just verbage, nothing more.  Here's farmville 2, read it's permissions:  [  Notice you give permission to Farmville 2:  >   modify or delete the contents of your USB storage>   test access to protected storage  ...and I'm guessing you've never posted about Farmville 2 deleting your personal data right? or hacking your secure data? Seriously it's fear mongering click bait that was roused up to exploit the ignorant. Don't buy into it.  tl;dr: you bought into lies. be skeptical about what you read online. "
Edge case crushing doesn't help anyone. Provide  real  benchmarks and I'll look again at them.,"I used PHP 7 with MarkBasedDispatcher (which is fastest for me here), maybe that's why I get different results… When comparing always use the options provided which work fastest for you.  Generally, for first route, TreeRoute is about 30-40% slower than FastRoute for me.  Also, you define a set of a thousand routes… obviously, regex doesn't like that in last route/unknown route cases. [with 10000 routes it's even 57 times slower.]With a hundred of routes TreeRoute and FastRoute are of equal speed. And with less routes FastRoute always wins.  Also, your benchmark is one thing: totally synthetic. The strengths of FastRoute don't show up in thousands equal length routes... Real world routes looks like: lots of common static prefixes, then eventually a few dynamic parts and maybe then a static suffix.  TL;DR: Edge case crushing doesn't help anyone. Provide  real  benchmarks and I'll look again at them. "
"First-class languages in CLR seem to be superior to Java, but upcoming languages in the JVM show incredible polish and innovation.","Well, as it stands the CLR is in many ways superior to the JVM in terms of capabilities, and I'd say that most people who know the intimacies of both would not disagree.  But it still stands that the JVM is Free Software and its designers seem committed to keeping it that way. Which means that only the JVM is a suitable target for FOSS technologies that attempt to go beyond mere experimientation.  I say this as someone whose day job includes programming in .NET. While appreciate the niceties of a well-designed language like C# against the more outdated Java, the JVM seems to have a much more vibrant community of language designers who are targeting it.  CLR currently has these languages as first-class citizens:    -C#    -VB.NET    -F#And these languages are actually runnable, though they have little to no support for real-life usage:    -IronPython    -IronRuby  Notice that the interesting languages, however great they may be, are owned and designed by Microsoft.Compare it to the JVM. Its major language is Java, however you can get these languages to run as well in a very productive manner:    -Scala    -Clojure    -Jython    -JRuby    -Groovy  With the exception of Groovy, the other languages are not funded by Sun, but they are extremely usable and useful, and they manage to get around the JVM's few limitations in compile time.Truth is, when you're doing language experimentation it might be best to go for a platform that is not controlled by a single party in the same way the CLR is, which is ultimately dependent on whatever Microsoft's whims may be. That has turned out to be pretty good for VB.NET and C# until now (read: LINQ and lambdas), but I fear that C# 4.0 is trying to ride the wave of success Python and Ruby have gotten by shoehorning features that make little sense for C#'s design philosophy.  There are advantages and disadvantes to both approaches, but if we look at it from a pragmatic FOSS point of view (pragmatic in the sense that it looks for independence from proprietary vendors) the JVM wins the day.  tl; dr: First-class languages in CLR seem to be superior to Java, but upcoming languages in the JVM show incredible polish and innovation. "
"Learn C++, then start modding or build your own engine.","First, learn C++. Not many game engines are written in Java these days.  The next step depends on what you want to do. If you want to do FPSes there is an easy answer. Pick up a copy of Unreal Tournament 3 or a Source game and learn to mod it. Add weapons, game modes, fiddle with the AI, etc. I would suggest UT3 unless you want to work at Valve because Source is used outside of Valve less often than Unreal Engine 3 is used outside of Epic.  Other types of games are somewhat harder to say, because there are a number of engines that could be used. Learning OpenGL/DirectX, some sound libraries, and other fundamentals of game engine design/programming could be helpful.  tl;dr Learn C++, then start modding or build your own engine. "
one way to solve communication problem of outsourcing is outsource to a country near your time zone,"Interesting how all these cases talk about outsourcing to Thailand, India and other far off countries. My group originally hired workers in Ghana. That didn't work out so well. They eventually closed the Ghana shop up because of infrastructure problems and the high failure rate.  However, shortly after my company started outsourcing to Mexico.  Previously the main problem had been communication: with workers on the other side of the planet, calls could only take place at certain hours of the day. With workers in Monterrey, 1 time zone off, calls and emails could be shot back and forth until specs were...specified and problems solved. I don't know if we were just lucky, but the workers also seemed eager to impress and tended to work longer hours than our in-country people (all were salaried since my company was a subsidiary of a larger, outsourcing company).  In short, I was impressed by the work that the Mexican workers did and by their work ethic. I think them being closer to my time zone solved many problems  TLDR: one way to solve communication problem of outsourcing is outsource to a country near your time zone "
HTML 5 Canvas is the cool new kid on the block. Everyone expects him to beat up Flash.,"HTML 5 is the latest iteration of HTML, the language a web page's structure is made out of (""This is an image"", ""this is a link""). It's often used as a blanket term for a number of new web technologies that are closely related (things like local storage, dragging and dropping files in and out of your browser, audio and video). HTML 5 features a few new HTML tags that are exciting: <audio> and <video> which do what you'd expect them to as well as a new tag called <canvas>. It basically tells your web browser ""this is a blank canvas that you can draw on"". On its own, it's not really interesting. It's a blank canvas, after all.  A programming language (on the web, this will be JavaScript) can draw things on that canvas. For example, you can do something as easy as ""draw a single dot at position 10,34 in the color #f0ad35"" or ""draw a line from 10,10 to 40,24"". But you can also create psychedelic mathematical patterns, particle effects (like the blood in OP's game) or draw existing bitmaps (sprites, like the backgrounds and characters in the game) onto the canvas. JavaScript can also handle input such as keyboard and mouse usage.  So browsers can now, easier than before, create interactive elements without needing a plugin like Flash or Silverlight. These plugins come with some drawbacks (Flash often uses a lot of CPU, although some Canvas applications do so as well). ""HTML 5"" is often touted as the ""Flash killer"" which holds some water, but they both have weak and strong points. Flash is seen as aging, bulky, slow and insecure  [citation needed] .  tl;dr HTML 5 Canvas is the cool new kid on the block. Everyone expects him to beat up Flash. "
"In both cases, a ""signature"" is just a proof the author possessed a particular key, and applied it to a specific message body.","It's actually the same concept... in both cases the ""signature"" is a short number of bytes generated based on the message and some secret key, which can be regenerated to prove the author possessed the key and the message... i.e., that they ""signed"" it.  The main difference lies in how the signature is verified. For MACs, it's generally based on a shared private key, so the recipient would also have to have the secret key, which they'd use to regenerate the MAC signature.  For public-key systems, the signer generally digests the message (via sha256, etc) and then encrypts the digest with their private key... the verification process therefore requires decrypting using the public key, and verifying the resulting digest. Only someone who has the private key should be able to encrypt the digest correctly, thus proving they had the private key to ""sign"" the message.  tl;dr  In both cases, a ""signature"" is just a proof the author possessed a particular key, and applied it to a specific message body. "
"In Sweden and Finland, there was a Feburary 30, 1712.  For insane reasons.","[I'll just leave this here](  > Instead of changing from the Julian calendar to the Gregorian calendar by omitting a block of consecutive days as had been done in other countries, the Swedish Empire (which included Finland at the time) planned to change gradually by omitting all leap days for the entire period from 1700 to 1740 inclusive. Although the leap day was omitted in February 1700, the Great Northern War began later that year, diverting the attention of the Swedes from their calendar so that they did not omit leap days on the next two occasions and 1704 and 1708 remained leap years.  > To avoid confusion and further mistakes, the Julian calendar was restored in 1712 by adding an extra leap day, thus giving that year a 30th of February. That date corresponded to February 29 in the Julian calendar and to March 11 in the Gregorian calendar.  > The Swedish conversion to the Gregorian calendar was finally accomplished in the usual way in 1753, by omitting the last 11 days of February.  TL;DR: In Sweden and Finland, there was a Feburary 30, 1712.  For insane reasons. "
Sum types are basically checked exceptions that can be returned instead of thrown so you can check if all error conditions were handled.,"The question here is...  did  you ignore the error code?  If you are using null/ptr then you can say whether you checked for null but if you didn't you can't say whether you were supposed to (function may always return a valid pointer).  If you are using a standard (val, error) return pair then you can check for handling error, but you can't say whether val needed to also be handled (without looking at the called function source).  Also since the error is returned as a single value it has to be a supertype, so you can't say whether you handled all of several possible error types unless the error handling always covers a default case (in which case you can't say if you properly handled the actual error types).  If you are using unchecked exceptions then you can't say if all possible exceptions were handled, but it is easy to recover at a higher level from unhandled errors.  If you are using checked exceptions instead of errors then you can say if all error conditions were handled.  If you are using sum types then the type includes the success value type plus every error condition type (could be 1, could be several).  So you can say if you handled all possible error conditions.  From this it should be clear that Google Go uses a pretty bad form of error handling.  tl;dr Sum types are basically checked exceptions that can be returned instead of thrown so you can check if all error conditions were handled. "
"exception safety is non-trivial, even if you use RAII consistently.",">Do you have an example of this that RAII would not solve?  Firstly, I did not claim that exception safety is an unsolvable problem, I just pointed out that exceptions affect your code design even if you do not catch or throw them. Your previous post suggested you were not aware of that. RAII is one example how exceptions affect you.  As for RAII, it sure goes a long way towards exception safety but it is not sufficient by itself. Another thing you need to keep in mind is API design. Some APIs are inherently not exception-safe no matter how you implement them.  This Cargill's article  method that removes the top element of the stack and returns it by value. This natural API cannot be implemented in an exception safe manner, at least not with the value semantics of C++. This is why, e.g., STL's stack implementation has a separate top() method which does not modify the stack, and a pop() method which returns nothing. See [this article]( for detailed explanation.  tl;dr: exception safety is non-trivial, even if you use RAII consistently. "
"If all you have is error codes, you'd best not ignore them.","Let me back off a bit and explain what I mean by ""actively idiotic"":  Suppose we're using a language with exceptions, but without checked exceptions, or Java with best-current-practices to avoid those evil things. Then, when someone writes  x = foo();  there's no exception handling, but neither is the programmer going out of their way to throw away information. When foo throws an exception, it propagates up to the top level and the program falls over dead, but leaves behind a message describing the problem where it was discovered and typically a stack trace.  Now, say you do something similar in C or Go:  foo(&amp;x); /* C: Ignore the return code. */x,_ = foo(); /* Go: That's easier than handling errors */  In these cases, the value x is invalid or corrupted, but the information that foo had about the error is gone. That's why in C, and presumably Go, you always have to check your return codes whether you want to or not, and it's one of the big advantages many people see in exceptions: if you ignore exceptions, your program crashes with a meaningful error message; if you ignore error codes, the information about the error where it was discovered is gone.  Now, what I mean by ""actively idiotic"" is someone writing:  try { x = foo(); } catch (Throwable t) { /* nope */ }  That  requires malice aforethought. The programmer is going to a fair amount of extra work to make everyone's life miserable later.  Note 1:  I did not call any hypothetical programmer an idiot. I referred to a given activity, the writing of specific code, as idiotic.  Note 2:  I apologize if I offended anyone by calling that activity ""idiotic"". yougivemelefebvre is right, that's not an argument. Actually, I don't apologize. If you write that kind of thing, I hope you get phone calls at 2:30 in the morning. From angry people. Often.  tl;dr: If all you have is error codes, you'd best not ignore them. "
"Maven has a fairly steep learning curve, but is a capable tool and is well worth devoting the time to learn.","Maven is a complex beast and has a not-insignificant learning curve.  Most people who scream from the rooftops how horribly broken the platform is have not taken the time to understand how it's supposed to be used and how to properly configure it.  When set up properly, it makes for very streamlined builds, CI, deployment, and even development.  But it has to be configured properly.  My company has some extremely complex build requirements, with large multi-module applications, strict CI/regression requirements, etc.  Maven (and Jenkins) makes this all possible, and it runs like clockwork.  This didn't happen overnight.  There was no small amount of frustration involved in getting to to this point.  No one ever claimed that complex systems were easy to build...  TL;DR: Maven has a fairly steep learning curve, but is a capable tool and is well worth devoting the time to learn. "
"The Robustness Principle is mis-named. It leads to software that is less robust, since most of the time doing nothing is vastly preferable to doing the wrong thing.","I have to disagree with the Robustness Principle. I have had far more trouble with software and libraries that attempts to do the ""right thing"" with incorrect input than I have had with software that crashes hard (preferably with a nice stack trace or error message) when it encounters an unexpected problem. In my opinion, the Robustness Principle (a.k.a Postel's Law) is one of the most pernicious bits of bad advice you can give a software engineer. It sounds really good in theory. We would all love to have software that ""does the right thing"" with the input we give it, regardless of how mangled that input is. But in practice, it's impossible to do the right thing with all error cases. And in cases where it is impossible to do the right thing, I would much rather have software that crashes reliably and predictably than software that does the wrong thing.  tl;dr: The Robustness Principle is mis-named. It leads to software that is less robust, since most of the time doing nothing is vastly preferable to doing the wrong thing. "
"instinct therefore becomes hard to fight. 
 But fight it I must, for great justice.","Feynman once wrote about inventing his own mathematical notation to avoid ambiguity. (He considered ""sin x"" to look too much like s times i times n times x, for instance.) He used these personal symbols for years, but abandoned them when he tried explaining something to a friend, who didn't understand his written notation.  Most of the math texts I've read explained introduced symbols with sufficient clarity to be approached by interested readers. Of course, keeping track of all of them (remembering, for instance, that a little squiggly d means ""distance function"" whereas a little squiggly b means ""epsilon ball"" -- probably utter hell on dyslexics) becomes a chore, and when confronted with a page full of equations the ""tl;dr"" instinct therefore becomes hard to fight.  But fight it I must, for great justice. "
built in vb get ip from hostname function spammed packets on network 1 with the ip address from network 2.,"This wasn't boneheaded in terms of code I  wrote , but something boneheaded was going on with the webpage and I'm still not sure what.  I wrote a webpage in classic asp which, in part, pinged a number of servers so it could display a green, yellow, or red bubble next to the name (for up, slow, or down).  The page automatically refreshed every 60 seconds or so, so we could keep it in a window and swap over to it periodically to make sure everything was up.  When I wrote it, I wanted to test the code, particularly the other functionality of the page (which included ftp'ing into an environmental monitoring unit, pulling the latest log file, and parsing it to display the computer room temperature on the webpage).  So I brought up ~10 copies of the page and left it running for around 20 hours until I got back the next day.  The next day, my supervisor is contacted by network security, trying to figure out why they were getting a shitton of outbound packets for an ip address which didn't match any on the mill's network.  They eventually figured out it was the ip address of the second network card on my computer which attached to a private network.  That's right.  My computer was sending out packets on NIC 1 with a source IP address of NIC 2.  When the destination computer tried to respond, the router had no idea where to route the packet.  So it sent it up a level.  And so on and so forth, until something tried to send it outside the mill's network where it got blocked by the firewall.  Now, surprisingly, my webpage showed absolutely no ill effects of this occurrence.  It had happily refreshed every 60 seconds for an entire night and was still working properly the next morning.  So why on earth was my computer sending out bad packets?  Well, the networking functions I was using required a destination IP address, but I only had the server name.  So I utilized a built in function to get ip by hostname.  Somehow this function both completed successfully AND was sending out bad packets.  tldr: built in vb get ip from hostname function spammed packets on network 1 with the ip address from network 2. "
"Smalltalk images might have been an bad idea. Erlang is OOP. OOP isn't well understood yet. 
 EDIT: Fix typos and spelling","You can get the transcript by clicking the little + button underneath the video. If you want something even shorter, here's my interpretation of what they said:  RJ: Ideas spread slowly. The smalltalk image may not have been a good idea - it creates a tight coupling between code and an entire system, threading is difficult, and managing changes from large amounts of people is hard. But by and large, the tools and dynamic nature of smalltalk are brilliant. The rest of the world is starting to catch up.  JA: Erlang is very object oriented. It holds the 3 basic tenets of OOP: message passing, isolation between objects, and polymorphism. Message passing is the most important aspect.  RJ: Erlang has 2 parts. A high level object oriented part and a low level functional programming part. There's potential for issues with large amounts of messages. Object oriented programming has been around for a long time but it still isn't as prevalent as structured programming. It takes a long time for ideas to spread, even good ones.  tldr; Smalltalk images might have been an bad idea. Erlang is OOP. OOP isn't well understood yet.  EDIT: Fix typos and spelling "
"Git flow is great, works well on many projects, but not all of them. Learn it and then adapt your own strategies.","Git flow works great for web development projects where you only have a single production version of the app at a time.  At my company, we have a product that has flavors for consumer, small business, and enterprise. Consumer and SB are relatively similar and we run the code on our own servers so we can control versions. However, with enterprise there is the option to license the code and run it on their own servers. We have many clients running different releases of our code and there was no way of knowing if someday you'd need to jump back 3 versions to make a hotfix. It simply didn't make sense to have a single master branch and to merge releases into it.  We settled on having an unstable branch (master) and release branches M1_R01 (Major version 1, release 1). Hotfix branches are made off of the release they are for, and then merged back into master as well. Periodically, we take stock of client versions and get rid of old release branches if possible. Developers are of course free to make local feature branches at will and optionally make them remote branches if collaboration is needed.  tl;dr Git flow is great, works well on many projects, but not all of them. Learn it and then adapt your own strategies. "
"You should not worry about ""future-proofing"" interfaces. They should specify one role and one role only, and higher-level features emerge from composition of classes implementing small interfaces.","Interfaces themselves are not a form of inheritance, and are actually the key to composition (instead of inheritance).  Intrusive interface specification is a feature. It uses the type system to ensure that objects composed together through the interface can safely collaborate, sort of like different electrical outlet shapes. The type system won't let you compose objects that aren't meant to collaborate. The interface defines a contract that the implementing class should adhere to, which a mere type signature would not necessarily communicate. This is the actual ideal of reuse through interface polymorphism - not inheritance, but composition.  Interfaces should not have too many members. This is one of the SOLID principles, Interface Segregation, to keep the interface focused to one purpose. In particular, defining as few methods as possible to specify one role in a collaboration. You shouldn't have to think too much about what all to include in an interface, because most likely in that scenario, you are specifying way too much.  The Collection interface is a good example. It mixes together abstractions for numerous kinds of collections, bounded/unbounded, mutable/immutable. It really should be broken up into at least three interfaces, Collection, BoundedCollection and MutableCollection. As well as Iterator, which includes remove().  contains() should be in the core Collection interface because this has a performance guarantee dependent on actual collection. map() and reduce() are higher level algorithms which are better served as belonging to a separate class or mixin (as in Scala) or in a utility class like Collections. These functions  use  Iterator, and do not need to be a part of it.There is no need to clutter the Iterator interface with any more than next() and hasNext().  TL;DR - You should not worry about ""future-proofing"" interfaces. They should specify one role and one role only, and higher-level features emerge from composition of classes implementing small interfaces. "
"voxels"" and ""fancy lighting"" are only a small part of the paper in question. It's mostly parallel memory management and clever methods for turning GPU compute weaknesses into strengths.","> Updating that oct-tree in real time is a bitch :)  Ooooh yup, indeed it is. That's actually a large chunk of the impressiveness of this paper, as they have found a fast method to do just that.  Pretty much the  entire  algorithm runs on the GPU, with little more input from the CPU than ""start drawing a frame"". Infact it actually puts the GPU in charge of the CPU, which is essentially relegated to the role of ""brain dead [MMU]( The ""Sparse Voxel Oct-Tree"" component of the paper consists of about 2 of the 15 chapters.  The bounces is the funny one. Because of the GPU-ness of this method, spawning another process to calculate say, the reflection of glass and the transmission of glass isn't allowed within GPU's, although on thinking about it the paper does outline a method of doing just that by marking a ray as incomplete and in need of another pass.  tl;dr  - ""voxels"" and ""fancy lighting"" are only a small part of the paper in question. It's mostly parallel memory management and clever methods for turning GPU compute weaknesses into strengths. "
What you want is bad and you should feel bad.,"Are you  crazy ? How would that benefit literally  anyone ?  Browser makers wouldn't be able to differentiate themselves to users, since normal people don't know or care about rendering engines. Why would they use any other browser than the one that came pre-installed?  Us webdevelopers would have more work to support all the different engines & engine combinations. In a perfect world, all browsers would follow all standards 100%. This isn't a perfect world and will never be, so we'd be boned.  And 90% of users would be boned too because they either don't know about extensions, or they'd install too many of them and ruin their browser completely.  Plus, how would you even enforce standardization of UI's across different platforms anyway? Good luck trying to get Apple, Google AND Microsoft to agree on what a browser should look like.  tl;dr: What you want is bad and you should feel bad. "
"Writing a web-app without an ORM is like saying that you want two codebases, not one.","Well, for me, the decision is largely dictated by tooling and shop experience.  If the team is used to using Django or JBoss, we're pretty much going to use an ORM.  Most of the time, it's an assumed way to get the job done - delving into views or stored procedures is purely a  de normalization step to improve performance on large data sets.  And even then, with web applications, most engineers are content with structuring their app to leverage a page cache of some kind in order to speed things up.  A looong time ago, I did work on an app that had a more traditional stored procedure approach to persistence.  We had a dedicated DBA to make that fly, along with database performance profiling and the rest.  My takeaway from that experience was that there were serious performance gains to be had here, but at the cost of more beauracracy within the development process.  Application data model changes had to be communicated across multiple layers of the application, amongst team members with different disciplines; the SLA/IDL had to be maintained or else the systems couldn't communicate.  The result was a slower development pace, which  encouraged  a waterfall style development model.  To look at it another way: A set of SQL stored procedures and views is effectively a second codebase. Proper cultivation of that code requires specifications, tests, and a way to ferret out regressions and other mistakes.  An ORM does all this for you, by coming to the table with a completely tested and ready-to-go set of capabilities that take all of that out of your hands.  From this I have concluded that ORM-style development is not only to serve as a database normalization layer, or to shelter app developers from learning SQL, but as a tool to circumvent the process drawbacks of maintaining application code at multiple layers of the architecture.  It was as if someone noticed that maintaining SQL on top of everything else was slowing every part of the development process down, and so they  wrote their way around it.  tl;dr; Writing a web-app without an ORM is like saying that you want two codebases, not one. "
9 for the C.H.I.P. doesn't make it competitive with supercomputers.,"I'm really curious about this. 31 MFlops is about what I calculated using the [cycles counts from ARM specification]( I actually got between 16.5 MFlops and 268.4 MFLops, but that's for division only and absolute value only respectively. It's not clear what a ""floating point operation"" is. This is why FLOPS are benchmark dependent. The distribution of floating point instructions varies between applications. Anyway, at ~30 MFlops, or ~3.5 MFlops / dollar, it's not exactly price competitive with a more traditional supercomputer component such as an Nvidia Tesla K20, at 406 MFlops / dollar. It's really not a fair competition though, because the Tesla is basically a dense array of computing cores, and the C.H.I.P has to pack in wifi, GPIO, audio, video, and more for each CPU core. So I really hope that anyone looking to build a ""supercomputer"" with these keeps in mind that their work will be an exercise in distributed computing at best. That said, I would really like to see a board like this that's designed to be part of a modular supercomputer, even if it isn't particularly space efficient. Something like that would be great for hobbyists who would like to scale their hardware without sacrificing thousands of dollars for a GPU rig.  TLDR $9 for the C.H.I.P. doesn't make it competitive with supercomputers. "
"This was awesome, but not mind blowing.  If you found it interesting, you should hack around and try it yourself.  It may be easier than you think.","Let me say first that the demo looked amazing and they were very upfront about the techniques used.  I really enjoyed it.  That said, given that all the planes are instanced (only has to be sent once to the GPU) and precise collision detection isn't really required, this doesn't seem all that impressive to me from purely a technical standpoint.  Screen space effects are typically independent of the geometry on screen, especially the particular effects they stated.  11M nearest neighbor queries is rather impressive, but not exactly mind blowing.  There are similar (and I stress similar... not the same technique here) techniques explored back in 2004:  tl;dr  This was awesome, but not mind blowing.  If you found it interesting, you should hack around and try it yourself.  It may be easier than you think. "
"if f(n) = C n^2, and g(n) = D n, then g(n) < f(n) for most n, but g(n) > f(n) for small n if D >> C",">...and for tricky reasons, sometimes O(n^2) can even outperform O(n) for very small problems.  It's not really ""tricky"", here's an explanation for anyone who's curious:  It's just that ACTUAL runtime cost for an O(n^2) algorithm is C*n^2 for some constant C.  You can think of this constant as something like ""the overhead involved in doing it this way to begin with"".  That just means if we have two algorithms, one O(n^2) and the other O(n), then for small enough values of n, the ""faster"" algorithm could actually be slower if its ""overhead"" constant is much larger than the O(n^2) algorithm's constant.  Vastly simplified, but you get the picture.  tl;dr: if f(n) = C n^2, and g(n) = D n, then g(n) < f(n) for most n, but g(n) > f(n) for small n if D >> C "
forwards compatibility for websites made with brand new but not yet widely supported standards until the standard is officially published and  hopefully  rigorously implemented.,"To answer your rhetorical question: to me, the point of a standard is that when it becomes a standard, all the browsers will have a firm guideline w.r.t. how they should render the various languages that make up the web. The fact that Mozilla and WebKit have both started implementing HTML5 in one way or another but differently from each other is not a sign of fragmentation—at least as far as I see it—but rather their attempts to adopt an unfinished standard with a eye for forwards compatibility. They could, of course, try to build it all along with what's currently there, but if something changes substantially in the standard, and then they change their implementation to reflect this, any page built in HTML5 with an eye towards compatibility with said browser has a greater chance of breaking.  I may be wrong here, but I imagine this is a major problem with Internet Explorer, where even if they decided to bring everything in line with standards, all of the pages built with IE workarounds have a chance of fucking up. With individual implementations of a nascent standard, pages built with  -moz  and  -webkit  properties will still work when the standard is finalized (or as finalized at it can really be). It's then up to the discretion of web developers to bring their sites up to code—as it were—or risk having their pages break at some point in the future.  TL;DR  forwards compatibility for websites made with brand new but not yet widely supported standards until the standard is officially published and  hopefully  rigorously implemented. "
"Instead of using a coding style that requires a monospaced font, I've adopted a coding style that works with both monospaced and proportional fonts.","Leading  tab stops work the same as in a monospaced font.  This kind of code doesn't work in a proportional font:  #test {    display:          block;    background-color: #123456;}  Nor does this:  var obj = {    a:                         'one thing',    something:                 'another thing',    specialTemporaryTestValue: 'yet another'};  But I don't like that style of coding anyway. What am I supposed to do if I remove that  specialTemporaryTestValue  property? Reformat the rest of the spacing to match the new narrower columns?  I prefer avoiding the column-oriented code entirely:  &#160;&#160;&#160;&#160;#test { &#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;display:&#160;block; &#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;background-color:&#160;#123456; &#160;&#160;&#160;&#160;}  &#160;&#160;&#160;&#160;var&#160;obj&#160;=&#160;{ &#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;a:&#160;'one thing', &#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;something:&#160;'another thing', &#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;specialTemporaryTestValue:&#160;'yet another' &#160;&#160;&#160;&#160;};  Besides being easier to maintain, I find that style easier to read: I don't have to scan left to right across a sea of whitespace to find the value for each property.  Here's another example:  function doSomething( oneArg,            // cool arg                      anotherArg,        // even cooler                      andAnotherArg ) {  // three's the charm}  What do I do when I rename the function as  doSomethingNew() ? Move everything over three columns?  Instead, I format it like this:  &#160;&#160;&#160;&#160;function&#160;doSomething( &#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;oneArg,&#160;//&#160;cool&#160;arg &#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;anotherArg,&#160;//&#160;even&#160;cooler &#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;andAnotherArg&#160;&#160;//&#160;three's&#160;the&#160;charm &#160;&#160;&#160;&#160;)&#160;{ &#160;&#160;&#160;&#160;}  This looks fine in a monospaced font too, once you accept the lack of column alignment on the comments:  function doSomething(    oneArg, // cool arg    anotherArg, // even cooler    andAnotherArg  // three's the charm) {}  tl;dr  Instead of using a coding style that requires a monospaced font, I've adopted a coding style that works with both monospaced and proportional fonts. "
I hate puzzles and I love puzzles. I hate pure trial+error puzzles and I love deduction based puzzles.,"He misses an important part. There is  puzzles  and  puzzles  and both are called  puzzles . If you are  puzzled  by the use of the word  puzzle  and the  puzzling  pointless markup around the word  puzzle  and variants, we are on the same level.  For example, a puzzle like he describes is boring. It mostly is search and trial and error over and over and over and over again. His second example, with the bridge, is a similar kind of puzzle. Basically, you try a certain number of strategies (the obvious choices would be slowest together first, fastest together first and mixed together first) and then work from there. Trial+Error again.  But there are other kinds of puzzles, which are far more fun and less about working, but more about actual thinking. Think of games made by  Zachtronic Industries , but much rather about deduction and concluding solutions in order to get things done.  tl;dr? I hate puzzles and I love puzzles. I hate pure trial+error puzzles and I love deduction based puzzles. "
I love Scala. Researches are smart guys. They might both be too smart for myself and other mainstream devs. Us poor suckers just want decent tools.,"I am happy to report that I did not say any of the three things you put in your bullet points. The reality distortion field is pretty strong over there.   Researchers are smart. I think that Martin Odersky is fucking brilliant (I also really like his language which I have said several times including in the comment responded to)  Uh, no. You should not use it in Java either. That is what ""deprecated"" means. I do believe though that wherever possible there should be a standard library for the JVM. I also think it should that it should evolve more quickly. That's why I mostly use .NET instead.  Absolutely it would. Sadly, Scala is more about ten years of adding new features and libraries and tens-of-thousands of people running to stand still trying to learn it. The people that do use it sure enjoy it though.   You were not serious about listening to me to begin with so no big loss if you completely stop now.  As for strawmen, I direct you to SeanMcDirmid's comment in the tree of my IDE comment (currently the top comment). About building the IDE support he has two interesting things to say:  1) First, ""...there were no resources to maintain this in the face of a very volatile Scala compiler and language"". So, even the academics do not seem to line up behind your ""ten years of bug-fixing"" and increasing maturity story.  2) Second, when talking about the demand for sophisticated tooling he says, ""I could never see the point, but then I'm a researcher :)"". In contrast, Kotlin is built by people whose entire existence revolves around sophisticated tooling. That is what I mean by ""industrial"" vs ""academic"".  TL;DR  I love Scala. Researches are smart guys. They might both be too smart for myself and other mainstream devs. Us poor suckers just want decent tools. "
what was my point?  I don't know.  Maybe I don't have one.,"Fascinating.  I earned a Bachelor's degree in Computer Science in 1988, have been working professionally as a software engineer since then,  in 2007 ""went back to school"" and am about halfway through a Master's degree in CS -- and I  still  had little-to-no idea what Dr. Sedgewick was talking about in most of the ""analysis"" section of the early part of this presentation.  The main message I get from this is that Computer Science has almost entirely transformed from what it was when I was earning my degree, and I might be better off starting over from scratch than trying to peck around the edges of something I seem to no longer have a good central grounding in.  Tonight when I get home I am going to forward to the Powers That Be in my CS program the link to Dr. Sedgewick's presentation and ask them whether there's any chance of this material being covered in the couple of classes remaining in my program.  If not, it may be time for a rethink of the whole Master's degree thing.  I'm a poor student -- in particular a poor reader of technical material (textbooks, papers, manuals) -- and so I don't believe I've really learned much in the classes I've taken so far, so why continue to throw good money after bad?  I can (attempt to) read online materials for free and not learn much less than I've already done by spending thousands of dollars.  tl;dr: what was my point?  I don't know.  Maybe I don't have one. "
this profession needs less jargon and more straight talk.,"The problem here is much of that list really is unfair to expect every frontend dev to know.   ""callback hell"": The concept matters more than the term. A good interviewer would write a pyramid of doom on the whiteboard and ask how to refactor it. Should we really care if he has heard the term before?   ""closures"": It's a bit more unusual to not know the term here, but again, the concept is what matters, not the jargon.   ""constant vs literal"": another example where asking ""what's the difference between this code snippet and that code snippet"" is the better interview question.   Angular directives: this one is just ridiculous on its face. Not every frontend dev should know Angular and there are good arguments [that nobody should]( Moreover, if Angular use is a requirement for the job, it's probably better to select for someone who learns quickly than someone who has mastered trendy-framework-of-the-year.   ""hoisting"": this one's pretty important to know, but the definition is  really  fussy. I generally award points if people can get it close to correct. Even experienced developers often have to look up the particulars every now and then.   ""var vs this"": this does seem pretty basic. Any good JS developer should be able to explain the difference here.   ""polymorphism"": Another example of a jargon-first question rather than testing for knowledge of the concept directly. I'm generally skeptical of asking people to explain CS concepty things, especially people without CS degrees. Ask a question that reveals an understanding of the concept instead rather than just asking for the definition of the term.   ""asynchronous functions"": This is a fair one.   ""method chaining"": Also fair.   ""strict vs identical operators"": Too jargony. Better to ask what the difference between == and === is.    TL;DR: this profession needs less jargon and more straight talk. "
If ZoL really does (did?) become a stable option for Linux I'll be happy.,"Yeah I really would like that as well.  About 1.5 to 2 years ago I tried to use ZFS on Linux on a production file server (nothing fancy, 4x3TB drives, 24 GB RAM about 15-20 users). Installing and configuring all went fine and also the load tests showed no problems.  However as soon as the real users started hitting the system it would lock up hard within two days at most (sometimes it only took a few hours). At the time it appeard to be some kind of memory leak. The developers were trying to help debug the issue, but there is only so much you can do while 20 people care constantly interrupted in their work.  In the end I reinstalled the system with FreeBSD (and continued to use the same pool I created with the linux version, which was nice) and havent had any problems since (besides having to put up with freebsd).  So tl;dr: If ZoL really does (did?) become a stable option for Linux I'll be happy. "
There is no such thing as genius.  Everything is obvious if you put your mind to the problem.,"I learned before the Internet as well.  One day I thought, I wonder how a program like the BASIC interpretor can do what it does.  I had some assembly under my belt, so I wrote a ""string separator"" (later learned was called a lexical analyzer), and a ""RAM mediator"" (later learned was a memory manager), and an ""Executor"" (later learned was a Parser & Interpretor).  My scripting language had its VM, but I didn't call it that.   I was a teen, I never stopped to think there were books on this sort of thing.  Making games was a black-art, so everything else probably was too, so I thought.  TL;DR: There is no such thing as genius.  Everything is obvious if you put your mind to the problem. "
"more current .NET support, future version support, and closer compatibility with the mono/.net specs.","Since nobody answered your question, I'll take a stab at it.  Basically, you're talking .NET 3 vs .NET 4.5+ language specs. [Tasks, bigints, etc.](  This also means, since it's something supported by the actual Mono team, that as .NET progresses (5.0 comes out at some point soon I believe), when new language framework features are implemented by the mono team, they'll be available in the Unreal Engine.  It also means that some of the liberties Unity has taken with their mono fork that cause it to drift, won't be an issue going forward since this is now at a 1:1 parity with mono.  TL;DR; more current .NET support, future version support, and closer compatibility with the mono/.net specs. "
"When you are the scale that Mailchimp is, there is almost never a drop in replacement.","Except all the code you probably have to change. Have you ever encountered a strange bug or odd behavior in an edge case while implementing an API? Usually you write some special code to account for it. If you switch to a similar API, even one with mostly the same functions, often you have to rewrite the code as well as everything it impacts.  And ""drop in"" replacement seems to indicate this will be easy. When you are running thousands of servers, fundamentally altering their architecture and services is never easy. Especially when you have written lots of code to optimize for the existing architecture. Deploying that is a nightmare, particularly when you have to rollback and start again when you hit a bug along the way.  Not that you should be unwilling to change when you are at a large scale. But often it will make more sense to directly fork the existing product and develop/optimize it instead of going through a large switch, because usually it is minor adjustments to it that make huge differences. I doubt anyone would be surprised if Mailchimp has already done this. I would not be surprised if they are running their own PHP fork too, just like Facebook.  tl;dr  When you are the scale that Mailchimp is, there is almost never a drop in replacement. "
"implicit type conversions vary from language to language, and JS's is certainly not the best :p","This is a quirk? It is a misuse of an Object declaration where a function call should have been used. In fact, I'd say any odd behavior is not the fault of the language, as instead of calling the function that does the type conversion, you're creating an object and relying on the language's implicit type conversion to evaluate your conditional. Yes, it's unfortunate that the Boolean object in a conditional doesn't behave in the same manner as one in a 'pure' OO language, but in JavaScript, the Boolean object is nothing more than a home for explicit type conversion functions, and should not be used in place of the Primitive types.  tl;dr - implicit type conversions vary from language to language, and JS's is certainly not the best :p "
"you just defined your system in such a way that it includes the very comments you were attempting to refute (because they, too, are inputs to the price).","You know, I wrote a response to this, then took a long hard look at the last sentence you wrote... and deleted the whole thing.  >The buyer IS the demand, the seller IS the supply and demand and supply IS the price.  So demand and supply IS the price. ""Demand and supply"" = price. Got it.  Okay, now let perform simple substitution on your original context.  > Are you trying to dispute the fact that the wage of developers is set by demand and supply?  ""Demand and supply"" = price.  So:  > Are you trying to dispute the fact that the wage of developers is set by the price?  ... which is a slightly snarky, but accurate, way of expressing how I feel at having discovered that you're talking tautological dingo's kidneys.  If your conception of ""supply and demand"" includes ""all actual inputs to the price, regardless of whether or not we know what they are"", then not only have you told us nothing, you've admitted that this concept which you think should be taught in grade school is not well enough understood to contain any actual information.  TL;DR: you just defined your system in such a way that it includes the very comments you were attempting to refute (because they, too, are inputs to the price). "
this isn't for organizing the reddit tabs you open for 3 seconds. it's for people who have lots of web apps they need to use.,"I feel like those are different kinds of tabs than this tries to approach.  At work I have my mail, calendar, bug tracker, and code review system each up in tabs, all day. They're on the left because they're the first tabs I open after I open the browser, which is nice. Then I'll have a pandora tab for music, a grooveshark tab for if I want to listen to a particular song, a reddit tab I use for listings, and then I'll have the browser for my company's repo so I can look up code. Three or four tabs for javadocs/pydocs that I reference occasionally. That's the most I can fit in one browser comfortable.  After that, I'll open new disposable tabs for things, even if I need them for a few hours, and close them as soon as possible, because the longer they're there the less efficient my other tabs are. Multiple browser windows isn't nearly as convenient in linux and it breaks my concentration to think ""which browser is this in,"" though I have tried it.  tl;dr: this isn't for organizing the reddit tabs you open for 3 seconds. it's for people who have lots of web apps they need to use. "
Anti-discrimination laws are awesome...however need to be re-evaluated after they have achieved complete success.,"You certainly can.  The problem is the aftermath.  Some of those laws need to be removed because they are no longer serving their purpose. Most minorities in my area now admit having an unfair advantage in... EVERYTHING.  Considering minorities are already preferentially hired in companies owned by minorities, the advantage isn't really fair or necessary in areas where the laws have become completely successful.  I am one of very few non-minority types where I work.  More than half my coworkers are not American born.  It's awesome... and at this company nobody has to worry about anti-discrimination laws because we're way over the minimum percentages for affirmative action.  It's great to be at a company like that, honestly...  However, where I used to work,  we would hire (then fire) some incompetent minorities to keep the numbers up.  Job fairs in projects are a great idea, but when you rollover almost the entire hired pool because they just don't do their jobs... wasteful.  TL;DR: Anti-discrimination laws are awesome...however need to be re-evaluated after they have achieved complete success. "
neither pickle nor cPickle blew my mind in this context.,"Actually I added pickle and cPickle after I posted this -- predictably non-c pickle was an order of magnetude worse than most, scoring something like 38 seconds on karel.json, which all the other modules did in under 7. cPickle was competitive with yajl, which is nice.  In the real world, pickle has always caused me trouble, with its opaque protocol levels (the highest of which has a thing for stratospheric Unicode values that are non-trivially annoying to stash anywhere) and its uselessness outside of python-land (as masklinn poins out).  I am sticking with json, despite msgpack's elegant efficiency -- the nice thing about having a shitload of json implementations to choose from is you can use whichever works best when you need to. ujson writes like a champ, but simplejson is a faster reader for some deeper trees. If you're just writing API calls that puke out some dicts to a browser, the json library's speed won't matter much in like 19 out of 20 situations; I often reach for simplejson because it can pretty-print something that I might need to debug. Programmer time vs. user time, etc etc.  TL;DR neither pickle nor cPickle blew my mind in this context. "
"human readable"" and ""human editable"" are really vague terms that are only somewhat related to text vs. binary. Text tends to help, though.","Text files and text protocols are a means to an end. The goal is to have  human readable  files and protocols.  The problem with any human readable format is that it doesn't state who the human is. Is it whoever came up with the format? Is it your grandmother? Is it any native English speaker? Is it any native English speaker with a good understanding of the content that the format should contain is and who is somewhat familiar with XML? It it someone who knows how to identify a gzip file by its header and has a JSON prettifyer? Is it James, provided he has access to the format spec and a perl interpreter?  All of these are humans, yet their abilities to read and perform modification to a document varies wildly.  Is a postscript file (text) more human-readable than a BMP (binary) file? Even without the spec at hand (or fully in my memory), I could write a program for displaying/converting/modifying a BMP file rather easily. I could not do the same with an arbitrary postscript file. I might be able to pick out some places where there is text. I could change that text. I could spend some hours trying to get the document to look right again after that modification caused the text to have a different length.  TL;DR: ""human readable"" and ""human editable"" are really vague terms that are only somewhat related to text vs. binary. Text tends to help, though. "
"I would be ok with it if my browser had a ""I'm shopping"" mode","I block all ads in my browser because they are really annoying. In fact, every time I have to use a browser that does not block them I'm surprised people can cope with it at all.  That said, when I'm shopping I like to see ads. Not in their current obnoxious form on most websites, but in recommendation engines (i.e: if you like the Raspberry Pi and emulation software, maybe you would enjoy an Ouya)  I wouldn't mind tracking if it meant:   better recommendations and relevant ads when shopping  no ads at all when not shopping  I decide when to activate/deactivate tracking   tl;dr: I would be ok with it if my browser had a ""I'm shopping"" mode "
The GIL usually isn't the issue it's made out to be.,"Yes, the GIL (global interpreter lock) is a problem in Python (and some other interpreted languages).  It is also poorly understood in general and often incorrectly maligned.  As with any other problem of this kind, please show me don't tell me.  Usually the GIL is not a problem.  When analysis shows it's a real issue, then it must be dealt with.  Often, as with the other Python bugaboo - being slow - it can be handled with extensions.  If extensions know that they can run independently of other threads, they can run outside of the GIL.  That said, I wish the GIL would just go away.  Guido has gone on record that removing the GIL isn't worth the effort (but welcomes any effort to safely do so).  TLDR: The GIL usually isn't the issue it's made out to be. "
"push != pull, and trying to change that is misguided.","I'm sorry, I don't buy the idea of consolidating push and pull systems; they solve different problems. Communication is largely a push system; I need to communicate with you, so I push you a message of some sort, be it a phone call or an email or an IM, etc. Media distribution is largely pull;  I  decide to turn on the TV or load a website or whatever;  you  decide what to send.  The similarity in both of these circumstances is that  I  am the user, and there is something  I  need to accomplish. To implement push systems via pull systems (or vice versa, as some have attempted to turn the web into a push system) is to confuse who is the producer and who is the consumer.  tl;dr: push != pull, and trying to change that is misguided. "
"It's not the language, it's the programmer.  Stop bashing PHP because you think it makes you look cool. It doesn't.","Why is it that any time a web development discussion comes up people are always so quick to bash PHP?  I've been a PHP + JavaScript developer for 5 years now (coming from C/C++) and it is a perfectly legitimate programming language.  Yes the language itself can be a bit archaic at times but in the end it is the programmer that determines the quality of a codebase.  I've met plenty of RoR and Django developers (not all or even most, just ""enough"") who think they are the Spaghetti Monster's gift to web development but when it comes to a real programming problem, they are stumped.  All too often I've heard things like ""What's a unit test?"" or ""What's this time complexity thing you keep talking about?"".  Just say the word ""design pattern"" and their head starts to spin.  It's one thing to be able to manipulate a framework (ie: read a tutorial online) and get your application to do what you want, it's another to actually understand what is going on under the covers, so when a real bug/performance issue arises you will know how to fix it.  On the flip side, I've also met plenty of said developers who are very knowable of the programming world and genuinely know what they are doing. It's not the language people, it's the developer.  I cannot stress that enough.  TLDR  It's not the language, it's the programmer.  Stop bashing PHP because you think it makes you look cool. It doesn't. "
"Flash video speed is comparable to HTML5, but HTML5 video is faster on the Mac on Safari only.","CPU activity depends system to system and browser to browser.  For video, Flash on Windows is great (~20% CPU) and HTML5 is a little higher (~25%). This is because Flash is optimized to use the GPU.  Whereas on the Mac OS X, Flash video is bad (~40% CPU) and HTML5 is only 12% on Safari but 50% on Chrome. The poor performance on the Mac is because Flash and Chrome is not optimized to use the GPU. Safari does. Now, the playing field is not quite level because Apple gives access to Safari to optimize and leaves everyone else in the dark.  source:  tldr; Flash video speed is comparable to HTML5, but HTML5 video is faster on the Mac on Safari only. "
"Floating point can suck in subtle, unexpected, and major ways that will waste you LOTS of time.","Floating point errors can manifest in strange ways. I had a problem with a system failing to divide 6.25 by 1.25 correctly on a particular test platform; it resulted in 4.0 instead of 5.0 consistently when performed as part of a larger equation. This was not easy to detect as it was one of a a few million calculations as part of the test suite.  Turns out the test system had a 32 bit OS on 64 bit hardware. After some research I settled on conclusion that the excess precision from the 64-bit architecture in conjunction with the default rounding mode (truncate) was the cause. If the value was taken out of a register and stored in a 32 bit space, however, the problem magically disappeared. There were other documented cases of this happening and an explanation by Intel about the bug.  tl;dr: Floating point can suck in subtle, unexpected, and major ways that will waste you LOTS of time. "
"before making snarky comments do some research, just because there is hype doesn't mean every piece of software under the trendy","As being part of a team that has deployed both Scala and MongoDB in production at a large (millions of uniques a month) site I can (not sarcastically) say that it can indeed end well for them, if they do it right.  Scala as a language is very stable by this point, the issue is finding the developers or people who are willing to learn it.  MongoDB is also pretty mature, but definitely more iffy then choosing scala as a language. There is some major development still going on with it, but it's backed by a good team, has solid docs, and does live up to the performance expectations.  tldr; before making snarky comments do some research, just because there is hype doesn't mean every piece of software under the trendy "
Same lack of input validation causes SQL insertion attacks so definitely a bug. Bug priority and severity depends on solution so not always a must fix.,"It's a bug.  Your valid characters for input are tightly defined - numerics, possibly with inclusion of decimal point and sign. You've classified the bug as UI/UX but I would disagree and say that it is far more serious as a general programming principle but benign in this situation.  The same scenario occurs for SQL insertion attacks where poor (or absent) input validation allows malicious users to corrupt the database / drop tables / elevate privileges etc. This is the exact same root cause. Always a bug, but the priority and severity of the bug depends on the usage - not all bugs need be fixed to ship. Generically, always validate all input from the user / code for security.  TL;DR: Same lack of input validation causes SQL insertion attacks so definitely a bug. Bug priority and severity depends on solution so not always a must fix. "
static analyzers aren't used only to validate programs; they're used to design programs to be easier to validate,"Excellent article as always. (I'm a fan of Mr. Might's writings).  It does side-step how static analysis is used in industry ... which is that we perform a number of these transformations by hand, on  purpose , to make the problem easier for the static analyzer.  To illustrate, a typical workflow: I write the following program:  unsigned int g(unsigned int);void f(unsigned int size, __in_ecount(size) int* arr) {     arr[g(size)] = 1;}  And my static analyzer complains ""error!! I can't tell if your access of 'arr' is safe! Please help!"". Well, I know that 'g' is supposed to pick some random number < input, so instead, I just write:  { unsigned int tmp = g(size);  if (tmp &gt;= size) exit(1);  arr[tmp] = 1;}  And then my static analyzer now says ""OK"". I say that the analyzer proves there are no array bounds errors, but what I  really  mean is that the static analyzer helped me design an alternate program that is easier to author proofs for.  tl;dr:  static analyzers aren't used only to validate programs; they're used to design programs to be easier to validate "
All Rust lovers should definitely go check out [ATS](,"Each time I see praises to Rust and its memory management precision I remember of ATS language which is often ignored, what a pity. It had similar (or even more powerful) features regarding safe manual memory management for ages. There you allocate some memory and along with a pointer get a virtual proof object stating where you got this memory, how big is the allocated piece and whether it is initialized or contains garbage. Then you pass around this proof object with the pointer, and this proof object has linear type (it's unique), and then compiler demands that you pass them either to deallocation routine or to GC. Compiler will not let you forget to free the memory, will not let you free it twice (thanks to linear type of the proof) and will not let you access garbage memory or outrun the buffer thanks to state and size information in the proof. During the compilation that proof term gets completely erased and you end up with a naked pointer in runtime but with safety guarantees checked at compile-time.  tl;dr. All Rust lovers should definitely go check out [ATS]( "
"having your hand held with examples in class is great. 
 EDIT: TLRD2; Yes, I am THAT bad at math.","I don't know if I could handle most of my lecture material outside of class times. I'm just too lazy.  Also that arrangement seems more scammy that the usual university paradigm already is. For dedicated professors, I could see it being really good, for lazy professors, they would just record once, then use it for the next 30 years, no matter how bad it was then, or how out of date it became  (yeah, I know ""What about those MIT SICP taped lectures? Legendary"" but I see those as an outlier. You really think Dr. Tenurecoaster could pull that off?)  Although one of the absolute best classes I had was when I took Calc I at a community college where we did devote a lot of classtime to examples.  The math dept at my university was horrible, and after having to drop Calc I on two other occasions, I decided to just go to the hassle of taking Calc I at a community college across town where I knew credits would transfer.  It ended up being a 90 minute class, 45 minutes of lecture, with 45 minutes to go over a worksheet. The guy in charge of the class would work with you until you got every one of the examples right. It was amazing. The homework was the back half of the worksheet, which was the same number of problems, at the same difficulty, if not less, than the ones on the front side.  Come test time, every test question was from one of those worksheets, with maybe a 5 in place of a 3, or something like that.  Ended up passing with an A. This is after doing so poorly at my university that I had to end up dropping class twice.  TLDR; having your hand held with examples in class is great.  EDIT: TLRD2; Yes, I am THAT bad at math. "
"use --as-needed,  at pre-install, store all functions&libraries a package uses, check that list against new programs installed/uninstall and see if stuff would break. 
 >","Semi, let me see if I can explain a bit more, with some examples taken from RPM-land for now ( cause they have features I like ).  An RPM file has at least 2 different kinds of dependencies.  The first one is .spec-inherited. Static depends, BuildRequires, Requires.    Depends on a package name+version+Epoch,   same same as portage. Just slightly different.  The second part is a post-build dependency scanner in the RPM suite.  As files are separated into the various .rpm's from a src.rpm, they get scanned for extra dependencies.  These dependencies are shown as the following:>>libfontconfig.so.1()(64bit) >libfreetype.so.6()(64bit) >libgdk-x11-2.0.so.0()(64bit)>libxml2.so.2(LIBXML2_2.5.8)(64bit) >libxml2.so.2(LIBXML2_2.4.30)(64bit) >libpthread.so.0()(64bit) >libpthread.so.0(GLIBC_2.2.5)(64bit)  This means that the package manager at installation time can check for both missing library dependencies,    functions  inside said libraries ( USE-flag depends in Gentooland).   This  also  works for perl, python and various other languages ( simply because the scanners function on these languages as well)  What this means is that you first have a package manager complaining that the .spec requires aren't installed. (RPM doesn't fix it for you, yum/zypper/apt-rpm does however)  and then when those are satisfied, it  also  complains that various  libraries  aren't in place.  The second part of this is a central database for the various RPM-repos which matches all packages => library names provided. This can then be queried for automatic dep resolving on installation time, by the ""smarter"" package managers.  But for Gentoo, I'd simply want a pre-install scanner of the packages, extracting function&library dependencies and storing with the package.  This can then be centralised and sorted on a per-function basis, and you can have a pre-remove scan to see if the package you are removing ( since in Gentoo we overwrite first, and then remove the old stuff) would break any other packages' recorded dependencies.  This would  not  fix the ""new libpng doesn't provide the same functions"" or such from happening, but would mean that a developer would see it first, or that a user would see it.  However, for this to ever work, you'd need --as-needed supported straight through, or it'd be too many intradependencies.  TL;DR:   use --as-needed,  at pre-install, store all functions&libraries a package uses, check that list against new programs installed/uninstall and see if stuff would break.  > "
"If you want to do complicated things with Unix, RTFM",">The packaging system breaks down when you have an old distro and you just want a new app to work.  That's a problem with pretty much any system. Newer software will require newer libs and newer everything. Either you're brave and borderline insane and trust the package manager to do it all for you, or you start from the configure script to see what it wants and give it to him in a location of your choice.  I suppose running Office 2007 on NT4 wouldn't be much fun either (haven't done Windows in ages though).  >Or if you're installing software that isn't part of a distro and ends up spewing files everywhere.  configure. Or poke at the install script.  Unix systems are there for you to tweak. If you want to step out of the path, you're free to do so. However, having done so, you're not supposed to whine about your boots being muddy. That's what it's all about.  >If you do make install on some tarball you have no way of uninstalling the file or even figuring out which files go with which programs.  There's stuff that logs that for you. Or you can even  gasp  write a script that does it. And often, all you have to do is ( eep ) :      make uninstall Crazy stuff I tell you.  >When the distro system works, it's great. But if you're doing something unorthodox it can become a huge pain.  When you do something unorthodox, you have to read more documentation.  tl;dr; If you want to do complicated things with Unix, RTFM "
Use named functions. There's no reason not to as compared to this.,"I agree. I cannot see a single thing that this has over just defining a function in the same place. You still have to name your ""blocks"", you still can't embed a ""block"" as a part of a normal expression.  Additionally, this:   def id(x): return x with my_block &lt;&lt; id() &lt;&lt; 'x, y = 3':     print(x, y) my_block(4) my_block(1, 2) print(my_block)  The syntax here is entirely non-obvious. id() should be an error as you're not passing it any arguments.  The fact that it does syntactic rewriting instead of rewriting bytecode also, from what I can see, means that you're effectively parsing the file twice, since the imported module is already parsed once by the time the codeblocks line gets to run.  tl;dr: Use named functions. There's no reason not to as compared to this. "
"in general people do their best, yet these sorts of errors still get introduced.","I was advocating that you program defensively.  IMHO at the end of the day, enterprise software either does what it's meant to do or it doesn't.  When it's not doing what it's meant to do, that generally means money is falling on the floor.  The resilience of production code can be increased tremendously by treating ""environmental"" errors as inevitable and programming defensively against them.  As to why people would believe some of the falsehoods I can only tell you that I am judging people's beliefs by the code they wrote, not by conversations I actually had with them.  In many cases the people who had introduced these bugs had long since moved on to other projects so it wasn't possible to get their perspective.  FWIW I do believe that most people are well-intentioned and that most programmers are well educated and careful.  When such errors are introduced, the cause is invariably found to be a complex interplay of factors.  tl;dr in general people do their best, yet these sorts of errors still get introduced. "
Know your skills.  Know what skills your prospective employer needs.  Show that company that you know the value of those skills.,"People tried to teach this skill to me, and failed.  So I have  no  idea how to teach it to someone else.  But here goes:  Have confidence in your skills.  Sometimes, that means even figuring out what your skills are.  Then you need to present them in a positive light, not negative.  ""I'm not good at GUI work, so I prefer backend stuff.""  -  Very bad.  ""I'm really into backend stuff."" - Good.  ""I've been concentrating on business logic lately."" - Better.  You also need to know that your skills have value.  Employment is a 2-way street.  They need you and you need them.  You should be interviewing the company as much as the company is interviewing you.  You need to ask some hard questions about their policies and work situation.  It seems like this should be under 'negotiation' rather than 'marketing', but asking good questions shows that you know what's going on and actually raises their interest in you.  Knowing about typical problems that companies have and how to solve them is invaluable when marketing yourself.  My father is much better at this than I am, so I'll use him as an example.  He's an industrial engineer, and he would go into interviews and say, ""I know that you have an XXX problem.""  They were always stunned that he could see that about their company so quickly and easily, and it was always why they were hiring someone.  The thing is,  every  company that's hiring for that position has that problem.  He didn't need to know anything else about the company.  This is just another aspect of knowing your skills and their value.  I have yet to interview a DBA that came in and said, ""Have you run into DB scaling issues yet?""  But I would be impressed if they did that, and then offered suggestions on improvement.  I've yet to work at a company that  didn't  have some kind of DB scaling issues yet, either.  Sometimes this is solved by optimizing the database in some simple way, but sometimes it's a lot more complicated.  tl;dr - Know your skills.  Know what skills your prospective employer needs.  Show that company that you know the value of those skills. "
"If you need a wrapper to link node and PHP, then chances are you don't need to even be using PHP for that project.","Why not just use node exclusively instead of a hybrid node-php system?  The logic makes sense to me, use PHP for basic CRUD apps and Node for real-time apps, but linking both languages together just seems like it could get very messy very quickly.  Node can handle CRUD applications very quickly, especially when using a framework such as express.  I've been using PHP for the past 6 years exclusively and I'm now just getting into node.  My plan is to make node my primary language of choice for all web development, but I'm just getting used to how things work with it.  My one mistake is that I try to compare it with PHP, but you can't, you need to get out of that mentality.  TL;DR: If you need a wrapper to link node and PHP, then chances are you don't need to even be using PHP for that project. "
Github is to programmers what portfolios are to artists.,"I interview programmers whenever my company has openings and I love looking at code they've put up in public. Being able to see their recent code provides a much better indication of quality than any tech question I could ask in an interview. And with sites like github/bitbucket I can see how you've grown over time as well; if your code from 3 years ago is crap but the code from 1 month ago is good, then I know that you've been perfecting your craft. That's big.  If I had two candidates that both did OK in an interview, but 1 of them had a github account, I would spend a lot more time vetting the guy with the github account.  Note: This effect does not require github, as long as you have a link to your work in your resume it works.  TL;DR; Github is to programmers what portfolios are to artists. "
"if you're using data providers to increase code coverage numbers, you're doing it wrong.","I don't agree with that, actually. Your  test  should have a descriptive name that describes the behavior. Ideally data sets in a data provider are all related to single behavior.  I wouldn't write the test in the blog, because it's doesn't really do anything than increase code coverage. Which is okay, but it's not a very well written test. Well written tests verify object behavior. The object in question has two behaviors:   cash payments have fee added  credit card payments without feed   Each of those behaviors should have their own test -- or the class in question should be refactored to use a strategy pattern to calculate the total, but that's a bit of overkill for such a simple thing.  public function testCashPaymentAddsFeedToTotal(){   // ...}public function testCreditPaymentDoNotIncurFee(){    // ...}  TL;DR: if you're using data providers to increase code coverage numbers, you're doing it wrong. "
With great power comes great responsibility. You need to be equipped to handle that responsibility.,"The alternative is a VPS (Virtual Private Server). It's like your own little server that you can do anything you like with.  The advantage is the sheer level of control. If you are an advanced web dev you can do all kinds of wonderful things with this control. Automatic deployments, installing packages and extensions, setting up a little mail server, tweaking the web server configuration and so on and so forth. Resulting in less phone calls to the host and the security of knowing that whatever you do in dev, you will also be able to do it when you push it live.  The disadvantage is that you are responsible for backups, uptime and setup. If you don't know what you're doing, a poorly configured server can be slow or even have security holes. If your website goes down you can't pass the blame onto someone else (unless the data centre has set on fire) and if you don't backup properly and you lose data then you are fucked with a capital ""F"".  TL;DR: With great power comes great responsibility. You need to be equipped to handle that responsibility. "
"one of these methods ""Has no obvious defects"", the other ""Obviously has no defects"".","For-loops tell the compiler  how , and can only be used to perform side effects. Functors/Mappables tell the compiler  what , and used properly will never perform side effects. By using a functor, you're simply creating a projection over an Iterable container. The exact method of implementation and order of execution is irrelevant.  From a performance perspective, this gives the compiler much more free reign to optimize (Such as by running the operation over multiple threads, or lazily). Javascript VMs currently don't really do much with this, but other languages do (And Javascript probably will at some point). However even in JS it does mean you can swap out data structures without changing your code, letting the 'map' function choose the most efficient underlying algorithm without having to change your code.  From a code maintainability perspective, avoiding side effects means your code has less moving parts and is easier to reason about.  It's also just simpler.  var ys = [1,2,3,4].map( x =&gt; x * 2 )  Is unambiguous about what it's doing, and lets the compiler figure out the optimal way of doing it.  Compare to:  var xs = [1,2,3,4];var ys = [];for( x=0; x &lt; xs.length; x++ ){    ys.unshift( xs[x] * 2 );}  This has a lot more moving parts. It performs side effects. It's harder to optimize. It's requires more cognitive effort to reason about what it does, and it's much harder to spot any potential bugs. (Did you notice them?).  tl;dr - one of these methods ""Has no obvious defects"", the other ""Obviously has no defects"". "
"If you want raw speed in a small project, choose C, C++ or other such languages. If you want correctness in a scalable project, choose Haskell.","Or maybe the people who solve AI contests choose C++, and the people who do real work choose Haskell instead?  Seriously, that's a non-argument.  C++ definitely has advantages over Haskell.  The performance of a C++ program is more predictable than that of a Haskell program. However, the mathematical meaning (semantics) of a Haskell program's result is more predictable than that of C++.  This makes C++ more suitable for small-ish projects with a serious focus on performance, and Haskell more suitable for pretty much everything else.  Haskell will make it easier for the project to scale in complexity, it will reduce the number of errors by a significant amount (eliminating whole classes of errors, such as dereferencing NULL), reduce the number of lines of code, etc.  However, Haskell will make it  harder  to write something that executes quickly where correctness is just not that difficult of a problem in the first place (e.g: Such small projects).  tl;dr -> If you want raw speed in a small project, choose C, C++ or other such languages. If you want correctness in a scalable project, choose Haskell. "
"sum types are unions with little extra tags called constructors. 
 EDIT: added missing is.","Sum types, or  discriminated  unions, are quite similar to ( non -discriminated) unions but they have slightly different typing behaviour.  From an implementation perspective they'll probably both have a tag. For a sum type the tag is a  value  that  maps  to a type but for a union type the tag actually  is  a type. Informally a sum type is 'one of these named things' while a union is 'one of these things'.  So with a sum type you have two operands that have the same type because they're discriminated by their constructor, e.g.  -- A very boring shop that only sells bread and milk but is sometimes empty, the Ints represent quantities.data Groceries = Bread Int | Milk Int | OutOfStock  A union type doesn't have constructors such as Bread and Milk, it just has types. So the above example can't be directly used. Instead we can emulate it by adding the constructor tag by hand, e.g.:  data GroceryTag = Bread | Milkdata OutOfStockTag = OutOfStockunion Groceries = (GroceryTag, Int) | OutOfStock  Note how this is pretty much just implementing a sum types constructors by hand. This is a common idiom in Erlang, where they use atoms as tags. You see it in some Lisp code as well but it's not as common.  Now it is totally feasible to have a sum type that itself contains the value of another sum type, e.g.  -- I need to add the T suffix so it doesn't clash with the constructors for Clothing.    data SweaterT = GrandmasKnittedWonder | SteveJobsTurtleNeckdata TrousersT = SkallyWagPantaLoons | Jeans data Clothing = Sweater SweaterT | Trousers TrouserT  For unions the same doesn't really hold. If you remove the constructors how can you really tell they're nested?  We could emulate the above declarations just like before but we have to use tuples (or records or some other exotic product type) and essentially just implement sum types with unions.  Now most languages that support algebraic data types don't both with unions, the little extra bother of adding constructors make code more self-documenting and a bit less confusing.  tl;dr  sum types are unions with little extra tags called constructors.  EDIT: added missing is. "
Don't take no for an answer with your livelihood.,"Happened to me once on a web design job - guy said ""Go ahead with the work"", got him to write down specifications and a date he wanted it by, we agreed to the price.  I did the job, but came back and he told me ""I got my family member to do it, doesn't it look good?  He did it for free, I'm helping him start up his web business, I don't know much about computers"".  Needless to say I was furious, because the design was done, so I demanded he pay me for my work.  He refused, so I kept onto him daily until I told him ""We had a verbal agreement and I have written specifications and a date.  If you refuse to pay further,  I will send you to collections ""  We ended up agreeing to 75% of payment.  He didn't want the credit hit to his business, and honestly I really didn't want to deal with collections because that means I'd only get 15-20% for the headache of sending it to the agency.  tl;dr: Don't take no for an answer with your livelihood. "
"not one of his better posts, he comes off as arrogant and doesn't provide any useful advice, except in the comments.","Sometimes responses like this can be pretty annoying.  In this case, the response may be (in my opinion) quite justified, but there are a bunch of reasons that someone might be asking a dumb question -   to prove to their stakeholders that something can't/shouldn't be done;   because it's a custom-built app for a specific set of users (eg within a specific enterprise);   maybe it makes sense for some other context-specific reason (this one may not make sense in this case, but this happens a lot with web application development).   Maybe the developer could have asked in a more diplomatic way, but mocking them for asking isn't particularly constructive.  Who the fuck are you to say that the CEO of the asker's organisation hasn't demanded that every desktop in the company have a widget that can't be moved?  Yes, it might be a ""stupid"" requirement, but that's not really the developer's fault.  I don't think that they should add the functionality to allow programs to do that stuff, but I'd be far more interested if this was a discussion on  why  it's not a great requirement, and some of the options that a developer in this situation could try (or at least outline what other UI elements could be appropriate in these types of situations).  In fact, it's only in the comments that Chen starts to actually provide any constructive comments (like ""maybe you should open an application on the taskbar"").  TL;DR: not one of his better posts, he comes off as arrogant and doesn't provide any useful advice, except in the comments. "
"There already is a fair, level playing field with multiple popular choices. Removing this option results in a narrowing of valid choices available.","Such rules should only apply to the market in general, and should not be used to squash individual companies or products.  Apple introduced iPhone/iPod/iPad into the market competing with existing products and companies, several of which companies are big players. People have choices there. iPhones are a big hit, but looking at the people I know most of them have something  other  than iPhones. Seems like a level playing field to me.  As a software developer myself, I'm looking at doing apps. Without apps, I have all the choices for releasing software that I've always had. With apps I have a choice between Apple iOS and Android, basically. If I go with Android I have little restriction on what software I make available through their app store. I'm not seeing an issue here either.  My beef with passing laws to break down the barriers of Apple's walled garden is that at this point Apple is operating in a pretty healthy, level playing field, offering products of a certain flavor among many others. They break no existing laws, do not deceive, etc.  I personally did NOT buy an iPhone (I bought an android instead) because I did not like several of the restrictions. That's how the market should work. I'm a shining example of free enterprise. Buying an iPhone and then bitching about it, or worse, bitching about it and then buying anyway, is a much worse way to participate in the market.  TL;DR: There already is a fair, level playing field with multiple popular choices. Removing this option results in a narrowing of valid choices available. "
Memory look up is still a constant of the largest look up time.,"I'm glad I'm not the only one who hasn't been bothered by the ignoring of the constants.  In real life, I've found the system degradation rate sometimes is more important than the overall speed.  I think you're wrong on the O(1) for memory look up, if anything it's a constant that is a range of time (but still has a max), but the constant time once you hit accessing the last cache level wouldn't increase any more.  In other words, if at 1000 items you start hitting the hard disk, the O(1) time doesn't grow with the size of the problem even if you went to 10k or 100k items.  TLDR: Memory look up is still a constant of the largest look up time. "
"Author doesn't really know what she's talking about. 
 EDIT: missed a word","As much as I like the lambda calculus (I agree that, historically speaking, it probably would have been a better choice for the de-facto theoretical computational model), this is a pretty bad post.  She seems to think that all imperative languages are exactly as expressive as Java, and seems thoroughly confused about the nature of the lambda calculus in relation to machine code.  Functional languages don't ""escape"" assembly somehow; they still need to be compiled into imperative machine code or interpreted by a compiled program.  Assembly doesn't cease to exist once you start using LISP.  The function composition argument is a bad one.  All imperative programs in good style consist of well-organized modules split into individual pieces (functions or procedures) that are easy to reason about individually.  And you can still approach the expressiveness of command-line piping with imperative languages: if you ask me, Ruby's method-chaining is exactly as eloquent as command-line piping or function composition.  tl;dr Author doesn't really know what she's talking about.  EDIT: missed a word "
"Wiki's are dumb, they are always out of date. Document your code, goddammit!","Pretty much. We have everything on our wiki... theoretically. Practically, it's all outdated, incomplete and hardly maintained. Especially when it relates to code.  As a developer, I think there's just no way around making it a habit to document your code. Yeah, I know you don't get paid for that. It doesn't make you look good, it takes extra time, etc. I do it mostly for myself. If I expect possibly having to revisit that code six months later, I know my future self will thank me for it. And if someone else has to do it, well, hopefully they'll thank me for it.  Maybe I'm still too optimistic for this job. But I do expect to move up, and manage people in the future, and thus setting the coding standards for everyone in the team, so I start working on those now. Because if you're leading by the ""do as I say, not as I do"" principle, you just don't get a lot of credibility and everyone will hate you.  TL;DR: Wiki's are dumb, they are always out of date. Document your code, goddammit! "
"use pnoise3 (instead of pnoise2) with the same x & y coords, but a random z.","I had the same problem as you initially (same results over and over) until I switched to generating 3d noise and using a random z coordinate. It will always generate the same noise given the same parameters (which is one of the nice things about it), but if you generate 3d noise (which this library allows) you can just take a slice from a random z coordinate and get something different every time.  The way most of these Perlin noise libraries work is by using a static precomputed vector field (usually from [Ken Perlin's reference implementation]( so the results will always be deterministic, but the same.  tl;dr  use pnoise3 (instead of pnoise2) with the same x & y coords, but a random z. "
"Regardless of whether or not you think a public healthcare option should happen, make this public so that those making decisions can make a more  informed  decision.","This is so incredibly important right now.   Make this public .  It doesn't matter whether or not it's fraud.  It doesn't matter whether or not it's illegal.  The entire point is that this is precisely why a public option is on the table right now.  And you know what?  I don't really care whether or not a public option ever happens, but serious regulation of an industry who immoral, unethical and often illegal things that causes suffering or death to innocent people just to affect the bottom line  must happen .  If stories like yours stay buried, it never will.  TL;DR:  Regardless of whether or not you think a public healthcare option should happen, make this public so that those making decisions can make a more  informed  decision. "
the quality is not due to being a women just to the fact that there are not that many.,I've actually met two females that have been excellent programmers(That ive worked with also by excellent it means damn im never going to be that good). The thing is it was not in college or at any small development shop it was in a top 3 company.  To be honest i think they are about as good as us males just that they are so few of them. I work with a lot of females and lot of males in my office i would say a 6 to 4 ratio but in 3 years ive only worked with 10 female programmers the rest are in either QA or some type of customer facing part.  TL;DR the quality is not due to being a women just to the fact that there are not that many. 
it's about a lack of education on what it is like working in that field,"The biggest problem that Software Engineering/Computer Science has are the stereotypes that go with it:   1) Risky, low job assurance.   2) Stupendously retarded hours.   3) Filled with fat, socially awkward, who'll oogle at you.   4) Unrewarding.    At the end of the day, Women want to work in areas that empower them.  My feelings on the matter that not all of those points are any more true than any other technically demanding role. Sure, there are employers out there who exploit their employees, but the same can be said for any industry. Same can be said with the rewarding/unrewarding nature of working. Of course, the industry isn't filled with fat, socially awkward, nerds, at least not significantly more that other knowledge based professions.  tl;dr it's about a lack of education on what it is like working in that field "
"JSON without schemas more error-prone, in particular for clients. Moves format burden from server to client - therefore popular with server developers.","Schemas can not only gives you validation, but also language bindings.  In many cases using JSON is more verbose and error-prone compared to using modern XML bindings like JaxB, where you would just do something like:  Person p = XMLFactory.parse(xml);  System.out.println(p.getFirstName() + p.getLastName());  .. and the compiler would complain if you used the wrong method. Now this has obviously problems like ""what if the schema changes"" and that not everything is easily captured in a schema (like format of a phone number).  What traditional 'static' approaches using XML and schemas do is to move the burden of getting your properties and names right to the developer making the data format.  What JSON does is to say ""screw that, just put out whatever map you want (or already have)"", making it the client's problem to figure out if it's ""last_name"", ""LastName"", ""surname"" or what. This is obviously better for the data producer, as it means less work, and you can get your service/data out there right-away. If you are also making the client - great - you can just peek into the server source code to see what the format 'really is.  For clients, data parsing is not a big issue with JSON, as one can just look at the JSON in a text editor, ""do it by example"" - something which is often much harder with the verbose and often over-structured XML.  This is assuming there are no hidden surprises like strings turning into arrays when people have several names, or the key ""middle_name"" suddently appears for 1% of entries.  For clients what is often hard with JSON is data entry, as there typically are no templates and documentation tends to be limited to hand-written examples (often outdated or with syntactic errors) on a wiki page.  For example, some RESTful services accept data input in a slightly different JSON structure than what they themselves output, like ""firstname"" instead of ""first_name"" - the reason is often just sloppyness as data structures are not defined anywhere in the code, just used directly as maps, and a key was renamed in one piece of code but not another.  It is possible to do JSON well - but it still requires much of the same discipline as making a good (as compared to most horrible) XML formats.  TL;DR; JSON without schemas more error-prone, in particular for clients. Moves format burden from server to client - therefore popular with server developers. "
small semantic differences between web service implementations restrict seamless use across different platforms.,"> Sorry, but that's incorrect.  I am afraid not. I have been working with WSDL for the past 7 years with implementations in Axis 1, Axis 2, .Net, Python (suds is the best client library), C/C++ gsoap, and other obscure platforms like Accelerys's Pipeline Pilot. In addition to the other can of worms that is grid services.  What you learn is that your best chance of success is by writing your WSDL manually and avoiding all complex types, or by learning which types work with which implementations. If you do that then your WSDL will be > 100 lines. The dream of the OP that you can just use [WebMethod] and have a WSDL file that works with gsoap or suds or Pipeline Pilot is just that, a dream.  If you have a sane WSDL file that plays well with all implementations, then by all means use whatever suitable toolchain for your platform. I wasn't particularly worried about whatever code written automatically does. But assuming that [WebMethod] or a @WebService would automatically guarantee compatibility is not the case today.  tl;dr: small semantic differences between web service implementations restrict seamless use across different platforms. "
"This is a home-made problem, not a search engine problem.","Why does the perl community even hand this over to a search engine? They are completely giving up control about the learning path of anybody who is interested in perl.   For everything Microsoft, there is msdn.   For everything Android, there is d.android.com.   Even Apple has ADC.  python has a Beginner's Guide on python.org -> documentation  For haskell, there is excellent beginner's documentation on haskell.org  The ruby community has localized beginner's guides on ruby-lang.org   ... while in contrast, perl.org has manual pages, core modules, a FAQ and some [atrocious]( tutorials in their 'documentation' section. A beginner's guide is lacking. They recomment books instead of giving a way to jump into coding. Everybody seems to have an ""introduction in 20 minutes"" ... but perl doesn't.  Incidentally, this seems consistent with most of the GNU world. Which hasn't exactly attracted developers either.  TL;DR: This is a home-made problem, not a search engine problem. "
"I'm an ornery son of a bitch, but eventually I got over my poor judgement of the cost of unit testing.","That's actually what coaxed me into unit testing in the first place. I'd previously considered unit tests to be ""too much work"" compared to just testing the behavior myself in the Python REPL. But eventually I realized it would be far less work to encode my manual testing into automated functions that did all the work for me, every time, covering more possibilities in less time than I ever could manually.  Even then, I still had a high cost mentally associated with testing, and it took doctest (tests in your docstrings, which also act as examples. 2 birds, 1 stone!) to push me into the world of testing. Eventually that became unwieldy as well, and I graduated to more traditional unittest tests. This gave me a ton of freedom for refactoring, which instilled a love of testing in my soul.  I've recently switched to Go and the builtin test framework is fantastic. Tests run so fast that I haven't even had to make use of features like  testing.Short()  there if/when I need them. It's an order of magnitude better than any manual testing I could do, and it brings the cost of testing so low that TDD doesn't seem completely insane to me.  tl;dr: I'm an ornery son of a bitch, but eventually I got over my poor judgement of the cost of unit testing. "
problem is that writing comment prose is  hard  to do well.,"The answer, as usual, is  it depends .  >A lot of people claim that ""comments should explain 'why', but not 'how'"". Others say that ""code should be self-documenting"" and comments should be scarce. Robert C. Martin claims that (rephrased to my own words) often ""comments are apologies for badly written code"".  >My question is the following:  >What's wrong with explaining a complex algorithm or a long and convoluted piece of code with a descriptive comment?  It depends on what the comment is. Most often seen ""comment error"", and most stupid thing to do, is that the comment repeats what the code does. Another is the ""apology for badly written code"". Yet another is a comment completely or slightly out-of-sync with code.  The author really should have given an example of the code and its comment, then we could talk.  tl;dr: problem is that writing comment prose is  hard  to do well. "
"Non-Sun hardware support sucks, so have not tried OpenSolaris. Pkg format is slow but acceptable. Live Upgrade rocks as a patching strategy.","Honestly haven't played with OpenSolaris. But my experience with x86 and Solaris has been hit and miss (though there stuff. In some cases where Sun have licensed other people's designs, such as the v40z x64, hardware support was terrible. I wouldn't want to run Solaris on anything other then Sun hardware.  Solaris package management is not a highlight. It's stable, it's possible to do roll backs on patches generally without getting into dependency hell (unlike RPM) but taking 6 hours to apply a patch cluster to a high end server is unacceptable. I can do the same thing on HPUX in 15 minutes.  But Sun do have Live Upgrade which makes patching and rollback pretty fool proof. You basically copy the live environment to a couple of spare slices (partitions) and patch the cloned environment while the machine is still live. Reboot to access your new environment and test it. Make it the new default if it is OK and if it all goes pear shaped, just reboot again to launch your old unpatched environment. It will even update your log files in your new environment when you reboot to activate. Only outage required is to reboot and test/rollback.  tldr: Non-Sun hardware support sucks, so have not tried OpenSolaris. Pkg format is slow but acceptable. Live Upgrade rocks as a patching strategy. "
"Keep away from buzzwords, learn your programming language, and implement your own ideas. I started with [this]( book.","If you are going for learned prediction/statistical machine learning, you will mainly need Google and sample data. As much as it sounds complex, this side of ""AI"" is based on very simple principles that require simple but lengthy implementations in code.  If you are choosing to learn about true ""AI"" (which I prefer to call Artificial Life), you will need to know your programming language inside and out first as generics/data typing/data representation is most of your work and is usually reserved for the advanced areas of <insert computer language of choice>. After that, things get murky and the only thing that I can tell you that will make sense now is that whatever the current research out there claims to do is mostly based on sand-boxed demos that do only about 1/2 of what you think they may and keep regurgitating the ""same ol' same ol'"". Personally, I have only seen a handful of impressive ideas over the years that I have been studying the subject.  TLDR; - Keep away from buzzwords, learn your programming language, and implement your own ideas. I started with [this]( book. "
I worked in the exact position necessary to not get in trouble for my pranks.,"Back in college in the text-only, command-line-based, central-computer-with-timeshared-terminals days, I quickly became well-known for being extremely knowledgeable about how to use the system; people would come to me before they would contact University Computing Services, in fact.  When UCS launched a Helpdesk,  they  asked  me  if I would be their first intern.  I accepted happily and from then on earned actual money for doing what I had previously been doing for free.  In the meantime, on our off hours my friends and I would pull scary-but-non-damaging pranks on people who left themselves logged in, ""to teach them a lesson"" about the perils of not logging off.  ""THIS time you only THOUGHT all your files were deleted!  NEXT time it could be for REAL!  Always remember to log off.""  We thought of ourselves as performing a public service.  One day I found such a terminal and made a few changes to the command definitions, such that a directory-listing command would say ""no files found"" and an attempt to discover the changed definition of the command would show no tampering.  Then I went to work.  That shift, I got a note from a professor in the English Department, complaining that someone had messed with his command definitions while he was out of the room for a few minutes.  Little did he know he was writing  directly  to the  very person who had done it.   I answered him with the standard UCS reply that ""that's largely out of our control; we can't tell who did it, and our standard recommendation to all users is to log off if you're going to be out of sight of your terminal.""  Then I deleted his message, and nothing was ever heard about it again.  *bow*  tl;dr - I worked in the exact position necessary to not get in trouble for my pranks.  "
"Sign up for github 
 Learn jQuery 
 
 PS: I'd suggest posting links to your github repo's up here asking for feedback. The community is pretty reasonable.","I'd sign up for a github account and push up some of your code. You'll figure out git basics, which is good because I don't see a VCS on your resume. More than CSS as a backend dev I'd encourage you to learn JS. I recommend jQuery as the best framework to learn, there's a reason why Ruby on Rails now uses it by default. I'd stick mainly to figuring out AJAX requests in jQuery they're simple.  If you're looking into MVC frameworks, I'd highly recommend codeigniter. It's a fairly straight forward, light weight framework and it's extremely popular right now. If you're comfortable with MVC you may also want to check out [Fuel]( it's an early stage MVC. You might learn a thing or two maybe even become a contributor.  Job Wise I'm not quite sure what the industry looks like in LA, I know that it's a decent size. If you have a hard time finding a job down there, SF has a much larger webdev scene  tl;dr   Sign up for github  Learn jQuery   PS: I'd suggest posting links to your github repo's up here asking for feedback. The community is pretty reasonable. "
learn to hack in 2-$x different programming languages and get a degree in computer science and/or software engineering.,"I don't work for Twitter but probably the following (roughly in order of importance):   ""web application fundamentals"": HTTP, DNS. TCP/IP. you should also know the OSI model is and why it matters.   a systems programming language: C/C++, Java, maybe C# or Go.   Javascript and at least some basic HTML/CSS. you don't have to be a UI designer, but you can't make a web app if you can't lay out some elements on a page.   a scripting language, probably Python or Ruby but Perl is still useful. bonus points if you also know shellscripting, but that's ""in addition"", not ""instead of"".   data layer theory/practice: learn SQL, learn about the engineering trade-offs between SQL solutions and NoSQL solutions. database scaling/sharding, etc.   operating systems theory: file handles, sockets, caching, memory management. use Linux (it's free and everyone huge is running their code on it!)   version control (probably git... you are using github already, right?)   bonus points for a functional programming language: Haskell, Lisp/Scheme, F#    tl;dr: learn to hack in 2-$x different programming languages and get a degree in computer science and/or software engineering. "
"Your attention span is too short to learn this idea. 
 EDIT: still learning how to format posts.
[haskell wiki link about partial application](","I've been happy to see mainstream languages like C# and Python embrace features of functional languages like list comprehensions/LINQ and anonymous functions. However, one thing I miss a lot when using most languages is partial application. It's something OO language users have been using for years but they often use an object type system instead of functions.  If you're not familiar with it, it is the essence of using a function argument to abstract over a value, and then filling in that function argument with different arguments to create new functions. So suppose I have some function that does a bunch of messy administrative stuff with IO and displays window saying, ""I'm done!"" You could use copy/paste to create many functions with all the same administrative junk, use a macro utility to expand the specialized part of the functions to include the administrative junk, use an administrative class which abstracts over the specifics, or you could pass the specialized part as an argument to the administrative function, creating new functions via partial application.  Ex.:  (ML syntax) administrative_function : (int -&gt; unit) -&gt; int -&gt; unit  (c syntax) void administrative_function(void (*specialized_function)(int), int);  (ML syntax) specialized_function1 = administrative_function special_function1  specialized_function2 = administrative_function special_function2    (c syntax) c and its cousins don't have a syntax for partial application, so making the specialized functions is less clean.     void administrative_function(void ( special_function)(int), int x)    {       do_stuff;        ( special_function)(x);        do_stuff;     }    void specialized_function1(int x)     {         administrative_function(special_function1, x);     }     void specialized_function2(int x)     {         administrative_function(special_function2, x);     }  Or in an OO view, this is basically the same as having a base class containing the administrative functionality but with the specialized stuff declared virtual, and then making new classes that fill in the virtual parts. When I see a class that take classes as arguments in the <> braces (e.g. generics in Java), I automatically think of arguments being passed to class constructors and member functions. In fact, you can do this in a functional style (i.e., with partial application) with OCaml, which has a really cool object system.  tl;dr Your attention span is too short to learn this idea.  EDIT: still learning how to format posts.[haskell wiki link about partial application]( "
I tried to test out of a class.  Was expecting C++ but got Scheme instead.  Thought I was having a stroke.,I do have a funny story about my first experience with Scheme.  I found out that during orientation I could test out of the first CS course so I decided I'd try.  When I read the description of the test it said that I'd be given two versions of the test and to complete one of them (my choice).  One would be Scheme and the other Haskell.  At that point in time I'd never heard of either language and so I just thought those were the names of the professors that had written the exams.  I had mistakenly assumed the test would be in C++ like the AP exam.  So I go in to take the test and I have no idea what any of it means.  The syntax was such a shock that I actually worried I might be having a stroke.  I was actually able to figure out a good part of the test after I calmed down but obviously had no idea what specific functions like car and cdr were supposed to return.  Then I got to the second half of the test and it required writing small functions but I didn't know how to do that.  Suffice it to say I didn't get credit for the course and had to take it like everyone else.  The good news is that it was written by the professor teaching the course so I basically got a sneak peak of what the exams would look like.  tl;dr I tried to test out of a class.  Was expecting C++ but got Scheme instead.  Thought I was having a stroke. 
"damnit, why don't students learn how to use unix/linux.  I like to ramble. 
 It def","I TA'ed an intro to programming for engineering course back in the day (2000-2002 I think), which focused primarily on teaching how to program in C, but with an emphasis on the problem solving side (e.g., the first assignment focused on figuring out the pattern in a series, then programming it and looking at convergence of your answer...another assignment was about the newton-raphson method).  We were still using the Solaris workstations at this time, forcing people to do stuff via the commandline (although for a basic C class, we didn't worry too much about the more complicated stuff...what we do would have worked on pretty much any platform).  I was really sad that the year after I stopped TAing, they replaced the Solaris machines with Windows boxes, and I think may have switched to visual studio and then later JAVA.  My friend TAed essentially the same course at another university several years later, where the ""programming"" that was taught was in MATLAB (useful for engineers, but it's scary that a lot of students wouldn't know how to write and compile a simple ""hello world"" app from the commandline).  During my time as a phd student working in computational nanoscience (and currently as a faculty member), most students I work(ed) with need(ed) to be taught basic unix/linux commands (as this is what powers almost all of our super computers clusters), C and C++ (C and fortran are the most heavily used languages in molecular simulation packages), basic bash and perl scripting, etc. all because they've only worked in the windows world, and/or have only learned to ""program"" in an IDE.  tl;dr: damnit, why don't students learn how to use unix/linux.  I like to ramble.  It def "
"Yes it does have a lot to do with bootstrapping, as the name suggests. It helps people with the initial stages of creating portable web pages.","> Also, my question was related to this library which from a quick review has NOTHING to do with bootstrapping, statistical or otherwise.  Bootstrapping is now commonly used to refer to all sorts of ""something from nothing"" scenarios, e.g., building a business without substantial venture capital, or the initial stages of loading an operating system. In this case it refers to the transition from ""there is no website"" to ""yay, there is a website that does nothing, but at least it does it consistently"".  The Bootstrap project is useful because that transition is surprisingly non-trivial to do at all well, given the unfortunate inconsistencies out there that will make your website look and behave differently across different browsers and operating systems.  TL;DR:  Yes it does have a lot to do with bootstrapping, as the name suggests. It helps people with the initial stages of creating portable web pages. "
"good programmer != good computer scientist, but also good computer scientist != good programmer","Let me clarify: you shouldn't study CS with the goal of learning how to write code. It won't teach you how to write code, just as astronomy won't teach you how to use telescopes. You can ""do"" astronomy without ever using a telescope and you can ""do"" CS without ever writing a single line of code.  Writing code is neither necessary nor sufficient for computer science.  What the quote doesn't say (but I would argue), is that the same is true in reverse: computer science is neither necessary nor sufficient for writing code (ditto for telescopes: astrologists can use telescopes without understanding astronomy).  Of course computer science can help you write better code (and thus maybe figure out what you're doing wrong) like astronomy can help you to make better use of a telescope (and thus maybe figure out that astrology doesn't really work), but while there doesn't seem to be much of a market for telescope-using there is quite a market for code-writing -- and plenty of people are pretty good at it without having a solid formal understanding of computer science (though they may in fact have become that good by unknowingly picking up some CS along the way).  That said, for 90% of the industry (e.g. most GUI programming or generic CRUD), when it comes to day-to-day programming, understanding how to use version control and writing readable code is more important than fully understanding Big O or the halting problem. Although a basic understanding of the general ideas is probably necessary, I wouldn't classify that level of knowledge as ""scientific"" any more than an understanding of physics that basically consists of ""if things go up they must come down"" or ""water hurts if you are too fast"".  tl;dr: good programmer != good computer scientist, but also good computer scientist != good programmer "
immutable already exists in JS if you want it and  const class  is the already existing sugar for immutable classes.,"If you want to make something immutable, make it immutable (e.g.  Object.freeze ). Not a fan of hiding what really happens. If people are mutating shared state across modules and because of that are able to ""accidentally"" change the inheritance of a class, there are other problems you should solve (proper modularization). And the whole issue resolves itself as soon as [const class]( is properly supported by traceur. So not sure why you'd want to have that level of obfuscation. I'd rather people like the author of the blog post contribute to traceur if they think it's a pressing need for them.  TL;DR: immutable already exists in JS if you want it and  const class  is the already existing sugar for immutable classes. "
"the way I understand it) with stateless services, the logic for management of state shifts to the client.","The nearest I can figure it is exemplified by logged in/out.  Disclaimer: I stand and welcome to be corrected as I have a superficial understanding.  The request needs to understand and persist state of user session and the response will be different depending on that state (logged in or out). That would be the opposite of a ""stateless"" service.  To make a logged in/out stateless, the service would need a parameter to identify the user session (usually called a token).  So the service request needs respond with some information. Let's say a collection of items related to the user's billing information.  In the stateless case the service takes a token and bill identifier, it checks the token then retrieves a collection of bill items based on the identifier. If the token doesn't check out it returns an empty collection. It doesn't care about the state of the user so it doesn't manage the response logic based on state. It retrieves a collection of items, empty or not.  In the other case the service only takes the bill identifier, it checks the request session then it too retrieves a collection of bill items based on the identifier. If the session doesn't check out, it has to return something else entirely (a redirect, exception information etc.) in order for the client to handle that. It cares about the state of the user and needs to manage the response logic based on state. It retrieves a collection of items or sends directives to the client in case something goes wrong.  So with stateless services you would need 2 requests to manage state on the client: one which responds with data and one which validates (and responds with true/false) for a token.  Not really sure how that makes SOA bad...  tl;dr; (the way I understand it) with stateless services, the logic for management of state shifts to the client. "
"Function types are structurally typed, Java lambdas are nominally.","> A first-class function, and a stateless implementation of and interface with that single method, are same thing for all practical intents and purposes.  No, they aren't. The problem is compatibility. When you have real function types, all lambdas that take the same number of arguments of the same types and have the same return type are interchangeable. This isn't the case with Java lambdas (as far as I can tell). Say I have this interface:  interface Handler {  void handle(String arg);}  I can create a lambda like (I think this is the right syntax?):  Handler handler = (arg) -&gt; System.out.println(arg);  OK, swell. But let's say I also have:  interface Applicable {  void apply(String arg);}  I can create a lambda for that the same way:  Applicable applier = (arg) -&gt; System.out.println(arg);  Note that these two lambda expressions are identical. But they aren't interchangable. I  can't  do this:  Applicable applier = handler;  And when it comes to  invoking  them, there's no consistent way to do that either:  handler.handle(""arg"");applier.apply(""arg"");  This means you can't write generic code that works with any function of a given type because there  are  no function types. You  could  make your own  Function  interface and establish a convention that everyone use that, which is what Scala does, but you can't easily retrofit that onto the piles of existing Java libraries that are either using their own different one-method interfaces.  TL;DR: Function types are structurally typed, Java lambdas are nominally. "
"but I think outside of the syntactic differences, the lambdas in Java8 are pretty close to how Scala's work.","Author of the post here.  To reiterate what I said in the article the only difference between Scala and Java8 lambdas is:   Java8 lambdas work with any SAM, not just a generic FunctionN SAMs (scala has Function0 to Function22 defined in its library).  there is no syntax in Java8 to generically invoke a lambda like there is in Scala, you must know the underlying type and use it's appropriate invocation method  there is no syntax in Java8 for specifying a Lambda type.  Instead of: (Int) => String, you'd use a SAM like Function1&lt;Int, String&gt;   I know my article was probably tl;dr, but I think outside of the syntactic differences, the lambdas in Java8 are pretty close to how Scala's work. "
"your boss had a point, albeit a small one.","Stuff grows, and that single, solitary place in the code might in the future become a giant clusterfuck of stringly typed if-then-else statements. With respect to the longevity of the code base, using enums is usually the way to go.  That said, making a big deal out of it is idiotic. If it's really just a couple of lines, it could be changed further down the line when an extension to that particular piece of code does come up on the todo list. However, your manager might have been afraid that the dude who will extend the code later on might be a moron.  tl;dr: your boss had a point, albeit a small one. "
"mysql_* can be used safely, but most people don't.","This might be more for 7 or 8 years olds. Sorry. :)  If you use mysql_real_escape_string() consistently and have the correct connection character set configured with mysql_set_charset(), then there's no effective difference between ext/mysql and the other options. You're safe.  The problem is that it's easy to slip into bad habits with ext/mysql. I would venture to say that most developers (probably not the ones who read /r/php) get at least one of those things wrong, not helped by the multitude of tutorials that advocate terrible practices ( direct interpolation of $_POST variables into queries . By contrast, [the first hit if I add ""pdo"" to that search]( talks about using prepared queries and parameters — teaching coding patterns that are much less likely to be used insecurely.  Also, it's worth remembering that mysqli can be used in a procedural manner if you want. Nobody's forcing you to use OO there if you don't want to.  tl;dr: mysql_* can be used safely, but most people don't. "
its not that you took a week to do something.  The problem is you took a week to do something no one cares about.,"Everyone is blaming management...and they are to blame, but your friend isn't totally innocent.  Ideally, tickets would have some sort of estimate in hours or story points or what have you.  They would be prioritized by the project manager based on cost/benefit.  The programmer then would take things off the  top  of the backlog and then the PM can look at performance in terms of estimated hours of completed tickets.  That being said, just because the PM isn't prioritizing tickets doesn't mean the programmer should just take what they want.  In a very real sense, font size can be far more important than an incredibly rare bug.  Font size effects all users where as this bug is a very small subset.  Its a perfectly legitimate complaint when a programmer takes a week to fix some pet ticket no one cares about.  If you took a week to fix that thing everyone is constantly complaining about or the one with a comment list a mile long I doubt you'd get the same dubious looks.  tl;dr its not that you took a week to do something.  The problem is you took a week to do something no one cares about. "
Your only motivation is to be able to easily steal other people's stuff. Expect a dirty fight from the people who don't want their stuff stolen.,"> If we used a public goods model for production  How do these producers now feed themselves, pay their rent - and clothe their children?  > if everyone in history was like you, the British would still rule North America [..] slavery would still be legal  Xenophobia isn't an effective argument against someone who was born in England, where slavery was abolished long before the US. And the WWW (the technology you are reading this with) was invented by an Englishman in Switzerland. You're welcome.  Your argument is also for the illegal duplication of non-essential luxury digital items, not human rights.  > Such lame hyperbole.  Licensing computers  is not my idea  are already illegal to use. It's not infeasible to severely tax motherboards that did not have a [Trusted Platform Module]( on the motherboard, and then build anti-piracy measures into the OS.  I agree there's a romance to free trade of digital information. But your ""win"" condition harms professional and independents alike. The technology (and legal framework) already exists to counter it.  You also misunderstand independent content: it's unpopular because most of it is crap. There are no distribution problems. It is trivial to host your own website and torrents. Print-on-demand will get your book, CD or DVD published - and Amazon will sell it. You can even get your stuff on [iTunes](  tl;dr - Your only motivation is to be able to easily steal other people's stuff. Expect a dirty fight from the people who don't want their stuff stolen. "
You don't get what you want by sitting there and hoping it's handed to you.,"So therefore... what?  I could go on a rant here, but I'll let Patrick Henry do my talking for me:  >They tell us, sir, that we are weak; unable to cope with so formidable an adversary. But when shall we be stronger? Will it be the next week, or the next year? Will it be when we are totally disarmed, and when a British guard shall be stationed in every house? Shall we gather strength by irresolution and inaction? Shall we acquire the means of effectual resistance by lying supinely on our backs and hugging the delusive phantom of hope, until our enemies shall have bound us hand and foot? Sir, we are not weak if we make a proper use of those means which the God of nature hath placed in our power. The millions of people, armed in the holy cause of liberty, and in such a country as that which we possess, are invincible by any force which our enemy can send against us. Besides, sir, we shall not fight our battles alone. There is a just God who presides over the destinies of nations, and who will raise up friends to fight our battles for us. The battle, sir, is not to the strong alone; it is to the vigilant, the active, the brave. Besides, sir, we have no election. If we were base enough to desire it, it is now too late to retire from the contest. There is no retreat but in submission and slavery! Our chains are forged! Their clanking may be heard on the plains of Boston! The war is inevitable--and let it come! I repeat it, sir, let it come.  >It is in vain, sir, to extenuate the matter. Gentlemen may cry, Peace, Peace-- but there is no peace. The war is actually begun! The next gale that sweeps from the north will bring to our ears the clash of resounding arms! Our brethren are already in the field! Why stand we here idle? What is it that gentlemen wish? What would they have? Is life so dear, or peace so sweet, as to be purchased at the price of chains and slavery? Forbid it, Almighty God! I know not what course others may take; but as for me, give me liberty or give me death!  tl;dr: You don't get what you want by sitting there and hoping it's handed to you. "
"is that it's a nasty connection. 
 And let's not get into the whole binary/textmode thing...  shudder","for a start, it pre-dates the standard encryption systems.  Your password is sent  in the clear  without encryption or hashing.  Secondly, it predates people knowing how to use TCP/IP ;)You connect to one port, login, request a file, and then  the server initiates a new connection to send it to you  instead of using the already open connection.  Apart from anything else, this plays merry hell with firewalls / NAT systems, since, from a TCP/IP point of view, they've no reason to think this incoming connection is related to your existing ftp connection, and so are liable to drop it.  There's workarounds, but the tl;dr is that it's a nasty connection.  And let's not get into the whole binary/textmode thing...  shudder "
"Stop lying to clients, and bill for each hour you do work for hire.  Feel free to invoice at $0 if you feel guilty.","Time to get clients is spent money, can't do anything about that... but after that point, bill for every hour.  Too many developers have reams of unbilled time.  Honestly, get a fair rate, and bill what you work -- stop giving away free hours.  A lot of this comes down to developers generally being a rather low-key type of individuals who would feel bad or guilty about billing non-productive hours.  Be honest with your client from day one, explain to them the time it will take to learn the system, explain to them the risks (in time) and then bill them fairly and honestly.  This ""invisible value"" is a huge negative for the entire IT consulting market (both contractors and clients).  It creates a false flow of work, ""I hired Ed and it only took him, lets see on this bill... 3 hours to learn our system"" ... then when I take 55 hours to learn it, I look dishonest.  If you insist on giving away your work, put it on the invoice with a cost of $0.  TL:DR - Stop lying to clients, and bill for each hour you do work for hire.  Feel free to invoice at $0 if you feel guilty. "
"you want something. How you ask does matter, but you shouldn't expect someone to do it just because you asked nicely.","Saying ""I spent my time writing a good bug report, so now you have to spend time to fix my bug"" is hilarious.  Projects have no expectations about well written bug reports. We see everything from ""X doesn't work."" to exquisitely written test cases and reproduction instructions, often from other developers. From this description alone, we can't tell which bug is more important. The only thing we know is one  may  be easier to fix.  This bug took 6 hours to fix and more time to backport to a specific version. ""X doesn't work"" can often be closed as ""duh, that wasn't supposed to work"" or as ""sorry, not descriptive enough"", taking ~5 minutes.  tl;dr: you want something. How you ask does matter, but you shouldn't expect someone to do it just because you asked nicely. "
don't rely on too many plugins. Cache your selectors.,"The overhead of jQuery is negligible. Load the latest version using Google CDN and there's a good chance that the user will have it cached.  The big problem is when you rely too much on jQuery and you end up with 300 plugins.. which I think is a big no no.  You should use jQuery for DOM manipulation. The jQuery team makes sure that their code works between different browser / js interpreter. (until the new jQuery version is up, which will drop support for old IE).  For example, you don't have to detect whether you can use querySelectorAll, jQuery does that for you with its sizzle engine. You can be sure that $("".classname"") will work.  The overhead for selecting is negligible, just make sure you cache the selectors. For example, don't do this:  $("".container"").addClass(""heyhey"")$("".container"").removeClass(""yoyo"")  Instead, cache them;  var $container = $("".container"")$container.addClass(""heyhey"");$container.removeClass(""yoyo"");  or chain them  $("".container"").addClass(""heyhey"").removeClass(""yoyo"")  By removing things like that from your workflow, you will have more time to focus on the logic of your program, which is a good thing.  Also.. I find some part of jQuery much more elegant than pure JS. For example, AJAX requests.  tl;dr: don't rely on too many plugins. Cache your selectors. "
Redditting from office is as easy as redditting from home. :P,">  So Marissa Mayer, who famously banned working from home at Yahoo last year, was wrong?  > It’s not so simple. There are lots of factors that could lead to such a ban, including a culture where remote workers tend to be slacking because of low morale. Also, we were studying call center work, which is easily measured and easily performed remotely.  I'm not a call center worker, but it seems that half of my open space cohabitants are... add to this that I'm a one-man-team (literally) and I have exactly  nil  functional contacts on site -- both customers and team are spread globally. So yeah, my kitchen makes a better office for  some unknown  reason and I tend not to be in the office when not required to by meetings etc. Which is around once a month.  On the other hand for last few months I've been hitting managerial meetings (calling in) and I've been struck by how many local managers in my company  ban  remote work for their teams. One guy literally said ""I need to have my  flock  where I can see them at all times"". First thing I did was looking that guy up and making a note of not ever moving there. He literally called his people sheep and sounded like young ""imma manage you so hard"" power-tripper.  So it all depends on factors, but people who want to get their stuff might be better off without distractions. 100 people open spaces are not work-friendly. If 10% if people are there only to fraternize (there's a space for that, it's called kitchen/lunch room/coffee machine spot) -- they will make it worse for everybody.  tl;dr Redditting from office is as easy as redditting from home. :P "
"not that it's right, but markets are advantage driven.","It is murky.  But it's kind of how it's always been and to think banking/trading is anything different would be incorrect :)  If I remember correctly (and I may not) Reuters (UK market data/news company - like Bloomberg) was started when someone spotted that cruise liners from the US didn't dock immediately on arriving in the UK (not sure why, quarantine perhaps? waiting for the harbour to open? dunno).  Anyway, this dude decided to row out to the boats and beg, buy, borrow or steal the most recent copies of the US papers/financial data.  In the days before transatlantic telephony this gave him and anyone he sold the data to a days head-start.  It's still pretty much like this.  You can guess/predict the market all you want but if you can get your hands on prices and execute quicker than the other guy you're always going to have an advantage over him.  tldr; not that it's right, but markets are advantage driven. "
I would love a more up-to-date formulation of Fitts's Law for touch interfaces.,"Fitts's law isn't just ""big things are easier to hit"", it has a [precise mathematical formula]( relating the time to acquire a target to the distance and width of the target.  The wrinkle with touch interfaces is, there is no pointer and most controls expect only touches close to them to register. The user interaction is different: with touch interfaces, users generally try to hit the control directly; with mouse interfaces, users observe the movement of the mouse pointer and adjust their mouse movement to fit. So effectively the distance to a touch interface target is zero? And since it is the width along the line of pointer movement that counts, what is the width in the equation?  Subjectively speaking, targets in touch interfaces are harder to acquire than in mouse interfaces because the finger is much larger than a mouse pointer and there is no pointer feedback. On the other hand, once acquired, accurate gestures are far easier. For example, in my app [Instaviz]( I was able to do decent shape recognition on the shapes users draw, but I wouldn't dream of asking them to do the same on a mouse interface... the phrase ""drawing with a bar of soap"" comes to mind.  Also, the ""infinite edges"" of the Law are different too. Users are often constrained by the need to hold the device or not obscure content with their hands. This leads to the best edges being left and right, then bottom, then top.  TL;DR: I would love a more up-to-date formulation of Fitts's Law for touch interfaces. "
I think that it's silly to call Radix sort O(n) on a computer that can't do a single bit compare in 1/32 the time of a 32-bit compare.,"Radix Sort is kind of cheating, though, because it's not a  comparison sort   and  swap an entire row in O(1).  To level the playing field, you could store each  int  as  bool[sizeof(int)*8] .  Now qsort's compare can look at individual bits.  qsort's compare would often be able to finish a comparison before going through all the bits.  Still, qsort would take longer than Radix sort's 1-bit but with this setup it would be in your best interest to have compare look at the fewest bits, so you'd select decreasing powers of 2 as a pivots instead of list members...  Et voilà, qsort is radix sort.  Or look at it the other way: Implement radix sort on ints and it's just like you did qsort with pivots as decreasing powers of 2.  tl;dr : I think that it's silly to call Radix sort O(n) on a computer that can't do a single bit compare in 1/32 the time of a 32-bit compare. "
elevator algorithms use a lot more factors than the article describes,"There is a lot of simplification in those algorithms. In real life there are far more factors at play.  The most important point as to why such modifications are made, is that some elevators do not have digital controls. Many are still elevators with relays to control the travel. No parking control is done with these.  For digitally controlled elevators, the running of the individual elevators is in at least one of three modes   normal  emergency  maintenance   For normal mode, the elevator needs to know how many other cars are available so that they can 'buddy-up'. In all but the simplest set-ups, cars work in groups per zone. Zones are areas of priority per group of cars when answering requests. Cars for a zone will typically wait close to their zones.  Zones themselves change throughout the day and week. In the morning, more cars wait at the bottom floors, whilst at lunch time and home time, there will be a bias for waiting in bands throughout the building. The placement of these zones and when they change during the day is customised per building because it depends on the use of the building, the habits of the tenants and visitors of the building etc. A Hospital will have different traffic patterns to an office block that has 9-5 workers. where there are shift workers, the pattern is changed again.  Nowadays, most elevator systems also learn from the call patterns they experience, and adjust themselves constantly. Data such as public holidays is also added to modern control systems to help optimise car availability.  TL;DR   elevator algorithms use a lot more factors than the article describes "
version]( for people who don't like walls of text,"Thank you for posting this.  As with most things, there's two sides to a story. It's quite interesting to see the contrast between the two side's arguments.  Google  - ""The parts we used were freely available.  Also, Oracle was all for Java on Android until we decided to not partner with them.  They're attacking us because they missed out.""  Oracle  - ""Freely available code does not mean we don't own them.  You need our blessing to use it and after negotiations fell through, you used them anyways.  You brought this on yourselves.""   Edit:   I'd love to see Google win this, but it's going to be rough.  Keep in mind that although they would love to fight Oracle on the grounds that you shouldn't be allowed to copyright core libraries, that's not what this case is about.  (Fighting the infringement is easier than fighting the law itself)  Oracle is going to try and show that Google knew they would need a license to use the Java libraries and decided to go on without them.  Google is going to have a hard time denying that head-on, so they're going to be trying to show that Oracle treats the Java libraries as free and open and is singling Google out because of a failed partnership.  They're also arguing that they even if licensing is required for the Java libraries, it doesn't apply to them because they're using Java as a language and using their own ""clean room"" implementation of the core libraries (which Oracle says they plagiarized).  This case is going to hinge on two things: did Google even need Oracle's blessings to use their libraries, and if so, did Google's attempt to get around that result in plagiarism/intellectual property infringement?   Edit 2:  Here's a [""tl;dr"" version]( for people who don't like walls of text "
"Great work, congratulations on the event. IMO you have some interface polishing to do.","First: Great work. The website is functional and I like it a lot.  But...  Please consider using non-web-standard fonts. There are a great many fonts out there that are free, lightweight, and look very good. When I see a site devoted to IT in 2012 and it's still using Times New Roman in headers and Tahoma in body font, I get a little sad inside.  The right side of the code view pages is covered in vertical scrollbars. Tiny ones, one per line. I'll screenshot it for you, but I'm confident you see it too.  When clicking ""more"", why are you using a Javascript alert to tell me there's no more results? I can't use my browser until I close it. Please consider just changing the text of the ""more"" link to say ""No more results"" instead of an alert(). Same line of code, just different functionality.  Lastly, on a more ""personal opinion and definitely not for everyone, just personal preference"" note, I think you should consider implementing Twitter Bootstrap. This would allow you to easily and consistently stylize your buttons and inputs, as well as allow you quick access to notification badges and stylized alerts. Who knows? Maybe you can implement cool functionality with the dropdown menus or carousel.  tl;dr: Great work, congratulations on the event. IMO you have some interface polishing to do. "
"I'm glad your a developer (I assume) and not an economist, because you would suck at it.","You are entitled to your opinion.  However suggesting that the presence of a competing product is unnecessary because the one you like is plenty good is plain and simple fanboy-ism and demonstrates a very weak understanding of economics, aka, ""how the world works.""  Competition is good for everyone in the market.  The presence of a competing product prods the other players in the market to innovate.  A sees that B is doing that awesome thing, A decides to do it too.  Everyone gets better.  If A doesn't want to spend the time to do said awesome thing, they lose market share and B gains users.  The only case where this is not totally true is when a product is serving a niche (C is for lovers of cats only and doesn't need awesome thing for cat lovers), and then why would you care about a niche product?  Taylor Otwell (creator of Laravel) clearly states above why he started Laravel in the first place.  CodeIgniter wasn't doing it for him.  He wrote his own, and as he wrote it he continually refined the feature set and the methods used to accomplish the feature set.  From that a product was born that many people prefer.  It is not necessarily better than all the others, but different (some say) in a good way.  Windows has a larger community and has been around much longer (can't speak to documentation), does that make it a superior product?  Trick question, it's a preference. (But no, it's not)  I do know that even though someone may prefer one or the other, that without the presence of both (and others) in the market the sole, unchallenged market leader has no incentive to innovate.  Your job market statement is just ludicrous.  Let me make a similar one.  ""As a mechanic, I sure wish Chrysler would just shut down because, gosh, they just complicate the job market by making cars.  They do the same thing that Fords and Chevrolets do.  The less cars to work on, the better.""  TL;DR - I'm glad your a developer (I assume) and not an economist, because you would suck at it. "
The Bitcoin protocol is easily duplicated and as such has no value.,"In this comment I will explain my views on the article. I will then give my opinion on the financial value of Bitcoin.  Good read, takes too long to start going (and god I hate the future description bullshit), but explains the key points clearly.  In regards to the actual value of Bitcoin, I think it's been a good way to get rich, but it will ultimately become worthless.I first read about Bitcoin in 2010. I knew it had potential, but something was bothering me. In hindsight, buying a few hundred bucks worth was not a horrible risk, but I don't really regret not buying.  The problem I see with Bitcoin is not that is is a ""finite resource"" - that's fine. It is the fact that it is a REPLICABLE concept, ANY person can create an identical, or better yet, an improved version of it and voila, Bitcoin becomes absolutely worthless in a heartbeat. As soon as people realize there are many identical ""bitcoins"" they will see it really doesn't make sense to buy it in its current state.  tl;dr: The Bitcoin protocol is easily duplicated and as such has no value. "
It's always easier to go from native development to using a library than it is the other way.,"Disagreement is healthy, but I think you missed the point which is that OP should be working to gain a better grasp of what the browser is doing under the hood.  Working with GSAP is handy, but if you never delve into the underlying architecture, you'll be missing out on a fuller understanding. For example, the only reason CSS3 animations natively don't have promises is because promises and CSS3 animation callbacks are completely separate pieces of technology. Are promises easier to implement in GSAP? Sure, but that's because that functionality is already there in JQuery, which Greensock is leveraging. Now, is it impossible to implement promises on native CSS3 animations? The answer to that is a hefty no since all that's required is to add the animationEnd callbacks to the promise chain, which will then for once all the animations are complete. There will always be instances where the use of external libraries will be frowned upon (mobile comes to mind, but that's beginning to lessen).  TL:DR; It's always easier to go from native development to using a library than it is the other way. "
"As the number of data scientists grow, so too will specialization; abstracting Predictive/Explanatory modeling from data preparation.","I would advise against taking my post as an excuse not to practice ETL, because it WILL be necessary at some point, but if the idea that data scientists in general spend the majority of their time scrubbing/munging isn't already a myth, it will become more-so as time goes by.  I spend very little time scrubbing/munging and why would I?  A business analyst with some SQL skills can do that for me and save me a boat load of time on deriving value from the data.  The BA can't do the latter (to the same degree) so it makes sense to specialize where possible.  I know that tons of data scientists aren't utilizing data from standard SQL DBs, so if your scale can justify it, it makes sense to hire a data engineer (which can be considered a DS who specializes in data management/big data tech) in addition to the DS who is building models, etc.  I realize start-ups are the exception - they may not be at the point where specialization is affordable/justifiable, so they either hire a Unicorn who is an expert in all DS areas OR more likely, they hire a ""full-stack"" guy who is solid in all the areas they need, but isn't an expert in any of them.  This isn't a knock against ""full-stack"" guys, they bring tons of value in this context.  TLDR - As the number of data scientists grow, so too will specialization; abstracting Predictive/Explanatory modeling from data preparation. "
you want to use sudo as little as possible because using sudo is so much less secure then keeping with your current user privileges.,"I think using a user-writable install prefix with Homebrew is probably more secure than a privileged one. Since Homebrew is a source code-based packaging system, there's always a build step involved in installing a package. Since building and installing package ""foo"" consists of just a single step, running ""brew install foo"", I'd much rather run that command and all of its child processes with my non-privileged user permissions than run it with ""sudo"". If there's a virus in the build process, at least it can't touch my system files.  A standard Macports install requires you to ""sudo port install foo"". I don't know about you, but I'm not comfortable running a build as root. And you need root privileges to update the local Macports repo, too. With the suggested Homebrew approach, you don't need root for anything.  tldr: you want to use sudo as little as possible because using sudo is so much less secure then keeping with your current user privileges. "
"I pulled out a dirty ""old man"" coding trick on my interns. 
 EDIT: gramer","One time I worked at a place where I was responsible for shepherding 10 to 15 programmer interns (read: cheap software talent from the local college).  I was only a few years older than these lads (like, literally 4), but I grew up learning code the hard knocks way.  K&R, DOS assembly, reading fucking text files from BBSes.  Basically, it was bullshit crap you had to do before Google made my life as a programmer a god damn heaven-sent blowjob.  Anyways.  They used a lot of C++.  Templates, STL, all that crap.  Since I spent most of my days teaching them how real memory works, and wrangling OSS PHP written by high functioning retards, that kind of stuff was never apropos.  I step into the lab to make sure no one is getting noogies one day, to hear them discussing the speed of writing parsers with STL/templates and all that jazz.  One of them pipes up, knowing I have, at best, a poor grasp of C++ and STL and challenges me to a coding contest.  We will be provided with 10 similar, but not exactly equal, lines of characters and numbers which must be dropped into variables.  ""Ok,"" I says, ""let's ride, bitch.""  So, we sit down at our respective workstations, and the time-keeper says ""Go!"".  Jeff, bless his heart, wrote this fancy-ass template/STL thing in about 2 minutes.  Very nicely done.  Extensible.  Correct.  Beautiful.  Go Intern and all that shit.  Of course, I did ctrl+C, ctrl+V on sscanf 10 times, and made a few minor adjustments for each line in about 30 seconds.  My code was scoffed at, scorned for using primitive techniques, and 25 year old library calls.  ""Yes,"" I said, ""but the requirement was for speed, not maintainence.  Now get back to work""  tl;dr I pulled out a dirty ""old man"" coding trick on my interns.  EDIT: gramer "
I did not feel the need to use any MVC frameworks because my simplistic approach worked so far.,"I didn't really feel the need to do that, I just create Data Access Objects that manage the database operations(with my database object), and provide an API for managing the data, I have Smarty templates and I have my business logic scripts which most of the time are just switch and case statements.  I am working on replacing the later one(business logic) with objects that contain methods, that map to actions.  I have done a lot of experimentation, I have wrote a lot of data model frameworks and stuff like that but then I realized that this is not what I should be doing because I don't spend most of my time on the server side, I work more on the client side.  TL;DR: I did not feel the need to use any MVC frameworks because my simplistic approach worked so far. "
"I feel like the same strategies are now appearing in all industries.  Hire cheap college graduates with no chance of promotions, fire expensive veterans. 
 EDIT: Forgot a word.","Dead on.  It's been an interesting progression, from my (narrow) perspective.  I graduated in 2007, rumors were that we programmers would have no chance to get a job, because of outsourcing.  But, it seemed that companies were finally wising up to outsourcing, realizing it may be more trouble than it is worth, and were hiring local programmers.  It seemed many from my class had no problem getting a job, even the questionable (bad) programmers.  But then... the economy started to implode.  Rumors of the veteran programmers getting fired and being replaced with cheap college graduates.  College graduates are desperate, willing to work long hours for little pay, where as veterans demand respectable pay and respectable hours.  The interesting thing is, I feel like I'm seeing the same type of strategy now used in many industries, outside of technology jobs.  This creates a gap: the young are worked to death but still don't have the money to buy property, have families, and the other normal purchases that strengthen the economy.  On the other hand, the veterans (AKA older) folk have a hard time securing any ""respectable"" (AKA what they are accustomed to) job, no longer can support their property, families, etc.  The middle class is vaporizing.  These are just my observations, mostly heard from college students since I do a lot of teaching.  Take it with a grain of salt.  TL;DR: I feel like the same strategies are now appearing in all industries.  Hire cheap college graduates with no chance of promotions, fire expensive veterans.  EDIT: Forgot a word. "
"Your example assumes too much, and is really best handled by understanding and fixing the API.","You're trying to fix a bad API design with language features.  Exceptions are more or less ""contracts"" in the sense that the method promises to do what it says it can do unless it encounters one of the exceptional conditions as defined in its interface (at least in Java, in C# for example this is implicit).  The onus is on the writer of the API to come up with the correct set of cases on when the method is unable to live up to its responsibilities.  Your example assumes that parseEntry is self-contained and not stateful, a log entry on each line I assume.  But what if the log file was in XML? Then it would make sense for parseEntry to throw an exception, because of an unclosed tag, for instance, throws off parsing of the rest of the file.  Tl;dr - Your example assumes too much, and is really best handled by understanding and fixing the API. "
"I wanted make sure people didn't get the wrong ideas about what Angular is doing with $watch and $digest, particularly my team members that read this subreddit.","So I saw the post the other day on $watch, $digest and $apply in this subreddit, and I honestly felt like it didn't do a great job of accurately explaining what they are and how they work together.  I'm currently the front end ""architect"" (lead) for a large team of developers working on a large effort in Angular. My worry is that since some of my team members are redditors the other article would give them the wrong ideas about what's happening... not because the other article was outright ""wrong"" but because it's not exactly accurate either.  The most important statement in this entry, if you're kind enough to read it is this:  **DON'T TAKE MY WORD FOR ANYTHING, [READ THE CODE ON GITHUB!](  This is the beauty of using open source frameworks. You can read the code and understand what's going on.  I'm not doing this for any sort of profit (I think Blogger.com may make something off of ads they inject or something, I don't know, I have AdBlock)... I'm doing this to learn and help others.  TL;DR: I wanted make sure people didn't get the wrong ideas about what Angular is doing with $watch and $digest, particularly my team members that read this subreddit. "
"If by ""free to copy that beat"" you mean ""could probably get away with copying that beat"", you might be correct depending on the circumstances.","> you can't directly copy any of these products she mentioned.  RTFA again, please. Or in this case watch the video.  There IS ABSOLUTELY NO PROTECTION (in the US, and  de facto  no protection overseas) for fashion.  > you are free to copy that beat and use it in your own music  This is 100% incorrect. Specifically, the recording itself has a copyright that is independent of the ""written notes"" of the song. So, yes, you could take a ""boom tisk, boom boom tisk"" pattern and ""copy it"" by playing it on your own drum set. It's not artistically significant. But if you take a sample, if you take the actual sound from the original record and put it in your song, you are infringing copyright. There are grey areas about amount of content used, whether it was ""the heart of the work"", and other fair use considerations. But legally, you have no inherent right to take ANY PART of the song. That's what ""All Rights Reserved"" means.  By contrast, in the fashion industry, there is NO COPYRIGHT in ANY PART of the dress/shoe/whatever. So you literally  are free (as in freedom) to take whatever you want  TLDR: If by ""free to copy that beat"" you mean ""could probably get away with copying that beat"", you might be correct depending on the circumstances. "
"they are hoping to get rich slowly. 
 I still remember Cisco buying Arrowpoint for $5.7B.  They couldn't have made more than $200M revenue from that acquisition.   boggle*","I saw an article recently (unfortunately, I don't remember precisely where; friend forwarded me the link and I'm too lazy to login to work to find out if I already deleted it or not) that took a more nuanced view on this topic.  Women  are  starting businesses.  Instead of the high-risk/high-reward VC-funded startup, they're often more interested in ""lifestyle"" businesses with a more organic growth pattern.  Having watched a few startups' hiring get out of hand, this type of business model seems wiser to me as the founder can adapt and develop at a more sustainable pace.  Interestingly, if I was going to do a startup, I'd do the same thing--grow a business with as little outside funding as possible.  I'd rather it be small, steady, and mine.  That said, if I did a startup, it's because I like selling product not that I want to create some technology that gets picked up by John Chambers*.  tldr; they are hoping to get rich slowly.  I still remember Cisco buying Arrowpoint for $5.7B.  They couldn't have made more than $200M revenue from that acquisition.   boggle* "
Schools and libraries having computers do nothing for childrens' and adolescents' general apathy towards learning about computers.,">I admit, my parents did buy a computer for me (which was a big deal, being poor in the late 80s) but I can't see why that would make a big difference to younger people, who have grown-up with fairly easy access to computers (at school and in libraries if not at home).  There are some problems with this, unfortunately. Despite being surrounded by computers, few people my age (I'm 18) have a damn clue about them. They just use computers, and often poorly at that. They don't know how to type at a reasonable pace. (Agonizing hunt-and-peck typing is the norm, it seems.) They don't care about how computers work. They just use them. By the time I was nine, I was reading computer books of varying subjects from the library and experimenting. I learned BASIC and HTML, later JavaScript and PHP. Now I'm playing around with Objective-C, building an iPhone application. Apparently that's not  normal.  I don't know anyone with the same level of excitement about computers.  Now, schools and libraries have computers, yes. Unfortunately, they won't do you much good if you have anything more than a passing interest in computers. They lock them down with software like ""CybraryN"" so you don't even have access to the Desktop or Taskbar in Windows, they filter websites, etc.. Forget doing anything remotely interesting like punching some basic HTML into Notepad... (It's laughably easy to get around the software if you know what you're doing, but you end up with a chicken/egg situation.) Public computers are pretty much only good for ""safe,"" non-intellectually stimulating uses that the tech-illiterate staff know exist.  TL;DR: Schools and libraries having computers do nothing for childrens' and adolescents' general apathy towards learning about computers. "
"this presentation is awesome, take some time to watch it","I witnessed his presentation at the ekoparty security conference held last September in Buenos Aires. Since argentine customs didn't allow him to enter the country with two ATMs (it seems customs are specially sensitive to this kind of machines) he had to ask his girlfriend in California to show the ATMs via UStream as he remotelly cracked them. It was, if I'¡m not mistaken, the last presentation in the conference and it was one of the best as well. He was able to rewrite the firmware of these ATMs and made them literally to do a 'jackpot' (victory music included) throwing money all over the floor.  It's incredible to see how ""easy"" it is to do it (once you had the previous knowledge and tools he developed, which probably weren't easy to make). Each ATM brand has a unique brand of key which can be used in any ATM of that brand, and once you have access to the inside you can simply plug an USB drive and start rewriting their firmware.  tl;dr this presentation is awesome, take some time to watch it "
"you're not hygienic, and that's dirty; but you don't actually
need heavy machinery, you could do things right with a little care.","{ do      var temp__ = `left      match temp__ true?          case true then `right          else temp__      end  end }  This is wrong if the variable  temp__  is used in the code of  right (and  left , if  var  is implicitly recursive). This is a classicalproblem with macros, which prompted the Lisp folks to design ""hygienicmacros"", which are actually not so clearly defined. Look at the workof the Racket people to see how modern lisps handle hygiene.  Hygienic macros are actually not necessary, provided you respecta simple rule: in the translation of a macro, always place a givenquoted code at the scope level it was intended for¹. In your case, bothexpressions where intended to be at the ""root"" scope level, thereforeyou need to use them both at the root, under no binder.  ¹: which generally means ""under no additional binding"", unless your macro is meant to be a binding construct.  Something like (I'm only making this up, don't know the real magpie syntax)  { do      var left, delayed_right = (`left, (fn () `right))    match left true?          case true then delayed_right()        else left    end  end }  Unless  var  is implicitly recursive (in which case you needa non-recursive binding construct), both  left  and  right  will beplaced under no additional binding. This is the right thing. Theshort-circuiting behavior is obtained by delaying  right  evaluation,using a 0-ary function for that purpose.  The code also uses simultaneous bindings of two names, using a tuplepattern. This is important because  var left = `leftvar delayed_right = (fn () `right)  would be wrong, as  right  wouldn't be in the root scope but undera  left  binder. Lisp and ML languages introduce a syntax forsimultaneous, non-recursive bindings, which would be slightly moreappropriate for this purpose.  TL;DR  : you're not hygienic, and that's dirty; but you don't actuallyneed heavy machinery, you could do things right with a little care. "
"It's useful, and I've had to some shenanigans to get the same result. 
 edit: I accidentally a word.","Here is a specific example of something I wrote which needed this:  I work at a university where I help maintain the main website and several department sites. We use a CMS which allows the departments to maintain their own content using content classes which we maintain/create. I was tasked with making a new content class which displayed items from a feed. This feed might be from twitter or a blog or whatever, and in the form of atom or rss. I used the Google Feeds API and needed a way to accept in the feed url as an argument so that I could pass that in to the Feed API and then output the results using a Soy template. The JavaScript and templates would be compiled.  I did this by creating a JavaScript object on the webpage below the dom element the feed was supposed to be displayed in, which then had an array as an attribute to that object, and I would push the arguments (feed url, how many items to get, the id of the dom element...) into that array. My script would have an object of the same name, with the push function overridden as a prototype of that object, and when that push function is called, the script starts. There might be several of these on a page, so I needed to iterate over each object which was received and make sure it was all in order before trying to output it.  This is pretty much how this server2.js works, only with more features, such as garbage collection, the ability to add more than one ""hook"" (which is their equivalent of my ""push""), etc.  tl;dr: It's useful, and I've had to some shenanigans to get the same result.  edit: I accidentally a word. "
"if you have to pay for a license, it's not Free Software anymore.","The author doesn't seem to understand that not charging for the software is one of the most vital aspects of successful open source. A proprietary license (because that's what it inevitably becomes if you have to pay before you can use the software) means you cannot freely redistribute, modify and build upon anymore: the software is no longer Free.  It would also disturb the evolutionary mechanism (survival of the fittest) that takes care of sorting free software packages by quality through exposure to a wide audience: first of all, the licensing cost is an incentive against switching to a different (possibly better) alternative; and second, licensing cost becomes a decision factor next to quality, which in turn leads to the project with the best cost/benefit ratio winning, not the project offering the best real-world usefulness and quality.  TL;DR: if you have to pay for a license, it's not Free Software anymore. "
My naive questions as a junior engineer may have changed an aerospace standard.,"It's been a long time since I've been in that world, so I don't know what current state of the art is. In my day, it was all manual, and for hardware, component by component.  Interesting anecdote: I was working on a complex timer module, which used crystal oscillators. The vendor I used asked me what kind of fill gas I wanted. Knowing nothing about crystals,  I asked about options, he explained the various typical fill gasses, dry nitrogen, helium, etc. I asked what would happen if the seal failed in a vacuum. He told me the frequency would shift significantly. I reasoned that a slow leak might never be noticed at ambient (diffusing very slowly with ambient air). I asked him if they could make crystals with evacuated enclosures. He said they could. I reasoned that with those, a leak at ground level would show up quickly, and in orbit would be benign. I spec'ed evacuated enclosure crystals for the module and explained my reasoning to the prime contractor. They thought it was such a good idea that they might make it the spec system wide.  TL;DR My naive questions as a junior engineer may have changed an aerospace standard. "
HTML and the people who write it are complicated.,"Yup. So I maintain [Day of the Shirt]( and a suite of 30+ different website scrapers for collecting daily t-shirts. I have to modify at least 3 scrapers a week, here's why:   a lot of website are hand coded, which means xpath alone isn't going to cut it.  Some html I have to preprocess and grep out some gnarly tags before feeding it into a parser.   some websites have no semantic structure whatsoever: the title is one of 5 P tags but ends with ""shirt"", the full-sIzed image within this arbitrarily ordered list ends with ""_full.png""   a page will be one of 4 different templates (2 for Tuesday, grab-bag sale, etc) of which I have to maintain a big regression suite of HTML fixtures to test against   stuff just changes.  Sites get redesigned. The new intern really likes strong tags. Knowing when something breaks (and why) with good alerts (but not too many cause sometimes I'll just update it manually for a day cause I know it's not worth adding to the regression library) is really valuable too.   Edit: also, you're gonna have to sanitize your results. Even if you find the most beautiful semantic xpath, some clever-kid is gonna throw a non-breaking space in there and ruin your day.    tl;dr: HTML and the people who write it are complicated. "
"yep, distributable packages should avoid pinning requirements, large applications pretty much have to.","So, I think this comes down to us both being right. In a way.  What you're saying is true, and I agree with it. A distributable package (like piprot, or requests) should endeavour to be compatible with as wide a range of versions of it's requirements as possible.  You'll notice that piprot's setup.py doesn't designate a version number for it's only requirement, requests, for this very reason.  I'm, however, coming from a web application background, where ensuring that you're running the same version of packages/requirements on all 16 or 200 (or 2) app servers, is absolutely paramount. The same goes for development and CI environments, and it's the reason why, I believe, everything in your requirements files should be pinned to a version.  Obviously there's a lot more that goes into it than just having a requirements file though, especially around ensuring those packages continue to be available.  I wrote the initial version of piprot for use internally to quickly find out what was out of date so that we could run our CI against the new versions of any requirements changes. I think it works a treat for that scenario.  tl;dr: yep, distributable packages should avoid pinning requirements, large applications pretty much have to. "
"You're a professional. Act like one and quit expecting handouts. 
 edit: spelling","> This is a huge fault about the IT field.  This is ridiculous. And is not exclusive to the IT field, but to every professional job. Doctors, lawyers, accountants, anyone with extra training has to get continuing education to continue being relevant in a changing world.  What you learned 20 years ago of course wouldn't be as relevant today. So if you hope to be, why wouldn't you expect to have to understand where technology is moving to.  > So in response to the article, it is a huge benefit to have an employer that trains their employees in new technologies  So you expect your employer to pay you a full salary and benefits and in addition, constantly train you on new things that may not be relevant to them at all? So as a primarily java shop, my employer should spend money to train me on Asp.Net MVC or Wordpress or Python or Node.js, all things we don't use or use very little of, to keep my skills relevant?  > It allows you to use your free time how you want to  You can spend your free time how you want. If you want to party with your friends, you can. If you want a long career as a programmer, you can spend your time learning.  I have several friends that are accountants. They have to do continuing education for 40 hours a year. This is on their own time, but they are required to complete it to understand the new financial rules.  Attorneys constantly have to stay up to date in order to understand new case law and precedent. I know a lot of attorneys, none of whom work for a law firm that would let them put one of their cases on hold in order to do so.  And would you want a doctor who still used techniques he learned in med school 30 or 40 years ago? Most doctors work alone or with a couple partners. There isn't anyone to pay to teach them new skills. They have to learn it on their own too.  TL;DR; You're a professional. Act like one and quit expecting handouts.  edit: spelling "
I've not met a lot of people with longevity at one company that really have kept up. Kudos to this man and I hope he gets justice.,"Totally agree. This could be a wrongful termination or discrimination lawsuit. If I was that man I'd do it.  He definitely is a rare breed though if he worked at one place that long and was truly current. Most people I know that spend 15+ years at one place seem to have a woeful ""It has worked here for X years so why change it.""  Hell, I have a professor that is part time teaching Software Project Management that is a developer and has worked the same place 20 years that completely dismissed nightly builds and CI. Especially with how its being done where I work. Told me it was a waste of time because it adds to the documentation tasks. Ours in no way adds to documentation needs and saves significant time as well as closes the regression feedback loop.  tldr; I've not met a lot of people with longevity at one company that really have kept up. Kudos to this man and I hope he gets justice. "
"Limits suck. Without them, you can easily start teaching calculus to any high school student who's studied algebra.","Calculus, like geometry, is a beautiful and intuitive subject that is easy to reason about.  Unfortunately  mathematics in the last couple of centuries has been overrun by formalists who want to start with the assumption that 1+1=2 and rigorously prove everything else.  It's fair to say that all of the calculus you could ever study in an undergraduate program was developed using infinitesimals - they're what Newton, Leibniz, Gauss, Euler, and all the greats used in developing calculus. However, Calc I starts with the definition of limits because, prior to about 1960, no one knew how to rigorously prove the mathematical existence of infinitesimals. Limits were developed in the 1800s as a way to rigorously get from 1+1=2 to calculating the slope and area of a curve (i.e. the basis of calculus.) They were of historical importance in bringing rigor to the subject and helping to resolve a number of apparent contradictions that had arisen from reasoning intuitively about infinitesimals, but they are no longer needed.  Limits are ugly, counter-intuitive, and unnecessary. Once students have been harmed by the teaching of limits, it is very hard to get them on track. I would liken their pedagogical use to requiring all computer science students to begin their course of study by programming a 1960s IBM mainframes using only binary machine language.  TL;DR:  Limits suck. Without them, you can easily start teaching calculus to any high school student who's studied algebra. "
use of a decent editor makes the whole issue disappear.,"Personally, I've never understood this controversy.  Tabs: Tabs are a single character with a user-definable width. Pretty much any editor will allow you to define that width pretty much however you want. Thus, if you one width of indent and I prefer another, then we both get what we want without having to convert anything or mess with the file or even thinking about it.  Spaces: I'm led to understand that some editors will interpret leading spaces according to the user's preference, so that we won't kill each other if you prefer 4-space indents and I prefer 2-space indents. This does require tinkering with the file, but we still don't have to think about each other.  Tab to spaces: The editors with which I'm most familiar allow you to use the tab key to insert a user-definable number of spaces.  If you are concerned only with the presentation, then use an editor that can do what you need it to do without also imposing your will on those around you. At that point it shouldn't matter whether you use tabs or spaces, but the tabs strike me as easier to deal with. If there is some text processing requirement that is screwed up by having tabs, the use an editor that can convert tabs to spaces and vice versa because we've all got better things to do than counting spaces.  tl;dr use of a decent editor makes the whole issue disappear. "
"If .Net scales well enough to be suitable, then we don't need it because so does JavaScript. If .Net doesn't scale well enough, then Java is a better fit.","The more exact equivalent is Java bytecode vs. PE, but CIL is simply the equivalent of assembly language versus the machine code of PE. Both can be derived from the other with very little effort. There are assembler like formats for java bytecode as well, e.g. java assembler language. The JDK even includes a disassembler program called javap. The distinction between CIL and PE is irrelevant here.  If we're _not_talking about garbage collecting gigs of RAM and running dozens of concurrent threads, then there's no real point in having this conversation at all, because for non-heavy workloads, JavaScript is fast enough, and if you don't like its syntax you can write a compiler for your favourite language that targets JavaScript. Lots of projects have done just that.  If we need a fast new VM, it's only because we actually  do  want to run applications that can't run well enough on JavaScript today, in which case the .Net runtime is painfully inadequate. And heap sizes of a gigabyte and half a dozen running threads are in no way shape or form limited to server workloads. It's what modern video games and graphics software use  today . So if we want to run those kinds of applications in the browser, .Net is not nearly good enough, but Java might be.  TL;DR: If .Net scales well enough to be suitable, then we don't need it because so does JavaScript. If .Net doesn't scale well enough, then Java is a better fit. "
I only write unit tests for functions/methods that I may need to alter in the future while maintaining the original functionality.,"Up until about a year ago, I only tested code in production after I had a good feeling that it wouldn't break.  After getting into some longer-term projects that impact a lot more people, I learned that even though particular parts of the code base might work today as I intended, bugs / feature requests will come up that require minor changes to those functions.  Since the functions were originally written for only one or two specific cases that I already forgot, I often find that the modifications that I make cause those original functions to fail for all of the cases they should work for.  With the above said, I now will write unit testing for anything that I feel may change in the future or become reused in another part of the code base.  This way, I can easily tell if the original intentions of a function hold while adding new functionality to the software as a whole.  TL;DR: I only write unit tests for functions/methods that I may need to alter in the future while maintaining the original functionality. "
"is: if you think ahead in your application about error/exception handling, you'll find it a lot easier to gracefully ""crash""/quit.","So when you catch a general exception, you cannot dump the state, grab the exception details, and present a message to the user that says ""Whoops, something went wrong, please restart your application and try again""?  With decent enough application design, you could even throw the general exception back up through the stack until a ""known good"" (i.e. a user clicks a button to perform a complex series of actions, your exception occurs deep within the code, and you capture and record details about the error and toss it back to your button handler where you tell the user ""Whoops, that failed.  Try again later"").  Think through the design of your application, and you can often handle these errors.  I think the biggest issues a lot of developers have is that they think of ""exceptions"" as something they don't really have to consider, and error handling as something they don't have to deal with.  Many program to the ideal norms, and rarely think of the common occurrences that can plague your application (such as network connectivity becoming unavailable, latency, full disks, etc).  I guess the TL;DR is: if you think ahead in your application about error/exception handling, you'll find it a lot easier to gracefully ""crash""/quit. "
we like crashing programs because we'd rather our programs crash than produce incorrect results.,"We like crashing programs because we like our programs to be in a known state.  We like them to be in a known state because then we know that our program will produce correct results if our state transition logic (IE: our code) is correct.  If an exception occurs and we have not specifically written code to recover from it, we are in an unknown state.  Our state transition logic might or might not produce correct results if we are in an unknown state.  The safest thing to do is to crash, as it prevents our program from potentially producing incorrect results because of it being in an unknown state.  TL;DR: we like crashing programs because we'd rather our programs crash than produce incorrect results. "
"please do not be so offensive. You have the right to be disagree, not to be rude.","Well,   first, I'm not the one who wrote this, I just expose an opinion that  might  interest some redditor, so don't blame me for  unsigned long int  vs  long int  stuff (show me your code, I will show you some similar errors!),  secondly, there are ways to implement pow in O(1), there is tricks like  y^n  <==>  y = exp(n*ln(x))  where exp and ln are optimized with SSE/SSE2 instruction to be calculate in O(1) time (well, the implementation uses a limited number of recursion to obtain a finer approximation of log/exp), and  yes , this is not true every time,  thirdly, I expose my own opinion nowhere, why do you consider I found this blog entry totally awesome?  finally, you have the link of the blog, I think that the author should be grateful for your clarification and would fix his post for nitpicking people.   TL;DR: please do not be so offensive. You have the right to be disagree, not to be rude. "
Version control for ERPs is not as simple as setting up a git repository and getting the programmers to use it.,"> ERP developers, in my experience, don't use version control systems because they have absolutely no care for what they're doing, or knowledge of how to develop software.  As an ERP developer, I have to disagree with this.  The ERP system I work on stores the majority (there are some processes that exist outside of it, but those are slowly being discontinued) of it's ""stuff"" in the database.  An online page is described by a number of records in various tables, code that fires when various events occur is in another set of tables, etc.  All of the development in the system is done using a development environment specific to this ERP.  In order to work with this ERP, a version control system would have to understand all the various object types that the ERP works with, plus somehow integrate into the development environment.  The problem with something like git is that it can only version files, it can't version all the stuff that lives in the database.  Don't get me wrong, these are solvable problems and various companies have solved them.  But those solutions cost money.  There is no free software in this space.  tl;dr:  Version control for ERPs is not as simple as setting up a git repository and getting the programmers to use it. "
"Go play with some shit. See what you like, see what the tools do, and then make some informed decisions based upon what you are trying to achieve.","I'll tell you what. Build a comment system. Write it in vanilla JS and have it bounce off an express API. Then write it with jQuery. Then write it with Angular. Then write it with React. Oh, and try to support IE8 the whole time.  Realistically, I could tell you React is end all, but I've been doing this for over a decade. You should see what tools solve what problems, and then you'll be in a better position to make technology choices moving forward.  Quick anecdote, my wife had our daughter and was breastfeeding, taking notes on paper. Me wanting to help out, I decided to make her a breastfeeding tracker. I did it in angular before she could finish one round of breastfeeding. It worked great and she used it for months.  I think what I'm trying to say is, check out the tools, and realize what problems they solve, and then pick the right tool for the job.  I use React almost exclusively now. Shit, I wrote McFly! So I can tell you that  most  flux libs support server rendering. Some more than others. I recently forked McFly into a project called Biff, because it wasn't the right tool for a project I'm building.  TLDR: Go play with some shit. See what you like, see what the tools do, and then make some informed decisions based upon what you are trying to achieve. "
Demanding unconditional respect never works. Dress for the effect you want. This applies to everyone.,"> Wearing dresses as a female programmer is like wearing a suit as a male programmer.  I think you are very near an important point.  Let me diverge a moment to mention an old book called ""Dress for Success"".  The point is that people judge you by how you dress, how you behave, how you overall look. If you dress like a salesman, people will initially treat you like that. If you dress like a jock, ditto. And so forth.  There is an old saying ""you can't judge a book by its cover"". But the problem is that we all do, at least initially -- it's a survival trait.  We can choose how to dress and behave however we like, but we cannot force others how to treat us. That seems to be at the root of complaints like the article.  I doubt that she would need to ""dress like a male nerd"" to get respect. There are nearly infinite combinations of dress style. Find one that results in the attitudes you desire and go with that. Dressing in a style that deludes the observer about your skills is edgey, but you're only hurting yourself. (Generic you, not you personally.)  Crap, too long.  TLDR:  Demanding unconditional respect never works. Dress for the effect you want. This applies to everyone. "
"When writing Python, implement the algorithm at the highest level you can instead of worrying about other things you would have to worry about in C.","It's  not  about which parts are ""good"" or which parts are ""bad."" It's about which parts are  applicable . For example, you wouldn't use a pointer for a small, local variable in C because there's no need to allocate the memory, then free it after you're done. In Python and other high-level languages, you don't worry about this at all. Not only is Python going to go ahead and do the necessary allocation and deallocation of memory on the heap, but it might even optimize that and use the stack (don't count on it though). More importantly (and this is what you should keep in mind),  you don't need to care .  In low-level languages, you worry about memory management because you want to make the best use of your resources, whether they be time or space. If you find yourself feeling like having to do this manual management is getting in the way of the  actual  problem, then you're using the wrong language. At that point, you need to use a high-level language because you don't care how resources are allocated, just that you get to implement the algorithm you care about.  Think about it it this way: when moving from assembly to C, you stop worrying (in most cases) about which registers are used and how. It's true that compilers these days are smart enough to handle this task smartly, but even though that's not the case with Python (that is, you will incur some performance penalty), you don't need to care because at the end of the day, these things will be taken care of for you, even with the associated penalty. If the penalty matters, you'll need to go back to a lower level language.  TL;DR When writing Python, implement the algorithm at the highest level you can instead of worrying about other things you would have to worry about in C. "
"Obvious shit is obvious 
 Only if you think rationally instead of in terms of power.","> Why would anybody respect their superior merely because they were higher up in a metaphorical chain?  Because we are primates and given to hierarchy. Some people believe that once you have the title, it means you've earned respect from the world and thus shouldn't have to earn the respect of individuals.  For these people - and then tend to gravitate to the top of the org chart - to not respect them for their title is to threaten their worldview. It's grounds for harsh punishment.  > No body does a job with the intent of making a mess of it - be that in retail, economics or engineering.  But plenty of people do their jobs badly, inefficiently, and make more work for others. In IT, this is a cardinal sin.  > Tldr. Obvious shit is obvious  Only if you think rationally instead of in terms of power. "
Your attitude is part of the problem. Put it aside and be part of the solution.,"If you view your I.T. department as a cleaner of pipes you're doing it wrong from the word go.  Most I.T. people  do  want change, they want things to work faster, work more reliably and work  better .  The problem is that lunatic executives don't understand the technology that they're trying to command so they tend to make idiotic decisions along the line ""Of the moon is too far away, we'll need to get it closer to make it easier to mine."" and then instruct their I.T. department to make the change...preferably by Monday which is when the sales team promised to deliver it.  As someone who sits on both sides of the table, and is well paid for it, THIS is the primary problem. The decision makers don't have the domain specific knowledge that they need and so they conclude that I.T. is not only  easy  but is a cost center.  I.T. done right, in other words directed by someone who knows what the hell they're doing with it, yields competitive advantages through efficiency and new initiatives that can drive business.  tl;dr Your attitude is part of the problem. Put it aside and be part of the solution. "
"Informix, although owned by IBM now, is still a completely separate DBMS product from DB2, and continues to works very well in a variety of business environments.","IBM acquired Informix back in 2001 and promised they'd keep the product alive and growing ,even after adapting many of Informix's more noteworthy features for DB2. The Informix DBMS still has a strong following in finance, retail, and even some embedded devices, and shows no signs of letting up. IBM continues to support Informix with patches and new features requested by the (fiercely loyal) Informix user community.  As someone who slings a fair amount of SQL for a living, I can generally accomplish what I need in Informix, but I do find myself missing the wealth of SQL options in DB2. I also encounter a fair amount of XML in my projects, and I don't anticipate Informix adding much XML support anytime soon.  TL;DR: Informix, although owned by IBM now, is still a completely separate DBMS product from DB2, and continues to works very well in a variety of business environments. "
Be a big boy and get up and go to work. It is more productive for everyone around you.,"I am really suprised to see how many people have bashed the team lead for trying to get everyone in at a decent time. I am a developer at a corporate company, and I think it is absolutely essential that we are here durning business hours. (9-5)  In my mind, it is important to have structure and routine, and in the long run, your work will improve from it. We have a 10 minute stand up meeting each morning, and it it is a great way for our team of 6 or 7 to start out the day. We get small updates from each person on how they are doing on their tasks, and whether or not they need any help.  It is also essential that everyone is around at the same time. If I need developer X for something, and he doesn't come in until 11 or 12, I have wasted all that time. The same can be said for supporting customers. Everyone who flamed the OP just came off a jerk to me. You have a big boy job. Go to bed earlier and get up at a decent time like everyone else in the world. You will feel much better for it in the long run!  TLDR; Be a big boy and get up and go to work. It is more productive for everyone around you. "
"If the rules don't ""make sense"", and if the workforce has ready alternative employment, you're going to have problems.","My thought was that 'seat in chair is not (always) an appropriate deliverable'.  Questions to ask yourself:  Why is arrival at time X effectively a deliverable for your staff?  How does it further the business' interests?  Do you agree with the value assign to this goal/deliverable?  Honestly?  If not, how are you going to sell it to your employees?  How will this impact your work environment?  Will there be resentment?  Will there be departures?  Will these be manageable?  If it is not enforced, how will  that  affect other areas of the business?  (E.g. resentment of ""those spoiled geeks"".)  Is anyone working to address/mitigate that?  (""Yes, they arrive later, but you don't see/realize that on average they are putting in 60 hours/week?  (Should we begin demanding the same number of hours from you?)"")  TL;DR:  If the rules don't ""make sense"", and if the workforce has ready alternative employment, you're going to have problems. "
"arrays use [] not {}, and for-in behaviour does not mean internal behaviour.","There are a lot of points here, so I summarized them into bullet points.  1) You said an array with an element set at zero becomes ...  MyArray = {""0"": ""Derp"", ""someArrayMethod"": function(){...}, etc...}  Not true, JSON notation supports arrays.  1.b) To clarify, that is what I was referring to when I said arrays are Arrays and not Objects.  Sure they extend Object, but you described the array using a JSON Object notation, which is incorrect. It would be an Array, not Object.  2) The behaviour of for-in does not prove that arrays strings for numerical indexes. Numerical and string indexes are treated separately for arrays, for example adding a string index to an array [in this example]( does not alter the arrays length.  3) for-in returning strings for keys is not  internal  behaviour, based on implementation details. It's how the language works, which doesn't make it internal.  4) for-in returning a string index does not prove that they are internally stored as strings. Again you said ...  > Array indices being stored internally as strings  It could convert them to strings on the fly, or return a cached string, or use some other approach.  The only way to prove the internal behaviour would be to go look at V8 or SpiderMonkey. If I were a betting man, I'd expect to see numerical indexes internally for most arrays, and other approach for corner cases (like sparse arrays).  tl;dr arrays use [] not {}, and for-in behaviour does not mean internal behaviour. "
IT departments don't like people who makes them look incompetent.,"In 2007 I was a student of a computer security course at my university. We used to run different tests against many sites (with whom we had previously signed authorization letters). One day we were learning how to sniff traffic using Wireshark and ended up realizing it was really easy to get other student's login credentials since they were sent as plain text (no HTTPS) and it was specially easy because there were hubs (yeah, hubs, not switches) plugged in various points of the building.  What was the mature reaction of the IT department once they realized we had discovered this (really obvious) problem? They disabled the university credentials of all the students currently being part of the computer security course. No explanations were given. Fun fact is, I was a volunteer at my university and I needed my credentials in other to access some network services. Other guys simply couldn't log in to the university's site and download their invoices so that they could pay their monthly university fee. To this day I still don't understand why they did what they did, the only explanation I could find is that they don't really like being exposed as the incompetent IT department they truly are. It's been almost 6 years since that incident and the uni's website is still using an expired certificate that forces everyone to add exceptions to their browsers so that they can enter the site. Even more, they TEACH people how to add those exceptions in different browsers, and by doing so they encourage a horrible security behavior.  tl;dr: IT departments don't like people who makes them look incompetent. "
Applying positive discrimination to female programmers doesn't help them.,"Although not the majority, there are plenty of female programmers at my workplace. Other programmers, both male and female, choose to treat female programmers in different ways. Some treat them worse than male programmers because, ya know, they're female and they don't know jack. Some treat them with super-special snowflake care lest they insult or somehow deter them since they have managed against all odds to become programmers. Finally, some, like myself, chose to treat them like programmers... I won't talk about the people in the first category, they are prejudiced and need to change their ways. I will however talk about the people in the second category: If you apply positive discrimination its still discrimination and it doesn't help. It doesn't help the females that are bad programmers and it doesn't help the females that are good programmers. The bad programmers aren't pushed enough to become better and the good programmers may feel they are only being praised because they are female.  I've worked in the industry for 7 years and every time I am given the chance to mentor a younger programmer (male or female) my number one goal is to make this person understand that they don't know everything, that they need to be humble and that they will make mistakes (oh, and to always read the source). I will treat both male and female programmers with equal respect and harshness.  TL;DR; Applying positive discrimination to female programmers doesn't help them. "
"fuck bosses who ask questions like that. Preferably with pineapples, and young developers should grow a spine and shove said pineapples up their bosses asses","You get them young and naive. That his it was for me anyway.  First day on first job, I worked 16 hours. Then worked on average 12 -  16 hours per day, 6 days per week. Regularly (once every one or two weeks) I would work day-night-day without sleeping, doing some 32 hours without seeing a bed. Did I mentioned that overtime was not payed? I wanted to show off, get as much done for ""my"" company and went with it all  After about 1.5 years of this, I was burned out, started making expensive mistakes, which obviously were all my fault, and I was ""let go to reorient myself""  TL;DR; fuck bosses who ask questions like that. Preferably with pineapples, and young developers should grow a spine and shove said pineapples up their bosses asses "
"what is important these days = tooling, documentation, packaging, automated tests - the actual language is strictly secondary to all of these","redesigning the language is kind of a minor thing.  What you would need to work (heavily) on is, packaging, automated testing, package distribution&discoverability (and automated testing in the repository), support for private package repositories, documentation, i18n, windows&mac os support, tooling for easier quick setup (akin to leiningen), and only then standard library stuff for better consistency.  In my pet peeves I would also integrate a easy to use semi-automated FFI and integration to the package repository to be able to directly import packages from other languages and automatically reformat their documentations/run their tests. And I would strictly forbit undocumented&untested packages from being in the global repository.  I'm working on these things, but the LISP I'm going for is quite far from Common Lisp because of other considerations. (I don't actually consider CLers as potential users at all)  TLDR: what is important these days = tooling, documentation, packaging, automated tests - the actual language is strictly secondary to all of these "
"Java is explicit in the ways I like (strongly typed), is general enough, and plays to my strengths as a developer.","Unfortunately I have to admit that I am conversant in VB.net and C#.  Windows development is pretty much  de rigeur  in some areas of this industry.  I can work with php, perl, and python but I have had a lot harder time getting into those.  A recent joy of mine has been Erlang.  I had to do this for a class and messing around with it really brought out its strong points.  Java is not a strong language for parallelism.  If you want to do this in Java look at Scala which borrows constructs from Erlang and runs on the JVM while interacting with Java code.  I like Java because I like being explicit about things.  I like doing architecture.  I often end up with a lot of half-finished projects with a badass architecture (or at least, I think) that never get finished.  For some reason I enjoy that activity more than finishing the thing.  I'm finishing a Master's Computer Science degree specializing in Software Engineering so a certain degree of mental masturbation is to be expected.  It helps me fine-tune my methods and development patterns for when I have to use them to make money.  Java is also nice because, as you say, its fairly all purpose.  I  know  that whatever problem I choose to solve: it can be solved in Java regardless of being appropriate.  Finally, for Java, Eclipse is the best editor I've found for any language hands down.  The #Develop series is decent for being free but I find that I have to use Visual Studio a lot with them.  Komodo is a competent editor as well but I don't feel like it pulls everything together, at least the free Komodo editor doesn't.  The full blown version probably is better.  I've really never understood the bashing that Java tends to attract either, for what its worth... but even still its better to have more arrows in your quiver if you can manage it.  tl;dr - Java is explicit in the ways I like (strongly typed), is general enough, and plays to my strengths as a developer. "
GC is a performance win only if you accept 10x bloat.,"> Java has remarkably fast memory allocation (faster than malloc), so it might pay for that speed with higher memory usage, but it's not good to exaggerate.  Yes, allocation in a garbage-collected environment is really fast.  In spirit, it can boil down to one addition:  allocate(amount) {    x = next_available_address;    next_available_address += amount;    return x;}  But you pay for that because collection is much more expensive than malloc()+free() ...  if  you do it often.  Garbage collection is an an overall performance win only if you collect infrequently, which means using more memory than the program really needs.  If I recall correctly, early experimentation found the break-even point to be around a factor of 10: garbage collection was a performance win when it was permitted to use 10 times as much memory as would be needed for explicitly-collected memory management.  (I'm terribly sorry, I can't seem to find the work to cite it here.)  However, since then   The science of garbage collection has improved.  We have better algorithms now (eg: generational collection).  But,  Processors are now much faster than main memory.  Programs must efficiently utilize small caches to achieve good performance.  The address reuse of explicitly-collected programs reduces cache churn.   Do these two factors cancel each other out?  As always, if you're interested in performance, benchmark.  tl;dr: GC is a performance win only if you accept 10x bloat. "
Replacing all the various spell checks we all run with one is certainly not bloat. It's an optimization. It decreases the total footprint and increases usefulness.,"> saying ""Oh well, this is arguably part of an operating system"" isn't reasonable.  What are you trying to say? That it is unreasonable of me state the role of an OS then provide a clear argument about spell check both fitting within that role and justifying the incredibly miniscule ""bloat"" it would add. If that's what you're saying, how was that unreasonable?  Your argument of less is more is a little misguided. No one really wants an OS to be as minimal as possible. If they did the Linux kernel sans GUI would be wildly popular.  The best OS IMO is one that weighs the usefulness of each feature against the footprint it requires. And a good spell check would take up what, less than a thousand lines of code? Plus a database measured in a few megabytes. And the trade off is a useful feature 90% of people would use and probably already have more than one of installed on their system.  tl;dr  Replacing all the various spell checks we all run with one is certainly not bloat. It's an optimization. It decreases the total footprint and increases usefulness. "
"Linux ""on the desktop"" is slow-and-steady grower and no immediate threat to Windows, but there are plenty of things giving plenty of people at MS ulcers ATM ;)","> in the market place Linux does not really amount to much  I agree with your larger point that RedHat (let's say) isn't going to dominate the end-user desktop market anytime soon :)  In the global marketplace, however, Linux (open source POSIX-alikes to be precise), have really caged MS in... Ubuntu and nationalized linux versions (Red Flag linux, etc), are genuine contenders for the future of computing in developing nations and force a terribly low price point on MS. MS is almost locked out of the embedded appliance space.  In the enterprise and high computing power server market side MS is an also-ran, though their Exchange-based offerings are powerful on the business side of things. High end consumer laptops are currently-dominated by Apple which is running on a modified BSD kernel.  In the mobile space Windows Mobile is a never-was (gotta see what happens with WinMo 7 + Nokia...), and they are currently having their lunch  eaten  by Android which is based on the Linux kernel...  TL:DR; Linux ""on the desktop"" is slow-and-steady grower and no immediate threat to Windows, but there are plenty of things giving plenty of people at MS ulcers ATM ;) "
IMHO there're many other factors besides energy saving which can affect developing or degradation of senses.,"There's opposite situation as well: if, per chance, specie gets ability giving it significant advantage over others, often it matters no more how much energy it is required to support it - this ability will usually provide this energy (by occupying new niche, getting dominant in existing one etc.). Moreover, often general lack of resources simplifies propagation of new mutation, since advantage becomes even more significant (see effect of gene bottlenecks on humans and cheetas, e.g.).  We still have little evidence if (given brain ability to retarget its parts to new uses) degradation of sensors provides energy saving sufficient for it to be an advantage. More probable, IMHO, that in situation when particular sense is not used not having it is not more a disadvantage, so such mutations become neutral instead of negative (like in  some cave fishes . In some cases (moles, e.g.) having developed eyes is a source of diseases, so it's an advantage to have them underdeveloped.  tl,dr: IMHO there're many other factors besides energy saving which can affect developing or degradation of senses. "
The list of things to read is long but the time/life I have is very short,Thanks for your comment. To be honest somehow your comment on my attention span resonates with me at a deeper level. Lately I have come to realize that after I discovered the internet (or the internet discovered me) I have become like that kid at the big candy shop trying to grab as much candy as possible before the bonanza comes to an end.  A summary is a good way of deciding if the given piece is going to be a good investment of my time. I would read every article that I came across if I were immortal but until then I depend on the good people of the internet to help a brother out. Peace.  tl;dr:  The list of things to read is long but the time/life I have is very short 
"if your connection is that slow IDK how you could use SM to begin with. Upload while you are sleeping, and eventually everything will be on Google Music.","I've used simplify, though maybe not to the same extent as you (I only ever used it for the music). If your internet connection is really that slow I don't understand how you were streaming the music with SM in the first place. Additionally, unless you are adding more than a hundred songs a day I don't see how you can argue that you'd have to leave your computer on all day to maintain google music. It should take just as much time to upload a song to google music as it takes to stream it to your phone with SM. I agree there is a large initial hurdle of uploading all your music, but once you've done that the difference is un-noticeable, minus the fact that you don't have to run your computer all the time.  tl;dr if your connection is that slow IDK how you could use SM to begin with. Upload while you are sleeping, and eventually everything will be on Google Music. "
"version: you can't make a C wrapper for std::sort, but that's mostly because you can't implement std::sort's behavior in C, no matter how hard you try.","And now you've demonstrated quite conclusively why people  wouldn't  write a library that way. Because, if designed in that manner, a program would be unable to read arbitrary .png files - only a strict predefined subset of them, thanks to how C++ templates work.  Templates are used for extreme performance when significant pieces of the prerequisites can be defined at compiletime. Yes, if you want to define those prerequisites at runtime, you can't write C bindings for the library . . . but at that point you can't write C++ bindings for it either because templates don't do ""runtime"".  Or, tl;dr version: you can't make a C wrapper for std::sort, but that's mostly because you can't implement std::sort's behavior in C, no matter how hard you try. "
"some of the reasons we don't hire you might sound stupid to you or there's nothing you can do about them.  It isn't like ""learn PHP gooder"".","I'm not google, but when I'm evaluating a web dev candidate it's kind of a ""whole package"" thing.  There's a minimum threshold you need to understand about the programming language we'll be using (or even a similar language).  You should know CSS, javascript, and preferably at least one javascript framework.  Then you need to know the basics about web servers, database design, sql, basic server administration, debugging, and source control.  If you know that, you're in a group with about 90% of your peers.  Everything after that is subjective.  I need to try and get an idea of ""how"" you think and how you approach a problem, which is nearly impossible in our limited timeframe.  If I explain something to you, do you understand it and discuss it with me or do you just sit and nod your head.  If I prompt you with an opening to discuss back, do you take it?  If I'm intentionally wrong, do you correct me or just let it slide?  Do your strengths prop up the existing team's weaknesses?  Are you cocky, annoying, etc.?  Suppose we need someone with an iota of web design experience.  It's not a requirement of the job, but it would give you a leg up.  If I tell you ""You didn't have web design experience... learn that and apply back in a year.""  That's not only misleading, but it won't help you since we already have someone to fill that gap.  Or maybe someone equally qualified is asking for $10k/year less than you.  Apply again next year but ask for less money??  TLDR; some of the reasons we don't hire you might sound stupid to you or there's nothing you can do about them.  It isn't like ""learn PHP gooder"". "
"Focus on the GTA, go for larger companies, and cross your fingers. As it's PHP, you have a lot of competition (including myself)","If your'e going to do the Canada thing, my search would begin and end in the GTA (Greater Toronto area). The amount of developer jobs there compared to anywhere else in the country is impressively higher. Also salary is the best in the GTA. PHP devs here I've found are in the 70-90k range for reputable shops for int-senior level devs. Not too many places I've found give much of a premium for team lead exp, but some do. In comparison, shops that I've interviewed at in Ottawa, Eastern Canada and Vancouver pay 40-60k for the same experience level. As for if they're open to sponsoring....thats' a good question. If it's a multinational maybe, but they dont' usually do PHP work. that's a aprt of the market that I've never looked into.  tl;dr Focus on the GTA, go for larger companies, and cross your fingers. As it's PHP, you have a lot of competition (including myself) "
"chin up buttercup, just be good at the language and at software in general.","It's already been touched on by a couple other comments but here is how I avoid being overwhelmed when looking at the morphing front end landscape: just keep getting better at two things, javascript itself and design patterns. Maybe a third thing which is CS topics that we could all learn more about like algorithms, algebra, etc.  I think it's so funny/annoying that many job postings have 'X years using backbone is a must'. My issue with it is if I know js (really solid/deep understanding I mean) and I have plenty of practical knowledge on design patterns then I should  have no issues getting into a backbone code base. Sure I'll be referencing the docs heavily at first, but that's just referencing syntax and 'how backbone does this'. I think a requirement of having X years in Y framework is garbage  unless  the job is for someone who will be playing an architect type role where framework specific knowledge will be important in making design decisions.  Sorry for the text well, hit a button.  Tl;dr: chin up buttercup, just be good at the language and at software in general. "
"jQuery abstracts browser differences, but browsers have become pretty uniform and will only continue to do so.","Sure, I'll elaborate.  jQuery was created to abstract browser differences a long time ago, back when web standards were almost nonexistent. You couldn't rely on browsers to treat anything uniformly; jQuery was created to normalize that and abstract the differences away from the developer. jQuery was awesome!  These days, though, web standards, as well as browser javascript - they've become very uniform. We no longer have to support IE5, 6, and 7. We no longer have to support old versions of Firefox and Netscape. Web standards have come a long way, as has the DOM; browsers are becoming unified in the way they handle DOM manipulation. As such, jQuery has really become a superfluous layer, unless you're dealing with very old browsers. The jQuery team knows this; their new library drops support for older browsers, making the library much leaner (because calls are uniform across newer browsers).  jQueryUI is still pretty awesome, though.  TL;DR: jQuery abstracts browser differences, but browsers have become pretty uniform and will only continue to do so. "
Probably a good idea if you're serious about make the career switch. Especially if university time/cost isn't a real option.,"Background: I've TA'd at a Fullstack Academy of Code, substitute taught at New York Code and Design Academy, and I'm in the current batch at Hacker School.  I've never heard of anyone learning to code completely on their own and get a high paying job (I'm sure it happens but it's certainly not the norm). Given that, bootcamps and university degrees are the most reliable option.  The variation in quality of bootcamps is a lot like colleges (to be clear, I have nothing but praise for all the programs I've been involved with). Some have lax admissions standards and attempt to admit people to make more money without regard for outcomes of their students. Others are selective and compete on the quality of their students' outcomes. When you apply to bootcamps, research student placement and examine admission standards.  I think almost any bootcamp can be a very positive experience (relative to online learning) if you put the work in but it's important to do research.  tl;dr - Probably a good idea if you're serious about make the career switch. Especially if university time/cost isn't a real option. "
"don't be scared by advanced vim, you can start with just 2 or 3 commands and build up slowly. It isn't bad to just use normal cursor keys :)","I'm a very recent vim convert. My advice for getting into vim Is to start as follows:  1) use a graphical vim (macvim on mac, dunno about others)2) know the following: i puts you into typing mode, escApe takes you out of it.  Macvim (and I hope others) make all the keys you expect (cut/copy/save/search) work, so you can start usefully using vim straight away!  Next thing is to learn a few commands ( . Is easy and cool, repeat last edit again) and also start working on you vim configration file. Google around for other people's but understand whatnyou are adding, don't just cut+paste the whole thing!  You'll find repeative tasks keep annoying you - go and google for them and learn new commands over time.  The only thing I haven't figured out a good way to do is get plugins, there are so many with overlapping utility that it is hard to see how to work your way through them all.  tl;dr: don't be scared by advanced vim, you can start with just 2 or 3 commands and build up slowly. It isn't bad to just use normal cursor keys :) "
"Work time is for work, out of work time is for non work activities, try to keep a balance.","I am actively trying to get out of the always thinking about work mindset, mostly because it's started to affect my life at home with my family.  The rule I try to hold to is that between 7pm and 7am, unless it's an emergency, I am 100% completely focused on being at home with my family or another activity.  While my daughter is awake I'm focused on her, I might go golfing or spend time with my wife or play a video game, but my I'm focused on a non work activity when I'm not at work.  I find that I actually unwind at night and the stress from work actually leaves instead of being constant. I'm not perfect and sometimes work creeps in to my family time / hobby time but I desperately try to leave work at work. I've been at it for about a month and it's resulted in me being happier and my family being happier and I can't really complain about that.  TL;DR - Work time is for work, out of work time is for non work activities, try to keep a balance. "
"Just start with one simple path. You can branch out once you get dangerous (I'm not yet, by far.)","I'm saying that information overload will keep you from getting results. I know that personally. Choose a path and follow it. The one I gave you is the one I've started following while working a full time job and balancing family time (as well as being remarkably scatterbrained), so I can attest to it being pretty mindless. You can read/watch one of the lessons from either Learn Python the Hard Way or Python Osmosis during a 15 minute break. Just keep working at them until you feel like trying something of your own, and then start doing it. I still plan on going through those tutorials, but actually putting down code is paramount, because now I have an idea that's all my own, so there's automatically a level of commitment to seeing it become fully developed.  The brain work starts when you want it to, when you actually decide that you have something you want to do with it, and you decide when to start, and you dedicate yourself to creating something that works (no matter how ugly). This is the hump to getting started. Actually starting to make something. So make your path there as quick and simple, but useful, as possible. You're going to get all sorts of suggestions on what and how, and they've all had varying levels of success for different people. So weeding through them to isolate the best or trying to do them all is counterproductive.  tl;dr - Just start with one simple path. You can branch out once you get dangerous (I'm not yet, by far.) "
I don't deny it has benefit.  I'm just skeptical that it's the 75% to 90% reduction in programming errors that some of these sweeping statements make it sound like.,"> which are very expressive and correspond to a logic.  So what if they correspond exactly to logic?   Correctly  writing down a description of something in formal logic is far from a trivial task.  As a matter of fact, isn't that one of the big gotchas of logic-based AI systems, that you try to write down a bunch of logical statements that are true about a world you're trying to model, and then when you're done you have no way of even knowing if what you wrote down is actually consistent and valid?  Using formal logic correctly isn't a given.  Reduce programming to formal logic, and maybe you have a powerful tool, but you don't have a silver bullet.  > In these type systems, any property of code (all of those error categories except ""errors in understanding the specification"") can be eliminated.  Any property?  So this type system can prevent me from writing an infinite loop, which means it has solved the halting problem?  I know you don't mean that, but aren't you exaggerating when you say ""all of those error categories""?  > Various bugs are statically eliminated which are not in other languages, specifically null pointer exceptions, ""action at a distance"" problems with mutation, inconsistency in concurrent reads (due to immutable data model), and some more that I can't recall at the moment.  My gut feeling is that, depending on the type of work you do, these sorts of errors account for maybe 10 to 25% of all errors.  I would describe these mostly as what you might call careless, mechanical errors.  I would even go so far as to say a lot of them are amateur mistakes, although certainly experienced programmers still make those sorts of errors from time to time.  But, no doubt, a programming language that can eliminate entire classes of errors like this is a compelling proposition.  And it's worth investigating and developing.  It just doesn't justify hyperbolic statements like ""if you can compile it, it's probably correct"".  Maybe the statements are just meant to be hyperbolic and come out of enthusiasm, and maybe I should be OK with that considering that knocking out (say) 10% of all programming errors would still be a pretty worthwhile achievement.  TL;DR:  I don't deny it has benefit.  I'm just skeptical that it's the 75% to 90% reduction in programming errors that some of these sweeping statements make it sound like. "
"I need a ColorFilter that replaces RGB values in a bitmap with a new, specified color, leaving the Alpha values alone. Help? :D","OH MY GOD THIS IS GREAT!  I just posted a [question about some of this stuff]( in androiddev.  I need to find a way to replace the RGB values of a Bitmap or Color(int) array quickly, ideally as its being painted to the canvas. I want to leave the Alpha values alone though.  As of now I have it changing the colors before it gets painted, but it takes a good second to do this, but the time between frames should ideally be 20 ms.  I know you can probably do this via a ColorFilter but I really have no clue how to as there is little documentation on the subject.  tl;dr I need a ColorFilter that replaces RGB values in a bitmap with a new, specified color, leaving the Alpha values alone. Help? :D "
"he's right that you can't take benchmarks as significant at face value, just quantitatively. But you also can't take his rejection of benchmark significance at face value. It depends.","1 or 10 ms is definitely imperceptible to the user.  200ms is starting to be perceptible though. The difference between a load time of 200ms and 800ms is definitely easily perceptible, and makes the difference between something that looks super speedy vs not.  And because a benchmark says a difference between, say, 200ms vs 400ms doesn't mean that will be the difference in your  actual app , neccesarily. The benchmark is measuring certain things; those faster things that resulted in, say, 50ms faster in the benchmark, may result in hundreds of ms faster in your actual complex app using more 'stuff' than the benchmark.  (And of course, the speed of your actual app can be as much about server processing time as browser rendering time).  tldr; he's right that you can't take benchmarks as significant at face value, just quantitatively. But you also can't take his rejection of benchmark significance at face value. It depends. "
Remove SharePoint from the title... Spice it up for the client facing part and mention some JS frameworks as pure JS is almost a lost art for the youngsters.,"Hi  JS Guru and SP Guru here... In addition to what other people are saying here... ""Junior"" and ""SharePoint"" not only aren't attractive they are almost negating each other entirely. When I learned web development (ongoing task obviously) JS was part of it. SharePoint? after years of project work... and a specific directive to learn and become familair with it.  No junior, unless they've previously been employed by a company that uses SP will know what SP is... therefore they think unqualified.  Additionally there is SO MUCH to SharePoint that theres almost no point hiring a junior... i know your probably working with the client side API but still... Juniors and JS though? hell yeah! Juniors love JS.  Again - outgoing and client-friendly developers are rare. Its like you can have the JS or the SP or the ""client friendly"" rare to have it all.  TLDR:   Remove SharePoint from the title... Spice it up for the client facing part and mention some JS frameworks as pure JS is almost a lost art for the youngsters. "
"I don't think it hurts (particularly if you have additional university coursework), but you may be better off showcasing course projects.","I've seen people list coursera (or similar) courses on a resume under a ""continuing eduction"" heading within the education section of their resume.  I, personally, think it shows initiative and dedication, which is a definite pro.  However, those courses are often less rigorous than similar courses taught at a university.  So, I don't think we're quite at a point where taking free (or cheap) online courses is a substitute for college education.  However, depending on your dedication and the specifics of the kind of job you're looking for, you might end up being very well qualified.  In my opinion, one of the biggest benefits of online programs is providing direction for completing independent data projects.  Your best bet would be polishing those off into a really kick-ass portfolio, rather than relying on course completion certificates.  tl;dr: I don't think it hurts (particularly if you have additional university coursework), but you may be better off showcasing course projects. "
"Indexing is a great arrow in your quiver, but first you need to know where the target is.","You book looks a great introduction to database indexing. Thank you for sharing it online for free. However, I must take issue with the statement in the preface, ""As it turns out, the only thing developers need to know to write efficient SQL is how indexes work."" The first part of any solution should be identifying the problem. Quite often once this is done, yes, indexing is the answer, but let's not skip that first step.  The first chapter of  Optimizing Oracle Performance  by Cary Millsap should be required reading for all programmers. It's not actually specific to Oracle or just database programming, and it's online for free too: [ Briefly, the method proposed is 1) Ask the user what is slow. 2) Measure it carefully. 3) Optimize only the part that would have the greatest impact. 4) Repeat.  tl;dr: Indexing is a great arrow in your quiver, but first you need to know where the target is. "
"Form structure and functionality should be defined once and rendered, validated, re-rendered and executed by the framework.","Looks interesting!  Do you plan on implementing any kind of form/UI framework? I haven't used many MVC frameworks, only Joomla and Cake, but I think that's a feature that can really save a lot of work.  Basically, in Joomla you don't get any help with rendering form HTML. You have to type it all, copy-paste or implement you own templating system for the input types. (I think that sucks.)  In Cake you get a nice interface for building forms where most of the time you (theoretically) only need to specify the model to update and the names for each field. (In practice, it's not flexible enough, so you end up writing a lot of HTML anyways, which sucks, but not as hard as Joomla.)  If you can get that right, I'm sold.  edit:  I should mentioned that I built a class for handling form rendering, validation, re-rendering on error and execution of code. I is by no means perfect or even very flexible. It is merely a set of classes I collected and reused over a few years from some of my non-MVC projects. But it still illustrates what functionality I would like to see.  TL;DR  Form structure and functionality should be defined once and rendered, validated, re-rendered and executed by the framework. "
"a judge could find a random file on your computer, claim it's encrypted and throw you in jail indefinitely for failing to give the password.","What's troubling are the differences between an encrypted file and a safe.  There are numerous issues with forcing someone to hand over a password. Some of which are:   They might not know the password. The file could be a random combination of ones and zeroes. Or you simply forgot the password. Or you never had it in the first place. A judge should not be able to throw you in jail  indefinitely  just because of a randomly generated file on your computer which he/she claims is encrypted.    I understand it is possible to produce an encrypted file which has not just one, but two or more different passwords, each of which decrypts the file into something else entirely. Let's say the contents of the encrypted file are benign and you give them the password. By the same twisted logic as in the first paragraph, they could ask you for the  second  password, which you do not have. Then off to jail you go because the judge then finds you in contempt of court.  tl;dr a judge could find a random file on your computer, claim it's encrypted and throw you in jail indefinitely for failing to give the password. "
"It's not that they are stupid, it's that they just don't care.","I think there is a slight variation of this that is a better way to think about it  90% of your users don't give a flying rat's ass  It's not that they are idiots (although some may actually be), it's that they simply don't care.  Most of them are thinking about the problems they are having with their girlfriend or who is going to make it past this round of the Bachelor or how to pay off their credit card.  The most important part to realize I think is that software is nothing but a tool, a means to an end.  Inventory software is not about the software, it is only about managing inventory. If a user can't figure the program out, it's probably not because they are an idiot, but because they care less about the software than they do about what they are going to be reading when they take their morning dump. All they want to do is do the inventory and then get the fuck out of there so they can meet their friends at happy hour.  Facebook? Nobody will ever care about Facebook as a platform or what language it uses or how it works. They do care about staying connected with friends in other cities and seeing what their neighbor had for lunch. Facebook is simply a tool to get that information. If they can't figure out how to easily creep their ex, they'll move to another site where they can figure it out.  TL;DR; It's not that they are stupid, it's that they just don't care. "
"I agree, but that's not the problem that ravioli code refers to.","I agree. But ravioli code is when even coherent modules are chopped up, simply for the sake of chopping up. I've had to deal with code whose only job was to copy a buffer from a hardware device and pass it to a third-party library once every five seconds. You'd think that would be what, 4 or 5 classes? This thing was some 18 C++ classes, including a separate class to allocate memory for the buffer, a separate class for reading from the hardware device vs configuring the hardware device, a class wrapping the third-party library that only one company made anything like it, etc etc etc.  This seems to be easier in more OO languages like smalltalk, where everything (including loops, conditional statements, basic arithmetic, etc) are methods on some other class.  Chopping things up too small makes things even less modular than having things too big, especially when one of your small modules doesn't stand on its own but is used by several other classes.  tl;dr: I agree, but that's not the problem that ravioli code refers to. "
From what I've seen Agile is most effective when it's a mindset and not a set of procedures.,"I stopped doing major development a little while ago when we engaged a company to implement a new Business Process Management Suite.  I'm currently managing one of the last major projects that will utilise the external companies resources as we have gradually upskilled our own employees.  The company has embraced the agile paradigm and from what I've seen so far they produce much better results with the same cost and time constraints that we would typically assign.  They still do some requirement gathering at the start, but it is literally two weeks (The last time this application was built, the requirements took 9 months,) and was more of a broader scoping statement so they could gauge a price.  From my observations the clearest indicator as to how effective at utilising agile values a company is, is this:  How pissed off do the developers get when you want to change something?  I remember  when I was a code monkey, if someone came to me with a change late in the game, the level of frustration was immense; these changes could throw to waste a huge amount of work.  I may speak to a SME or Line Manager and require a complete go-over of the UI, rules and Data mapping based on that conversation; I speak to the developers and they go yeah, cool, I can see why that's beneficial and get to it.  That being said, I've worked with companies that claim to implement agile, but still bristle at each and every minor modification (I'm talking things like required fields, and label changes.)  I think a key difference between companies can be seen at quote time, is it time + materials or Fixed.  TL:DR; From what I've seen Agile is most effective when it's a mindset and not a set of procedures. "
people have to put up with shit that contributes to less-than-ideal code.,"> Never put your name on something you're not happy with  > say ""Sorry, that isn't possible.""  Unless you need the job to, like, you know, live. Not everyone has the luxury of being able to say ""no"" to their employers. I worked at a place that fired me for exactly those kinds of reasons.  E.g. They wanted a stable platform, I tried to make it so, then got got let go because I wasn't as fast as others who were quite happy to publish code with a higher rate of errors and idiosyncrasies. They used to have this one frequently used and awful SQL query that literally ignored incomplete customer orders under certain conditions, making them angry and sales staff puzzled. I fix it and spend a while testing to ensure it works correctly on historical data and won't bugger up new orders. Then, I got crap because it was ""only a few lines of code"", and the boss has a rant about shitty quality control. No more lost orders but no thanks and no apology even after I show him in Git (thanks Linus) that it's been that way since before I worked there.  tl;dr  people have to put up with shit that contributes to less-than-ideal code. "
"After almost half a century since FORTRAN, these problems have  been solved . Why start from scratch?","Authors of articles like this tend to forget that programming is a very old profession.  People today aren't writing all new low level libraries because ALL of the low level libraries are readily available, free and better than anything I could produce within a year. People are always scrambling to fill the gaps as either a business model, a research opportunity or just giggles.  You absolutely should get to the point in your career where you could write those libraries for fun or education, but why put that on your organization when the tools already exist and you could be working on the more important problem at hand? Its a form of premature optimization to write the damn thing in C without external dependencies because you can.  To me, from reading HN and reddit all the damn time, it seems like organizations value rapid learning, high level technologies and sensitivity to the demands of business over byte-cramming, obfuscated C, hand-rolling loops, and running BSD on a breadboard of NOR gates. The pattern seems to be: Business Case? -> Proof of Concept -> CRUNCH -> Profit. I'm not saying optimization has no place here, but unless performance is a measurable problem for 1-4, it's probably not going to get the focus of development until after the profit stage.  TL;DR : After almost half a century since FORTRAN, these problems have  been solved . Why start from scratch? "
"My ""Eureka!"" moment with regular expressions was ""fuck regular expressions"".",">The tricky thing about regular expressions is that it is its own language within a programming language. It has its own collection of symbols and syntax. Those long, scary strings that regexes are made up of are collections of these symbols and are used to make the pattern you want to match. Think of a regular expression as a sentence, and each one of the symbols as a word.  My ""Eureka!"" moment with regular expressions was that I would much rather express my regular pattern matching with  sane logic statements which are   READABLE  because having a language within a language to do what a sequence of system library calls performs is a little too 'yo dawg' for me.  tl;dr; My ""Eureka!"" moment with regular expressions was ""fuck regular expressions"". "
If ignorance was worth a penny reddit would have trillions.,"This is not copy paste - I have personally integrated datamapper, cancan, dragonfly, bencode and a custom cms for a website. Using libraries is not copy paste, really it isn't.  >The main thing about each of the above gems is that they provide some basic DSL for the developer to follow - seldom will the developer have to touch actual Kernel methods or learn about ruby itself beyond basic syntax rules and the difference between strings and symbols.  Really this is getting ignorant, you obviously haven't used rails or these tools as anything beyond absolute uselessness requires pulling general bits out into an application specific library.  TL;DR: If ignorance was worth a penny reddit would have trillions. "
"He probably doesn't want just 'full-time work', but rather to run a business.","I think your analogy is wrong. I have an experience very similar to dustlesswalnut, in that I dropped out of college to work for a startup. The only difference is that I came on full time as IT director and we're still growing every year...  I can say with great confidence that had I stayed in school, I would not have received the business experience that has changed my entire philosophy on 'jobs' or 'working for money' or however you decide to categorize that activity.  It is  extremely likely  that dustlesswalnut, like myself, has discovered that there is more he can offer to the world than just programming and it is much more rewarding to control your entire business from lead generation, to sales and marketing, to customer support and relationship building, to actual service. I can't speak for him, but I know I would not be satisfied at working in just a 'programming' position ever again. And the lessons that I've learned that changed my outlook are not taught in any college that I know of, because they require actual hands on interaction with paying customers.  TL;DRHe probably doesn't want just 'full-time work', but rather to run a business. "
"If you're going to go to college, get a liberal arts degree and teach yourself to code on the side.","I have a college degree but NOT in Information Technology (or science or math or engineering).  In 20 years in Information Technology (first as a software trainer, now a middleware developer), I've noticed two things -   Any technology that is in demand when you graduate college has a good chance of going away in 10-15 years.  If you can't keep educating yourself, you'll have a very short career (or you'll become a project manager).   I use the soft skills of my college education (in English literature) daily.  I can write and speak informatively for the correct audience.  I can plan my time effectively.  I can stay awake in meetings.  Frankly, I can't think of a better source of insight into human nature (and interpersonal politics) than Shakespeare's plays; King Lear should be required reading for anyone in management.    tl;dr  - If you're going to go to college, get a liberal arts degree and teach yourself to code on the side. "
"as php loves to typecast, try to avoid == in favor of === 
 not tired yet? 
 $var1 = ""10000000000000000000000002"";
$var2 = ""10000000000000000000000003"";

var_dump($var1 == $var2); // true","well, if you use == (equal) and not === (identical), then PHP will do a lot of typecasting depending on operands  in case 1) it will typecast everything to boolean as there is a boolean present, thus it actually compares false == false, which is true as 0 equals false (but is not identlcal to false)  in 2nd excample, again both is typecased to 0 as the int_value() of 'foobar' is 0, thus again we compare 0 == 0  now the last example, we compare string and boolean, in this case everything is typcast to boolean again, and 'foobar' is typecast to ""true"", because this is the ""default"" behavior of all strings EXCEPT very few like the string '0' or an empty string check  TL; DR; as php loves to typecast, try to avoid == in favor of ===  not tired yet?  $var1 = ""10000000000000000000000002"";$var2 = ""10000000000000000000000003"";var_dump($var1 == $var2); // true "
"developers try to jump over where the wicket seems lower, and inadvertently destroy functionality and never realize.","I don't know about the specific case in question and i don't really want to investigate it, but my qualified guess is this is a result of them implementing their own links without inheriting from the default link object.  This usually happens because a developer wants custom functionality for some case and it can sometimes seem easier to just inherit from the most basic element and build your own custom element on top of that. This has the downside that the basic elements often have a lot of behavior associated with them the developer might not know about, sometimes it may be browser specific,  and in general its very easy to miss something when taking this approach, which is why its recommended developers don't use this method and instead inherit from the default objects that most closely resemble the desired behavior.  Unfortunately it is very easy for new developers to fall into this trap of creating your own elements basically from scratch, since it can sometimes be difficult to overwrite certain behavior and nobody wants to read the manual on how to do it. This is also rarely something developers feel the negative effects of since they are likely never to use or have used accessibility tools like screen readers, which depend on a lot of the features from the default web objects.  If you really want them to fix it submit a bug report or send them a mail about it, they might fix it.  TLDR: developers try to jump over where the wicket seems lower, and inadvertently destroy functionality and never realize. "
"Use array_merge. If your concern is memory, choosing the C-code over the PHP-code should always be right.","Well, if you're  really  worried about memory usage, you should - in your foreach loop - make sure to unset the values from $b after you've set them. I.e. ($b as &$val) { $a[] = $val; unset($val);}  Unless you're doing it on function arguments, then copy-on-write will bite you.  But that's just silly. array_merge is there for a reason, and you should use it. This is exactly the kind of micro-optimization that you should  not  do in PHP, simply because it makes no sense.  Not to mention that your foreach-loop there, it  does not do what array_merge does , so replacing code with that would break things. i.e. if you have non-numeric indices, array_merge will replace the existing keys with the new ones.  tl;dr:  Use array_merge. If your concern is memory, choosing the C-code over the PHP-code should always be right. "
"Google trusted user input.  This is stupid, because most of their users are stupid.","> In what way was Buzz integration a disaster?  Essentially, the issue exploded when a chick complained that Google Reader said it was sharing the contents with her ex-husband, who I guess was abusive and out of prison.  This ended up not being the case, with the issue merely being a display error to her, and nothing was actually shared.  However, their was an issue with the way Buzz worked.  While their were privacy settings available for people to use, most people never look at privacy settings until after something happens. Anyways, when Buzz launched, it used existing privacy settings, assuming people were smart.  This was a serious bug, as people aren't smart, and we all know we shouldn't trust user input.  Anyways, what ended up happening is that people that didn't want to associated with other people got automatically Friended, which was bad because these highly secretive people logged into Buzz, and had each other on a friends list, and were using their real names and linking real accounts together.  The net result was a lot of people got shown as speaking with journalists.  In the end, this caused a lot of bloggers to RAGE on Google for assuming that the existing privacy settings that could have prevented this were accurate, and they asked that instead next time Google introduce a feature, we are presented with 20 screens of Privacy options with 20 page T&C making us state over and over again that this is what we want.  This hasn't happened, luckily.  Anyways, these bloggers and journalist-blogger types have essentially done nothing else, having completely forgotten the incident, only dredging it up when they need a ""like the Google Buzz fiasco.""  In the end, everyone complained about privacy, but didn't do a damn thing to solve it or learn form it; instead they just focused on getting page hits.  TL;DR: Google trusted user input.  This is stupid, because most of their users are stupid. "
"Why don't open source projects have good, beautiful UIs, when they are so good feature-wise?","It seems like the user interface design of OS-software is rarely on par with commercial counterparts. Many open source projects excel when it comes to functionality, but I have yet to see a really well designed open source UI (apart from maybe the Chrome browser, which I think is beautiful in its simplicity).  Unfortunately, the more ""desktoppy"" an application is, the more important the user interface becomes. Apache, the Linux kernel, Open SSH, etc are all fabulous projects without user interfaces, but compare Open Office to iWork / MS Office or The Gimp to Pixelmator / Photoshop.  I love the ""movement"" in Os X, to create desktop apps that are not only functional, but beautiful as well (and so does most of the rest of the user base, it seems). Unfortunately, the more beautiful an app becomes, the more likely it turns commercial. Is UI / UX planning really that expensive, compared to feature programming?  I don't like to separate usability from aesthetics either. It seems obvious these are connected (many users will probably feel much happier using a beautiful app compared to an ugly version of the same, even if the controls are in the exact same places).  TL;DR Why don't open source projects have good, beautiful UIs, when they are so good feature-wise? "
pattern matching is awesome; monads are even more awesome.,"Right, but it's just boilerplate. And it's a form of boilerplate that's actually rather common. Here's the equivalent code in Haskell (though its just as easy in ML, and I assume Scala):  printMaybe Nothing = print 100printMaybe (Just x) = print x  In fact, even this is more boilerplate than is strictly required, and it can be very easily generalized to:  x &gt;&gt;= print  At the call site. (i.e. there's no reason to declare this as its own function if you wanted to, it'd be printMaybe x = x >>= print).  Yes, I left out the type declaration cause it's not strictly required (though idiomatic Haskell  will  include the type declaration).  The first code example is easily transferable to any language that supports pattern matching on function arguments. The latter is rather Haskell specific, though not for any fundamental reason.  tl;dr pattern matching is awesome; monads are even more awesome. "
be willing to hire someone with interests different than yours as there's always someone who'll be excited about a particular task.,"In my experience, the answer to this question has little to do with incentives or the ""github wouldn't make a bad choice"" answer below.  It's all about hiring people with a broad variety of technical interests.  To use the several examples that spring immediately to mind, I find the idea of working on an installer, filesystems, and box hardening/security auditing/certifications ungodly boring.  However, I've interviewed and hired people who absolutely  love  those areas.  In fact, one of my most memorable and endearing interviews was a guy who gushed about the extensible installer he wrote.  Likewise, I've had interviews with masochists, I mean, people who excitedly talk story about previous FIPS or Common Criteria certification efforts.  Finally, if worse comes to worse, you'll have some coworkers who are just good soldiers by nature and it won't be a sh*  task to them.  It'll be the answer to ""what's the most important thing that needs done right now?""  These people aren't as strictly engaged by the technical content of their work (resume-based development wouldn't even occur to them).  They're motivated to work on important things  regardless of how interesting they are*.  TLDR;  be willing to hire someone with interests different than yours as there's always someone who'll be excited about a particular task. "
"They are both still beta, but core features are well bedded down.","Thanks!  I use the Angular version for some light lifting in an AU.gov project I'm working on and it is also being used by a company in the US for skinning, but I haven't heard a report from them in a few weeks (no bugs, so that's good).  I started the VJS version a few weeks after the NG one as my first attempt to make a VanillaJS library (which has been going really well). Both are only around 4-5 months old so no major heavy lifting has been done, but I really want to get them out there so please give them a hack and bug report away! My next major milestone this month is to complete and publish Jasmine tests.  I'm also in the middle of a rewrite of part of the  eval uation logic to include the IFRAME's  sandbox  attribute, but that is at least a few days away yet for CjsSS.js and a good week out for ngCss.  TL:DR; They are both still beta, but core features are well bedded down. "
There's a lot going on just under the surface that's exciting and will be formally announced soon!,"SitePenner here,  I wanted to thank you for your reply. Dojo is indeed 'old' in the sense that the project has been around for 10 years now however the latest release of Dojo is quite modern in what it offers, at least as far as a 'toolkit' is concerned.  As pointed out in other comments, Dojo 2 work has been under way, sort of, but a bigger announcement is coming very soon. The repo posted by /u/kenman only has a few contributors because it was a private repo until recently.  Dojo 2 is being reworked for the modern age as are the docs, tutorials etc.  As the repo indicates, Dojo 2 is going to be written in TypeScript and drop a lot of legacy stuff that was there due to Dojo 1.x needing to support earlier versions of IE.  As to the OP, if there are specific things you're looking to do let me know and I can hopefully point you in the right direction. There's also the #dojo IRC channel or mailing list. The tutorials are up to date but it may be difficult to know what you need to know. This is something we're addressing for Dojo 2.  There's also the other tools that are built for Dojo like [dgrid]( [dstore]( [Intern](  tl;dr - There's a lot going on just under the surface that's exciting and will be formally announced soon! "
Non-technical person tries to interpret complex process: EPIC FAIL,"I work for an ISO 9001 software house. We have a few small teams, totalling about 30 developers.  Somehow a very non-technical person has become Programme Manager, in charge of planning the software products and managing it when there is slippage.  The role of ISO 9001 is to ensure orderly business processes and involves an endless stream of paper forms that all need completing and multiple sign-offs (along with internal and external auditing of said process and forms). This is usually be considered overkill for small/medium companies, however in our industry/sector it is a requirement as we deal with a lot of public sector contracts who all require it.  So, when I saw the diagram on the whiteboard in the office I was shocked. When I heard it was conceived by our Programme Manager (a senior position) I was even more shocked. What really took the biscuit though was when he got our technical author to copy if off the board and put it in to Visio. I hear that now the plan is to print and laminate it and put it up around the office... as you may have guessed he is quite proud of this diagram.  TL;DR Non-technical person tries to interpret complex process: EPIC FAIL "
I need to find a college that will let me take classes specific to my needs/desires.,"I'm 25 years old, with 2 years of college under my belt, but the University I was going to didn't really teach me what I wanted to learn.  My major was computer science, but I had to take a bunch of Gen Ed courses, and programming languages I could care less about. Not that they suck, it's just not really what I wanted to be doing. I'm a designer I guess. The UI is what's important to me, though I still love to program it after I design it. I know the basics: HTML, Javascript, Java. But I crave so much more!  I have a list of things I want to learn, and I need to know where to look to find a school that offers ALL of them.  phpphotoshopxmlc++mysql  maybe unix/linux (cant hurt)maybe Flash, HTML5?  (not to limit it just to those! Networking has always interested me.)  The actual degree is not THAT important to me (it would be nice). I want the knowledge, so where can I go that will allow me to take those classes, and ONLY those classes? And still maybe get a degree...? Is that what a Tech school is?  (I'm  moving to the Colorado or Seattle region in a couple years, if at all possible)  TL;DR: I need to find a college that will let me take classes specific to my needs/desires. "
What is the development/exploratory process for finding these bugs.  Each step seems like an edge case.,"It really answers my question.  I guess whenever I see a vulnerability written up, I am always surprised at how the person came up with the attack vector.  For instance, local vulnerabilities ( at least in the old days ) are usually find a program with elevated privileges then buffer overflow it and put your payload at the pointer return.  When it comes to the web, its never that simple, I have no idea of how they find these vulnerabilities.  It's always a chained together sequence of events to break out of the security setting like the code you just posted.  Not to mention, any time you load code in a runtime setting you have just introduced a vulnerability so I would assume that Oracle would have vetted the shit out of this JVM feature.  TL;DR What is the development/exploratory process for finding these bugs.  Each step seems like an edge case. "
"Programming is hard.  Python is gentler than most languages, but still hard.  Stick with it.","HTML isn't a programming language.  It's a (hyper text) markup language.  Sort of like how typing `[link text]( in a reddit comment inserts a link like: [link text]( -- you just described the data but didn't program anything.  You can't use a markup language to compute or manipulate data in any way on its own.  E.g., you can't define a function to sum the variables in a list like how in python you can simply go to the interpreter and type:  &gt;&gt;&gt; my_list = [1,2,3,4]&gt;&gt;&gt; sum(my_list)10  Python gets rid of a lot of nastiness, has great libraries for almost everything, and makes clear readable code.  It is a very good choice for a first programming language.  Relevant [xkcd](  It's downsides are that   (a) native python code is generally much slower than similar code written in lower languages;   (b) its in the middle of a transition from python2 to python3 -- which is a significantly different language;   (c) some people hate dynamic typing (not explicitly declaring an integer is an integer); and  (d) some people dislike that python uses indentation for blocks versus curly braces:   E.g., the following C snippet works; where white space doesn't really matter.  int x = 0;while(x &lt; 3) { printf(x); x += 1; }  while python3 would require:  x = 0while x &lt; 3:    print(x)    x += 1  though most agree that proper identation should be used anyway.  TL;DR: Programming is hard.  Python is gentler than most languages, but still hard.  Stick with it. "
"TP3 rocked my boat, too. Making code smaller is expensive.","Economics.  Development time: How much extra would you pay for a smaller touch utility? TP was a major product by then - and a product of love, trying to pit a low-price product against the behemoths. Touch is simply a baseline utility: not a distinguishing feature, not worth much attention, but needs to be there, the quivalent to showing up in pants.  Storage Economy: I won't tell you what  I  paid for a floppy back then,  but it's easy to scale: cost of disk storage has dropped by roughly 6 orders of magnitude back then, so you are actually pitting those 39K against 44mB. (yes, that's millibyte). While not only disk storage is involved, all other aspects have seen similar improvements.  Functionality: That scale has allowed us to replace standardization with abstraction: instead of making components work the same, we can afford to make them work well in many scenarios, and put a shim inbetween to make them work as required. If you want to follow ""touch"" down to the same level of hardware abstraction as TP, you pull in a huge stack of code (and probably even firmware). That's more or less a result of mass production and diversification: compare the market for TP3 in 1986 ot the market of OS X - ish operating systems that can live off the same code base. I don't know if touch contains its own messages, but if it does it might as well be more text than TP.  TL;DR: TP3 rocked my boat, too. Making code smaller is expensive. "
"Try to build without the libraries to get a better feel for what problems they solve, then it will be easier to look at the source code","When I started using JavaScript heavily and learning it 'for real' so I could move into webdev, I had a similar worry.  Whenever I wanted to do something, I searched stack overflow and google looking for a library to do it for me thinking this was the Right Way.  This got more and more difficult as I started using things that I had no idea how they worked. This was especially true for node & backbone.  So I purged everything and decided not to use any library unless I absolutely needed it.  I ended up with a process kinda like iterative refinement.  As an example:  simple client-side app.  hmm, I need to change this text and the class.  Okay, I look up getElementById and use it, but its a pain to type over and over again. How would I abstract this?  Hrmmm.. Some kind of query library would really help.  Okay, now lemme look at jquery's source.  Since I already have an idea of what I need, it makes more sense and things don't seem as divinely inspired.  Awesome, now Im much further and I notice I keep doing the same things. I want these pages that load a model from the server, then I want to load a template based on that data, and attach a series of event handlers. Could I abstract this better so I have some reusable classes that glue these operations together? Oh, that's Backbone. Now that I see the need, it makes more sense how its put together.  Ugh, look at all of these library files I have, how do I control which ones are on which page and load only what I need? Oh, requirejs, how handy! etc  TLDR: Try to build without the libraries to get a better feel for what problems they solve, then it will be easier to look at the source code "
The loop will always exit because of the characteristics of the random number generator.,"Here's the documented behavior of the random number generator, at least for one version of Java:  Inlining and evaluating constants, we get something like this (pseudocode):  x = random int between 0 and 9do {    do {      seed = (seed * 25214903917 + 11) % (2**48)      bits = (seed &gt;&gt; 17) % (2**31)      val = bits % 10    } while(bits - val + 9 &gt;= (2**31))} while(val != x)  The inner loop makes analysis a little tricky, but it's using a linear congruential generator with:  m = 2**48a = 25214903917c = 11  I don't have The Art of Programming in front of me, but according to [Wikipedia]( this will cycle through every possible internal state iff:   c and m are relatively prime: m is a power of 2 and c is odd, so yes.   a - 1 is divisible by all prime factors of m: 2 is the only prime factor of m, and a is even, so yes.   a - 1 is a multiple of 4 if m is a multiple of 4: yes, they're both multiples of 4.    So, given enough time, the seed should go through all 2^48possible states. In particular, it will hit all the states from 0   2^17to 9   2^17, which will give you results from 0 to 9.  TLDR: The loop will always exit because of the characteristics of the random number generator. "
"When you download Ajax-powered webpages, set a timeout for your script. Otherwise you may miss some content that didn't load in time. 
 Edit:  typo","There is a problem with this approach. When you fetch an Ajax-powered webpage, you can't know for sure how much time it takes to fully load everything. Take this page for instance:  . If you open it in your browser, at the bottom you will see a progress indicator. It takes  several seconds  to fully load every Ajax part.  So, the solution is to integrate some waiting mechanism in the script. That is, we need the following: “open a given page, wait X seconds, then get the HTML source”. Hopefully all Ajax calls will be finished in X seconds. It is you who decides how many seconds to wait. Or, you can analyze the partially downloaded HTML and if something is missing, wait some more.  I also faced this problem and managed to solve it with my scraper called [Jabba-Webkit](  I made a test with the [page above]( Your script stopped after 10.7 seconds and downloaded 145,009 bytes. Jabba-Webkit was launched with a 20 seconds timeout and it downloaded 3,568,301 bytes. I compared the two contents and large parts were missing in the first case.  TL; DR:  When you download Ajax-powered webpages, set a timeout for your script. Otherwise you may miss some content that didn't load in time.  Edit:  typo "
I totally agree with OP. Especially the comic in OP. My social skills are not what they used to be.,"I just finished 6 months of working from home on a grad school project. At the same time, a lot of my friends were out of town/leaving town and we had the crappy northeast snow situation. There were stretches of 3 days, maybe 4, where I didn't leave the house. Because my roommate has been staying with his GF a lot, many of those days, I literally didn't speak a word aloud.  While I was productive overall, there were stretches of 1-2 weeks where I would do almost no work (beat DA:I in about a week... 80 hours.) Then I would set a goal with my boss and work 80 hours a week so that I would meet that goal. I think in the end, it all balanced out to about 40 hours per week, but not consistently. IMO a consistent work schedule is healthier than an inconsistent one.  Because I'm a grad student and not a professional programmer (i.e., not much $$), I talked myself out of buying a really nice chair. I have an Ikea markus, but it really isn't great for me. I wish I had spent the money on a nice chair. Just do it.  It also struck me, since most of my friends had just left, how much I missed being at a regular workplace. Although work friendships aren't always ""real"" friendships, at least you have someone to talk to.  I don't know how /u/taelor managed 6 years of remote work, but I'll definitely be looking through that comment thread after this post.  TL;DR I totally agree with OP. Especially the comic in OP. My social skills are not what they used to be. "
In the end you would probably be better off using an existing general-purpose upscaling solution like the one implemented in mplayer.,"AFAIK it should be possible to upscale  anime  videos using this algorithm. I can't measure the time it takes for a single pic to upscale since I don't have a Nvidia graphics card, but I would guess that you would need a rather powerful machine for real-time upscaling and noise-reduction. You would have to rewrite most of the image input/output code and maybe adapt the algorithm itself if it makes use of quirks in the original image compression algorithms. You would also probably want to rewrite it in C/C++ to make it fast enough, because Lua seems to be used for the main parts of the program, although the important computations are done with CUDA. Afterwards you would probably have to feed it with some lossless Blu-ray rips.  And even when you overcome all this and get a decent framerate, you probably can only use this for Anime/Comics since these have large, we'll distinct fields of singe colours and this algorithm is optimized for that.  Tl;dr: In the end you would probably be better off using an existing general-purpose upscaling solution like the one implemented in mplayer. "
"MySQL ran a query that contained a syntax error, deleted a multi-GB table.","I've used MySQL for several years now.  While I wouldn't call it a toy, it does feel like an unfinished product in several ways.  For all the hot air the author is blowing, a few good points come out.  Personally, the biggest fail I've seen in MySQL was when I was writing a query to clean up some bad inserts on a data warehouse.  The query looked something like:  DELETE FROM sometable WHERE NOT EXISTS (subquery);  The subquery had a syntax error because it had been cut-and-pasted from another query improperly.  Well, guess what, MySQL actually ran this query anyway -- and treated the subquery as returning NULL.  It deleted the whole table.  It was a MyISAM table (data warehouse, remember?), so no ROLLBACK.  Yeah, we had backups, and we fixed it, but... really?  A syntax error doesn't mean NULL, it means you print an error message and you don't run the query!  TL;DR: MySQL ran a query that contained a syntax error, deleted a multi-GB table. "
"I used to feel the same way, but I've changed since I've realized times are changing.","I agree, it feels weird to use more declarative templating than just HTML, and separating JS from HTML ""completely"" is indeed commonly thought to be  ""best practice"" .  The truth is, ""best practice"" is dictated by a number of circumstantial factors involving each project on a case by case basis.  In the case of separating JS from HTML, it's fuzzier than you might think: These days, anything you can do with HTML, you can do 100% programmatically with JavaScript. HTML is a declarative way to manipulate the DOM. JavaScript can be/is a programmatic way to manipulate the DOM. ""best practice"" here (and everywhere, really) isn't some sort of by-the-book religious thing... it's on a project by project, case by case basis. If you must have one immutable rule, IMO it should be  Best practice is whatever is easiest to maintain .  All of that said, declarative MVVM, MVW templating like you have in Knockout in Angular are simply more expressive means of doing what HTML does for you.  Think about it another way:  You don't complain about having a few loops or model/object references or if/thens in your server-side templates.... be it HAML, or Razor, or PHP, or Handlebars.  These frameworks are moving all of that forward. Gone are the days of rendering every piece of HTML on the server. More and more servers are just data sources (RESTful apis, web services, hypermedia hosts, etc), and the HTML rendering work has been offset to the client.  People have been mixing HTML and programming elements to great effect for more than a decade. Angular and Knockout aren't doing anything new or alien... they're just doing it in a different place than what you're used to. Furthermore, in both cases, it can be done 100% unobtrusively.... it just usually isn't because there isn't much point.  TL;DR: I used to feel the same way, but I've changed since I've realized times are changing. "
"Something like ""LRU Cache"" could be an answer to a useful interview question for an experienced developer. It makes no sense for it to be a question.","I did implement a LRU cache in a class I took in college, but I couldn't re-implement it from scratch in an hour under job interview pressure. This is just another person complaining they can't find any good programmers because they want someone who knows everything, and who can produce complete results instantly in any situation without ""cheating"" by doing what a any programmer would do in a real situation, look it up.  Yesterday, I had a situation at work where I needed to perform a topological sort. I haven't been out of school that long, it's been less than two years since I saw topological sorting in class. But, all that I remembered is that a topological sort is what was required. I didn't remember the various algorithms, etc. It doesn't really matter, because they're ""well known"" in the sense that I could just look it up.  Then, what I did is write a generic implementation that has three methods, ""addObject"", ""addDependency"", and ""getSortedList"". This means that not only have I never had to think about topological sorting since I saw it in a college lecture, no matter how long I am at my current job, I will only have thought about it one time since then.  Now, going from recognizing the need and producing the generic implementation took maybe an hour. Two weeks from now I probably won't even remember how I implemented it. If I were asked about it in an interview in two weeks, I doubt I'd be able to reimplement it without looking it up again, because I will have worked on 20 other things in the meantime.  The point of all this is, the longer someone has been working, the less they will remember how to re-implement well known algorithms, and there's nothing wrong with this. It's trivial to find and implement them when necessary, and if possible, they will be implemented in such a way that they do not have to be implemented again. The only important thing is to make sure they know what algorithm should be used given the description of a problem.  TL;DR: Something like ""LRU Cache"" could be an answer to a useful interview question for an experienced developer. It makes no sense for it to be a question. "
"If your code encounters an unknown exception, log it and GTFO.","I have recently taken over projects from a few developers and here is the biggest flaw in most of the programs I see.  They don't GTFO.  They have exception handling all over but they seem to want to keep the program ""rolling along"" even when serious problems arise.  I had one program where you could rename the connection string and this program would just keep barreling though the code skipping the parts where it encountered database queries.  That program obviously is a serious example but I see this so often on a smaller scale...  Try(unknown error happens)catch(log it, and CONTINUE!?!?!?)  Don't do this.  TLDR If your code encounters an unknown exception, log it and GTFO. "
Linux  will never become   is already   mainstream  until   because   granny can do everything without ever touching a terminal.,"Why are you using future tense in that sentence?  Thanks to Linux in both her TV and phone -- she  can  do everything without touching a keyboard or mouse (assuming that's what you mean by terminal).  That's why Linux  is  mainstream.  Most houses have more CPUs running Linux ([TVs]( cameras, [kitchen appliances]( than Windows these days.   It's exactly  because  you can do so much more without needing to touch a terminal that Linux is so much more popular than Windows on such no-terminal devices.  > IMO Linux will never become mainstream until granny can do everything without ever touching a terminal.  TL/DR:   Linux  will never become   is already   mainstream  until   because   granny can do everything without ever touching a terminal. "
Your company can be sued for millions of dollars for not following W3C standards if disabled people cannot surf it using alternative browsing technologies.,"I understand that the OP thinks W3C validation is a ""harebrained scheme"", in your words.  It's not. The validator has been around for quite some time now, modern web development best practices almost universally stress the importance of validation, and the argument that ""well the big sites don't do it"" doesn't hold any weight whatsoever, especially in light of the fact that Target Corporation, for instance, was just the defendent of a multi-million dollar class-action lawsuit brought against them for breaking section 508 compliance under the Americans with Disabilities Act.  This is not ""harebrained scheme"" stuff; it's real, and it could cost your company millions.  tl;dr - Your company can be sued for millions of dollars for not following W3C standards if disabled people cannot surf it using alternative browsing technologies. "
App development is something that both non-programmers and programmers should be able to do (and not get flak about it). NO fart apps!,"I'm probably going to get downvoted into oblivion for this, but here we go.  Application Development is just as much about the design as it is about the codebase that the application is built on top of. Sure, not everybody is cut out for programming, but I sure as heck have seen enough programmers who can't design a halfway decent user interface. That's where visual tools such as this one come in. Anyone can create a crappy program, be it in C, Java, VB, Python, Ruby, PHP etc, etc (these languages can be used to create very good programs as well). There are enough ideas floating around out there, that if one person were able to create it (not just a programmer), it could be pretty useful to someone else in the end (as long as it's not another one of those fart apps; I'm sick of those).  tl;dr  App development is something that both non-programmers and programmers should be able to do (and not get flak about it). NO fart apps! "
"would be ""parsing is the slowest part of the compiler"".","As someone else who has learned the hard way, I can say that the compilers class would have helped. LL(k) with backtracking is slow (but really nice), copying strings is slow when you do it 100000 times, even traversing giant trees is slow if you do it enough.  The problem is that parsing is just simple enough that anybody can look at it and think ""I could do that""! Still, learning all the tricks is fun.  I'm now at the point where I knew that the problem would be the parser even before I read the article (just after reading your link title). So maybe the real tldr would be ""parsing is the slowest part of the compiler"". "
"Show enthusiasm for learning more PHP. Know how to write secure PHP. Also know SQL, Javascript, HTML.","Having no relevant commercial experience isn't a big barrier to getting an entry-level position :) the most important things are demonstrating that you show good promise and have enthusiasm to learn more. Make sure your own projects are secure and functional. Provide code examples with your CV. Provide links to working demos, or make the code available for them to setup themselves to have a look at what you've built. Your previous projects are your portfolio, show them off.  If you've attended any PHP conferences or seminars, point these out. If I see an applicant has attended something like PHPNW or PHPBenelux then they stand out above other applicants.  You absolutely need to know about things like SQL injection and XSS and how to prevent them - not knowing these are dealbreakers for hiring a dev from my perspective. If the people looking over your code find places where you've let these issues through, you will not be hired. And to be honest, if you can't identify and prevent things like SQL injection, then you aren't ready to be exposed to a commercial position!  Knowing OO PHP is a big plus point, and exposure to a major framework like Zend is another big plus.  version control of some kind is a nice-to-have but not necessary for entry level stuff, but it'd be another big plus point if you know at least some basic Git or SVN.  > I developed a few scripts which use MySQL databases and whatnot.  I may be misreading this, but that sounds like you're saying you haven't done a great deal with manipulating databases within PHP? Improve that - reading/writing from a database is a solid part of a dev's workload and you need to have at least basic sql skills in most projects.  You'll also want to have at least a passing knowledge of javascript as you'll likely need to tie in front-end functionality as well as back-end. Your HTML should be solid as well.  TL;DR Show enthusiasm for learning more PHP. Know how to write secure PHP. Also know SQL, Javascript, HTML. "
you're a moron interviewer if you shoot me questions that google can give answers in 5 seconds.,"This happened to me in a phone interview with Yahoo.  -So you say you code in C? Do you know who invented C? ""Ritchie"". How do rate yourself on a scale of 10 given that Dennis Ritchie is 10/10?  What kind of question is that? So if I were to be 5/10, does that mean I am half as good as Ritchie? I'd think most rockstar programmers would be happy to rate themselves 2.5/10, as that makes them one-fourth as good as Ritchie. Secondly, if I were to describe myself half as good as Ritchie, are you going to pay me half as much Ritchie would make working for Yahoo!(assume he makes 500K a year). For a subjective and stupid question like that, I would be happy to give him links to some apps I have made in C, instead of crowing about my ability and expect him to believe it.  The technical questions were just as bad: ""Tell me what 100 in Hex format translates to in Decimal format"". ""Do you want me to explain how my converter function looks - like I'm doing this iteratively or recursively?"". ""No, I just want the final answer"".  I do it on paper and say 256. ""Yes, lets move to the next.""  Even in interviews where I got rejected, I have enjoyed the event and the exchange of ideas, the different approaches to solving stuff. This was the most unstimulating interview I ever had.  TLDR: you're a moron interviewer if you shoot me questions that google can give answers in 5 seconds. "
"This code needs more bit-wise operators, especially in the return statement plz fix.",">The list node contains a pointer and some data, most likely another pointer, or a primitive type, or a struct of primitive types. Which means, that on all systems, a list node is aligned to at least 16 bits, much more likely 32 bits.  You would normally be dynamically allocating the elements, would you not? So the actual size of the node structure is irrelevant, since malloc by no means guarantees contiguous memory.  But I know at least dlmalloc guarantees 8  byte  aligned memory.  So you normally have the least 3 significant bits of a pointer at your disposal. Why did you waste [four full bytes]( of memory on ret, when you could have stored your return value in one of the least significant bits of some pointer?  TL;DR This code needs more bit-wise operators, especially in the return statement plz fix. "
"Naming: good, State: bad. Naming without State: Double plus good.","> [...] allow the editor (e.g. Emacs) to display a wrapped line.  Displaying a wrapped line puts the onus of formatting on your editor, which hopefully knows less about your code than you. Also if you work with colleagues and use different editors, you'll have to adapt your cognition to their settings.  > I'm not sure how it reduces diffs. [...]  Because differences in formatting don't show up. Even if you hide whitespace changes, differences in formatting can still disrupt the flow of reading a diff.  > As for reading, I find long lines to be clearer.  To each their own.  > They (intermediate variables) introduce temporary intermediate state, couple statements temporally and then enable the wide dispersal of those statements, and depart from the functional style that I favor.  Not if you just use them as a binding, e.g. in java you can declare them final, in C/C++ the compiler will automatically figure out that a variable is left unchanged within a certain scope. Even in functional languages, you can use let-bindings or where-clauses to name a term.  TL;DR: Naming: good, State: bad. Naming without State: Double plus good. "
"It's not that we want the front end language to be unsafe, but we want it to be flexible, and those two goals are arguably at odds.","I don't want to speak for /r/timeshifter_ but in my opinion front and backend code have two very different goals.  Web pages are designed so that the consumer has complete control over them.  I can chose to inject my own JavaScript into the page, I can overwrite your CSS because  ugh look at that backgrond and tiny font .  I can inspect and modify the very structure and content of the page.  Web pages are designed for flexibility.  This is how we get glorious products like AdBlocker, StyleBot, and my personal favorite, Vimium.  This level of  flexibility  is what is fundamentally at odds with safety.  Obviously some of these features are specific to the platform of a browser, and might be avoided in NodeJs, but a lot of the flexibility needed by the browser is built into the language - you can inspect, overload, and modify behavior at runtime of any object you can see.  The flexibility is very beneficial for customizing behavior, but is a security nightmare.  TL;DR: It's not that we want the front end language to be unsafe, but we want it to be flexible, and those two goals are arguably at odds. "
"Binary serialization is not the be-all and end-all, JSON is a competitor which retains compatibility","Exactly. IMO it's quite a feat.  Binary serialization depends on an external schema and nicely structured data.Compared to that, JSON is much more chaotic.  There are lots of things that can slow down an unoptimized parser, be it insignificant whitespace, unknown order of properties (or even properties that are completely missing), escaping / encoding rules... the list goes on and includes a  LOT  of branching which can kill the instruction prefetch if not designed properly.  By deploying similar tactics to binary serialization and having an external schema which in turn compiles into specialized JSON writers and parsers, it is actually possible to compete, and even beat binary serialization in terms of speed. The key point of interest is to think about the end result of the JIT compiler and design the VM bytecode to account for the compilation step.Payload size, on the other hand is a different topic as the JSON format inherently demands that the property names are duplicated in lots of places. If you're having issues with this, consider streaming compression.  With the recent MongoDB/PostgreSQL JSON craze I'm finding it quite interesting that we are able to talk in a textual format without wasting too much time in the serialization step, and that I'm still able to reuse the same method to chat with a browser in a language it understands (as opposed to pickling bytes on the client side).  TL;DR Binary serialization is not the be-all and end-all, JSON is a competitor which retains compatibility "
ServiceLocator is a disguised version of a hard coded global variable. Don't do it.,"There's a place for integration tests, but I think that place is a live, running version of your application on a QA machine. I've been developing software a long time and the root of much evil I've seen is the use of service locator patterns.  It creates a tight coupling that becomes pervasive and pernicious across your code base. Try refactoring a couple million lines of code where the dependencies are all circular. Try writing unit tests so you can feel more confident in your refactoring but you can't because the references are all static. Try dealing with thousands of integration tests that affect global state and trip all over each other (sometimes). Want to play build roulette where the results are not deterministic? Want to run all those integration tests locally? It can take a few hours and maybe they'll pass locally only to fail on the official build.  TL;DR ServiceLocator is a disguised version of a hard coded global variable. Don't do it. "
Never went back to Sourceforge again and lived happily ever after.,"Downloaded 7-Zip from Sourceforge a few months ago, and was bounced to a mirror that gave me a different executable. I started the installer and knew right away that something was up.  Maybe I accidentally clicked an ad? My adblock extension should have blocked it. I went back, downloaded the file again from a different mirror. This one wanted to install some ""Mind Spark"" bloatware and toolbars.  I got mad at 7-Zip. It never crossed my mind that SF would do something so insane. I went to the 7-Zip forums and sure enough, the first thread had people like me complaining about the issue and discovering that 7-Zip had nothing to do with it.  Tl;dr Never went back to Sourceforge again and lived happily ever after. "
For someone who has used both svn and hg for large repos... does Hg feel faster?,"Yes, but I get the majority of the delay BEFORE I connect to the repository, say when I select commit - the delay happens before the files show up on my local box I can select to commit.  The actual commit isn't that slow.  It looks like you can port the Svn source database on the server to a Hg Served database - and then incrementally update that Hg database from Svn as needed.  That way I can test Hg against our real code but there will be no penalties for not switching over the other developers.  Unfortunately, this method has no easy way to take anything commited to the Hg database and push it back to Svn.  (Yes, I know there are methods to do this as and individual Hg developer - but I want to get some opinions about how a completely coverted Hg setup will feel for remote use with a large repository.)  tl;dr - For someone who has used both svn and hg for large repos... does Hg feel faster? "
"in 1997 we thought Java was the hot new coolness.  After playing with it for a month or two we said ""meh""...","I can remember evaluating Java in about '97 or '98.  We thought it might make a nice cross-platform GUI for our app which needed to run on Solaris and Windows.  At first I recall being kind of excited about Java.  But as we dug into it more and tried using it problems became apparent: the main one being it was slow compared to our then current development language, C++.  The more we played with it, the more we thought that what Java could have been was something much better than what it was.  It seemed to us that Java didn't move far enough away from C++.  Yes, there was garbage collection, but there were no templates (no generics at that time and there wouldn't be any for many years).  It lacked many of the higher-level features that languages like Python or Perl had at the time.  After a couple of months of playing around with Java we decided to drop it, it just didn't offer us anything compelling enough to switch.  Sometimes I wonder if the C++ Boost libraries had existed at that time (and compilers that could support them) would Java have gotten a foothold?  I haven't done any Java coding since then.  tl;dr: in 1997 we thought Java was the hot new coolness.  After playing with it for a month or two we said ""meh""... "
"Programming"" in the sense of ""the set of mental skills and disciplines it teaches you"", not in the sense of ""a particular language or API"".","> Computer science is about the Oh-notation, the halting problem, turing-machines, etc.  Fair point - the course I was doing was CompSci, but there was plenty of programming in it, and those without existing skills (or who weren't prepared to put in an  ungodly  amount of effort) definitely had a hard time keeping up.  That said, I'm not so much arguing that learning the syntax of a specific language or a specific API is important (you're right; it's not), so much as the idea that programming forces you to get very good at things like task decomposition, logic/control flow, concentrating on details and holding and manipulating large, complex structures in your head, all of which are important skills for understanding and using even for the areas you list (O-notation, turing machines, etc).  I'm not trying to say programming is inherently important, so much as the skills and mental abilities it encourages in those who do it a lot, and I haven't seen many other ""practical"" skills than programming that really seem to exercise and engender those kinds of mental tools.  TL;DR: ""Programming"" in the sense of ""the set of mental skills and disciplines it teaches you"", not in the sense of ""a particular language or API"". "
It's a pain. I wouldn't recommend it. Just start from scratch and try to iterate on what is already in existence.,"I have done this before and it's very difficult especially if you aren't the original developer. Mainly because there is always a lot of small, undocumented functionality that users have gotten used to. Often when you start developing on a frame work there is a much simpler approach to take to replicate a page but you ultimately miss the small hidden functionality on that page. So you end up having to play catch up with the original app or occasionally adding functionality which was a result of a bug but was something that the users expect and don't want to get rid of.  TL;DR It's a pain. I wouldn't recommend it. Just start from scratch and try to iterate on what is already in existence. "
"anonymous functions are like glue: they are very useful in some cases, but getting them all over your project is probably a bad thing.","Generally, anonymous functions are a code smell.  In most cases, anonymous functions are a good starting point for refactoring your code into something more readable and maintainable. In my experience over the past decade (working with JS on simple websites, client-heavy web apps, on the server with Node.js and in the database with CouchDB and ArangoDB), it's a good idea to keep anonymous functions as simple as possible and extract as much code into shorter named functions as possible.  The benefit of having lots of small named utility functions is that they are a lot easier to test, allowing you to be certain that some part of your code will simply work as expected even if you can't/won't test the entire codebase properly.  The main code smell with your example is that it's all one big event listener. A lot of jQuery code looks like this, sadly, so it's not surprising. I think a good first step would be to use a module system (I'd strongly recommend CommonJS via browserify), so you can more easily extract and isolate your utility functions.  tl;dr: anonymous functions are like glue: they are very useful in some cases, but getting them all over your project is probably a bad thing. "
Get subversion and tortoiseSVN.  Read the rest of my post.,"I highly recommend getting subversion.  I use this for work (remote repository) and also for fun projects and consulting work from my [DNS-323]( SAN device.  For integrating with windows, get [tortoiseSVN]( which integrates smoothly with windows explorer.  I also recommend [SVN Notifier]( It sits in your system tray and lets you know if someone else has checked something in and you need to update your local code.  I don't consider myself a power user, probably just the opposite.  There are only 2 other people on my team so there is not much that we need to do in coordinating- however talking to your team mates ahead of time is the best way to make sure you aren't working on the same stuff.  How I recommend using SVN for a website (php):   Set up a local branch by checking out the code that is in the repository to your web directory   Make whatever changes.   Check out the code on your server in the web directory.   Make changes locally and test them.  When things are working 100%...   Go to your server and update the directory for the web stuff.    It is recommended that you have at least a 2 tiered system:   Do an update on a 'test' server.   Let some people test the site and confirm that it is 100%.   Then do an update on the production server.    By this point if it gets more complicated than this you should have either learned enough to make informed decisions for the next steps or you should hire some outside consultant to help you figure it out and maybe lend a hand (consultants are a very good way to spend little money to get a big problem solved quickly).  tl;dr : Get subversion and tortoiseSVN.  Read the rest of my post. "
"He may simply be giving two disconnected characterizations of the problem, the ""O(n^2)"" problem and the ""chokes even for as little as 101 files"" problem.","This is possible. First of all, big-O notation only talks about long term behavior, so you can choose the runtime of a function for any finite set of inputs without changing the big-O analysis.  Second of all, even if we were to assume that the runtime absolutely had to be related to a polynomial, and we weren't allowed to make exceptions, there are many solutions to  2 * (a * 100^2 + b * 100 + c) = (a * 101^2 + b * 101 + c) , for example,  a = 1, b = 0, c = -9799 . So the runtime  f(n) = n^2 - 9799  is  O(n^2) , yet  2 * f(100) = f(101) . (You can throw in a  max  term if it will make you feel better:  f(n) = max(1, n^2 - 9799) .)  tl;dr He may simply be giving two disconnected characterizations of the problem, the ""O(n^2)"" problem and the ""chokes even for as little as 101 files"" problem. "
The GIL doesn't work like you think. Profile or GTFO. Scale horizontally.,"Dropping the GIL has been done a few times before and never has resulted in any non-niche performance increase.  This is because the GIL is not held for anywhere between 50% to 99% of the time depending on what your application does.  I have yet to find anyone who has told me ""the GIL made my application suck"" that actually profiled their app and interpreter.  If you actually do find yourself in a situation where you need to scale, it's usually better to scale horizontally with modules like ""multiprocess"".  Vertical scaling just has too low of a ceiling anyway.  The cases where someone actually needs HPC-style threading behavior are vanishingly rare outside of science (see also, numpy and umpteen other C extensions that don't hold the GIL).  tl;dr The GIL doesn't work like you think. Profile or GTFO. Scale horizontally. "
"Make shell scripts to automate tasks in the shell, not ""real"" programming, don't do more than 100 lines or so of shell in one script.","I've come to love ZSH scripting, but generally I  only  right shell scripts that I call during an interactive session.  I don't want to make this a Bash v. ZSH think (especially since I think the problem mentioned here is present in ZSH as well), but overall I find the way ZSH behaves less surprising than BASH, but it's certainly more arcane & terse. (quick what do $i:r, $i:e, $i:h, $i:t, or even the very useful $i:t:r do?)  Generally if they're getting over 100 lines or so, I'm doing something wrong.  Now that said, I got really good at shell scripting by making a screenscraper (HTML Parser) in sed + zsh. This was an interesting exercise in futility (parsing HTML with regexes into a language that only understands lists).  Though to be fair I was mostly extracting links from the HTML.  When I eventually rewrote in Python I got parsing & querying (with XPath) for ""free"" with LXML, but spent a lot of time essentially rewriting ""pipe"", but in doing so I was able to get concurrency and caching (of parsed trees not of HTML) to avoid re-fetching.  I should release that code some day, it's ugly as hell, but  TL;DR: Make shell scripts to automate tasks in the shell, not ""real"" programming, don't do more than 100 lines or so of shell in one script. "
"humble yourself, bro. Solve problems rather than whining about them.","Bit of a whinge. I think I know what working 'with' you would be like. A few points:   if your project isn't interesting, make it interesting. I dived into that less-than-perfect code base and found juicy low-hanging performance optimizations we could (and did) implement. Then some fundamental security holes which Google also suffered from.  if other people say your code is being too clever, then it isn't ""obviously correct"", which is the goal most of the time. If they don't understand closures, metafunctions, and type inference, then teach them. Quietly push them into the light.  as mentioned elsewhere, don't burn bridges. Microsoft has very many teams who are working on interesting , Hard problems. If you're good, you'll gravitate towards them. Talking about a former employer with such words is a red flag to many interviewers. When they ask you about previous employers, only ever subtly point out their flaws, and do it in a way which is constructive and which the company you're interviewing for has a solution to.   Enough said, but final word is you need to sit in a tight loop incrementing your maturity for a while, then try again.  TL;DR: humble yourself, bro. Solve problems rather than whining about them. "
Once? I wish it was once. I really do.,"Back in the day, I, among others, used to push backspace to unident one line.  Its wonderful that we've come so far with spaces. I have much more choice now. I can:   backspace   4**: Most compatible, except for the exception noted below. This option is most profitable with clients that are billed per-keystroke.   Shift + Tab : Not as effective price-wise, provides a good balance if needed, but application support is more limited   (ESC|CTRL+C) + < < : much less compatible than shift + tab   1x Backspace : Some smart applications optionally allow the user to configure backspace  to erase 4 spaces. This feature seems to result in several more keystrokes.   With all of these choices PSR-2 has definitely increased my productivity-- especially when working on hundreds of differently configured servers with editors that all seem to prefer tabs.  This change may force me to stop using PSR-2, I'm not sure if I'll be able to make up the lost income.  TL;DR  Once? I wish it was once. I really do. "
Operating System 'Feature' where it automagically 'fixes' the endianness of certain data types behind the scenes.,"Around 2 years ago I was writing some code for reading some old data that was stored in the Mac OS Resource fork. I ran into the problem that certain fields of certain data types (like  snd ,  PICT , and  STR#  were having their endianness flipped for no apparent reason.  I looked at the hex from DeRez and compared it to what was being loaded into my program and confirmed that wasn't making a mistake like incorrect offsets... 2 different programs were showing different contents or the same resource entires.  After a few days of on and off investigation, I discovered that there was a  ""feature""  that you could register callbacks that would automatically flip the data in a resource entry to use native endianness.  It turns out that several of the builtin types had flippers pre-registered for them, breaking assumptions made by my code.  TL;DR: Operating System 'Feature' where it automagically 'fixes' the endianness of certain data types behind the scenes. "
"makes life easier for >95% of cases, write SQL and auto map to an object when needed.","It's not to avoid writing SQL per se, it's so that we can to set up complex business object models and map them to storage easily.  In EF, if I have done my mappings properly, I can write complex queries to get objects out, modify them as needed, then chunk them right back in without worrying about the details.  I'm actually extremely comfortable writing SQL, but why do it when you can programmatic ally map your objects there then write something that is easier to write than SQL and integrates right into the language.  Customers.Where(c => c.HasPremiumPackage).ToList().GroupBy(c => c.RegistrationDate).OrderBy(g => g.Key) gets me the same thing as a similar SQL query, but I am able to do my grouping and ordering on my app server rather than DB (far more scalable, those are expensive operations), and the intent of the code is extremely clear, rather than writing the SQL, then writing group/order code on the app.  I've not yet had any problems with it for crud, and even complex analysis stuff.  If one were doing some good old OLAP across a lot of tables, SQL is probably the way to go.  tl;dr: makes life easier for >95% of cases, write SQL and auto map to an object when needed. "
the wordpress coding style isn't anything but a steaming pile of manure. Manure oriented programming.,"No. It's not. Wordpress is a pile of procedural soup which has been built upon and built upon for years and years. It is what you get when you don't design something and allow it to grow organically. It is what you get when you ignore best practice. It is what you get when you refuse to refactor. It is what you get when you value backwards compatibility over maintainability.  Even if it was performance oriented (which I do not believe is the case), the cost of developing against something which is marketed as a CMS solution which does not have a focus on maintainability and ease of development will outstrip the cost of buying/renting faster servers by at least one order of magnitude.  tl;dr the wordpress coding style isn't anything but a steaming pile of manure. Manure oriented programming. "
"10s of billions of your taxpayer dollars are going into making sure you end up on a compromised site. As a layman, you're fucked.",">  Anything we can do to make sure we don't end up on a compromised site?  Yeah. Don't use the ""darknet"" to buy drugs.  Instead, head down to your local ghetto, and participate in a cycle of violence and punitive policing and community destruction.  Or don't buy drugs.  Seriously though, there's nothing you can do to make illegal activities ""safe"". Morally, adult drug use is arguable a victimless crime; nevertheless it is a crime, and as a country we spend 10s of billion dollars annually to try to destroy the lives of anyone involved in production, distribution, and usage. The War on (Americans who use) Drugs is incredibly dangerous, and you need to understand the only reason you don't go to prison when you make a buy is luck.  TL;DR: 10s of billions of your taxpayer dollars are going into making sure you end up on a compromised site. As a layman, you're fucked. "
Great site for learning Python if you're interested in data analysis because of how it exclusively uses data analysis-themed mini-projects to teach Python.,"I've been going through the missions on this site and I'm on Mission 6 currently. The general structure is a KhanAcademy-esque short video (about 5 minutes in length) followed by 3 or 4 coding challenges that build off of the topic in the video. The pacing is very well done and the coding challenges seem to have very little errors.  As someone who is learning Python for the sake of data analysis, what really sets this site apart from others for me are the themes of the coding challenges. They're specifically tailored toward analyzing data, which is a huge motivation for me to work through the problems. I went through the Codecademy Python course and learned a lot, but the coding challenges were rather disjointed from one another and didn't engage me as much. I can't speak much for how well it presents the more advanced topics, but for learning basic Python in relation to data analysis, this site is so far the best I've found.  TL;DR: Great site for learning Python if you're interested in data analysis because of how it exclusively uses data analysis-themed mini-projects to teach Python. "
"Don't worry about where you put the silly things; as long as you are consistent, you are fine.","Bracket placement is a matter of  personal preference . (Although you generally want to use the same standards if multiple people work on the same project.)  One thing that is worth keeping in mind is that vertical real-estate is nowadays more valuable than it used to be; widescreen monitors mean we aren't cramped in width any longer and the available height for 'comfortable' reading has become cramped in comparison. This on its own is a reason to collapse the starting { on the first line.  Also, CSS does have a different nature to actual programming languages. For one, every 'rule' (or whatever it may be called) is always followed by a pair of brackets. It isn't optional. Additionally, you don't nest CSS rules either; there's all separate in the 'top level'.  This makes many of the coding standard styling rules that govern brackets in programming languages irrelevant for CSS. In comparison, lining up the brackets isn't as important; it will be obvious where a group ends and a new one starts.  TL;DR: Don't worry about where you put the silly things; as long as you are consistent, you are fine. "
"Try them both out on real work, feature lists are only telling you how it can be used and not how you'll use it.","I chose PyCharm (apple) because it makes my life easier, my code more consistent, and development for large projects quite a bit faster.  My orange is vim, because when I do need something light and fast it's often on the other end of an angry server in some far away place that won't let me do remote debugging.  I've never used Sublime, but when i compared them myself a while back I chose PyCharm because it fit my needs better than other tools did.  I could give you more specifics but past experience tells me that it's not going to help advocate in either direction.  For every way that I ""do things faster"" in PyCharm I could think of at least ten other ways that are even faster given the right situation or the right person at the controls.  TL;DR:   Try them both out on real work, feature lists are only telling you how it can be used and not how you'll use it. "
I probably could have gotten my underlying point across better...,"I'm starting to see your point actually...  To continue on own analogy, which I thought was 'ok', I wouldn't show them the ingredient list because it's something they'll either already know (they handed me the cereal) or will have on hand to look at for themselves before trying it.  The original question was why did I choose PyCharm over ST to which I answered I chose vim and PyCharm over ST because that's what worked for me.  I contradicted myself because I wanted to say that although I knew what I needed for myself one should base their own decisions on experience and if you don't have that you should try it out.  Looking back, without the analogy and the TLDR I probably could have gotten my underlying point across better... "
there's more to scientific computing than the odd researcher smashing out a few scripts.,"This is nonsense. A spreadsheet user vs the team that wrote it -- what's the difference!?  Been doing scientific computing for just over ten years. The field is much larger than scientists banging out scripts now and then. Somtimes entire tools are developed and released. This may involve computer scientists, software engineers (real software engineers), mathematicians, project managers and domain experts working on the same project for several years.  Just an example... clinical genomics is an area that has a  very strong software engineering side to it. The data and code involved in those piplines is managed better then the average 'real world' project. Yes, there are researchers getting data off pipelines for analysis using ad hoc methods but the backbone of these systems are certainly not the result of a bunch of quick and dirty scripts!  tl;dr there's more to scientific computing than the odd researcher smashing out a few scripts. "
They didn't want to fix the algorithm so they stuck duct tape over the bug.,"Their algorithm is broken if the machine implements it using the typical 80-bit floating-point arithmetic present in Intel processors. It works fine if you use SSE or if you truncate the results to 64 bits at each step in the computation. One GCC compiler flag,  -ffloat-store , has the effect of forcing all results into memory (e.g. 64 bits) at each step. Some people were suggesting this until the patch was released, as it fixes the issue, though this would slow down FP math in all the PHP source.  By adding volatile to the variables given, the compiler is not allowed to keep its values in registers, so it will spill the value of those variables to the stack after each assignment statement to them. Volatile is typically intended for things like memory-mapped IO or values shared by other threads, so that the generated code can respond to the value changing ""out from under"" the current function's execution. This will truncate doubles to 64 bits even if the compiler is using the x87 80-bit FP math. They're using volatile here purely for its side effect of dumping to memory.  tl;dr: They didn't want to fix the algorithm so they stuck duct tape over the bug. "
a CE major has no more excuse than a CS major for having written the code in TFA.,">> Where I went to school you would have had to pass at least a data structures and possibly an algorithms class before getting into an OS class.  > Did your school offer a ""computer engineering"" degree, or are you talking about computer science? ""Computer engineering"" is a degree that is basically ""mostly electrical engineering but with a couple CS type classes tossed in"".  I am a computer engineering student, and at my school our core courses up through third year are virtually identical to the computer science majors. In my case at least, CE majors actually take  more  core math courses than the CS people do (CE majors do take discrete mathematics, but CS majors are not required to take introductory differential equations, mechanics or electromagnetics).  I would be shocked to find a CE graduate whose coursework did not include, at a minimum,   Introductory object oriented programming and design  Computer architecture  Data structures and algorithms  Operating systems   Most of the differences between the two majors do not become evident until the third year - the CS people move on to functional languages and theory of computation while we double down on systems programming and pick up some topics from basic EE: circuits, digital logic design and RTL hardware synthesis. Many of the common electives that CS majors take (e.g. compiler design) remain open to us as well.  tl;dr  a CE major has no more excuse than a CS major for having written the code in TFA. "
"a free SSL signing service would likely be treated no differently than self-signed SSL certs, so what's the point.","Google would never open themselves up to that legal liability.  A certificate authority is just a trusted entity that verifies the SSL identity. They put their name on the line when verifying you are who you say you are. That is why it is such a pain to get a SSL certificate through Thawte or Verisign. Granted, I haven't tried getting a SSL cert through them within the past few years, so I don't know if they have laxed their paperwork recently. It used to be a big hassle that involved a lot of paperwork and big wait time.  We ended up doing so many SSL certs that we enrolled in SRSplus to make it easier and faster to register and renew SSL certs. The fine print said that we were responsible for all the certs we issued, not Network Solutions. They wiped their hands clean of legal liability for any SSL certs we issued through their service.  If Google offered free SSL signing, I doubt they would twist in the wind for any of the people using that service. Since it is a free service then I doubt they would do identity verification as part of the issue process. Combine that will Google's hard-on for automation and general lack of support, you end up with something barely better than self-signed certificates and I wouldn't be surprised if browsers would treat Google-issued certs the same as self-signed certs.  TL;DR - a free SSL signing service would likely be treated no differently than self-signed SSL certs, so what's the point. "
read the implementation of some monads. Ignore overly complex monad tutorials.,"The little I understand is that they seem to describe some mystic, arcane concepts which only those within the circle understand.  Actually, the concept is very simple. I had trouble understanding monads as well, primarily because most tutorials are utterly confusing. Then I decided to just look at the implementation of some simple monads ( Maybe ,  Either , list), and Monads turned out to be very simple: some expressions evaluate to values wrapped in 'boxes' (e.g. a  Maybe  box or a list box). A particular monad just specifies how such 'boxed' computations are combined.  E.g. take the relevant part of the definition of the  Maybe  monad:  instance  Monad Maybe  where    (Just x) &gt;&gt;= k      = k x    Nothing  &gt;&gt;= _      = Nothing  So, if we have an expression that evaluates to a  Maybe  (left-hand side of (>>=)), and want to combine its boxed/wrapped value with the function k, there are two possibilities: 1. The  Maybe  value packs a value with the  Just  constructor. In that case, the wrapped value is pried out, and passed as an argument to  k . 2. The  Maybe  value was constructed with the  Nothing  constructor. In this case, there is no wrapped value to pass to  k , and  Nothing  is 'returned'.  So, an expression such as  f1 &gt;&gt;= f2 &gt;&gt;= f3 &gt;&gt;= f4  where each expression returns a  Maybe  value, says nothing else than if  f[1-3]  evaluates to Nothing, the whole expression evaluaties to Nothing. If the function evaluates to  Just x ,  x  is passed to the next function in the chain.  tl;dr: read the implementation of some monads. Ignore overly complex monad tutorials. "
stick to your guns if you can back it up.,"...or be able to change your own coding styles when there's a valid reason to.  Case in point. I was a PHP programmer at work and one of the conventions was not to put quotes in array key names (so $array[foo] = 'bar'; ).  The problem is php treats foo as a constant first, if no constant exists (which never should since by convention constants are all upper case) then it assumes it's a text value for the key.  I knew this, another programmer knew this.  We both told the first programmer this and he REFUSED to believe it.  Even when I gave him a link to the documentation on the php site explaining all this.  His reason for no quotes was ""It's less characters to type"" &#3232;_&#3232; The other programmer did change to it and I absolutely refused to.  Fast forward a year or two and that programmer turned on the showing of NOTICE level errors while developing.  Half the page would be filled with notices about undefined constant errors in array key names.  Then he changed it.  TL;DR: stick to your guns if you can back it up. "
if you get stuck take a break and maybe even a shit,"Take breaks, meaning leave the office and go somewhere and DO NOT think about the problem you were trying to solve.  (Don't force it either though) You'll get to relax for a bit but your subconscious is still going to be sorting through all the data and the algorithms you introduced it to.  As we all know a programming solution is usually a hierarchy of different levels of solutions, and sometimes all you need to get started is for one of those solutions to be revealed.  So while you're taking a crap, or whatever the hell you're doing, that process could just randomly start, especially if you've just been sitting there programming on and off for the last couple hours.  I've had solutions to both small and big problems come to me on the toilet, in the shower, on walks, etc.  tl;dr if you get stuck take a break and maybe even a shit "
When dealing with human lives 1 mistake should not be fatal.,"A circuit breaker is the sort of manual lock you're thinking of.  Switch it off, and the apparatus ain't fucking starting no matter what.  Once, my father was working on some wiring.  He had the circuit breaker shut off, since he was up to his elbows in the cables.  His boss found that something didn't start up when he hit the button, so he came to the circuit box and flipped the breaker.  Fortunately, it was non-fatal.  Could you say the same about monster screw pumps when some moron decides they should be on?  By the time the moron has turned off 3 separate safety system, there's a good chance you'll realize he's fucking with switches and stopped him.  TL;DR:  When dealing with human lives 1 mistake should not be fatal. "
"this would inevitably make Java more flexible. Is the complexity cost of that increased flexibility desired by the Java community, or would it hurt the ecosystem?","Serious question: would something like this improve Java? I don't personally use the language anymore so I'm not concerned either way. I know that Java is used for some heavy-duty stuff some places and that proper value types could improve performance, however, I wonder if the increased language complexity could do more harm than good.  I like that C++ and Rust have only value types. It makes the languages simpler. Java is similarly simple, having only reference types, though obviously at some cost to performance. C# and D have both value and reference types, distinguished by  struct  and  class , and while I can't imagine a simpler way to support this from a language perspective I still think the languages are made more complex by it. I believe VS colours structs and classes differently, which greatly eases the burden, but D's tooling isn't as good (NB: I haven't tried the VS D extension).  tl;dr: this would inevitably make Java more flexible. Is the complexity cost of that increased flexibility desired by the Java community, or would it hurt the ecosystem? "
"The comment about storing a lifetime of video recordings in the space of a sugar cube is probably hyperbole for now, if not outright bullshit.","Hmm. Several cameras around me my whole life stored in the size of a sugar cube? Seems... wrong.  Being generous to his position, let's say you live to be the average global human age (lowball figure depending on the country you live in) of  67.2 ]( giving us [473967000 bytes/hour](  You'll end up needing 589063 hours * 473967000 bytes/hour = 279196422921000 bytes = 279196 gigabytes per camera. Multiply that by 3 cameras and you get our total storage requirement of 837588 gigabytes.  Assuming the average sugar cube follows the standard on wikipedia of  3/4 a teaspoon of sugar per cube  of volume.  So we need to fit 837588 GB into 4.46875 cubic centimeters.  We're almost there. If you believe [xkcd]( the best memory storage commonly available is microSD. The best I could find is a [128GB card]( which has the standard dimension of [15mm x 11mm x 1mm]( or 0.165 cubic cm. So using this we could fit just over 27 SD cards in our sugar cube space. Let's be generous and say 28 and we get 3584 GB of storage. No where near enough.  Now what if we use the best possible storage anyone has ever physically done? Electron quantum holography (impractical as it currently is) has been shown to have a data density of [3 exabytes/in^2]( This means we could fit about 818099568 GB in our sugar cube volume. Overkill! So somewhere between the future tech we're working on and the reality of what we have, he may be right, but right now I'm calling BS unless someone can show me a practical storage mechanism which has 234x the densityof microSD.  TL;DR The comment about storing a lifetime of video recordings in the space of a sugar cube is probably hyperbole for now, if not outright bullshit. "
Just [tell your objects to do stuff]( and provide them the dependencies they need to get it done.,"Services (or commands + handlers, if you want to go that route) should be completely self contained. The service's clients should only have to make a single method call to tell the service to do something. The rest should be up to the service. So, option B in your example.  If your service expires projects, then that probably means something along the lines of checking the date and updating something in the database. The clients of the service shouldn't have to worry about that second part. Just  tell  the service to do its thing and done.  Why? Because you -- more specifically, your service's clients -- shouldn't care that a service does its job now or later or how it accomplishes its work. Maybe expiring a project takes to long for a web request, so one implementation of  ProjectExpirationService  sticks something in the queue to do later. The queue consumer picks it up and uses another expiration service implementation to do its work. Option A in your example above doesn't let you do that. There's a coupling between the  specific implementation  of the service and whether or not a  save  call is needed later. That's extra cognitive overhead for whoever is working on the system as well as some additional complexity. On a small scale, no big deal. Multiply that times 5, 10 or 100 service objects and you're probably going to have a bad time.  > I feel #A is much better for testing since I can test the service with just PHP objects, leaving the database alone.  Not really. What's much better for testing is only having to mock a single dependency (the service) instead of two (sevice + repository). Then you can test your service by mocking the repository and probably run some integration tests with a real database backend on the repository. It's a chain: make sure each part works with fake objects until your objects start touching the outside world. When that happens add in some integration tests and finish things up with an end to end test or two that hits the entire stack (and probably only follows ""happy"" paths).  TL;DR: Just [tell your objects to do stuff]( and provide them the dependencies they need to get it done. "
you proved my point that people can't evaluate good programmers by giving an example of a really bad way to evaluate programmers.,"I did, how does that change anything?  I argued that it's a fallacy to say that only one in two hundred isn't crap and that managers are shit at determining who isn't complete crap.  You then argued that you get lots of people who don't know Java applying for Java jobs is a contraindication to that, except it's not, in fact you've kind of proven my point. Knowledge of a specific language, technology or stack is a shit way to determine whether a programmer is any good. Everyone except for some reason hiring managers knows this.  You then mentioned going for clojure next which would be wrong even if specific languages were a good idea and used JavaScript as an example to belittle my point.  TL;DR you proved my point that people can't evaluate good programmers by giving an example of a really bad way to evaluate programmers. "
I'm a noob who wants to be great and would like criticism in order to reach my goals.,"I want to also add I just started programming this year. I'm 19 years old and a business major in college. I only recently got interested in computer science because of the possibilities the world of programming has. I'm learning programming on my own at the moment and it's pretty gritty but Im taking it one step at a time. I have at least 5 failed projects in my Eclipse folder as well as many more basic programs, but this one was the most satisfying to me, because I was able to use ActionListeners which I just learned about yesterday. I hope to become a greater programmer and possibly have my own gaming company, but that's a goal later in life I have in honor of my late best friend. Anyways, tips, advice, anything will help thank you!  TL;DR : I'm a noob who wants to be great and would like criticism in order to reach my goals. "
"so people write rants about how it was totally wrong here, from assumptions about the conclusions drawn in the post that don't actually appear in it.","The sequence of events went like this:   The press release contained virtually no analysis of the data, just making claims about correlation. It would be nice to see just how correlated it is.  Curious, the logarithm of the data is virtually flat, except for one point.   Linear regression of remaining points yields a very good fit. Curiouser and curiouser.  This statistical curiosity is interesting enough to share.   Wrote blog post.  Someone submitted post to reddit, vastly exaggerating the conclusion.  People read the reddit headline, skim through the post and at a glance it seems to fit the headline.   Text was TL;DR, so people write rants about how it was totally wrong here, from assumptions about the conclusions drawn in the post that don't actually appear in it.  "
"seems to make sense, reified generics provide more possibilities for run-time JIT/optimization, but these possibilities have a cost themselves and have to be used.","> It doesn't make any sense  I'm not sure about that actually, reification is done at runtime and so are the type checks for generics, so it would make sense that reified generics have a cost. Theoretically, because more type information is conserved, the runtime would also be able to better optimize storage and access of those collections (by using backends more adapted to the exact stored type for instance), but if that's not implemented then there's no gain. Plus the checks for implementation-dispatch would add still more cost, and memory pressure to boot (with erased generics, you only have one instance of the List class in memory, whereas with C#'s reified generics you have an instance for each generic type).  FWIW I believe GHC uses type erasure, and I've seen that cited as yielding performance advantages.  tl;dr: seems to make sense, reified generics provide more possibilities for run-time JIT/optimization, but these possibilities have a cost themselves and have to be used. "
pick something that would actually be interesting to you and would help you do something.,"> I enjoy coming up with projects that involve pulling some data from the web and doing something with it.  Two python projects that I actually count as ""successes"" (in that they did mostly what I expected, and taught me a lot) were an IRC bot, and a tool that would automatically post things to a message board (I'd queue up images in a folder, it would automatically upload them, keep track of the list, and post them to a particular thread.)  Both are the type of project that can be as in-depth and challenging as you want them to be.  tldr pick something that would actually be interesting to you and would help you do something. "
"Germany is a banana republic with politics on the level of third world countries. Nothing will happen. 
 ~ a German.","This is pathetic. Nothing will actually follow here unless the bigger companies in Germany see an advantage in having actual limitations on software patents.  German politicians are completely incapable of getting anything IT related right. Also, they never fuck with the people who pay them e.g.: the industry.  Our politicians blew millions on buying dysfunctional malware to spy on random people. Needless to say, it never worked and was torn to shreds by the [CCC](  Also they keep hiring IT companies as suppliers who are widely known to screw things up. Why? Their relatives/friends are the owners of said companies. Germany would have a country-wide fiber-channel network without those relationships... nuff said.  TL;DR: Germany is a banana republic with politics on the level of third world countries. Nothing will happen.  ~ a German. "
You are making all sorts of wrong assumptions and arguments.,"> No matter how sophisticated the strategy, if you know it, you can defeat it.  That's actually a claim that the halting problem can be solved, so you're not just wrong, you're wrong about one of the most fundamental points in computation.  > Of course for both players to know each others strategy perfectly implies a draw.  How would you come to that conclusion? Player one always chooses 1, and player B always guesses that payer one chooses 1, both knows the others strategy and still there is a winner. Just knowing someone elses strategy is not enough to win. You have to both know it, and adapt you strategy to counter what you know. Now if A knows that B's strategy would be B(1) if he didn't know A's strategy, and A adabts his to A(2); then he also knows that B will know that A isn't following A(1) but A(2) and therefore adopt his to B(3)... etc. You end up with recursion, and a situation where it's even possible that there will be no possible algorithms for A and B, it's impossible for them to even start the game because they are stuck in an infinite loop trying to figure out how to play.  Anyways TLDR; You are making all sorts of wrong assumptions and arguments. "
java sucks and is 550 years behind modern languages.,"Still more verbose and still retarded like shit:  x.getName()  <- that's a fucking vomit inducing crap that makes java look like shit, I prefer real properties and a proper casing such as  x.Name  also  asList()  <- WTF is that bullshit. Totally unclear where that crap comes from, it could be  public static Dick asList()  from the  MyHardJuicyDick  class.  comparing()  <- this is totally retarded. It's fucking obvious that you're comparing when sorting. It's the most redundant thing I've heard.  and finally  List&lt;Human&gt; list =  - Yes java please make me type 400 times the same fucking type name for every single variable in the world because your compiler is so fucking retarded it's unable to handle any kind of type inference.  TL;DR = java sucks and is 550 years behind modern languages. "
version: You're book is full of crap. Use array notation without specifying the length.,"OK, first things first.  The type of a string is  not  char*. The type is char[x], where x is the length of the string (including null terminator). Corollary: a string is always an array of characters.  Now for the tricky part. C has a weird array-pointer duality. We say that in a scalar context, an array will decay into a pointer to its first element. Almost everything in C is a scalar context, so arrays almost always decay into pointers to their first element, but to assume that this makes them the ""same thing"" is intellectually lazy.  OK, now for an extra twist. String literals are read-only. So if you say,  char *ps = ""This is a string"";  OK, so the string ""This is a string"" is stored as an array of characters in static memory. Then ps (in automatic memory) is set to point to the first element of that array. You are not allowed to modify the array.  But, if instead you say,  char as[] = ""This is a string"";  instead, as (an auto object) becomes a copy of the string literal. Since it's auto, you're allowed to modify it (but note that you're not modifying the literal itself but a copy of it, not that anyone cares). Specifying the length of the array in its definition is a bad idea, unless you plan on possibly expanding the string.  One final caveat: If you are going to make lots of copies of an identical string literal on a system with a small amount of stack space, it can be beneficial to go the pointer-to-literal route.  tl;dr version: You're book is full of crap. Use array notation without specifying the length. "
why is VS compilation so disk-bound for even small projects in the presence of copious free RAM?,"Can someone explain this to me:  My 32-bit dev PC has 4GB RAM installed, 3.5GB usable.  I typically have less than 1GB in use, leaving over 2GB unused RAM available for file cache.  Why am I waiting on the hard drive (I can see the light) when I compile even a small project in VS?  I can understand why a product like SQL Server will bypass windows file caching so it can manage its own disk writes.  But my expectation is that I should only see the disk light come on for lazy writing -- when I compile a small project, why do I see any disk activity at all?  tl;dr: why is VS compilation so disk-bound for even small projects in the presence of copious free RAM? "
"In the Netherlands you'll get statistics and problem solving skills, at the cost of some calculus skills, and computers are NOT necessary for this.","Through experience and through the information of my dad (math teacher), I can say that real world examples are important. The ""Dutch"" way of teaching math to students revolves around problems. Along with very essential computation skills you do get actual problems all the time. You have to find out what is the right question and then compute the right answer. You do not only get to hear some terms but actually understanding what terms like ""deravitive, integral and normal distribution"" mean is part of the maths. You need to recognize these terms in real problems.  On top of this, statistics are very very important here on top of algebra. They've already long ago embraced the fact that academics and even non-academics have a lot more use of statistics than any other math. The calculus is nearly solely for technical studies. For me this was an inconvenience because I was going to study a technical subject but I am a minority and this disadvantage is something you can overcome. The difference between the Dutch students and practically every non-Dutch student is noticable, but it slowly fades.  However, one thing is absolutely different in the start. When posed with real problems during our projects mostly the Dutch guys/girls came up with the ideas and equations for solving the practical problems and the foreign students had noticeable troubles seeing how our equations even related to the problem. Then we noticed that we sucked quite hard at solving the difficult equations and the foreign students had an easy time. And here is where Wolfram's point is of interest. We might as well have fed our equation into a computer. The other foreign students would have taken much longer and would have needed much more assistance in solving the problems despite their math skills. They will eventually catch up to this too, but I figure it's not as easy as simply practicing solving equations over and over again. Where Wolfram is wrong though is that you need a computer to teach like this and you should do even less computations. The computations are still important and if you are interested in another way to teach match look at the Netherlands.  TL;DR - In the Netherlands you'll get statistics and problem solving skills, at the cost of some calculus skills, and computers are NOT necessary for this. "
"kid pix is  facking m-azing . entertaining read. Now if only I, as a programmer, could create something amazing and successful ^^","This is actually really awesome. I had played with previous versions on school macs, and even have an old Apple 2 lying around with it on it. Imagine my delight when I discovered a couples copies of Kid Pix Deluxe 3 (Windows Color Edition!!!) lying around in some educational institution (I forget, but I got it for free. And no, not on the internet, on a real cd). I had a lot of fun with it, and got to show my younger brother it as well. Truly fun stuff, especially playing around with stamps. I think it's what got me pretty good at shops and interested in drawing.  tl;dr kid pix is  facking m-azing . entertaining read. Now if only I, as a programmer, could create something amazing and successful ^^ "
"I found both to have a learning curve(iOS: Syntax; Android: Lifecycle/timing) , but if done knowing what the OS provides, both can create very good applications.","I found both had a learning curve. For iOS, it was largely due to the syntax (didn't know objC or Cocoa until I started). Android just required I understood their intricacies (for instance ensuring layouts conform to different screen sizes, which I have never had a issue dealing with). Some things definitely don't ""just work"" as I have had rage quits on each platform. Both required that I took the time to understand what each OS was able to provide.  One thing that I do really like in iOS development is the Interface Builder, and also how there are methods that are in place for orientation changes. The other thing is that iOS feels very mature as it has a lot of synergy with OS/X.  To address the ""thrown together"" portion. I would fault this more to the developer rather then the OS in general. If you understand what you are working with, then you should be able to create a good user experience on the application. I find some apps that I get from the market that I don't like come from people that for whatever reason feel that iOS and Android apps should look identical. I prefer to have a general theme of each that are consistent (icons, type, organization, etc) while allowing for the app to have the look and feel of the OS.  TL;DR I found both to have a learning curve(iOS: Syntax; Android: Lifecycle/timing) , but if done knowing what the OS provides, both can create very good applications. "
On windows Flash is just as good or better than HTML5. On Macs it isn't but the blame lies on Apple.,"Flash player uses less CPU when playing video than HTML5.  More backing evidence.  >When it comes to efficient video playback, the ability to access hardware acceleration is the single most important factor in the overall CPU load. On Windows, where Flash can access hardware acceleration, the CPU requirements drop to negligible levels.  >after noting significant playback efficiencies in Flash Player 10.1 on the Mac, respected technologist and AnandTech founder Anand Lai Shimpi commented ""with actual GPU-accelerated H.264 decoding I’m guessing those CPU utilization numbers could drop to a remotely reasonable value. But it’s up to Apple to expose the appropriate hooks to allow Adobe to (eventually) enable that functionality."" So it looks like the ball is in Apple's court.  >Apple complaining about Flash being a CPU Hog while not exposing ""the appropriate hooks"" to enable Adobe to access hardware acceleration seems disingenuous at best.  TLDR: On windows Flash is just as good or better than HTML5. On Macs it isn't but the blame lies on Apple. "
"Cheers for the fix, I look forward to trialling it. But please resist the urge to be yet another abrasive internet guy in a FOSS project.","Well, as a ZSH & Git user, I applaud your efforts and will be examining them with interest. I strictly avoid Git completion for the exact same performance reasons as you.  However, you had a stoush with the ZSH devs and linked to the mailing list with (faux?) outrage at their clinging to purity over pragmatism.  I feel I need to point out to you that [the tone with which you begin the discussion]( with the ZSH developers was one that would easily cause offense, and I found it to be rude.  You had a valid contribution to make, but you got their backs up by being rude, and then you erected a strawman about purity, when the  email you link to explains why they're rejecting you :  > No.  I find your attitude offensive, not your intended meaning.  TL;DR - Cheers for the fix, I look forward to trialling it. But please resist the urge to be yet another abrasive internet guy in a FOSS project. "
"doing the simple thing is  hard  much of the time, applying principles/patterns is easy once you adopt the mechanics.","> I can't see the wood for the trees - the domain logic is hidden behind hundreds of lines of patterns.  This is the bane of patterns, and the key is,  always , moderation.  Looking at the  examples in Fowler's articles about SOLID, they  do  help exposing the domain logic. But they are always applied to a simple situation, and one principle is applied to help, that's how examples are supposed to be made.  But when confronted with real life, things change to the old ""I wanted to write a short letter, bit I didn't have time, so I wrote a long one instead"".  tl;dr: doing the simple thing is  hard  much of the time, applying principles/patterns is easy once you adopt the mechanics. "
"if you're having trouble keeping up with the latest tools and whatnot, you're focussing on the wrong things.","While new software and frameworks pop up everyday, barely do they introduce revolutionizing new technology. In many cases, it's the same ideas and technology in simply another jacket. Look at how many programming languages there are for instance. You might be overwhelmed, but when you look at them, in the end they're always the same with just some syntactic differences. When you see a new language, you can bet your ass it's either imperative, OO or functional; and if you know these, you pretty much can learn the 'new revolutionizing language' in a day. In fact, you probably don't even have to 'learn' it. Just like it's nonsense to 'learn' a new framework or API. When you know what you want, you can simply look it up in a matter of seconds. You can't memorize everything.  Another example are all these package managers. There's a million of them for linux: pacman, yum, apt-get etc. Then there's npm, apm, chocolatey, cabal, that one thing for ruby, etc. In the end however, they're all resolved around the idea that you have a central repository, and you can fetch the latest versions easily. Their syntax differs slightly, but if you know one, you know them all, and you can easily use the rest by just looking up the --help command.  TL;DR, if you're having trouble keeping up with the latest tools and whatnot, you're focussing on the wrong things. "
Most well structured Javascript code can be inferred and type checked.,"Classical OO languages are a terrible example of a good type system. If you think ""Java"" when somebody says ""Static Typing"", then you're missing out on  decades  of  advancement in type systems.  ML-family languages have had supernaturally good type inference for a long time. People use these languages in production on a day to day basis.  SML and Purescript (among others) both implement anonymous structural types that effectively do ""Duck typing"" at compile time. i.e.  val bob = { name = ""Bob"" }  Even more relevant - Facebook have also recently announced the tool ""Flow"", that implements a very comprehensive type system on top of normal Javascript. It can infer structural types, and perform flow-typing. At ""compile"" time, not runtime.  tl;dr Most well structured Javascript code can be inferred and type checked. "
"If your understanding of static typing is ""Java"", then you're really missing out on some really powerful tooling.","This is a very common, and completely incorrect misunderstanding about Javascript.  Javascript has a perfectly valid types, and they can be inferred and checked at compile time for a majority well-formed code. There's tweaks to the language that would need to be made to really do it properly, but it's not far off today.  There's certainly things you can do in Javascript that inherently cannot be type checked. But you can typecheck a lot more than you would think, and fall back to runtime type checking for the edge cases.  In terms of existing compile-to-js languages that you can use in production  today , Purescript is a pretty good example of structural typing at work, and Opa - although I haven't used it personally - appears to do a good job as well.  Java is not a particularly good language to use as an example of static typing. It alone is probably responsible for an entire generation of programmers completely misunderstanding what a good type system can be.  tl;dr If your understanding of static typing is ""Java"", then you're really missing out on some really powerful tooling. "
"I think his comments are baseless, he doesn't use the software often and thinks everything he doesn't use when he uses Vim is useless.","> Code folding is overrated  That's your opinion, useful to tidy up large files.  >clearly wasn't included in the original design  Yeah fuck add-ons I like my software default, changing changes or add-ons means I am using it wrong. OH you say you use Emacs, no you have confused me.  > kludge all the way through to the keystrokes  Ctrl+w + [another key] does all the split window managing, hardly kludge.  >they're not necessary if you only use the keyboard  Some people like them + you can navigate tabs with the keyboard any way.  Ultimately it sounds like you have barely used Vi/Vim I make this assumption based on this statement.  >I learned to use it properly, navigating with the proper commands and h, j, k, l.  If you think have learned to use Vi because you use the proper navigational commands, you're seriously mistaken. Sure let us all comment on stuff we barely have a grasp on.  tl;dr I think his comments are baseless, he doesn't use the software often and thinks everything he doesn't use when he uses Vim is useless. "
"this is a reasonable solution to a stupid problem caused by nosql style storage, blog author is a troll or stupid.","I actually worked with the founder of BoxedIce for some years. Irrelevent point, but lets just say that he's not an idiot. While this might seem like a completely silly thing to do it actually makes a lot of sense if you do use nosql style stores.  Lets assume you're using a document based nosql storage platform, you're going to store all the field names per document, and as much in RAM as you can for speed (the nosql in question, mongodb, just mmap()'s files). If you're planning to scale to up to say, a billion documents, then you actually DO have to worry about an extra 15 bytes per field per document after they're BSON encoded if you want to actually have effective indexes. If your fields are, for example, smallish integers, having a field name with each int that's 4 times larger than the int itself is even more stupid.  Obviously. having fields called tA, tB etc. is seriously irritating and error prone to work with, but assuming you have a sane interface library you'll be remapping these values into sane names at quite a low level. You should have some kind of interface library anyway with open schema storage formats to stop a typo arbitrarily creating fields with typos in and other silly by-products of nosql schemas being very flexible.  The actual saving for 10-30 bytes per field name per document when you take into account extreme numbers of rows, planning for future scaling and production server hardware is a lot LOT more than the completely arbitrary $0.05c figure the trolling blog post author made up. BoxedIce are UK based anyway :)  If you stuck a billion rows into MySQL (which is certainly possible with enough hassle, tweaking, sharding and clustering) you'll almost certainly have similar ""stupid workarounds"" for the myriad of problems MySQL would throw in such a deployment. (N.B. ""billion"" figure taken from the current amount of data stored at boxedice on mongodb).  tl;dr: this is a reasonable solution to a stupid problem caused by nosql style storage, blog author is a troll or stupid. "
"FreeBSD: kernel is great, userland and infrastructure sucks. (But that does NOT mean the horrible KFreeBSD Debian abomination is a good idea)",">I've learned the quirks, and the ports system beats the binary installers most distro use.  Really? The outdated userland overall, and especially the crappy ports system actually drove me away from FreeBSD. There haven't been any notable improvements to the ports system framework since FreeBSD 4! It's still based on  make , still horribly slow, still needs various external utilities like  portupgrade  for management (and all of these don't really work well, IMO) and feels very kludgy to use. Same for the installer, sysinstall, but I heard they've been working on a new one. I gladly would take the minimalistic OpenBSD installer over sysinstall bullshit.  The software selection shipping with the base system also needs a lot of work. I mean,  tcsh , seriously? Who is really using that nowadays? Just give me  ksh .  TL;DR FreeBSD: kernel is great, userland and infrastructure sucks. (But that does NOT mean the horrible KFreeBSD Debian abomination is a good idea) "
"16ms every other frame is better than 9ms per frame. 
 * I use the DMD version because I assume Ingrater's above comment is referring to that version.","That's not the case at all. GC takes some amount of time relative to the number of frames that need to be collected.  Let's play with some numbers using the DMD version*.  Time per frame is 14ms - 9ms for GC = 5ms of other work per frame.  That gives us a theoretical framerate of 196fps w/o memory management.  So if the garbage collector is left to it's own measures, it runs once per 10 seconds, or X = 1960 frames.  If it then runs for 3 seconds, the GC has an  amortized  cost of ~1.5ms per frame.  Clearly, there is a tradeoff to be made.  We can see that manual memory management takes roughly 2ms.  Let's estimate that the GC takes 5ms to scan live objects and 4ms scanning/freeing dead objects.  Thus for X = 2, total GC time will be around 5+4+4 = 13ms, lowering the amortized cost to 7.5ms while still offering totally acceptable worst-case performance.  I am confident that you could further adjust X to yield better results, especially considering most frames are very similar to the previous frame.  tl;dr 16ms every other frame is better than 9ms per frame.  * I use the DMD version because I assume Ingrater's above comment is referring to that version. "
"multiply your first intuition x 3, and make them suffer for feature creep","Start by estimating how long it would take you to implement all the requirements. Then make an explicit condition that you based your estimate on these specific requirements and that any change would reflect in the estimate. Then add 50% for changes of the requirements that they expected you to have based your estimate on, even without specifying them. Then add 50% because there is always something stupid that takes much much longer than you expected. And then you add 50% to manage expectations in such a way that you always finish before everyone thought you would, thus becoming a hero.  tldr: multiply your first intuition x 3, and make them suffer for feature creep "
GC in D can be improved to leak less but can hardly become much faster.,"Rainer, author of VisualD, made an ""almost precise"" version of GC (it's used in VisualD itself) where at least for the heap for each allocated word there is a bit in a separate bitmap saying whether it's a pointer or not. So while stack scanning is still conservative, this gets much closer to precise GC and leaks significantly less, at the cost of some reduced speed.  It seems possible to make non-moving generational GC (there is some literature about such GCs) that will not change data and so can work even with not-precise GC. However generational GC needs write barriers (either as additional code generated by compiler or as clever usage of memory page protection to mark some pages for scanning) and D folks will never agree to allow that, so GC in D will never be generational and hence will never be fast.  tl;dr: GC in D can be improved to leak less but can hardly become much faster. "
"Regexes -> Kernel Compiling -> ""THERE IS AS YET INSUFFICIENT DATA FOR A MEANINGFUL ANSWER.""","Compiling kernels is a hell of a gateway high. If you're not careful you'll wake up after a binge and find that you've written a lightweight kernel module for intelligent memory leak detection and auto correction in brainfuck. The road from that point is either to support groups or madness where you attempt to rewrite the entire kernel in LOLCODE and then get the bright idea to create a script that does it for you. If you take the first road you will eventually recover, but it will be hard. If you take the later road you'll wake up one day in the middle of a server farm with kernel hackers standing around you reciting a chant in the old code so as to summon the Cosmic AC back into this universe. tl;dr: Regexes -> Kernel Compiling -> ""THERE IS AS YET INSUFFICIENT DATA FOR A MEANINGFUL ANSWER."" "
make a determined effort to do things you're not doing now. And watch yourself grow.,"A bit late, and I'm not a great programmer, but I'm on the path to becoming one, and I think my little experience may help.  You want to be an intermediate programmer? Do intermediate programmer stuff. I'm a djangonaut (although I use Flask, and I still do for small projects) but all I ever did was small learning projects...until a few weeks ago.  I read 2scoops of Django, an intermediate book of best practices for Django developers and learned to do things like separate my requirements and settings files into local and production and write tests, things the beginner in me just wouldn't do. I learned some of the best ways to organize project files, use relative imports and host on Heroku.  After this, I learned to host my media files on Amazon S3 and I'm now learning to host my websites on AWS. All stuff I didn't know how to do before. The trick was, I knew what my other advanced colleagues were doing and learned them, so I could be a better developer.  You're probably more advanced than I am but I'd suggest writing a Flask extension...no matter how trivial it seems (but make it relevant) and maintain it. That will force you to write better code and make you more familiar with the kind of code contributed to Flask itself. And you'll be giving back to the community too.  One thing I know Flask developers do is to write boilerplate code for quickly setting up projects. You could do something like that and get others to contribute. You'll be helping the community and leading a project that will force you to do ""intermediate stuff"", if you get what I mean.  tldr; make a determined effort to do things you're not doing now. And watch yourself grow. "
"I don't think Java is a ""weird, lazy choice"".","Exactly.  But for one of you, the cool heads, I'd still get nine guys telling me their favourite language's name here, before I even finished asking that question.  And for some reason you too called Java ""weird, lazy choice"" before, prompting me to try this trap. Is it? Java is new COBOL and the de facto standard of the industrial/corporate world. JVM is everywhere and can now be programmed in a bunch of languages, providing nice paradigm cushion and common language runtime (LOL). For these languages Java is the semi-assembly language of the JVM.  And yeah, there are things that will get you close to metal, will provide decades of math discoveries implemented in very optimized way, or a way to formally prove your program is correct when it compiles, when this is what you need. And indeed, there are also new, exciting things. But there is no shame in Java. Java works and has pretty wide and field-tested library ecosystem, JVM lets you get things done and deliver. It's also present on every box I touch these days. And it's not going away.  tl;dr I don't think Java is a ""weird, lazy choice"". "
I would argue (somewhat simplistically) that it's not AI without an aspect of learning involved in the solution.,"The very term ""AI"" fell into disfavour years ago, after years of promising spectacular results and delivering very little. These days people talk about Machine Learning much more, and are usually more realistic about the promises they make.  (Yes, there are other names and related fields.) Some Computer Vision approaches could be seen as AI (for example, when combined with techniques from Pattern Recognition).  And others (such as patch finding here) are straight image processing.  As per your definition, the ""ability to  acquire  and apply knowledge"" is the key part.  Hard problems may require complex solutions, but that doesn't make them AI.  It's just the application of skills from the developer.  TL;DR: I would argue (somewhat simplistically) that it's not AI without an aspect of learning involved in the solution. "
"A company had connections and know-how on IBM technology, but decided to buy from Oracle. For a project that could have lived with flat files (at this stage).","There was once a project which just had to collect some data from a AS/400, process it, and display a web page. Easy enough to just use flat files. But of course you never know in which direction such a project can evolve, so you certainly want to use a database.  My suggestions were: PostgreSQL (because we had know-how and is free), DB2 (in case customer absolutely wants to spend some money on a big one and the AS/400 has a DB2, too, so it should be easier to let them work together). At the meeting we were told they already bought an Oracle DB for the server.  tl;dr: A company had connections and know-how on IBM technology, but decided to buy from Oracle. For a project that could have lived with flat files (at this stage). "
"PostgreSQL is greased lightning compared to MySQL, and I've been very happy with MySQL for years, but will implement PostgreSQL in the future.","I've been running a fairly high traffic cluster for a few years now, with MySQL.  Recently, I've been working with something on my local computer that involves a fairly large database (30 million row tables being joined). I initially implemented this in MySQL and was having major slowdown issues. After some twiddling with the SQL, I think I got it down to a 20 second query.  Anyway, I ended up having to switch where I was pulling my data from, and they happened to be using PostgreSQL. So long story short I switched to PostgreSQL for ease of importing data and holy shit is it awesome.  TLDR : PostgreSQL is greased lightning compared to MySQL, and I've been very happy with MySQL for years, but will implement PostgreSQL in the future. "
ranting about work but actually do like the idea,"As much as I'd like some nicer shorthand, my experience with the sort of code people actually write with that type of thing makes me worry a bit. People seem to be very, very keen to cram as much stuff on one line as possible, and it can lead to some unreadable code.  I sometimes see up to six nested ternary operations on one line. Then one day there's an error on that line. A method call on a non-object. The only way to figure out which operation caused it is to refactor the whole thing into the 20 lines of code it should have been all along, which takes a while. And now just because I've added a null pointer check that should have been there from the start I've got my fingerprints on somebody's horrible code because I've done such a hefty refactor. And next time it breaks I'm guilty until proven innocent. All because one of my colleagues is in love with writing this sort of 'concise' code.  tl;dr ranting about work but actually do like the idea "
MVC with a HTTP Request / Response framework is nice.,"You're doing great, keep it up. There is no reason to think you aren't ready now. Companies want self-starters who can learn and you've shown that much so far.  I am currently developing a Symfony 2 website and hopefully my tips will help. Here's an [example]( of the basic symfony 2 structure.  In this example the UserController.php will have a LoginAction and ChangePasswordAction. Example.com/u/login and example.com/u/change_password will route to those functions. In the app/config routing file you would configure Example.com/u/* to route all requests to the UserBundle. In Reddit/UserBundle/Resources routing file you would route /login and /change_password to their functions in UserController.php.  When the LoginAction request is a GET it will display the Login.html.twig view and if it's a POST will validate the users information, set session info, and redirect to the account page or if there was an error show Login.html.twig with the error message.  Going from traditional PHP to a full stack framework can be confusing at first. What would have been 1 file, Login.php, becomes 4; 2 routing configurations, the controller and the view. Once you become familiar with it you'll appreciate the well organized structure and the separation of concerns.  In regards to jQuery/AJAX, definitely focus your studies around PHP, HTML, and CSS. Well designed websites don't rely on Javascript to function only to enhance them.  tl;dr MVC with a HTTP Request / Response framework is nice. "
"Feminism"" is not the cause of whatever grievances men have about treatment in the legal system.","I am familiar in a general way about complaints men's rights groups have about how divorce law, sexual assault law and other domestic legal issues seem unfairly biased towards women.  While I agree that changes in the law have not always and everywhere kept up with changes in society (for example, alimony laws are slowly changing to acknowledge women's increasing ability to become breadwinners) I don't agree that this is the result of anything like ""sexism.""  And, most importantly, I strongly disagree that it is correct to lay these grievances (even when they are justified) at the feet of something called ""feminism.""  When most of the laws and legal precedents upon which our current family law system are based came into effect (in the 1960s, 70s and 80s) women were a very small minority in the legal world as lawmakers and law influencers.  And, the influence of feminists in particular on the formation of those laws is small.  Tl;dr ""Feminism"" is not the cause of whatever grievances men have about treatment in the legal system. "
"C# is led by microsoft, but it's in no way microsoft only.","It's true that not many of those other ones are complete and alive, but mono is very much alive, and very much complete.  I also didn't say ""many besides Mono"", I said ""many like Mono"" (generally quotes are for exact quotations, use brackets and dots inside if you are changing the statement). What I meant was more of Mono, and all of it's many implementations, like MonoDroid, and MonoTouch, and it's tools like MonoDevelop. It's a completely mature platform to build programs on, and it's a drop-in replacement for microsoft's versions minus a very small set of features that are either completely unused, don't map to other platforms, or are too new.  Also as mentioned, microsoft's tools are freely available to students, and express is available to all free (subject to a few conditions, depending on the version).  TL;DR; C# is led by microsoft, but it's in no way microsoft only. "
"if you never built a 4-bit computer using raw minerals after proving its logical correctness with the latest in bleeding-edge physics simulations, what did you ever understand?","To say that complete understanding was ever possible is blatantly missing the point of digital computing -- abstracting away the pain-in-the-ass process of analog modeling by setting very simple circuit constraints and allowing logic elements to completely ignore the implementation of other logic elements so long as the represented data gets through unscathed. Nobody ever needed to fully understand any computer in common use outside of a laboratory -- they would never get very far otherwise. Or, to put it more simply: if the computer requires a complex hardware understanding to program, you're dealing with a shitty computer. If your software framework requires a complex understanding of its inner mechanics in order to use correctly, you're dealing with shitty software.  Note that PC hardware has been absolutely terrible (because we trusted the people who focussed on obsolete 16-bit interfaces for years to build proper firmware) and software frameworks develop such legacy inertia that good design often seems to be a rarity. That the title is ""A Complete Understanding is No Longer Possible"" rather than ""A Complete Understanding is No Longer Necessary"" betrays a subconscious admission of the failure of software engineering, I'd think. But it doesn't really mean anything about understanding as it ever was.  tl;dr: if you never built a 4-bit computer using raw minerals after proving its logical correctness with the latest in bleeding-edge physics simulations, what did you ever understand? "
"It may be that haskell seems nicer because the people inside are less annoyed by the questions because the questions don't come from raw, beginning programmers.","They definitely are friendlier, but I don't think that is because the C++ people are meaner or the Haskell people nicer.  If you spend a lot of time in places ##c++ and such, you realize that questions are being asked over and over again and this just causes frustration among the channel regulars.  Even the nicest regulars will get frustrated sometimes. For example, lurk in ##c++ and you see that every day it seems there is somebody asking about the proper way to convert a string to an int.  Hang out in haskell, and the questions typically aren't revolving around concepts like that because practically no one starts with haskell as their first language.  The new haskell users are typically people with experience in another language already.  tl;dr  It may be that haskell seems nicer because the people inside are less annoyed by the questions because the questions don't come from raw, beginning programmers. "
It's the  people  that make an education -- most specifically your own person and the attitude you take to it.,"There is something to be said for being an impressionable 20 year old learning the same things as a whole cohort -- staying up late to study or finish an assignment, or arguing in the student lounge about favorite languages, struggling over the same homework assignments, hearing your professor relate the point to you in a personal anecdote -- there is a lot more to a university education than what goes into a university textbook. It's why many top universities have open course ware. The trick is the entire environment, not just dry text of a lesson.  Another difference is the mentality. Many of my class mates went into very different professions, like lawyer or industrial designer. If all you want is to be a programmer, you can effectively limit the depth of your education. If you what you want is to learn because you're curious, and new things feel magical, then you can pick up whatever book you feel like reading regardless of whether any given CS program would consider it central.  tl;dr It's the  people  that make an education -- most specifically your own person and the attitude you take to it. "
"it's because too many people selfishly take without giving that help is made explicit, often too late. then people are suprised when things die.","another community that isn't the first to offer FOSS and is in need of help because too many people take for fr without helping them stay alive.......who's fault is that?  do you blame the dev's because thy never made their efforts sustainably bullet-proof, or do you hold responsible the people that take without giving?  if you treat people like assholes and kids, then that's what they'll be. you shouldn't have to ask for help, interested people should already know what's happening in the community because they  are  part of the community.  if your kitchen's on fire, you don't think  ""oh i'm fine, i'm in the bedroom"" .  if you're part of it, look after it.  this guy is surprised that MAME needs help, as if he's an outsider. yet he cares enough to make himself a custom cabinet for his long-term pleasure of all things MAME.  if less people selfishly gorged on the free-lunch offered, and instead offered to help, then maybe more people could enjoy free-lunches. maybe we could all have lunch cabinets.  it's like citizens complaining about high taxes and wanting free state services. they don't want to pay anything, but they think they should all get free lunches.  it's because of people that selfishly take without giving that help is made explicit, because there are too many selfish people that take without any concern for what it cost to make the product. no one cares, so this is why we have situations where dev's are forced to make demos, make crippled time-bombed products, ask for donations, micropayments, and all sorts of creative ways make sure they get the resources they need to continue doing what they love......  ..if only there weren't selfish assholes that take without investment, then the dev's wouldn't have to resort to these measures and they could take invest even more time and effort into great products instead of trying to be their own fundraisers.  TL;DR it's because too many people selfishly take without giving that help is made explicit, often too late. then people are suprised when things die. "
"This is not HTTP specification issue, this is an API design issue.","Oh my, people quoting 100 year old RFC standards left and right without any argument of their own.  Anyways, ''Violation of HTTP' thing is silly. It's more of a bad API design as /blogs/last is not an end-point that should support DELETE.  I agree that blog post in question is ridiculously hard to follow, for API design you can either quote RFC's all day or simply define static resources that can take DELETE and return 20x if they were deleted or do not exist because the API client will not care if resource was 'really' deleted or does not exist and was deleted.  Typical API flow is that in order to delete something you must obtain data that you are looking to delete.  TLDR: This is not HTTP specification issue, this is an API design issue. "
"Vanilla JS (focus on Objects), then basics of jQuery so you can use it, then MVC framework of your choice","I would continue learning vanilla JS, paying particular attention to how to create Constructor functions and how JS Objects work.  Those 2 things will be vital when working with any of the MVC frameworks.  Once you feel you have a good understanding there, I would learn the basics of how jQuery works.  Most important though, keep in mind that jQuery is best looked at as a DOM manipulation tool for extracting browser differences away, so don't get so hung up on learning and using it that you don't remember raw JS.  It's easy to go down the jQuery road and have it significantly affect the way you look at and write JS, which many people then have to unlearn when working with the MVC frameworks.  The you can work on the basic MVC style frameworks.  I would pick one and use it, regardless of ""complexity"".  Later on when you understand one pretty thoroughly, you can try using the others and the knowledge from the first will help you pick up the other, and you'll have a better understanding of the differences between them.  tl;dr: Vanilla JS (focus on Objects), then basics of jQuery so you can use it, then MVC framework of your choice "
"you are correct, but only insofar as it would affect old negative submissions, which would remain buried after a recalculation anyway. It does  not  affect the ""top"" algorithm.","First, to address the score: score(ups, downs) = ups - downs. What you're probably referring to is the  rank , which is algorithm(score, date).  Old algorithm:  order + sign * seconds / 45000  New algorithm:  order * sign + seconds / 45000  Where order is  log10(max(abs(up - down), 1))  and seconds are the seconds passed since 2005-12-08 time 07:46:43.  On positive submissions sign = 1, therefore the new algorithm will not impact those. There is no need to recalculate old scores:   the old scores that were negative are already buried   old, negative scores will still be buried even if recalculated under the new algorithm   the ""top"" algorithm does not use the rank of the submission, but rather the score, which still uses the same algorithm of ups - downs.    It seems pretty clear that the original submission was a bug: the  hot  ranking is supposed to falloff with time  or  score. The old algorithm falls off with time  xor  score -- if score is negative the rank increases as we go older.  Credit to /u/someone_smarter for  their great matlab plots .  TL;DR: you are correct, but only insofar as it would affect old negative submissions, which would remain buried after a recalculation anyway. It does  not  affect the ""top"" algorithm. "
"without frequent commits and merges to a shared branch, it is neither continuous nor integration, so you reap few of the benefits.","CI in a distributed version control system means my code and your code don't get integrated until many hours/days and many commits later. The fact that everyone seems to think DVCS also means ""feature branches by default"" makes this problem five times worse, because even when I push it I still don't know if you or I broken anything until we merge.  And since I get your code in great big chunks I don't get to witness your masterful refactoring job in a way I can learn from, or tell until it's too late if you even know how to refactor.  TL;DR without frequent commits and merges to a shared branch, it is neither continuous nor integration, so you reap few of the benefits. "
all that has changed is that the glut of new capabilities in linux means that even experienced linux users again have to face what new linux users always have.,"That particular error message needs an upgrade; I just lost a lot of time on that myself.  I had polkit running.  I had a custom rule for polkit to allow users in the storage group to mount usb without prompting.  I had my user added to the storage group, which was it's own google adventure, as it was an ldap user.  I still got that error.  It turned out that my ldap settings in pam.d were one line too high, and so the systemd session wasn't being saved during sign-on.  Polkit relies on this, somehow.  So after far too many hours, moving that single line made it work the way it should have.  Of course, I don't think this means linux has ""lost its way.""  It's had an explosion in capability recently and some of these capabilities interact in complex ways.  Do they need better documentation?  Absolutely!  Do they need some re-design for simplicity and coherence?  Possibly.  But i think that anyone who thinks linux wasn't  always  ""code first, document later"" needs to update their rose colored glasses binary.  TL;DR: all that has changed is that the glut of new capabilities in linux means that even experienced linux users again have to face what new linux users always have. "
The highly vocal scam artists have spoiled the term SEO.  Sorry.,"The trouble is that two distinct groups of people are calling themselves the same thing.  One group is advocating clean, accessible design that allows search engines to accurately identify relevant content on a website, which, over time, will lead search engines to rank the page higher in the search results.  The other is advocating astroturfing, linkbaiting, linkfarming, and spamming to increase the rank of a website in the search results, without regard to the impact of that behavior has on the reputation of the site's owner.  The problem is that the second group, the ones who are spamming everything, well, they're spamming everything.  They've got  way  more visibility and they use the term SEO all over the place.  The 'good' kind of SEO companies, generally speaking, aren't doing that, and therefore aren't visible to people who aren't in the market for SEO.  That's why most people hear ""spammer"" when we read ""SEO"".  To put it another way: There's lots of advertisements on TV for companies that buy gold scraps: Cash4Gold, Gold4U, SellGoldNow, whatever.  These companies are scam artists.  You can take your gold scraps to any jeweler and they'll give you a fair price for it, but they don't advertise that.  If the high-class jeweler ""Snobby Jewelery"" started advertising their gold-buying service under the name ""Snob4Gold"", everyone would think it was a scam too.  tl;dr: The highly vocal scam artists have spoiled the term SEO.  Sorry. "
"There will be no expectation of privacy from governments, other corporations, or your cloud vendor.","I completely have to agree... just look at the whole AT&T domestic wiretap program. Any one who thinks that their data will be safe from scrutiny by whatever group is in power is living in a place I like to call la la land...  [this article lays out the problems with Privacy and Cloud computing- pdf](  The largest concern is the Legality and how Privacy changes once your data is offloaded to a third party. Your rights are not the same as on a local desktop.  I have dealt with the emergency of Privacy laws at the strictest level working in the health care industry. Imagine if the rules of HIPAA applied to every piece of data you shared or dropped in the cloud, talk about a nightmare...  The problem works out like this:  The people who hold the purse strings of congress are the people who want to know when the last time you used cotton swabs to clean your ears....  Law enforcement wants the right to search through every piece of data on you the moment that someone makes any accusation against you...  the Music and movie industry wants to know every piece of media you own so it can pursue those who do not follow their copy protection rules...  There are more entities concerned with having your information than there are those who wish to protect it.  Mark Zuckerberg from facebook has openly said [""the age of Privacy is over""](  Google founder and CEO Eric Schmidt said “If you have something that you don’t want anyone to know, maybe you shouldn’t be doing it in the first place,” A video of the interview is available [here]( and [here](  Privacy in the 21 century is going to blow past the 4th amendment like it was not even there because there is money to be made and crimes to be solved by those who know how you think.  Put it like this If the morality police of Iran or the Religious right wing of the USA or the Draconian government of China had the opportunity to dig through their citizens data to determine if they were following prescribed moral code do you really have any doubts that it would happen? To borrow a phrase from Sarah... You betcha!   TL;DR... There will be no expectation of privacy from governments, other corporations, or your cloud vendor. "
"Herp derp, I know nothing about software or security but criticise it anyway!"" <:-)","We've had cars for hundreds of years, but they still crash and people are still killed in them.  The bit you're missing is that Javascriupt does exponentially more than it used to, and with any advance in functionality you open up  new  attack vectors that weren't possible before, that need to then be discovered and fixed.  ""Secure"" in computing isn't a static target - it's a Platonic ideal that you'll never, ever reach, but you strive to get as close to as reasonably possible.  You could make a system as secure as possible, but it would also be completely inflexible and practically useless - by making it more flexible and functional, you inevitable make it harder to secure.  So saying ""Javascript still has security holes after 15 years"" is really only the same as saying ""Javascript has got consistently more powerful and advanced for the last 15 years"".  Plus, even if you locked down a system to the point it's practically useless, as things like Buffer Overflows showed us  entirely new classes  of attack will be discovered that nobody knew about before, and that your locked-down system is still vulnerable to (in this case, because of the design of the compilers that produced the code that runs your site - not even anything you personally did wrong).  TL;DR: ""Herp derp, I know nothing about software or security but criticise it anyway!"" <:-) "
"you are completely correct, nobody outside my family gives a shit about me","Growing up, my family moved around a lot, like I never stayed in one city for more than 5 years (and typically more like 3).  This persisted through college and now in my chosen profession as well.  The thing I envy the very most about people who never moved around a lot is the fact that they have several local friends who they have known for decades, who are basically as good as family.  Currently there is nobody within a thousand miles of me who I have known for more than a year.  Furthermore, since I will undoubtedly be moving on in a year or two, nobody with whom I am currently acquainted would rationally be willing to make any kind of sacrifice for my sake (which is pretty much how I define friendship).  tl dr; you are completely correct, nobody outside my family gives a shit about me "
I say 'fuck yes' but UX that shit before any VC demos.,"It is totally awesome actually; as someone who has learned what little C I know from hacking other peoples' academic python extensions, I can immediately get the top-down view that I've been unable to pick up on my own -- but please please please slap some UX on it, for the love of fuck, or cajole a designer into doing the same. Looking at the staccato ceaseless literalism of all the 'insert' labels makes me feel like a crazy person, plus the fact that the word 'insert' has no intrinsic meaning  qua  the C language is, like, potentially confusing to the very C-neophytes your interface paradigm could otherwise maximally assist.  TL;DR  I say 'fuck yes' but UX that shit before any VC demos. "
"Buzzwords are bad, and they will hurt someone who wants to be a good programmer.","Programmers should not memorize complexities like buzzwords for two reasons.   They form a false intuition about what big O notation implies. Everybody should know the  precise  definition of big O, because it's simple math.   They lack the understanding of why the algorithm has the complexity it does; why is merge sort n log n? Can you prove it? Can you analyze a recurrence? Why is depth first search O(m + n)?    Memorizing an algorithm's complexity and parroting that it's better because of it can often burn you. The simplex algorithm for solving linear programs is of a worse time complexity than the ellipsoid algorithm, but only through understanding how the algorithms work and apply can you see why simplex is often used in practice.  tl;dr Buzzwords are bad, and they will hurt someone who wants to be a good programmer. "
"borders suck. Also, double check everything in your bag before traveling anywhere, because the stuff you leave in there accidentally can be used against you.","> Heard of people being reject for just having an updated copy of a CV on their laptop.  I once accidentally left a pre-filled out IRS form in my travel folder (!!) because I was doing some remote work (GSoC) and travelling to the United States to visit some friends from my old employer. I thought that because I was passing through the U.S. while doing this programme that I had to pay taxes while I was over there.  I realized a few days before departing that I would obviously require a work permit. So I quickly contacted the GSoC organizers, explained that I made an error and submitted some paperwork saying that I just wouldn't work on the project while I was there.  When I was in line at customs I suddenly realized that I had left the IRS form in my travel folder instead of the new ""declaration"" that I wasn't going to do any work. By the time I got to the counter I was noticeably panicking and got pulled aside (which happens like nine times out of ten for me now). First thing they pulled out was the IRS form of course.  I don't even know how I managed to explain myself out of such a massive red flag as that one. I think I ended up showing them some email correspondence to the contrary and they could clearly see that I was trying to do the right thing but made a massive mistake in the process and got let in.  Naturally, I was terrified of opening my laptop the entire time I was there. On the upside, I did have a nice holiday and nobody noticed that I was gone for a week :).  tl;dr; borders suck. Also, double check everything in your bag before traveling anywhere, because the stuff you leave in there accidentally can be used against you. "
Follow a few simple rules to make life with cx_Oracle easier.,"There are several ODBC interfaces that are more or less easy compared to cx_Oracle to get working. It is a generic interface, so you are limited to doing only those things that ODBC supports, of course.  There is an ADO interface that you can use on Windows systems. Again, it's not very portable.  ODBC is not as portable to non-Windows systems.  If you are using Jython, you should be able to use the JDBC interface.  If you are using IronPython or another .NET enabled version of Python, you should be able to use any available .NET db library.  But, there are a few simple rules that can help work with cx_Oracle. First and foremost, you must use a version of cx_Oracle that matches your version of Python, and matches your Oracle client. Oracle recommends that you match the Oracle client version to your Oracle server version, but there is a lot of flexibility there, and all clients above level 9i are backward compatible with Oracle servers down to level 9i. Clients are not fully upward compatible with higher level servers, although they will usually work for basic CRUD sql.  You can use either the Instantclient or the full client with cx_Oracle.  I'm going to assume you have exactly one oracle client and one python installed. If you do, remember that cx_Oracle will only work with one client at a time.  So, install Oracle client software that is on-level or higher than your highest level server. Make sure it works with SQL Developer or SQL Plus.  Install the cx_Oracle version that matches this client and your Python version.  Open the python console and ""import cx_Oracle"". If this fails, check the ORACLE_HOME environment variable. It should contain the path to the directory 1 above where the sql plus executable resides. E.g. if the path is c:\oracle\home1\bin\sqlplus.exe, ORACLE_HOME is c:\oracle\home1  Setting up cx_Oracle on linux or anywhere else where you do an install from source is often a lot easier than trying to setup on Windows. Just make sure you have ORACLE_HOME set properly before running setup.py.  TL;DR Follow a few simple rules to make life with cx_Oracle easier. "
"Don't be naive in your goals, but don't avoid trying.","If you don't have a fallback plan, you may end up homeless.  All or nothing is not a great choice when it comes to your long-term personal finances.  Take the risk when it's a good chance, but have some kind of exit plan when it's obvious you missed your window.  Launching out of a job is like launching a missile into space.  You have a number of things working for you and against you making it, the more of them you align in your favor, the better chance you have of succeeding.  But your window for launching is always limited, unless you are independently wealthy, so gauge your window size, and set up your launch parameters to give you the best chance of making it through the window, them attempt launch.  If you fail to make the window, have a plan to get back to stability and not be under crushing debt.  I have gone through this more than once in an attempt to have a situation I really want, and have paid for missed launches as well.  tl;dr   Don't be naive in your goals, but don't avoid trying. "
Pretty much everyone uses Q3's model for async networking if that's what you want to do.,"Disclaimer: I'm not a network engineer, I'm an AI engineer. I worked on Halo 3, ODST and Reach and took an interest in how the networking worked, but I definitely don't know the implementation details.  Two things to note. Halo uses two different networking models:   Asynchronous networking for multiplayer, where the game state is published from a server to the clients. Some client side prediction is performed, which means each player sees their own avatar move smoothly.  Synchronous networking for co-op campaign and firefight, where each client runs in lock step, waiting for player input from each client before continuing.   The AI in Halo is entirely not networkable; it's virtually impossible to make as expressive an AI as we had with an asynchronous model. In an average network environment, synchronous networking actually works really well but you will experience very slight input lag (I could do the maths but IIRC on async. networking it's 1 frame and on sync it's 3 frames, best case). For very competitive players, like Halo has so many of, this is unacceptable.  May I recommend watching [this]( presentation which my colleague David Alridge presented at GDC this year. I've actually not watched it myself but I know it was one of the best presentations at GDC this year and should be an invaluable lesson.  tl;dr: Pretty much everyone uses Q3's model for async networking if that's what you want to do. "
Intro to NLP isnt' a license to be a dick.,"There is so much dickishness to your post, I'm not sure where to start.   Different smoothing techniques are better for different applications.  Paul Graham's approach that the author uses has proven to be pretty good.  Also Laplace smoothing is not necessarily +1.  α can be any number of values.   Yes, but don't be a dick about it.   Punctuation has an individual isolated significance in analyzing spam.  Exclamation marks have a higher probability in spam  by themselves.   So, yes, actually it does make sense.   Unigrams  are  n-grams.  He can consider using higher order n-grams, but he's not wrong for not doing so, and several bayesian spam filters have been successful in doing so.   A naive bayes filter is like 10 lines of code.   It's hardly wheel reinvention not to dig up a library that would include much more.  Particularly when he's using an uncommon smoothing method and a simpler lambda weighting system than most libraries would provide.   No, no, no.  Naive Bayes is not ""popular because it's easy, but actually a piece of shit.""  In fact, it's quite far from the bubble-sort of NLP that you make it out to be.  It is absolutely the correct method for certain tasks exactly like this one.  When you need decent performance over a very large training corpus and a set of conditions for classification that are not easily generalizable, naive bayes is the right answer.   Again, yes, but don't be a dick about it.    TL;DR:  Intro to NLP isnt' a license to be a dick. "
"If the Go “community” would be able to deal with criticism, people wouldn't need to take precautions to not get attacked.","I think it is amusing how you defend a language which you don't even know. If you knew Go, you wouldn't need a clarification on the things I said. Maybe you should just read Go's documentation instead of wasting your time hand-waving.  I think the up-/downvotes tell a clear story (even with reddit's vote fuzzing).  The reason why I use a throwaway account is because of Go's community. I want the stuff I say to be considered purely on factual grounds. I don't want to get stalked by Go fanboys. I don't want to give Go people a chance of attacking the messenger. I don't want to enable people like you go through my comment history and come up stuff with like “hey this guy commented in /r/gay so he obviously has mental health issues, so what he says can't be true”.  TL;DR: If the Go “community” would be able to deal with criticism, people wouldn't need to take precautions to not get attacked. "
I think it actually is less computation time if your number doesn't go to about 100 places past the integer when you don't need it...,"I don't get this deep into Pi, ever, because well, shit I've seen how crazy the guy goes in the movie;)  j/k...  Think about it this way:  in what can be represented as a circle in a grid:  (and I'm sure math purests will hand my ass to me on this)  3x3 is the least amount of resolution possible to represent a circle  0X0  XXX  0X0  that's technically a circle as represented in a 3x3 grid, so whether you have pi as a 3, or a 3.14 that really doesn't matter  now imagine it as 4x4 (hell this doesn't even look right, seems to work better with odd numbers)  0XX0  XXXX  0XX0  00X0  and 5x5:  00X00  0XXX0  XXXXX  0XXX0  00X00  Continue through, say 1,000,000x1,000,000  As you increase in number of ""pixels"" you need more resolution (precision) to determine what aspects of the grid are presented as X or 0.  As I understand it, and again, I could be VERY wrong...  The more precision/resolution you require you're adding to the complexity of the calculation.  I don't believe it's only about how many zeros follow that 3.  I'd spend more time on it, but I'm no math wiz and I really suck at explaining things:)  tl;dr: I think it actually is less computation time if your number doesn't go to about 100 places past the integer when you don't need it... "
it used to make a difference but compilers optimized that difference away.,I took a compilers class recently and it certainly did matter. Newer compilers are optimized so there isn't a difference but back in the day it did.  I'll give you an example of why:  i = 5;  (++i) + 5; in this instance the ++i can resolve to be (i+1) so you get machine code to first store (i+1) into i and then now compute: (whatever is stored in i+5)  (i++) + 5; in this instance you compute (i+1) and store it into i. But because you need to use what was originally in i for the i+5 you have to store that value in a new register so you can later computer (original i +5). So it takes up register space and time in saving that old value.  Now compilers are smart enough to possibly move the (i++) set of instructions till after the other stuff resolves in order to not take up that register.  Sorry if that doesn't make sense I'm a little tipsy.  TL; DR - it used to make a difference but compilers optimized that difference away. 
"bit-tricks give at least 20x speedup, as long as you don't care about readability.","If you really want to optimize for performance there are a variety of tricks to count in parallel. For instance, you can treat a single integer as a vector of bit-fields. I threw together the following code as an example. It uses 64-bit integers and treats them as a vector of 4-bit-wide fields. Since each field can count from 0 to 15, we can sum the bits from 15 different random numbers at once. However, we have to repeat the operation 4 times:  For bits 0, 4, 8, 12, ...For bits 1, 5, 9, 13, ...For bits 2, 6, 10, 14, ...For bits 3, 7, 11, 15, ...  Here's the code, which runs about 20x faster for me than the code in your example (with gcc -O3), but produces the same output:  static void count_slice(uint64_t rng_values[15], int shift) {    const uint64_t mask = 0x1111111111111111L;    int i;    // Counts every 4th bit of rng_values, storing results in bitfields of the accumulator.    uint64_t accumulator = 0;    for (i = 0; i &lt; 15; ++i) {        accumulator += (rng_values[i] &gt;&gt; shift) &amp; mask;    }    // Unpacks the accumulated results.    for (i = 0; i &lt; 64; i += 4) {        bitcount[i + shift] += (accumulator &gt;&gt; i) &amp; 0xF;    }}void count15(uint64_t rng_values[15]) {    count_slice(rng_values, 0);    count_slice(rng_values, 1);    count_slice(rng_values, 2);    count_slice(rng_values, 3);}  TLDR: bit-tricks give at least 20x speedup, as long as you don't care about readability. "
"Don't go basing your career path, or company's future on this information.","I am not sure how much faith I'd put in the Tiobe stats, or at least, I'd be very careful in the interpretation of it.  [Here]( is the definition, and I am not particularly sure what exactly we are looking at.  According to their site:  > The ratings are based on the number of skilled engineers world-wide, courses and third party vendors.  Okay...  but then:  >  The popular search engines Google, MSN, Yahoo!, Wikipedia and YouTube are used to calculate the ratings.  And looking at the way it's calculated according to the link above, it has very little to do with the number of skilled engineers world-wide.  All in all, it might be an interesting view on the relative and current popularity of languages, but other than that, it's not extremely informative.  TL;DR: Don't go basing your career path, or company's future on this information. "
"Sharing is nice, but you can't force people to do it.","> comparing yourself to Socrates  Well, no, that's not exactly what ""socratic"" means.  > exactly what the entire american economic system is already founded on  Be careful you're not engaging in [left-conflationism]( I don't apologize at all for the current state of things, which I'd be more than happy to call flat-out fascism.  Look, the purpose of any theory of property is to resolve conflicts. Two people say ""x is mine"". You need to be able to systematically know who is right. The problem with saying that ""the means of production"" should not be privately owned (or owned by 'all', which is basically the same thing) is that the ownership of a good changes based simply on the uses it gets put to. The question ""who owns it"" changes depending on whether or not you can classify use of the item as ""productive."" But how do you define ""productive?"" That's entirely subjective. Everyone has different value scales. Is a rubber stamp a means of production, or just a nifty piece of organic matter?  The beauty of laizzes-faire capitalism is that this democratic distribution of wealth happens naturally. It doesn't solve all problems, but private property has gotten us very far. No socialist regime has any hope of reproducing the explosive rise in the standard of living and level of liberty enjoyed by the comparatively laizzes-faire West from the Industrial Revolution through ~1910.  There's nothing wrong with socialism if all parties sign on willingly. It's just not very efficient. There were many experiments in socialistic communities in the latter half of the 1800s and they all died in obscurity. One of the earliest and longest-lived, the Shakers, only stuck around as long as it did because it was held together by religious faith. When statists decided to impose socialism by force, the result was very bloody.  TL;DR - Sharing is nice, but you can't force people to do it. "
In theory CMMI is great if you are doing waterfall but I have yet to see any organization adopting CMMI to be successful.,"My experience with CMMI and the organisations that try to adopt it has been less than thrilling. To me the whole CMMI camp seems to value procedure and process over pragmatism. That is not to say that you do not need proper change management, requirements gathering, technical documentation, configuration management, etc. Since these aspects of software development are critical to high quality functioning software. Its just that in my experience those who are ""going CMMI"" are usually from an ISO-9000 mindset and just try to pump a bunch of process on top of an already broken development cycle and do not consider the overhead costs involved. CMMI is then eventually abandoned by management in an effort to get the next release out the door with trace aspects still visible(We will do quality testing sometimes, we don't have to document that now, we don't need to fully understand the requirements, etc)  CMMI is a joke and a failure. ISO-9000 is a joke and a failure. Waterfall development works but is very slow. This is why anyone with a slight bit of competence as a software developer is at least ""trying"" iterative development.  I bet your one of those people still using CVS for source control or CS-RCS aren't you? ;) /jk  TL;DR In theory CMMI is great if you are doing waterfall but I have yet to see any organization adopting CMMI to be successful. "
"It's late and I'm cranky and I'm currently working on a ""PHP Test"" for a job interview.  It's not going well.","These types of tests are not fair assessments.  I'm not a savant that can recite all the nuances of PHP from the top of my head.  I rely on access to php.net, this community, and SO for development and  I have a Computer Science Education  (as in, that's not nearly enough).  I think a formal background or college education in understanding computer programming, OOP, principles and design patterns is a completely different metric than understanding web development.  To be a great developer, you need something from  every  category.  To have potential you need to understand the fundamentals - the Tao of the Web Developer.  Anyone can pick up php and get creative with it, but to be a rock star you need to know why things work the way they do.  It's like the difference between a Cook and a Chef.  A Cook can prepare a meal from a recipe and knows his way around the kitchen.  But a Chef knows what ingredients go together and why they work and what tastes like crap. A Chef knows the chemistry, a Chef is an artist.  tl/dr; It's late and I'm cranky and I'm currently working on a ""PHP Test"" for a job interview.  It's not going well. "
"Don't act rude or cocky, but definitely act confident. If you don't believe you deserve the job, then what does that tell the hiring manager?","Don't forget that there is a  huge  amount of noise in the process. People don't know what they want, you need to tell them that  you  are what they want. Here's the facts: if you have a strong math background, you can pick this stuff up. Period. Also, applying to jobs is a skill in and of itself.  My data point, for what it's worth - I applied to about 80 ""data scientist"" jobs (for the record, my grad dept. is ranked #2 nationally by NRC). I only got 8 interviews and scraped in 2 offers at the tail end. I work in condensed soft matter theory, so I have a decent stats background and solid programming ability, but nothing amazingly data science oriented. I really think the reason I got offers towards the end is that I stopped feeling like an impostor and instead just acted very confident, even if I didn't feel that way. The fact is that you  can  learn on the job, but nobody wants to hear that you intend to. Hiring is risky business, so you can't fault the companies for being hesitant.  tl;dr: Don't act rude or cocky, but definitely act confident. If you don't believe you deserve the job, then what does that tell the hiring manager? "
don't assume that MySQL is the right solution just because it's the most common one.,"There's nothing wrong with using Cassandra for a tiny site, whether they know MySQL or not.  The idea that MySQL is the ""simple stuff"" (protip: SQL isn't simple, nor should it be regarded as such, that's why so few people know about their database properly) and anything but an RDBMS is advanced stuff is just plain wrong; they're two different sets of technology that aren't necessarily comparable and that serve different objectives.  If his site tended to lend itself to a more column-oriented data store, than he made the right choice going for Cassandra, his MySQL knowledge (or lack thereof) is completely irrelevant.  tl;dr: don't assume that MySQL is the right solution just because it's the most common one. "
It's just lazy MIS Management using only thing they know - Waterfall - as a standard,"It has never been a 'Standard' that I have seen but I expect in places where the MIS Management for one reason or another are not at all good with Software/Programming/Programmers so they latched onto the term as it was taught them in some 'Get your MIS degree in 24 months' program and therefore force it upon the masses.  Most of the time when I have lead Software Organizations and made a supportive argument for not doing it the Waterfall way I hear an immediate...'But that's what the books say to do' at which point I whip out any number of articles, links or books that counter it from the 80's on.  TL;DR - It's just lazy MIS Management using only thing they know - Waterfall - as a standard "
Programmer's job is theoretically harder because he needs to deal with exponentially high complexity while writer does not.,"Ok, I see your point. Perhaps I did not word my very well.  When you're writing a story you pick  one  possible realization of event flow. Once you pick it events are totally ordered (and potentially overlapped) and you can write them down linearly, in an interleaved fashion or however you want.  Of course, nobody picks a whole realization at once, but as they are totally ordered you can make one piece and join it with another piece -- either sequentially or in parallel or in an overlapping fashion. (Later two are harder but not too hard.) I.e. divide&conquer approach is usable and so complexity grows linearly depending on size. (Probably more like N*log N).  Programming is totally different: you shouldn't pick one story you like, you need to consider  all possible outcomes , i.e. all possible different event orderings.  It is a common mistake to pick one story you like and pretend that it will work this way. It is called 'race condition': programmer bets on one event winning the race.  When you need to consider all possible outcomes rather than one you like you can no longer do it in small pieces and combine them into larger ones: at least in a general case everything can affect everything else. So complexity grows exponentially, not linearly.  There are well-known idioms to deal with this: 1) make sure that events are ordered the way you want through events/mutexes etc. -- but then it all depends on your imagination, one can easily overlook some connectin between entities and forget to grab a mutex; 2) isolate things from one another, like in functional programming -- but then you're limited in your possibilities to do stuff.  Of course these problems are not limited to concurrent code -- you need to consider how program reacts to all possible inputs rather than to ones you anticipate (e.g. in tests).  For example, if program receives events from outside world and updates its state accordingly (very wide range of prorgrams can fit this model) you need to understand how program reacts to each possible event in each possible state. As number of possible states and events usually are combinatorially high (exponential on size) it is impossible to solve this in general case explicitly.  tl;dr: Programmer's job is theoretically harder because he needs to deal with exponentially high complexity while writer does not. "
Good writing isn't the report on marmosets you turned in to your 8th grade bio teacher.,"And there we go.  I think the misunderstanding here is that when we refer to ""writing,"" you think that we're talking about the process of transcribing pre-constructed modules of thought, when we're actually talking about the entire process from vague notion to complete treatise. That includes making sure that the conceptual underpinnings of what you're writing are logical and causal, insofar as it needs to be to get your point across (and if not so at face value, then from some meta standpoint, e.g., I am writing in the voice of a crazy person, therefore it will be disjointed stream-of-consciousness). You're also probably going to want to try to anticipate the reaction your reader will have to the things your write, and adjust accordingly (something that can be more intuitive, and yet infinitely more difficult to do with people than with computers).  tl;dr Good writing isn't the report on marmosets you turned in to your 8th grade bio teacher. "
"Looks very time consuming, Application question may point at large amounts of painful PHP fixing, However I'm not a JS/PHP guru.","I don't know enough PHP/JS (So if those are stupid simple I may be missing it) to do those without a bunch of Googleing but i'm surprised you can't find applicants to do those.  They look fairly straightforward.  However, I have been looking at lots of job postings a lately (Hence finding this gem on CL) and I think you're asking for quite a bit of effort right out of the gate.  As an applicant I could spend time doing these and never hear back from you.  I think it would be more effective to only have one or two test style questions on there (the slow/crashing JS JQuery question looks really interesting) and use the other questions during an interview.  When i went back to the page to re-read it as I was typing this post I saw the test question section is called ""Can you program?"" and my initial reaction was ""Why yes I can program.  But why do I need to prove that four times."" Imho, chances are if an applicant can do one of them, they are not far from being able to do all of them.  Another option could be keeping them, but presenting it as ""Github or test"".  Either show me something you have done or do these.  This would be really awesome, coming from someone without an established github account or code sample archive (I hate not having code samples but I don't have tons of time to create one and considering it would be for a job it isn't something i'd want to quickly hack together), the test questions would give me a perfect stage for demonstrating that I can in fact actually program.  Just some (hopefully not-uninformed) ideas for you. YMMV etc.  (Also the refactoring PHP question akes me think that as your future hire I'd be working on crappy legacy PHP code.  That is probably not helping your cause either, unless that is the case).  TLDR; Looks very time consuming, Application question may point at large amounts of painful PHP fixing, However I'm not a JS/PHP guru. "
If I had to sell you on just one thing:  phpStorm . Best $100 I've ever spent.,"I code in Windows 7, then run it on an Ubuntu Vbox (LAMP+phpMyAdmin) which has share-access to my local (Win7) code-projects folder, so it's ""save & refresh"" w/ no delay.  For the code-wranglin': 85% phpStorm (w/xdebug), 5% Dreamweaver, 10% assorted: Notepad++, vim, and nano.  Version Control: Windows Explorer + Tortoise for both SVN and Git (and also msysgit / Git Bash), but since phpStorm does such a fine job on that front, I've been using them less. Also, I've come to prefer the Git Bash CLI over Tortoise Git... it just feels like I achieve my tasks quicker, versus using the GUI.  Putty/Pageant for SSH, and lots of ""tail -f /var/log/apache2/error.log"". Firefox/Firebug for frontend stuff. And Python for the mundane tasks that can get scripted.  TL;DR:  If I had to sell you on just one thing:  phpStorm . Best $100 I've ever spent. "
"Whilst Java might be able to compete with native languages for cpu intensive tasks, it's still going to struggle when it becomes memory intensive.","Citation on the 1.1x runtime claim I suppose. I can absolutely accept that in arithmetic/cpu intensive tasks the JVM with JIT may come into the same level of performance as C++ no problem - but ""equivalent applications""? If somebody wrote Crysis 2 in Java, and it performed as well as the C++ version, I'd be fucking shocked, I promise you I'd eat my own hat - fuck it 5 of them.  The main issue really is memory, the same sort of issue that Ruby was having that Java helped with. C++ with its manual control is going to outperform Java massively in this regard. So really, going Java was massively half-arsed with a memory intensive application.  tl;dr Whilst Java might be able to compete with native languages for cpu intensive tasks, it's still going to struggle when it becomes memory intensive. "
MongoDB is good for small simple projects as a big productivity boost. If you need to build a large scale complex application there is no one solution fits all.,"MongoDB is good for small projects with low complexity. You can quickly get up and running without needing to worry about a schema for your data, you can quickly iterate and develop until you come to a good balance.  However, as soon as you need to start thinking about interactions between objects in a relational way, which is nearly guaranteed to happen, with large projects, you'll find yourself trying to work around the database limitations rather than working with database features.  I would highly recommend something like Postgres for larger more complex tasks. Postgres now also supports json objects at speeds similar to or better than mongodb.  You really don't need to worry about scaling up arbitrarily 99% of the time, and if you do you should be thinking a lot more about the structure of your data, the trade offs you're willing to make, and the associated risks.  A lot of people think mongodb is this amazing black box that just takes care of that for you, but I assure you it does not do this out of the box with no or little configuration and careful structure of your data. Even then some people argue that it doesn't handle scale out anyway.  tl;dr: MongoDB is good for small simple projects as a big productivity boost. If you need to build a large scale complex application there is no one solution fits all. "
"From my understanding, dream scenario where a small, brilliant team came up with the solution and everyone listened to them.","Well, my understanding is that there had been multiple groups working on standards for this sort of thing at the same time, but this particular approach had the luck of being done at Bell Labs by the team that *made* Unix and C - whom were asked by IBM and  a lot of other people who were working on Unix at the time   proposal be a good idea? If not, give us a standard by Monday, and we'll use it."" And Thompson/Pike/et al said no to the initial proposal for various reasons, and came up with UTF-8, and everyone listened to them (for good reason).  So, you had a small team who designed the OS this format would first run on and the language in which it would be implemented, and then had everyone listen to them - sort of a 'dream scenario' as far as planning goes.  I think it'd be hard to have that sort of thing happen again, though; Pike and Thompson are still around, but everyone ignored  plan 9 , and nowadays you have just the people on the ""9fans"" email list shout about the benefits of their OS to anyone who takes a look at them, they as a whole vaguely resembling a manic crazy person on a sidewalk who just happens to be right. Pike heads development of [Go]( at google, but that's not likely to put him in a position like C and Unix had Bell Labs in the 90s.  tl;dr : From my understanding, dream scenario where a small, brilliant team came up with the solution and everyone listened to them. "
"I was in the exact same situation as you. A career-pivot is not gonna happen overnight, but it can if you really want it.","Just give it an honest shot. You have to spend the time to work through these things enough to learn them.  Some background:I'm 36. I am not a data scientist. I am an electrical engineer who used to HATE coding.  A few years back, I hit a big wall at my job and felt stuck. I learned some Python basics, but knew that there was no way I could get a real coding job which would match my salary at the time. So, I was much in the same situation as you are now.  I decided to stick with my career, but I did find a much much better job. But I vowed to myself that I would devote some free-time to something challenging which could eventually dovetail me into a new career, if I needed it.  That thing was Python.. at first.  I started reading books, taking online classes, reading blogs... Six months later I started using Python at my job, for test automation. Holy shit, I am getting PAID to code.  Eventually I wandered over into data analysis and starting taking stats classes, machine learning classes, etc.  Fast forward another six months, I'm pretty good at Python and now I know R: At work, in addition to electronics design, I now routinely do data analysis and visualization. In fact, when people have big sets of data that they don't know how to deal with, they throw it over to me. They're all like ""Well you're the stats-guy so..."". Holy shit, I am getting PAID to do data analysis.  All of this was accomplished by taking online classes, coding 100 stupid example projects which made me feel like an idiot but taught me stuff, and just wanting to learn it.  So here I am, less than two years later and I can code well, and while I wouldn't call myself a full-blown data-scientist, I'm definitely an accomplished data-enthusiast.  I predict that within another year, I could probably actually work full-time with data.  If you want to learn it, and you apply yourself, you can do it. There is no prerequisite other than will.  Pick two nights a week and devote yourself to something. If you decide it's not for you, pick something else, but keep those two nights for self-improvement.  TL:DR - I was in the exact same situation as you. A career-pivot is not gonna happen overnight, but it can if you really want it. "
I can dig it if the other person is competent.,"I am a psychology/creative writing major who took an intro to programming class for fun  (I was the only one that i know of in my lab of 20 who it wasn't required for).  Every week we had a lab where we had to program in pairs.  I quickly learned that when I got paired with someone who knew what he/she was doing, the process went well.  Each of us would bounce ideas back and forth about how to correctly do the assignment.  When I got paired with someone who was just there to get a grade, it tended to just be me sitting there programming by trial and error while the other person stared blankly.  TLDR:  I can dig it if the other person is competent. "
"in reality your Amiga experience actually put you in a much better position to work on UN*X systems. 
 /sigh 
 Yeah I don't know. I don't have a Porsche either.","IIRC the state of Windows 4-5 years before the Amiga was such that Windows could  barely  do  overlapping windows. Oh maybe you are thinking of the AMiga 4000 or something. Me, I got in right with the A1000. I had them all, from the 1000 to the 4000. The 4000 was AWESOME, but I still think the 2000 was the best machine I ever owned. Well, maybe tied for first with this MacBookPro I'm typing on now.  Anyway I had both AMigas and PCs, and I recall the Windows guy where I worked having a nerdgasm over shared libraries which on Windows were/are quaintly called ""Dynamically Loaded Libraries"". I had already worked with shared libs and preemptive multitasking on the Amiga for a long time so I yawned and said yeah, so?  TL;DR, in reality your Amiga experience actually put you in a much better position to work on UN*X systems.  /sigh  Yeah I don't know. I don't have a Porsche either. "
Add customizable key-bindings and you'll have more customers.  Very nice job though.,"I have to say, it looks very nice.  But, I have to say I won't be switching from emacs.  I've used emacs going on 20 years now.  I've started working for a windows shop in the last 4 years, and I still use emacs and only use dev studio to debug.  I might be tempted to switch if it had the ability to adopt customizable key mappings.  So, I can move to emacs key bindings if I choose.  Having Python as the customization language would be much preferable to elisp.  The hard-coded window layouts seems odd to me as well.  Why can't I have infinitely customizable panes?  In truth the given options are good enough.  I wasn't able to try out the build system, simply because I was too lazy to customize it to ours.  Might try it at home though through make.  Also, are the silly animations (like sliding text on folder open in the project) disable-able?  Cute, but it's just annoying after awhile.  tl/dr: Add customizable key-bindings and you'll have more customers.  Very nice job though. "
Euler solvers like this have orbital precession errors; use at least Runge-Kutta instead.,"I'm surprised, and possibly disappointed, by the differential-equation solver shown.  The code sample uses [Euler's method]( which has many bad characteristics for orbital dynamics.  The biggest problem might be a systematic bias in the error.  The acceleration is computed only at the start of each interval, and assumed to be constant during that interval.  However, during that interval, the direction to the large mass will surely change.  If the error were symmetric and random, then it would tend to average out but, as it isn't, this will cause a consistent drift.  This means that if you use Euler to simulate an eccentric elliptical orbit, the orbit will incorrectly be shown precessing around the large mass along the direction of the orbit.  However, if this is an early assignment, and a later one explores better solvers, then I have no problem with this assignment.   Fourth-order Runge-Kutta  some of the Euler problems.  tl;dr: Euler solvers like this have orbital precession errors; use at least Runge-Kutta instead. "
"No, I'm not on pills, and I think (and have shown) I can make it without them. If I need them later, I'll take them later.","> So, you're untreated? If so, why?  Yes, I choose not to undergo chemical treatment (aka, I don't take pills) of my own volition.  I see a therapist, and like to think I'm fairly self-aware. My bipolar, even while untreated, does not significantly hamper my day-to-day life.  My therapist also believes that I am managing my illness well enough to not require medicine.  Some diabetics can treat their diabetes simply by eating and living properly. Others need to add insulin in order to keep themselves safe and in control.  Some people with mental illness can treat their illness simply by living properly. Others need the help of medicine to help treat their symptoms.  With all of that said, I frequently review whether or not I've 'deteriorated' enough to need chemical help. So far, I've been managing and mitigating my symptoms through sleep, diet, and living my life well.  tl;dr: No, I'm not on pills, and I think (and have shown) I can make it without them. If I need them later, I'll take them later. "
"It's a good article, worthy of up-votes, but I think it ""splits hairs"" over semantics in order to connect a cool idea to a catchy title.","This is a good, thoughtful article, as I hope the length of this response implies.  I cannot confirm that coding is the new literacy, but I disagree with this explanation of why it isn't. It seems Granger says that literacy is ""external, distributable storage for the mind.""  I agree.  However, his use of the terms ""coding"" and ""writing"" seem to be contrived, such that they don't qualify.  Granger describes writing as ""making marks on a sheet of paper"", and coding as the computational equivalent.  Because coding and writing do not imply ""composition and comprehension"", we must instead turn to modeling as the true means of literacy.  The problem is that ""writing"" and ""coding"", in common use, already imply modeling.  If a layman were presented with meaningless scratches, they wouldn't identify it as writing.  A similar person, presented with a random collection of bits, would only identify it as code if the context implied it (eg. the bits were zeros and ones organized in a grid).  Also, the phrase ""Coding is the new literacy"" is clearly not referring to the ability to store things external to the mind, but the medium used for it.  Long before the first written words, there were spoken words, which filled the same purpose, but used a different medium.  Literacy became important once it was a widely accepted medium, and allowed a greater degree of freedom of communication to those who used it.  Can the same be said of programming today? I believe the answer has little to do with the use of the term ""Modeling"" in the place of ""Coding"".  Perhaps I'm missing the point.  The article is clearly trying to encourage ""computational thinking"" instead of, for example, the mystical output of a GNU/Linux terminal.  To this end, the article is excellent.  However, the title and thesis of the article implies that programming is a far less important part of the world than many believe.  TL;DR: It's a good article, worthy of up-votes, but I think it ""splits hairs"" over semantics in order to connect a cool idea to a catchy title. "
"The specializations catalog had a rather substantial core which included rigorous classes in areas like systems, theory, and software engineering. Threads™ does not. 
 -- gtg620b","I get a bit ranty near the end. Feel free to stop reading when you feel like I've answered your question :).  Specializations (which is actually the catalog under which I graduated, after the CS2130/ECE2030->CS2110/CS3240 shift), as I'm sure you well know, had a fairly substantial core which included such classics as CS2200 (Systems and Networks), CS3510 (Design and Analysis of Algorithms), and CS3240 (Languages and Computation). I believe ~15 hours of CS classes were elective.  Threads™ has no explicit core. Any given thread (of which there are eight) provides some selection of core classes that are specific to that area. Each thread has about half of its classes overlapping with each other thread, and is supposed to be roughly 2/3rds of a CS degree.  Some combinations (the one I particularly like to harp on being Media and People) don't require students to take any of the classes I mentioned above. The only exposure to algorithms, complexity, or data structures that students see is in a 1000 level class (which is, admittedly, a very good class). I've known people to pick their threads explicitly so they can  avoid  a class like CS2200, which is really the opposite of what we should be striving for. People should choose a curriculum because of what it contains, not what it omits. Additionally, can a student who is so scared of systems that they'll avoid a (relatively easy) 2000 level class on the subject really be called a computer scientist?  tl;dr: The specializations catalog had a rather substantial core which included rigorous classes in areas like systems, theory, and software engineering. Threads™ does not.  -- gtg620b "
"Win95 was a horrible OS, modem drivers fall down go boom.","I believe the OP is talking more about building the cpu controller into the CPU. A few SUN cpu's do this with network, and crypto. It has to do with shortening the pipeline so that data can be accessed faster. Here is a interesting [article]( about it.  Regarding winmodems.. Ya that was a horrible idea, mostly because it was ahead of its time. Offloading the function of a modem to a cpu isn't a big deal, its that the drivers were all horrible. Not to mention this was a driver that ran through the OS that performed the hard work of a modem so that they didn't put as much logic/chips on the actual modem itself. It allowed modems to be upgraded and to fix bugs.. It wasn't actually built into the CPU. tl;dr Win95 was a horrible OS, modem drivers fall down go boom. "
"only for non- content on  pages, because these violate the expectation of encryption","As that page describes, the problem you're thinking of is the  other  way around. Browsers complain about a  page with http assets, not the other way around.  The reason is that those http assets could result in a ""leak"" of information that the user would otherwise believe was submitted an an encrypted form. Form example, if I go to:  And it includes a reference to:  Then it's possible that that remote Javascript will use Ajax requests to push data to example.com over HTTP, which is unencrypted. Those requests could include data that was delivered to me encrypted from example.com, thereby undermining the value of the encryption.  Conversely, if a non-secure webpage includes secure content, your browser will usually not even mention it to you. It was never  implied  to you (by a  in the URL) that your connection was encrypted, so there's no need to warn you that parts of it might not be.  tl;dr: only for non- content on  pages, because these violate the expectation of encryption "
there is a directive that lets you get a perfect upward-compatible ABI between C and C++.,"You see this in every conversation about C and C++.  And this is basically wrong - you simply use the [ extern ""C""  directive]( which marks a function or a block full of functions and declarations as using C's ABI.  Of course, you can't declare functions that use C++-only features that way.  And you can only use  Plain Old Data  - but that's all you get in C, so you can't expect any more.  More details are given on page 40ff of [this interesting article]( on calling conventions.  And remember - the functions that are marked  extern ""C""  can contain C++ code within their bodies - it simply ""turns off name mangling"".  I have successfully done this multiple times in production environments with never a problem.  tl; dr - there is a directive that lets you get a perfect upward-compatible ABI between C and C++. "
errors are hard; exceptions (in Java or elsewhere) are merely mechanics that require human attention to do it well.,"The problem with the ""correct"" Java solution (""The Mountain of Encapsulation"" part) is that it it obscures the original error; the more layers there are (here we have 2: our function and the Java API, which is nothing, really), the more it is obscured. This makes it harder to report it well and it makes it harder to act on a particular error programatically.  Reporting is hard because a good error report contains more context. E.g. it is not ""Not enough lines"", but rather ""Not enough lines in configuration file %1"". That context really should be added carefully as you go, and it has to be looked after while the software evolves. In particular, the number of ""layers"" tends to break it.  Acting on a particular error programmatically (a rare occurrence, but still) is hard because depending on the number of ""layers"", one will not ""catch"" that error, but the one of unknown type, that will contain unknown number of inner exceptions, and an unknown nth exception in that chain (not necessarily the ""last"" one) will be of actual interest.  TFA also presumes to know/guess what failure conditions are noteworthy. This is IMO actually pretty hard in practice, as there is a serious disconnect between how different people use software (and therefore get things wrong), and what importance and what meaning they give to any possible error.  tl;dr: errors are hard; exceptions (in Java or elsewhere) are merely mechanics that require human attention to do it well. "
"Don't bother reading this article, it says nothing worth your time.","I was expecting... I don't know,a rationale, a thesis.  Perhaps a comprehensive deconstruction of our over-reliance on conditionals in modern programming and what problems arise as a result.  Maybe some sort of take-home lesson about how we can mitigate this problem.  Instead, this just talks about one extremely specific situation where you could use a conditional or not, depending on which is more efficient in the particular system and setting you're working in.  There's some hand-waving about clean code.  It's essentially fluff.  I came away from this article with absolutely no new knowledge or understanding.  It was a waste of a couple of minutes of my time.  tl;dr: Don't bother reading this article, it says nothing worth your time. "
"on one hand, there's that desire/need to stay C-compatible, on the other, there's complexity, and only one can be chosen.","Of course, unique_ptr is too new to be widely used. It's inferior, but perfectly functional for the purpose above, counterpart, auto_ptr is here since... 2003, I think.  Having said that... ""Discipline"" is the key word here IMO. I am one of those old farts for whom the great interoperability with C is an absolute must in C++ (not total interoperability, that's not practical). And if that is so, whatever is done in C WRT memory handling should be possible in C++, and that alone produces ""myriad of ways"". In this case, in particular (""what came out of this function is yours""), there are arguably two ways: the C-way (naked pointer) and the C++ way (auto_ptr up to C++11, unique_\ptr now).  tl;dr: on one hand, there's that desire/need to stay C-compatible, on the other, there's complexity, and only one can be chosen. "
Developer and Team Leader/Manager are two different career paths. One doesn't necessarily flow into the other.,"I think the move from awesome developer to management is a false ""career progression."" I've made this move myself, but that's because I wasn't an awesome developer, and actually preferred the management and people side of things.  Just because someone is a great developer, doesn't necessarily mean they will also be a great team leader or manager of a software team. Being promoted to a management position isn't necessarily a reward for doing a great job. I personally see them as two different careers, with a different set of skills and aptitudes required.  This is a big problem in smaller organisations where there isn't room to promote someone to a ""most senior developer"" or ""lead developer"" or whatever. In larger organisations I've found these people usually become consultants, experts in their particular field.  TLDR; Developer and Team Leader/Manager are two different career paths. One doesn't necessarily flow into the other. "
It's a tough problem people are looking at atm,"That's a tough one, and a problem a lot of people are interested (like national labs). Even without using OCCA the challenge comes from:  (1) Usually we assume memory is not connected across devices  (2) Performance  For (1):  If enabled, OCCA can emulate an ""unified virtual address"" space to let you use pointers (void*'s) rather than occa::memory objects. This is used if you want to let OCCA manage your memory transfers like in:  &nbsp;&nbsp;&nbsp;&nbsp;  An issue with this is that you end up duplicating memory needed when using GPUs or Phis (X in the device -> X in the host). To make matters worse, this still doesn't solve the issue of:  &nbsp;&nbsp;&nbsp;&nbsp;I want to let GPU ""A"" work with data from GPU ""B"".  especially with GPUs having such little memory  For (2):  Memory transfers are painstakingly slow (relative to computational speed). This would have to be taken into account when approaching work-balacing. Same as (1), memory management also becomes an issue  TL;DR:  It's a tough problem people are looking at atm "
"use a browser like Safari for ""top visited sites"" and check out renotecontrol.com for a better way to bookmark specific factoids.","I think the bookmarking concept is wrong/obsolete. Usually, if I bookmark something there are two reasons:  1) I visit that site frequently (aka reddit)or 2) there is some specific piece of information on the site I want to refer back to later  Problem is these are two very different tasks. For the former, a simple learning 'top sites' system like Safari 4 has (and other browsers) is the best solution. I don't have to do anything other than go to sites, it takes care of the rest.  For the latter, I now have to be concerned about organizing information as you mention, but I can't be sure what information on the site I was looking at originally and I have to rename the bookmark manually or put it into a folder manually to have some idea of why I bookmarked it. Then I have to worry about finding the info I wanted from the site next time I go back, assuming that it hasn't changed or just gone away in the intervening time. I also have to actually visit each site in a group to get all of the information I may have been collecting about some subject. In other words, it's not really what bookmarks are supposed to do.  My solution was to make a notetaking/bookmarking webservice. It handles the things in the #2 category and I let my browser handle #1 for me.  The site is renotecontrol.com if anyone wants to use it. It's really abre bones at the moment. I'm planning to add some features like searching/saved searches/folder nesting/etc in the near-ish future but it's good enough for me to use it which was kind of the original motivator so development has stalled as other things take priority, blah blah.  TL;DR use a browser like Safari for ""top visited sites"" and check out renotecontrol.com for a better way to bookmark specific factoids. "
"Suse makes broken gui configurators to make your life easier, but if you have half a clue about administrating linux they'll blow up spectacularly.","Absolutely.  There was one really bad moment that completely caused me to lose faith in the distro.  When I started my job I inherited a bunch of servers.  None of them had been properly maintained because there was no single person in charge of them.  One of them had Suse on it.  I decided that since this was a public facing server it should have a firewall.  I'd been trying to learn to use Suse features instead of just ignoring them.  I noticed it had a firewall configurator and it seemed like a reasonable idea to see what that could do for me instead of making another pass through the iptables man page.  The configurator was a little overzealous.  As soon as I launched it, it turned on a firewall.  Did I mention I was doing this over ssh because the machine wasn't at our building?  Anyway I had to call up someone else and get them to flush the firewall before I could get access to the machine again.  I could accept this as a personal screwup had I blindly clicked okay and activated a firewall, but there was no okay.  Program launched, and I got locked out.  Shortly after I spoke with a friend of mine who worked for Novell at the time.  He knew some Suse devs and said they were aware of the problem.  They regarded it as one of many deathtraps and were content to leave it in the distro.  Maybe it's been fixed since then, but I'm not about to try and find out for myself.  The other major problem I had was with yast.  This was at least 5 years ago, so it may no longer be relevant.  Basically launching up the Suse equivalent of a control panel rewrote several config files based on Suse's memory of the state of those files.  This included xorg.conf (though at the time it may have been x11.conf).  I was helping a user set up his laptop and Yast had no option for selecting the right video card driver.  I could fix it in the text file, but those changes got rewritten whenever Yast was opened.  I never figured out if there was a way to fix this.  tl;dr Suse makes broken gui configurators to make your life easier, but if you have half a clue about administrating linux they'll blow up spectacularly. "
"copyright meaning is almost entirely literal. If you haven't sold them the exclusive copyright, then it's yours and only yours to sell.","The layman's version of this is: (It's similar to buying software today, and ensure that you use words like 'software piracy' so they understand the similarity.)  You need to look at both of these as different products, source code is a product which they can resell and make money, or code can be a means to perform a function/service that they need/want.  When you buy software, you can't resell it - you've bought the rights to use the code, not the copyright to reproduce the code.  You alter your pricing accordingly depending on what they want, and you must be clear about it. If you have not specifically stated to them that you are transferring copyright, in most cases you have not.(For example they would own the copyright if you are an employee of a company where writing the code is your job - i.e they've hired you to write code so they can use it any way they see fit.)  tl;dr  copyright meaning is almost entirely literal. If you haven't sold them the exclusive copyright, then it's yours and only yours to sell. "
I pity the fools who think manual memory management can be easy in complicated apps.,"No, don't worry, someone on the internet said it was easy, so it must be.  Like, you call a singleton representing a database connection, then you don't release it, because, you know, it's not yours ! And if you are working with exotic data structures such as chained lists, well fuck you: just release the list and hope for it's elements to be into a pool. And also don't even think about objects without owner, because you know it's too complicated anyway, so this is irrelevant.  Last but not least, if you don't want memory leaks and are using a dangerous library (like the ones that are using object factories), then rewrite it: it's not about being lazy, it's about being RIGHT.  tl;dr: I pity the fools who think manual memory management can be easy in complicated apps. "
he used to use 2 data formats: JSON + ISO strings. Now he uses 1 format: JSON. Parsing 1 format is cheaper than parsing 2 formats.,"This is getting a fair amount of flack, namely the idea that parsing dates are so slow (or slower than parsing a JSON object). However I think what the author is really talking about is comparing between:   a JSON object filled with multiple ISO dates  a JSON object filled with multiple integer/string values   The first needs to be converted from a JSON string to an object, and then each date string needs to be converted from a string to a date. This means if you have 100 dates, you are parsing ISO strings 101 times. In his example he was parsing 1,400 separate ISO dates.  With the second, all that parsing is done the once, when you first convert it from a JSON string to an object. Something you had to do in the previous case, but removes 100 calls to parse the ISO date strings (or 1,400 separate ISO dates in his example).  tl;dr; he used to use 2 data formats: JSON + ISO strings. Now he uses 1 format: JSON. Parsing 1 format is cheaper than parsing 2 formats. "
The benefits of a privately owned company often condemn them to a very slow death or an aborted start.,"Like all things, it depends a good deal on the company and the market in question.  Let's say for instance you had a software product that you started to make a decent amount of headway with, you've got customers, a steady income for the business and a decent plan for the next few years. More importantly, you own the whole business outright; no investors. Then the calls from VCs start coming in, you of course turn them down because you don't need them.  The VCs then provide seed money to the founder of another company who enters the same market as you. The only difference is they have 3 times the engineering staff, they offered your top Sales guy double the salary (so now they're stealing your talent and know exactly the pain points of your customers) and quadruple the marketing budget.  Other times the market may simply be too expensive to enter without investment. Let's say you have a great idea for some new enterprise class router. You're looking at multiple millions of dollars of cost in development staff, hardware prototypes, market research before you sell to your first customer. Unless you can afford those millions out of pocket, you're going to need investors to risk their pesos on your little venture.  TL;DR The benefits of a privately owned company often condemn them to a very slow death or an aborted start. "
don't use globals. Try printing values to console and compare.,"It's very hard to follow your code because of all those globals: it makes it very hard to work out what the function is doing.  For example, it looks like the line causing the problem is  document.images['rc'+tmp].src  So  document.images['rc' + tmp]  is undefined - what is the value of tmp? What does it even mean? I assume 'temporary' but it can't be - it's a global variable! So I now have to look a lot of other functions, and these function themselves have globals:  function minimax(board,currentplayer){    // make a random move; used for easy mode    minimax_recurse(board,currentplayer,0);    tmp = best_move;}  The easiest way to debug would be to do  console.log(tmp);  just before line 648, and see what the output is when you get the error (which I failed to reproduce).  tl;dr: don't use globals. Try printing values to console and compare. "
"Made an app and gave to friends. Friends gave app to colleauges without telling me, I pushed 'joke' update to app and shit hit the fan.","My last work place had a 'call queue' display on large monitors around the office, that looked like it had been written in 1995 - complete with animated GIFS.  So in my down time I wrote an application that was 1/4 the size and displayed more information - which I only gave to friends.  Now, I write scripts and utils for a lot of things at work so I've got a custom update module that will update certain scripts/programs automatically. I ran the update program to prank a few of my friends - it was essentially one of those ""I'm watching GAY PORN"" pop ups.  Within 2 minutes of pushing out the update I had a phone call from my manager saying that he'd received a call from the folks upstairs and their monitors were flashing bright colours with GAY PORN on them.  Turns out somoene had distributed my util without telling me and a superviser had decided to use my 'prettier' call queue manager.  I've been unemployed for a week now.  TLDR: Made an app and gave to friends. Friends gave app to colleauges without telling me, I pushed 'joke' update to app and shit hit the fan. "
"JS is becoming a very nice language, but that doesn't mean CS doesn't have its merit.","The point is that JS is quite hairy for historical reasons (implicit globals, implicit semicolons, ...). CoffeeScript changes provides an alternative syntax with lots of syntactic sugar (e.g. adding language features you'd normally need frameworks for) and a purer structure (generally, everything is an expression).  It has some warts (IMO) owing to its ruby heritage (e.g. the slightly awkward ""implicit parentheses"" for function calls), but they are far easier to avoid than JS's. It's ""bad parts"" are not as omnipresent as JS's either.  It's not a universal replacement of JS and I'm certainly interested in seeing JS mature further (it's come a long way and is progressing nicely), but it's nice to have a choice. If your development process already involves minification or a tool like Sass, it's easy to integrate CoffeeScript into it. If you prefer to edit your JS ""live"", it's probably not an option for you.  If you're doing both back- and front-end development or recently picked up node.js but come from a Ruby or Python background, CoffeeScript is a very nice and astonishingly robust language with some very useful idioms.  In other words: it's not a better way to write JavaScript, it's a different scripting language that can be converted into JavaScript (which makes sense, considering browsers only reliably support JS). If you're too lazy/inept to write JavaScript, nothing can help you.  But if you like some/most of the features of JS but want a language that doesn't carry its historical baggage and allows you to write your code with less clutter, CS is worth a try.  I think comprehensions, splats, splicing and the existential operator are a better example for the power of CS than conditional sub-expressions (which are nice to have, but ultimately mostly about aesthetics). The difference between  big = true  (CS) and  var big = true;  (JS) may be minute, but the nature of JS's scoping makes CS's syntax quite useful: The  var  keyword is necessary in JS to avoid cluttering the global namespace, but provides a false sense of scoping because all declarations are yanked to the beginning of the function and re-declarations are silently ignored. CS simply does away with implicit globals altogether, thus avoiding the issue of watching your declarations.  tl;dr  JS is becoming a very nice language, but that doesn't mean CS doesn't have its merit. "
I thought the original post was silly until I read your comment,"When I read the original post I laughed: ""Haha! Silly journalists! They don't understand that the cars computer system just tells the door locks to activate when the accelerator is pressed, it doesn't mean you could control the accelerator from the locks, the system is way to simple to be used like that! It's the equivalent of suggesting that because an interlock stops a microwave operating with the door open, you could hack the microwave to 'lock' the door closed""  After reading your comment my initial thought was: ""What!? Cars have become this complex? The gas pedal is connected to the engine via a computer network that has bridges and routing rules to prevent inappropriate control of the throttle?! It's not just a simple connection!?""  tl;dr: I thought the original post was silly until I read your comment "
"Yes, this is a great job for an RDBMS like MySQL and PHP will handle it just fine.","Maybe work your way up to the project, but you can tackle it.  Really all you're doing is going through a preset set of conditions. So you'd start out with say, SELECT   FROM districts. When they select a district, you do a SELECT   FROM schools WHERE district_id = <district>. When they select that, you do a SELECT * FROM subjects (maybe go so complicated as to only pull subjects present at that school).  You'll end up with a series of ids by which you'll do a final SELECT * FROM documents WHERE district_id = <district> AND school_id = <school> AND subject_id = <subject> which will give you the final curated list.  Commit yourself to a prototype phase. Do it short, sweet, and dirty. No design, just a proof of concept to yourself to try out techniques. Most important rule about prototypes though is you burn them when you're done (not really delete it, but start from scratch for the final product, just use the prototype as a reference, don't attempt to refactor the prototype).  tl;dr  Yes, this is a great job for an RDBMS like MySQL and PHP will handle it just fine. "
sometimes it's okay to ask before trying to solve all problems on your own.,"I completely agree with the sentiment. It's a well known theme among discussion forums of technical subjects. And noone will argue that people  demanding  help suck.  However I, sometimes, also happen to be on the other side of that problem. I noticed that sometimes I spend DAYS researching something (and not really learning much from doing said research) that someone who's informend on the subject matter could've answered in mere minutes.  An attitude I sometimes notice in my behaviour is that I usually try to investigate EVERY possible aspect of something before I ask for help. As I've grown up using technical forums, I'm very used to people demanding that I've done my due diligence before asking for help. Oh the hours I could have saved if I only went out and asked for help sooner! But of course I don't want to get embarassing answers and ""RTFM!"" messages that people are very quick to hand out on discussion boards etc. I mean, it's understandable and all: we're all fed up by kids who just want to get you to do your homework, or lazy-ass people who can't even be bothered to google their problem, or newbiew who didn't look through the FAQ, ... Still, we oftentimes forget what it was like when we were new to a field. Ten years ago, plowing through a man-page was a monumental task, not only because my English wasn't up to par, but also because I just missed so much background information to grasp the little details and nuances of what I was reading. When I first started writing C++ I had a hard time wrapping my head around pointers, and problems that seemed trivial to more experienced people were just UNSOLVABLE for me, and all of their ""go google it for yourself"" answers weren't helping, at all.  TL;DR:  sometimes it's okay to ask before trying to solve all problems on your own. "
I bet there was a better female/male ratio in the School of Biology.,"That sounds awesome. I kinda wish I had done that. I'm often caught by the fact that I'm not doing anything to advance our understanding of the real world. Real science and even math is so much more rewarding. You're reverse engineering God's work. Me, at best I'm building an application used by millions of people, but so what. You know? Even if I go academic I'd have to go pretty far for to get to anything fundamental. I think the term is natural science? I mean, a binary tree, let's say I invented that. Is that real? Or is it just applied? Is it as real as discovering the connection between electricity and magnetism? Even if you go mathematical, the math that describes electricity seems so much more real than the math that describes, say, algorithmic complexity. I suppose boolean logic and combinatorial math and math if sets and stuff is all pretty fundamental, but, I don't know... I'm rambling now, but im not the first to question where the science in computer science is, or in my case where the nature is.  I will say that quantum computing really appeals to mw because something about having all those states in a system at the same time really sounds like what real life must be like (if that makes any sense).  Tl;dr: I bet there was a better female/male ratio in the School of Biology. "
"I don't want to read shit from dumbasses, and sometimes a dumbass needs to hear they're being a dumbass. 
 Edit: parley (dialog, converstion), not parlay (gambling bet, reinvestment)","He may know internally he's acting like an idiot, but 1) the external acknowledgement may cause a moment of reflection and pause the next he might act like that, and 2) a third party observing the conversation might consider it fun to also act like an idiot (given the comical parley) and an again a small but visible acknowledgement that the behavior is unacceptable may likewise cause a moment of reflection and pause.  Both of these have a low probability of happening, but the cost of sending the message was also quite low, thus possibly proving some small value, at least in my interaction with the community.  tl;dr I don't want to read shit from dumbasses, and sometimes a dumbass needs to hear they're being a dumbass.  Edit: parley (dialog, converstion), not parlay (gambling bet, reinvestment) "
"If you're changing operators to try to get a speedup, you're doing it wrong. There are much bigger fish to fry.","Clearly switching to C or C++ is going to be more expensive in developer time than changing a few operators. My point is that optimization (when done sanely) is targeting the biggest bottleneck over and over until you're running acceptably fast. Switching a few operators is going to be drops in the bucket - if you've optimized your code to the point where the modulo operator is the bottleneck, you're probably being held from doing much more by Java itself.  Real programs aren't just iterating through one array like this benchmark, and cache misses are a  huge  performance hit - going to main memory can cost hundreds or even thousands of cycles. Java, being a language that uses reference semantics and heap allocation extensively, is going to be very cache-unfriendly. The resulting cache misses are going to completely overshadow any gains you might get by changing  %  to  &amp;  and shaving a few instructions.  tl;dr: If you're changing operators to try to get a speedup, you're doing it wrong. There are much bigger fish to fry. "
"That'd be way easier, but we have developers pulling from multiple places and using tools that are not fully compatible with virtualenv and pip.","Well, the problem with virtualenv is most of our clients/developers use conda (the scientific packages package manager), which contains its own virtualenv-like-thing that is not compatible with pip. (If I could rely on pip and virtaulenv, I'd probably be leaning more into the new wheel format)  We're currently doing something like this with our own app (I'm on the DevOps team). We'll probably continue doing something similar to this, but what I really want to avoid is moving venvs, then having to edit the shebangs on each machine as part of the deploy. This seems to be a fairly common practice but it just feels so hacky to me. Of course that could be mitigated by creating a new virtualenv on that machine, then moving the packages over, but then at that point perhaps I would be better off having a 'lib' or 'vendor' folder that contains all third party dependencies and ensure that those are on the path (maybe using a venv for the interpreter?). Those dependencies could easily be gathered during the build, since the build process gathers up those packages for testing. Something like that supports our conda based clients a little better maybe?  Ideally, instead of virtualenv, I'd like to really isolate an application using a Docker image that already contains all the dependencies, then I don't care if you're a pip/venv or conda developer, but I have to build a middle ground solution first.  TL;DR: That'd be way easier, but we have developers pulling from multiple places and using tools that are not fully compatible with virtualenv and pip. "
"ffs, it's a for loop not a neural net.","Sure, there are other relevant factors to someone's values as an employee, and yes, everybody (not just programmers) likes having their ego stroked.  However, none of that invalidates the importance of having an employee who can do at least some of his work without having to do research and ask other team members (who have their own jobs to do) for help.  A good interview would judge someone's ability to collaborate and perform research, but it would also attempt to determine if they can actually do the job and how good they are under pressure (both individual pressure such as a deadline set by their manager and group pressure such as a release date set for their team).  All of that aside, this is not a highly complex problem that is designed to test those things, this is a 'freebie' designed to weed out employees who lack basic programming skills.  If someone can't handle this, the best possible scenario is that they effectively mask their lack of contribution to the company by directly copy/pasting code they don't understand out of google searches and forcing other employees to cover for the work they can't do.  tldr: ffs, it's a for loop not a neural net. "
It's useless to complain about a problem and do nothing to fix it.,"Great work. This reminds me of the hiring issue around  Millennials .  Companies for several years were slamming  Milliennials  because they seemed lazy, unmotivated, low-skilled, etc. But very few companies did anything to work around those issues -- they just continued slamming  Millennials  and writing op-eds about how shitty the  Millennial  generation is.  Lo and behold, the companies that invested time and energy into investigating and fixing these issues with  Millennials  are enjoying good successes. These organizations re-tooled themselves to provide extensive training and support services, pathways to advancement, relocation services, work-life balance measures, etc -- which are all popular with  Millennials .  TL;DR It's useless to complain about a problem and do nothing to fix it. "
I think you're partly right. A lot of good people panic in interviews and perform poorly.,"Way back in 1995, a friend of mine brought me a stack of floppies full of slackware. I was hooked. This friend taught me the wonders of shell accounts and the beauty that is *nix.  Many years later, I got him an interview at the company where I was an engineer (Unix/wireless core networks/etc). One of the questions my supervisor asked was ""Name ten unix commands"". My buddy choked. Not because he didn't know ten unix commands, but because he was  panicking . He'd been unemployed for several months, had five kids (!!!) and was desperate.  I wanted him on my team anyway (I was a team lead), and my supervisor said, ""OK, but it's on your head."" A month later he came and asked me if I knew any more dudes like my buddy, and could he hire them if I did.  TL;DR - I think you're partly right. A lot of good people panic in interviews and perform poorly. "
"Ambiguity is part of the game, and your coding skills aren't the only thing a company cares about.","Posts like these always seem to critique interview questions on the basis that the interviewer is some smug jerk and the interviewee is too afraid to ask any questions or expose his/her thought process, and say ""see, the questions are flawed!""  But in a situation like that, the interview process is flawed from the get go because of the mentality of the people involved and no amount of precise requirements-setting and word wrangling is going to save it.  In every interview I've been a part of, you're allowed to ask questions back to the interviewers to clarify the problem.  Heck, it actually makes sense to test if people will do this because real-world problems always seem to be poorly defined at first.  I've never heard of someone just being handed a problem on a notecard through a crack in the door and told ""We'll be back in an hour to grade your code"".  Usually the whole point is to get a sense of the person's thought process, and ability to communicate.  Usually the rebuttal is something like ""Well, what if the person's too nervous because of the pressure in the interview situation?"".  And while I agree that you shouldn't try to make your interviewees nervous if you want good results, if someone is too nervous to demonstrate their capabilities, then frankly they're just too nervous to be hired. It's unfortunate, but an employer is going to go with someone who can demonstrate their abilities every time rather than gamble on someone who may or may not be a secret genius.  Then there's ""what if the interviewer is hostile?""  Well if that's the case, the process is doomed from the start.  tl;dr - Ambiguity is part of the game, and your coding skills aren't the only thing a company cares about. "
"Whether his argument is valid depends on 
 
 whether language implementors should have the freedom to optimize tail calls 
 your definition of semantics","His argument rests on the statement that language implementors should have the freedom to do the tail call optimization.If they do have that freedom, then efficient tail calls might not exist in every implementation of that language and hence should not be included in the semantics of that language but should instead be included in the cost model (since the semantics doesn't include cost).  What you might argue is that the semantics does or should include cost.  I am inclined to agree with the author though.  The semantics of a language should describe  what  it does but not  how  it does.  tldr: Whether his argument is valid depends on   whether language implementors should have the freedom to optimize tail calls  your definition of semantics  "
"Intel says they now have Lego bricks for building arbitrarily large multicore processors, but still haven't solved problems of making arbitrarily large multicore processors work well.","I would note that the actual quote is, approximately, ""This is a scalable building block; you can stack them out to however many cores you want such as some really large number, but then of course you have problems with getting enough bandwidth across the edges to feed all those cores.""  The fact that the arbitrary really large number that they mentioned was ""1000"" is not the important information point here.  And the important information is not that they have a functional design for such a chip, because they pretty clearly said that they don't have such a design.  Just a design for a piece of it.  The important point is that they've solved (so they say) the problem of designing individual cores for large multicore chips, and that they now have other -- and much more fundamental -- problems to solve before they become practical.  tl;dr: Intel says they now have Lego bricks for building arbitrarily large multicore processors, but still haven't solved problems of making arbitrarily large multicore processors work well. "
"To be fair, the current kernel is an entirely different beast now, but it wasn't  their  kernel, it is still based off of someone else's work.","~eh, gonna nitpick.  Haiku's kernel is a fork of  NewOS  six years ago. He's not involved with the project anymore, but he is still active in #osdev & #haiku on irc.freenode.net (as am I :P) if anyone wants to know more about it :) This man is a genius, and has worked for BeOS, Apple, Palm, and several other companies, and is a very interesting person to talk to if you're interested in operating system design. If you have an iPhone or a Palm Pre, you're running bits of his code. Google also forked his latest lk kernel, although I'm not sure what they're using it for. I think its the basis for the 2nd stage bootloader on some Android models, but don't take my word for it.  tl;dr  To be fair, the current kernel is an entirely different beast now, but it wasn't  their  kernel, it is still based off of someone else's work. "
I was given fake code to test my problem solving skills.,">Been handed printouts of code and asked to describe what's going on.  In one of my most unique interviews I was given some code and asked to tell them what it did. I was immediately terrified because the code was like nothing I had ever seen before. After about 10 minutes or so of thinking out loud the interviewer told me I did a great job, which confused the hell out of me because I still had no idea what it did. Then I she told me the code was something one of their workers made up specifically for interviews, there was no compiler for the language and the syntax was intentionally ambiguous. The point of the whole exercise was to test my problem solving skills and see how I handled looking at confusing code.  tl;dr I was given fake code to test my problem solving skills. "
Even interviewers hate the interview ... so just relaxe and have fun.,"Here's an interview tip from my experiences over the last 20 or so years.  Don't bother being nervous.  The people interviewing you hate interviews as much as you do.  They brought you in because they liked your resume, so you've already got a foot in the door.  Relaxe ... be yourself, answer the questions honestly and to the best of your ability.  If they don't hire you ... well, you probably didn't want to be there anyway because they didn't think you were a 'fit'.  Talk to your interviewers like they're your friends.  Use colloquialisms and jargon.  Throw something up on the board, talk out your thought process, and say, ""there ... something like that should work"".  If they start quibbling over syntax, that's just another avenue for discussion that  you  actually get to guide.  The toughest thing in an interview is finding interesting things to talk about.  TL;DR : Even interviewers hate the interview ... so just relaxe and have fun. "
"I would rather know how to code, than how to do math.  Not everyone should be a programmer, but everyone should now how problems are addressed programmatically.","This is as good of place as any to interject my opinion.  Programming != math, programming is way more abstract but also way easier for the average person to get their head around its usefulness as a skill.  When I enrolled at my state university in 2000 my advisor was very critical of studying computer science.  I had stopped math classes after Precalc in high school (basically taking a year off of studying math) and as was told I would have a math deficiency. That it would put me ""way behind"" when it came to getting a computer science degree.  I had monetary issues as well coming from a single parent home, so I gave comp-sci a pass and studied political-science / Asian languages hoping to do some sort of work for a consulate or company located abroad.  12 years later I'm studying python as a hobbyist, I have done the first 20 or so problems on the Euler project and about half of thepythonchallenge.  The concepts I learned from programming literally made math accessible to me, not the other way around,. The world has changed, in a ""post-google"" world knowing how to find the things you don't know is way more important that actually knowing them.  Entry level programming (or mastery of Excel functions) is the logical way to know how ""do math"" without being highly math educated.  I will probably never be at the level necessary to work in IT, but the things I have learned will help me to accomplish things i wouldn't have been able to imagine doing without learning to code.  TLDR: I would rather know how to code, than how to do math.  Not everyone should be a programmer, but everyone should now how problems are addressed programmatically. "
"look very, very carefully if you see something claim to do a nontrivial operation in  O(1)  time.","Apart from the worst-case performance gotchas that people know about, there are also some subtleties with the commonly claimed  O(1)  lookup times. The basic issue is that information theory says  O(1)  lookup is impossible, because to distinguish  n  elements, you need to examine at least  O(log n)  information. Of course, in practice, we have 32-64 bits of information to distinguish the keys (the hash code), and real-world hash tables all have fewer than 2^64 elements, but including real-world considerations like that goes against the spirit of asymptotic complexity analysis.  So basically, if you were to deal with arbitrarily large hashtables asymptotically, the hashes would need to be logarithmic in the number of elements to be useful, and thus common operations on the structure would take logarithmic time.  The same arguments can be made about balanced tree-based associative maps, but way more people acknowledge those to have logarithmic-time operations. They do tend to be slower, in practice, than hashtables, but that probably has more to do with a lack of locality/convenient structure than asymptotic considerations.  tl;dr : look very, very carefully if you see something claim to do a nontrivial operation in  O(1)  time. "
JavaScript is not so bad. Learn it (because it's really simple) and get over it.,"I don't think it's so bad. I'm a C# dev by trade and a fan of Python as well as functional languages. While these languages are arguably much better than JavaScript, I think that JavaScript's ubiquity alone makes it a compelling choice for adoption.  Static typers complain that is dynamic -- well, neither is Python.Dynamic typers complain that it is weird -- well, it's gotchas are well-known and only a problem if you're really looking to lambaste the language.  I also have hope that JavaScript will continue to improve as a language. In fact, it is probably the most agile language out there. It's future is being shaped by the likes of CoffeeScript and TypeScript and its features are being standardized by a diverse collective in EcmaScript.  tl;dr: JavaScript is not so bad. Learn it (because it's really simple) and get over it. "
out of touch ex-developers who need to pay the bills among others.,"I've worked with someone like that. He was previously a project manager and hadn't programmed for a while so he was totally out of touch with the most basic elements of programming.  I was junior back then and when he asked me how to write an if..then I became really worried. Plus the client had given my one pretty intense technical interview so I wondered how the hell he managed to pass that one.  As it happens the client was having trouble finding consultants with their tough technical interviews so they went easy on the other guy. In the end I went on vacation and when I came back I guess the client found out I was doing all the work and fired the ""senior"".  The poor guy just couldn't find a project manager job so he took what was on the table even if it didn't suit him at all. It didn't help the guy couldn't be arsed to learn the country's language after 4 years. Not even a little bit.  tl;dr;: out of touch ex-developers who need to pay the bills among others. "
The tech was sacked for not keeping Windows up to date and I got twice as many hand cramps for a month or so.,"Whilst not an old DOS virus, I remember Sasser taking down my schools network in 2004, it infected the domain controller servers that the school used and then jumped to the XP machines that I and every kid, teacher and staff member were using.  It was a nasty little bastard, the teachers talked to the techs and only after a few weeks did it finally get mentioned that the computers (Both clients and servers) weren't really up to date (So Blaster was probably on there too).  It caused the network to go down and take all IT classes with it, which didn't bother anyone except me, who to this day has a seething hatred for writing with a pen due to the hand cramps I still get.  At first, I laughed my arse off at the computers going down thinking Sasser was an awesome worm, it decided to put me on the list of shit to fuck up that day because now I had to write. ¬_¬  TL;DR The tech was sacked for not keeping Windows up to date and I got twice as many hand cramps for a month or so. "
"Downvote me if you dislike, but learn assembly. You can thank (and upvote) me later.","Running the risk of voicing an unpopular opinion, I miss one language: Assembly - if you haven't learned it yet, you should do so on a spare weekend.  Pick your favourite architecture; I recommend 6502, 68000, ARM or MIPS, but feel free to use x86, it's not as clean as the others, but workable nonetheless, and if you have a PC you can dive right in (Btw. there are cool, sometimes even visual emulators for any of the aforementioned architectures, so don't feel restricted to your actual hardware).  Note that I don't recommend that you actually program anything of significance in assembly (though if you like, have fun). Just knowing the basic building blocks of the actual computation your CPU does (well today even machine code is not what actually runs on the hardware, but let's not go into that detail at the moment) gives you a greater appreciation for the heavy lifting higher-level languages perform to make it easier to program.  TL;DR: Downvote me if you dislike, but learn assembly. You can thank (and upvote) me later. "
If you overengineer FizzBuzz I'm going to wonder if you'll do that on real world tasks. Keep it simple.,"Don't over-engineer Fizzbuzz. It makes me wonder if you're going to do the same for every little task you get instead of just writing a simple working solution. For a simple task, the simplest solution is probably the best. I'm also well aware that people sometimes try and impress on interview questions by going a little overboard, so I don't directly penalise for it.  Realistically, I only put Fizzbuzz in as a litmus test. If you can't even google to copy-paste a solution, then I'm probably going to pass on you for a developer role. I don't directly score questions and don't have any specific pass-fail scenarios, but I do notice a strong correlation between Fizzbuzz answers and real-world overengineering.  As an example, I had one guy make a big deal about implementing the fastest possible solution, microoptimising the crap out of it - despite the fact that the question was bounded to print from 0 to 100. Apart from the fact the IO operation is the bottleneck (By several orders of magnitude), the data size is so small it doesn't matter anyway.  Once hired, he kept doing the same thing to every line of code - trying to write the fastest, leanest possible code even if it wasn't even remotely a bottleneck. As a result he ended up wasting a lot of time producing overcomplicated code. (Incidentally, he had no concept of Big-O runtime, and would often optimize individial lines in an O(N^2) algorithm when an O(logn) solution was available)  He was otherwise an excellent developer, just focussed far, far too much on microoptimising things that didn't matter and kept arguing when I told him not to.  tl;dr If you overengineer FizzBuzz I'm going to wonder if you'll do that on real world tasks. Keep it simple. "
"You can, people have been doing for six to nine months at least, you probably shouldn't.","Oh, for the love of god, this shit again?  HTML5/JS delivers considerably less features than Flash.  Flash is reasonably efficient.  Flash has a powerful development utility that puts control in the hands of game developers and designers, and not just hardcore codemonkeys.  Having an application built in HTML5 doesn't further any kind of standards, it just makes a mockery of semantics.  You have some media?  OH NO, BUT IT'S TRAPPED IN A MEDIA FILE?  Well, don't worry, we'll soon have it rendered into some nice, understandable indecipherable javascript in which alpha'd .png images are instructed to fly around a box.  Completely different from your flash media file.  You didn't want that to work in any internet explorers below nine, did you?  Some more of my favourite men of straw:  ""Iphones can't do flash"" - neither do they have a keyboard.  Games would still need to be developed especially to make use of iphone's unique limitations / abilities.  I anticipate they will run slow as shit down a drystone wall.  And if HTML5/JS games came to rival app store games, I would also anticipate a JS nerfing.  ""Flash is closed source""; well, suck it up.  What you want is a rival to Flash, not some shonky bullshit halfarsed library to recreate Amiga classics if they aren't too strenuous.  TL;DR:  You can, people have been doing for six to nine months at least, you probably shouldn't. "
"Microsoft just remotely nuked a major functional aspect of most of the phones that run their OS, presumably to force customers to upgrade to newer WM devices.","The thing is, even though most people laugh at Bing, for WM devices it actually a rather nice program -- it essentially acts like an internet-streaming GPS device.  A couple of days ago, for most WM users, it worked.  Then Microsoft pushes out a new version -- a version that is incompatible with all but the newer WM phones.  Nothing sinister there -- except, in cases where the new version is incompatible with the phone, instead of not installing, it actually installs the incompatible version, which then gives a popup explaining that the program is no longer compatible and will never work again.  No way to reinstall the old version, either, as even if you have it saved somewhere (Microsoft only supplies the new version via download now) the program checks for an update almost instantly and declares itself incompatible again.  tldr; Microsoft just remotely nuked a major functional aspect of most of the phones that run their OS, presumably to force customers to upgrade to newer WM devices. "
"Google does it because they come out ahead in the end, not because they wanted to give $5m to charity.","Google probably spent money to keep those tools available to themselves and to people using their other software, preventing those tools from being bought out by someone else, or from being abandoned and legally unusable.  The second part is passing part of the ongoing future development and maintenance costs to the community. (Not nefarious by any means, but certainly in their self-interest.)  Sure, they  could  have taken the tools completely in-house, spun up teams of employees to develop and market and create ""premium edition"" copies you need to spend $$$ to buy... But instead they can open it to increase the number of people using their other software assets and defray the cost of additional development.  TL;DR:  Google does it because they come out ahead in the end, not because they wanted to give $5m to charity. "
support for higher level AJAX validation and dynamic form schemas,"The big two features that I couldn't find implemented well elsewhere were good support for dynamic forms and underlying support for AJAX validation. The only form library I've used extensively was deform, so I can't speak much to the others. To my knowledge deform is the only library to support AJAX submissions out of the box, and that was a big feature for me. Professional sites validate things on the fly, give hints and tips etc, but aside from coding these by hand it seemed like there weren't great facilities for doing this conveniently. However deform had significant shortcomings that I couldn't work past. For reference, here are a few of the issues I had.   AJAX validation consisted of simply re-rendering and then replacing the whole html block. I didn't like this mostly because it seemed messy (I know, it still worked fine...) and it made trying to trigger JavaScript based notifiers (like bootstrap's tooltips) a pain. In general it just seemed inflexible.  By default it used Chameleon. I tried to use it for a while but gave up and have since moved to Jinja2. I know you can switch the rendering engine, but inevitably some of the frameworks design decision end up reflecting what it was built for. I'm sure Yota does.  Stuff that seemed like it should be easy was sometimes very complicated, such as trying to change the ""mapping"" template.  Although it was possible to generate a form with a variable number of fields, it involved no small amount of hacking with  new  and other messy things.  Validation that involved input from more than one node was a bit strange and hacky.  All validation errors prevent submission of the form.   tldr:  support for higher level AJAX validation and dynamic form schemas "
"prefer-dist is faster on the first install/update, but subsequent operations will be slower","IIRC, using that option means composer will download a tar.gz (or some other archive) of the latest appropriate version of the library. This means it won't clone the library's repository. So the next time you do a composer update, it will have to download the whole library again.  Compare this with not using this option, composer will clone the repository for that library so that next time you issue a composer update, it's just doing a git fetch and checking out the relevant commit/tag for that library (which is usually much quicker than having to download the whole library as a tarball again and again).  tldr: --prefer-dist is faster on the first install/update, but subsequent operations will be slower "
"abusing words to mean their opposite. subjection evaluation.  I really dislike the term ""clever"" in computing.","> If you're being as clever as you can be, to me it means you're using the most advanced features or language constructs that you know  While I understand what you're saying and the definition you're using, I find that definition  abhorrent . You're using ""clever"" to mean ""compounding complexity in an ill-advised way"", and while this usage is frequent in computing, it's also antonymous with the common, non-computing definition for cleverness.  Consider Homer Simpson renting a giant backhoe to dig out a small number of weeds. He's using the most advanced tools he knows how to solve a trivial problem. Do we call Homer ""clever""?  But semantics debates aside, the other problem with ""clever"" is that it's usually used to deride code one doesn't understand, irrespective of objective complexity or difficulty. I frequently cite a case where a collegue had observed 100s of lines of custom code replaced with the equivalent of a 5 list comprehensions. His critique was that the new code was ""too clever"", and he liked the old code better. ""At least I can debug the old code"". He doesn't want to learn about list comprehensions.  The new ""clever"" code worked flawlessly the first time. I'd just call it clever in the conventional sense, but that's my subjective opinion. The point was that  he  didn't know how to debug it, and that's what made it ""clever"" in the pejorative sense.  tl;dr:  abusing words to mean their opposite. subjection evaluation.  I really dislike the term ""clever"" in computing. "
"Don't over think and and specifically, don't over engineer it. You'll end up wasting too much time debugging unnecessary things!","> I'm not sure if I should write forgiving code or strict code.  Your functions should be descriptive and expect a specific type of input. If you want to keep it simple (aka KISS), make sure they take only a specific input type most of the time. I'm not sure what your example is trying to achieve (besides logging things), but it should say  logThisObject  to point out that it's only logging objects. It's really useful for future testing and ensuring your codebase doesn't trying to do above what it needs to do. If you need to log something different, then either create a new fn for it or just use your built in fns that are better at logging nearly everything.  TL;DR - Don't over think and and specifically, don't over engineer it. You'll end up wasting too much time debugging unnecessary things! "
"I lost everything in a head crash. 10 years later, I found it.","I had a similar experience. Only it was a hardware rather than user problem. I started up my computer and heard a very loud grinding sound. It was a head crash as the disk was seeking, so I had a beautiful spiral groove across the platter. Lost all of the code I had written (this would have been when I was around 12-13, so 8 years worth).  Then about 3 months ago, I was going through an old journal from 3rd grade that I was about to throw out, and found some BASIC code that I had written. You see, in those days, I wasn't allowed to spend much time on the computer. So, I wrote code out by hand, in class or when I was supposed to be doing homework. In fact, pretty much every program that I remember writing was there, including my first ever assembly program.  TL;DR: I lost everything in a head crash. 10 years later, I found it. "
"It doesn't make you safer and there are far cheaper alternatives, it shouldn't be required by law.","First, most tire pressure sensors in cheaper cars (to my knowledge, only have experience with a few) don't alert you when you have low pressure unless it's low enough to affect safety, so no fuel benefit there.  Second, good fuel efficiency isn't necessary for the required role of the vehicle, just desired.  And third, I know a GREAT way that for a couple of dollars I can check my tire pressure without spending hundreds or thousands on wireless transmitters and sensors, etc.  It's called a tire pressure gauge, and you can get a cheap one at just about any quickie mart or gas station, or a good one at any store that sells auto parts.  And if that thousand of dollars is the difference between you owning a car, and you not owning a car, that's a significant loss that was caused by unnecessary legislation.  TL;DR:  It doesn't make you safer and there are far cheaper alternatives, it shouldn't be required by law. "
I would rather my government make laws that prevent myself and others from being inconvenienced when there is technology that can prevent it.,"Shit, this is going to make me sound fascist, but fuck it, here we go.  If sensors are cheap, can report non-optimal air pressure, reduce wear on components, and provide better fuel efficiency, then the government should mandate that they are a necessary component on new vehicles.  Most leaks that cause flat tires happen relatively quickly.  It is in my best interest if my car notifies me as soon as possible that I am losing pressure.  That way I do not end up stranded or blocking traffic  when I finally realize there is a problem.  A tire pressure gauge does not help much while still driving, and tire noise only shows up if you are close to being totally flat.  TL;DR:  I would rather my government make laws that prevent myself and others from being inconvenienced when there is technology that can prevent it. "
Success does not come from whether you use frameworks or not.  The main benefit from frameworks is maintainability and ease of new developer integration.,"long time .java coder, as of late I have been consulting at a company in the .php world.  The company I am at made all of its success specifically by NOT using frameworks but just hacking things together.  But now maintainability is a nightmare.  So many n00b programming styles that make my Java soul cry. Oh useless singletons in .php, Tigga Please!  Memcache, Redis, you name it.. they have it.  Anyway, if they had have used a framework at the beginning they would definitely not have this maintainability nightmare, although I am also sure they would not be the success they are today as at the beginning they did not know what they wanted and tried everything and hacked it all up hellishly fast!  tldr  Success does not come from whether you use frameworks or not.  The main benefit from frameworks is maintainability and ease of new developer integration. "
"Technically ranges don't  require  GC, but that clarification doesn't get you very far with real D code.","Look, maybe you know something I don't, but from the documentation at  D Operations That Involve the Garbage Collector  Some sections of code may need to avoid using the garbage collector. The following constructs may allocate memory using the garbage collector:   NewExpression  Array appending  Array concatenation  Array literals (except when used to initialize static data)  Associative array literals  Any insertion, removal, or lookups in an associative array  Extracting keys or values from an associative array  Taking the address of (i.e. making a delegate) a nested function that accesses variables in an outer scope  A function literal that access variables in an outer scope  An AssertExpression that fails its condition   ...which is sort of my point. You have to be careful what you do without the GC in D. And that's not even the whole list. There are more bad moves listed in the ""Pointers and the Garbage Collector"" section from the above link. Granted, most are obviously bad ideas, but you've possibly done at least one of those things in your examples.  > auto p = cast(T ) malloc(n   T.sizeof);  That works, unless T or its members point into the GC heap. Again, see the above link.  On top of all that, it's going to be pretty hit or miss in the standard library trying to figure out which functions work without GC.  For a more concrete example of what I'm talking about, see Also,  is great and maybe you've read it. But read it again from the perspective of a kernel developer and see how one of them might say ""Nope!"" at certain parts, especially the ""Determinism"" subsection.  tl;dr  Technically ranges don't  require  GC, but that clarification doesn't get you very far with real D code. "
"It's society that needs to be yelled at, not just the nerds. So please stop yelling at nerds who for the most part want to help.","> Why such an insistence on one or the other?  The culture that exists at the professional level is an artifact of the people who get to the professional level. You're not going to convince a bunch of very practical-minded engineers that they need to change their culture for the sake of a bunch of strictly hypothetical women. Once there are women in appreciable numbers, you'll have much better luck. At that point it'll be an easier sell, easier to implement, and more effective with less chance of backsliding.  Beyond that, one is the  cause  and the other is an  effect . If you want to be efficient with limited resources, you target the  cause . Unless you are completely unable to, in which case you consider targeting the effect if such action would be meaningful.  Challenging CS and IT professional culture will do very little to affect the number of women in that sector, though. You cannot hire people who do not acquire the necessary skills. Even the most egalitarian, non-confrontational, and open culture is unlikely to do significantly better than 10%-15% lady engineers.  > How is it not likely that a culture that dissuades girls, at a very young age, from an interest in CS/IT persists at the professional level?  Once is a culture created by a whole society. The other is a culture created by a strongly biased subset of society. There is every possibility that the culture in this biased subset may be different from the larger culture. This subculture is known to possess significant traits in other realms, so it is not impossible or unreasonable in this regard.  tl;dr: It's society that needs to be yelled at, not just the nerds. So please stop yelling at nerds who for the most part want to help. "
"they can be useful and can be a tool to help get your work done, but they aren't the only tool.","I personally use unit tests for some things to ensure that they are correct and working as I planned. They can be good in theory, but they just become bloat if not maintained or if they are just too crappy. In my opinion and from my experience, it is far too easy to write a crap test than a useful one. I think they can be usable if you are dealing with the crowd on /r/programming (meaning, a crowd of programmers who care about what they are doing and want to keep learning) but I think that for the average (to sub-average) programmer, it isn't worth the time spent because if you spend an hour writing a test that is crap, you don't get anywhere. I work with a guy who put in 20 or 30 unit tests that he knew sucked just so he could get code coverage.  tl;dr  they can be useful and can be a tool to help get your work done, but they aren't the only tool. "
"Code duplication is wrong, but sometimes (rarely) it's the lesser evil.","So the theory goes, and it often works in practice. But I've come to believe that saying ""code duplication is bad"" is much like saying ""tables are bad"". While mostly true, there are exceptions. For tables, you should still use them for tabular data. I never thought I'd find an exception to the code duplication rule, but it does seem to have come up.  I think the big problem for us is that the site isn't as uniform as it should be. Everything that uses a core module wants it to behave a little bit differently. So you either end up with working around core modules in the other modules, or you end up with a hairy special-cased nightmare in your core modules.  tl;dr Code duplication is wrong, but sometimes (rarely) it's the lesser evil. "
"just like with any other code, use good practices. 
 edit: 'does support recursion' => 'does not support recursion'.","I actually disagree, pretty heavily, if you do it right!  The regular expression I posted actually doesn't represent the reality. That is how the regex looks,  after it is generated! . In my code, it is about 3 or 4 pages long. That is because the parts of it are split up, and cruft/boiler plate regex is generated or tacked on appropriately.  Above the regex is a list of rules of what it matches, written in english, with example code of what it does and does not match, and links to tests.  The regex was built for a language parser in an editor. Version 1, was done 'properly', with no regular expression, on top of Code Mirror. After about 2 or 3 months of work, what I had was slow, error prone, and hard to extend.  I ripped it out, and had it replaced with a regex version built on top of Ace, within a  single afternoon!  Ok, it does not support full recursion, but for syntax highlighting, I don't need a full parser. The regex above, also points out syntax errors, so you can do some clever stuff.  Plus because it's just a regex, I can easily slot them in or out. i.e.  // error'ing syntax gets a 'syntax-error' class applied to themsyntaxHighlight( errorRegex, 'syntax-error' );  Obviously you can have things optionally turned on or off with a parser approach, but it tends to come more naturally with regex based solutions.  The problem is that developers sit down to write regular expressions, and write them out like how I have posted it above. If you do that, yes, it sucks. If you just split it up though, write out example tests, and run them as you build the regex,  and keep that stuff around , then it's really not so bad.  tl;dr; just like with any other code, use good practices.  edit: 'does support recursion' => 'does not support recursion'. "
"Next time you're debugging a NullPointer exception, remember that your compiler could have found it for you without any extra work on your part.","Yes it really is. Imagine something simple like renaming a function in a 1,000,000 LOC codebase.  Now go and find all the places that the function is used and update them, and guarantee to me you got them all. Your job is on the line, and if you make a mistake it will cost the company $100,000 an hour in downtime. Keep in mind you're using first-class functions here, so the call-site might not name the function directly (i.e. grepping the source code isn't going to cut it).  In a statically typed language, you're compiler is going to find every single instance in a few seconds and give you a clear concise list of things that need fixing. A good IDE will do all the work for you with a few keystrokes, and the refactor might literally take a few seconds.  You  could  make an argument that unit tests will catch it, but let's be honest - unless your testing is as thorough as something like SQLite (Far, far more tests than actual code), there's no guarantee you caught everything.  Now taking it a step further, think of every time you've ever had a NullPtr exception. Or a bug caused by the wrong value making it into your state somewhere. How long did you spent tracking that down? How much time would it have saved you if you were using a type system that guaranteed at compiler time that couldn't happen? And did so in a way that required virtually no extra effort on your behalf.  Even Java's type system, which is ridiculously verbose and primitive, is still a fantastic trade off in terms of verboseness-vs-reward. Languages with type inference can reduce most of this verbosity while still retaining the compiler-time benefits.  tl;dr Next time you're debugging a NullPointer exception, remember that your compiler could have found it for you without any extra work on your part. "
"edit: SFML was popular because it was faster and better than SDL 1.2, but SDL 2.0 raised the bar again and a lot of devs have flocked back.","I'm not really 'in' the scene either, but I've used SDL 1.2 and 2.0, as well as SFML 2.1. I think this is more or less how it went:  SDL 1.2 was the dominant 2D graphics library for a  long  time. It was pretty much the only decent option, and did its job well. Then SFML showed up on the scene, did more or less the same thing but much faster and more efficient (since it was better optimized for modern hardware and rendering systems, while SDL 1.2 was fairly dated by this point). I personally switched one of my ongoing projects over from SDL to SFML, due to the not-insignificant performance increase.  But more recently still, SDL 2.0 -- which has been in development for a very long time -- was released as stable, and pretty much does everything SDL 1.2 did, but with performance comparable to SFML, making better use of modern hardware and such. Since SDL was the 2D king for the longest time and a lot of devs are very familiar with it, many have flocked back to SDL once more, since SFML's popularity seemed to be largely due to being a better alternative to SDL 1.2.  Anyone who knows better, feel free to correct me, but that's pretty much my take on what happened.  tl;dr edit: SFML was popular because it was faster and better than SDL 1.2, but SDL 2.0 raised the bar again and a lot of devs have flocked back. "
small teams can be better run with hands-off management (if you have good people),"I agree completely with this man - based on a 5-person project that I just completed. This was an engineering capstone project, where students are asked to manage themselves, and come up with a built solution in 8 weeks. My team of 5 knew our strengths and weaknesses very well, and we were all able to claim chunks of the project to complete independently. We finished with an excellent proof-of-concept prototype and a tight presentation and report.  One group got bogged down in their gantt chart (even had one student grading others on goals met). This group was so demoralized by their slave driver (she gave one of them a 27/100 for an action item) that their main drive was simply to survive until the end of the project. Their prototype did not function, and has several safety problems.  tl;dr small teams can be better run with hands-off management (if you have good people) "
"Awesome that you've built and worked on this, but the syntax isn't for me.","Cool that you are doing this project but this syntax immediately turned me off when I checkout the simple example:  &lt;title&gt;&lt;?=$title?&gt;&lt;/title&gt;  I guess at this point in time I just see it being much simpler to use:  {{ $title }}  As that type of double curly seems more universal for an echo. I also guess I am just spoiled to not want to use php open/close tags anymore. They are just a blight on the eye.  Your sections seem to have it right but I don't get the need for $this->stop(). After using blade with @stop its much nicer to look at.  tl;dr - Awesome that you've built and worked on this, but the syntax isn't for me. "
it was late and I was talking out of my bum.,"I made a test last night and it worked -- there's was no runtime errors. Though the test itself might not be the best one to illustrate it working/failing, I was mainly after runtime errors. (This is on AMD, Nvidia, the ref and WARP, both debug/release dx runtime).  Trying again today, the runtime was displaying it as an INFO rather than a WARNING, and I was filtering those (because of excessive INFO spam in most things).  > D3D11 INFO: ID3D11DeviceContext::Draw: The Pixel Shader unit expects an Unordered Access View at Slot 1, but none is bound. This is OK, as reads of an unbound Unordered Access View are defined to return 0 and writes are ignored. It is also possible the developer knows the data will not be used anyway. This is only a problem if the developer actually intended to bind a Unordered Access View here.  > [ EXECUTION INFO #2097373: DEVICE_UNORDEREDACCESSVIEW_NOT_SET]D3D11 INFO: Create ID3D11Query: Name=""unnamed"", Addr=0x00566E24, ExtRef=1, IntRef=0 [ STATE_CREATION INFO #2097279: CREATE_QUERY]Flushing GPU via query..  I was surprised the HLSL compiler didn't complain about RWTexture2D being explicitly bound to t1. Instead it just accepted it and made a token stream that had it bound to u1:  dcl_uav_typed_texture2d (float,float,float,float) u1  but I also didn't check that last night. grr. So I'd made a shit test that didn't properly illustrate failure.  TL;DR it was late and I was talking out of my bum. "
"Encryption only works when you care that the data gets to its destination, the rest can be circumvented.","This won't stop pirating. In fact, I'd daresay it'll be cracked within a week of release.  If AC2 works the same as AC1, that is with checkpoints, the game will probably call home every checkpoint to save. This makes it easier to capture the network traffic generated, and to trace the code that's transmitting the savedata - just keep triggering the savepoint to gather data.  It's very likely that it'll be using asymmetric encryption. Most people think of breaking it (which is nigh on impossible). Why not just disable it? Jump past the block of code where it's encrypted/decrypted, and then make it transmit to a local server. This seems far more trivial than what needs to be done for most other forms of DRM (with code locking you out all over the place etc).  Ubisoft won't be putting game logic on their servers - it'd be too expensive for them to maintain the servers. Handling savegames is one thing, that's just some fairly trivial I/O. Game logic that can't be easily emulated is a) not nearly fast enough over the internet and b) too heavy for their servers with a few thousand players connected.  So no, the emulated server won't have to be complicated. Receive savegame, write it to disk, and send back whatever data the program wants (this may be the harder part, but again, likely fairly simple to get from the binary).  TL;DR: Encryption only works when you care that the data gets to its destination, the rest can be circumvented. "
"Optimisation is complication and there's no magic bullet. Check your algorithms first, but asm-level optimisation is still relevant.","This is useful, but with a serious caveat: make sure your algorithm is optimal first. Sure, a profiler will tell you that you're spending 90% of your time in your sort() function, but if that sort() is running a bubble sort rather than a quicksort, than no amount of instruction re-ordering/stall-cycle elimination/cycle counting is going to get it going as fast as using the right damn algorithm.  On another hand, if you're writing code to take advantage of SIMD instructions like the SSE stuff in x86, your algorithms are probably going to look quite different to the traditional single-data pipelined algorithms. The thing to be careful of there is that you begin to obsess about parallelising your algorithm to the extent that you forget about the overhead of all the shuffling and packing you have to do to get your data sorted such that you can take advantage of SIMD stuff - for example, I've been writing colourspace transforms recently and a first-pass SIMD implementation only gained about 15-20% over a reasonably sensible C/C++ implementation, because so much time was spent in packing/unpacking operations.  On the other other hand, assembler-level optimisations can be really useful, when you get to that level: seeing that you're spending 5% of your time on an instruction following a load because it's stalling on a fetch from uncached memory can give you concrete, real-world gains. The trouble is, a lot of those gains can be very CPU-specific - sure, stall-cycles are an issue pretty much universally, but cache behaviour, register renaming, microcoding issues and so forth can vary even within a single CPU class, let alone across several CPU implementations from multiple vendors.  tl;dr - Optimisation is complication and there's no magic bullet. Check your algorithms first, but asm-level optimisation is still relevant. "
"You may not need to optimize the code, just make sure the code gets called less often.","On a side note; the profiler will tell you where the time is being spent  but it's not always the time that's important.  Example: I profiled an application that was supposed to be refreshing data and reflecting that data to the screen every second. When I looked at the profiled results, I saw that it was likely going to be pretty difficult to optimize the code enough to make a difference while still preserving functionality. But when I looked at the number of times the function was being executed, I could see a glaring bug: the data for each element on the screen was being updated in a dead loop despite the fact that this data was not being reflected on the screen.  tldr: You may not need to optimize the code, just make sure the code gets called less often. "
"webapps can replace 95% or GNU/Linux'  native apps, but we need a personal cloud platform first.","I think it is perfectly fine to replace most desktop apps with HTML5/python webkit bindings.  Native apps are only usefull when there is a real need to interact deeply with the GNU/Linux system. Other than that, HTML5 should be fine. Why bother choosing a GUI platform, programming platform, creating build scripts, compile farms, etc when there is no single need for them?  WebApps have a spectacular usability advantage. Instead of downloading, installing, finding shortcut, running, users can simply go to a URL and have them run the app. HTML5 makes short work of ""native apps"" in that sense.  The big problem I have with webapps is that most of them are written such that they are tied to the cloud. This is not a problem on itself, but the cloud is almost always ""server at evil company x"", where I rather do not want to put my data.  It is ok for me to use apps that connect to my personal server, and maybe lookup some generic data at some cloud service. A personal cloud server does not exist (yet), the thing coming closest near that is probably Amahi. But Amahi is far from usable, it needs leaps of development to get it's usability near that of say, Google Apps or Microsoft Live Apps.  Simple example: Contacts. How do you organize them? Using a native app on GNU/Linux, and then syncing them with your phone? Or Google Contacts syncing with your phone, and accessing them from your browser? Both is bad; the native GNU/Linux app for usability (you can't use the contacts on say, an iPad or in your CarPC. You'd need a sync protocol implemented in both clients, not to start about connecting your workstation (which might be a laptop which is on the move sometimes) to your iPad or CarPC). The Google Contacts solution is elegant, but then again: I don't want Google to have my information. Search queries are bad enough, contacts is simply not necessary.  So webapps are great, but the thing we all miss is a plugin-and-it-works homeserver with a one-click install of webapps for personal usage.  Off course, a lot of webapps can be created without any connection to the cloud, those apps are immediate candidates to create using HTML5.  TL;DR: webapps can replace 95% or GNU/Linux'  native apps, but we need a personal cloud platform first. "
The tapir has one of the longest penis to body size ratios of any animal.,"I am 21 and barely any of my classmates out of college have jobs. I am the lucky one. I work with 3 devs over 40, also my dad and his co-workers are >40 devs.  So basically, besides me, all of the devs I know are over 40.  Languages may change, principles generally don't. Knowing how to solve a problem is 90% of the challenge. Only 10% is writing the solution.  The 3 devs I work with are divided into 1 expert at everything and 2 experts at selected things. If I ask the latter for help with say... Java and they barely know how to write a "" Hello World! "" class, they can still help me.  Experience is extremely valuable and I think businesses know this, as none of my classmates have jobs.  I didn't like this article.  (Also, as a side note: my dad is in management and is one of the best devs I know of. He's in charge of 40 others and writes just as much, if not more, code than them. My boss is the same)  tl;dr:  The tapir has one of the longest penis to body size ratios of any animal. "
Learn as much as you can from the smart people you'll be working with.  Concentrate on architectural design rather than implementation detail.,"I've worked for three of the major investment banks, and there are both positives and negatives to the experiences.  On the positive side you're going to have the opportunity to work on systems the scale of which you'll rarely see outside of the industry.  Banks in particular also have a habit of attracting a LOT of smart people to work for them in I.T., so listen to what they have to say and learn as much as you can from them about architectural design.  Don't worry so much about language semantics or syntax - if you're right for the I.T. industry then you should be playing around with those on your own time anyway.  The biggest down-side to working for large banking organizations is the amount of politics involved in any project.  You'll likely start finding that decisions aren't driven by what's best for the project or the bank; rather decisions are driven by building and maintaining various power-bases within the companies.  Well, that's been my experience over the last twenty years or so.  Your mileage may vary :)  tl;dr Learn as much as you can from the smart people you'll be working with.  Concentrate on architectural design rather than implementation detail. "
"why people call Ruby and Python scripting languages as opposed to ""real"" programming languages?","That was quite an interesting read.  However, one thing bothers me. Let's have a look on how Steve Yegge classifies languages in his answer to Q7:  > Scripting language: Ruby. <snip> Perl, Python, Tcl, Lua, Awk, Bash <snip>  > Programming language: <snip> Java <snip>  This is not the first time I see people calling Ruby, Python and Perl scripting languages, sometimes  dismissing  them as such, often contrasting them with ""real"" programming languages such as C++ and Java, and listing them alongside shell scripting languages (Bash) as if there is no substantial difference.  Personally my definition of a scripting language has always been this: a language that is used to automate a range of tasks within a specific  application , and is not used outside of said application. Remarkable examples: MAXScript (3ds max); AutoLISP (AutoCAD); Bash and PowerShell (used to script respective shells). By this definition, JavaScript could be called a scripting language if not for NodeJS, which took JS from web browsers to outside world. But surely this is not the definition Steve Yegge uses, since Ruby and Python (and Perl for years before them) are widely used to build standalone applications and web sites?  I fail to see an objective criteria which will put Ruby or Python in the same group as Awk and Bash, and Java in the other. Surely it can't be the execution method (interpretation as opposed to running native compiled code) --- with byte-code and JIT compilation and platform VMs there is no longer any meaningful distinction between them. Is it dynamic typing? Relative ease and speed of programming? Execution speed? Not requiring a full-blown IDE? Dubious suitability for large-scale ""enterprise"" projects?  What are your thoughts?  TL;DR  why people call Ruby and Python scripting languages as opposed to ""real"" programming languages? "
"Read the post.  Then comment. If you have something intelligent to add, I would gladly continue the discussion.","hi, author here.  @cr3ative, either you didn't read the post or you are trolling.  In either case, you are not adding any intelligent discussion on the topic.  To address your comments:   ""It hides outliers""Screenshot illustrates how average latency can be misleadingly normal when 20% of requests are too slow. Screenshot is from our tool, on which the post is based. Read the post.   ""This is dishonest""Your comment is non-sensical.  Average latency for a website in our example includes all requests.  Whether any particular tool that uses Apdex measures only a subset of requests is irrelevant, you can substitute requests to images with requests to any other page on in your site that is less critical than the pages that are slow.   ""Apdex is the same as satisfaction score""Did you read the post? We didnt claim to invent the Apdex index. Apdex index is an industry standard score that we apply internally to better measure application performance in our tool. The post is about moving away from avg. latency to Apdex index type metrics to make better decisions about application performance.   TL;DR. Read the post.  Then comment. If you have something intelligent to add, I would gladly continue the discussion.   "
"do not rely on the ""top level license"" in an OSS project.","I doubt that it is by ""accident"", but the local rules are that the devs are supposed to declare up front that they are going to be using OSS code. I come along afterwards to find those instances of code they used, but ""forgot to mention"".  IME the vast majority of new OSS projects give no consideration at all to licensing. Most of the substantial OSS projects (the ones you'd actually use) are good, and try to adhere to licensing terms, but there are many, many small projects that don't. Even among the substantial ones it is not uncommon to find, e.g.,  MIT-licensed projects with GPL'd code in sub-folders.  And then there's all the MIT-licensed JSON parsers which include test data from JSON_checker, which is [MIT-Not-Evil]( licensed, hence unusable commercially. That one really is ""my old enemy"".  TL;DR - do not rely on the ""top level license"" in an OSS project. "
Python is good when dev speed is significantly more important than execution speed.,"The main reason to avoid Python in a general-purpose setting is speed. Python is very slow to run compared to a compiled and  strongly  statically typed language like C/C++ or Java. Generally you're choosing between development speed and execution speed, and Python is very far on the development speed end of the spectrum.  Any other reasons are generally domain specific. For instance, if I'm writing web middleware I'd probably use NodeJS. If I'm prototyping machine learning algorithms I might use MatLab. If I'm doing pure statistics I'll use R. If I'm writing a Windows GUI application I'll use C#/WPF. There are too many of these domains to enumerate.  So maybe it would be better to say when to use Python: I would use Python if you're writing scripts that have limited responsibility and high turnover, meaning they'll change fairly often and don't need to be particularly efficient. I would also use Python for prototyping, especially in a scientific computing setting. If your needs don't fall into these areas then there is probably something better out there.  TL;DR: Python is good when dev speed is significantly more important than execution speed. "
"if you are in math, either Mathematica or SciPy will serve most if not all of your tasks better than Java.","Since we were discussing programming languages in a technical subreddit I assume the person above meant Java when they said Java. So I'm going to set aside the JVM part of the discussion.  For dispatching, it sorta doesn't matter which language you use. Also it might very well be they don't do that kind of thing at all.  As for computations per se, I can see three options in mathematics:   The computation involves really heavy number crunching (double check, is it really math?).  The computation involves a bit of number crunching.  The computation is mostly symbolic.   So 1. and 3. more or less exclude Java. 2. doesn't, but there are better options.  tl;dr: if you are in math, either Mathematica or SciPy will serve most if not all of your tasks better than Java. "
The term “high level” is fuzzy and you seem to have a different idea of its meaning.,"> * Native string type is not actually a text type, but rather an array of bytes  I'm not sure what you mean. Could you elaborate?  > * No comprehension syntax  Go has  for range  loops which are somewhat similar, but I don't think that comprehension syntax is needed for a language to qualify as “high level.”  > * No native immutable types for the common data structures (immutable dicts, immutable lists)  Go has one immutable type, that is the type  string .  I don't see how the lack of immutability disqualifies Go from being a high level language.  > * No generics  While Go has interfaces which covers some use cases of generics, I agree with you that Go does not have generics. I don't see how the lack of generics disqualifies Go from being a high level language.  > * No exceptions  Go has exceptions ( panic  and  recover ), but they are sparsely used by the standard library. I don't see why a high level language must use exceptions for error handling.  > * Has pointers  So? I don't see what the problem with having pointers is.  > * No emphasis on laziness  If emphasis on laziness is required for a language to be “high level,” then almost all languages out there except maybe Haskell are not high level.  TL;DR: The term “high level” is fuzzy and you seem to have a different idea of its meaning. "
"Ruby is popular for Rails, but Lua is used in WOW and Adobe Lightroom and a lot of other apps! :p","Sure. I still occasionally use Ruby, though honestly Lua's library support has gotten good enough that it's pretty rare that I feel the need to fall back on Ruby.  And Rails....obviously for what Rails does, it must do pretty well, at least for some, though there are a lot of people who seem to think that it's over-hyped.  Can't really comment on it; I've used Rails all of about five minutes--to the point where I determined that it required a bazillion files to be set up in order to work at all. That didn't...feel right...to me. I want ""hello world"" to have 2-3 lines, not 40 files and 400 lines, you know? (made those numbers up, but it certainly seemed that complicated)  And...frankly, at this point I think Ruby has a much larger community almost entirely consisting of Rails people. I was a fan of Ruby well before Rails was written, though as I said I've since defected to Lua. And if you subtract out the people using Ruby for Rails, I think the Lua community is actually larger at this point. So since I don't use Rails, the advantages of the Ruby community are not so strong for me.  See the [list of programs that use Lua]( if you don't believe me. Ruby dominates in the server space, but in the desktop application space it doesn't even come close to Lua, unless there are a lot of stealth Ruby apps out there I don't know about.  A big part may be Lua's small size, and/or Lua's more open license (which many of its libraries share). Another part may be Lua's speed advantage over Ruby. Regardless, I do feel the language is better designed, and far more elegant.  Except I would have preferred zero-based arrays. :/  tl;dr: Ruby is popular for Rails, but Lua is used in WOW and Adobe Lightroom and a lot of other apps! :p "
"Postgres has bugs. People give bad advice on the internet. Use EXPLAIN. There's usually a workaround. 
 Also, try the Postgres mailing lists. It will save you time and heartache.","FWIW, I agree with you on 4 and 5, those are both beefs of mine. Not sure I understand complaints 2 and 3 though. Alter table support has been pretty solid in Postgres since the beginning, and outer joins have been around for almost 10 years. You sure you're not mixing it up with SQLite?  ""subselects are notoriously slow on postgres"" is a pretty broad generalization, and one example doesn't make it so. Perhaps a better phrasing is: ""NOT IN (SELECT ...)"" subselects can perform horribly on large datasets where the subselect is an aggregate across most of the table.  You are looking at O(n^2) versus O(n) in other approaches, so I think your were following some bad advice. This looks like a bug, as EXPLAIN indicates it will run the subselect once for every outer row (~1 million in my test). One workaround is to add an intermediate subselect, and then it finishes in seconds/minutes as opposed to days/weeks:  DELETE FROM foos WHERE id NOT IN (  SELECT id FROM foos WHERE id IN (    SELECT MAX(id) FROM foos GROUP BY bar, baz  ));  TL;DR:  Postgres has bugs. People give bad advice on the internet. Use EXPLAIN. There's usually a workaround.  Also, try the Postgres mailing lists. It will save you time and heartache. "
"don't be intimidated by what other people say. Work hard, do interesting stuff and have faith in your own abilities.","I don't know the original poster and he might be a genius for all I know. His post is obviously well intentioned and this is not meant as a personal slight on him.  However, I have noticed a trend in computer science students to do a certain amount of chest-beating that is often intimidating to those around them. When I started as a CS undergrad, I was terrified of all the stories from my peers about the mammoth projects they had conquered and their guru-level programming exploits. Later, I realised that a lot of it was bull-shit. Some of them were shit-hot, sure, but many of the louder examples were all sound and fury signifying nothing.  It's unfortunate, because even when the chest-beating is well-intentioned and formed as advice or passing on of knowledge, it does put people off. Later when I taught computer science, I noticed it was particularly bad for women. These 1st year undergrad girls, whose mates probably think they're weird for wanting to learn to program anyway, see all these alpha-nerds around them and think ""I'm not good enough"" and quit. They could easily be wrong.  tl;dr: don't be intimidated by what other people say. Work hard, do interesting stuff and have faith in your own abilities. "
"Voiced own thoughts -- they say it helps 
 [1] Yes, /r/{proggit,compsci,physics} is primarily entertainment, despite the temptation to categorize the time spent here as productive learning. 
 EDIT: [2]","Resonates, however  > Our brains need interaction with other people.  Perhaps this holds for the majority, but I wouldn't generalize. If I take away the constant outside pressure that ""checks up on"" whether you ""interact with other people"" (and thus lead a normal, accepted lifestyle),  then I don't see this need.  Also, ime, connecting and having friends is not enough. I've been brought up to make an effort to and was constantly in the position to interact with widely different people (believed to be a good thing, and it does have benefits) -- the result: a good number of dear, good, and appreciated friends practically none of whom share my interests, priorities, tastes in entertainment, and life role-models, and life goals. Trivial example to implement the abstract re, e.g., ""tastes in entertainment"": love for using the Friday to pile into bars and scream conversations about ""what did you do last weekend"" with the end goal of seduction -- I'd rather be alone at a concert of a good band or even rather be on /r/compsci^1. I found it hard to find people who would agree, despite having voiced this in plain text on multiple occasions [earns you no more than a ""weirdo"" label].  One symptom of my depression worth noting is: I don't want to be like most  non-depressed  people that I happen to interact with. They seem to be living in their own rosy world whose end-goal distills into consumption and procreation (or into worship, but let's keep religion out of this discussion.) Given this, how could I possibly get out of this state since I neither want nor can accept the obvious other?^2  For those of you who link depression tightly to our profession: seems unwarranted -- so which professions are so fundamentally different that they can't fuel one's depression? Even the exclusive ones (e.g. research scientists or art-related things) have plenty of potential for that. The one thing I'm happy about is to be in the field I am (although as most, I could use a  job with more interesting code to read and write -- those do exist and is something to work towards!).  TL;DR Voiced own thoughts -- they say it helps  [1] Yes, /r/{proggit,compsci,physics} is primarily entertainment, despite the temptation to categorize the time spent here as productive learning.  EDIT: [2] "
"Disregard bad food, acquire health and happiness. (Sorry for the digression)","I had a little bit of help because I started seeing a naturopath/dietician in order to determine the cause of my eczema (instead of continually treating the symptoms with steroid cream). Just from eating a natural ""anti-inflammatory"" diet (feel free to ask for details) for a couple months shaved off 20 or so pounds (and I wasn't even  that  overweight). Since then, it's easier to pick up my generally stationary ass and get more motivated to go to the gym, and commuting a measly 2 miles to/from work by foot/bicycle has really whipped me into shape.  I guess the moral is to start small, because something as simple (but not easy, it is definitely a challenge) as changing your diet can have a huge initial impact. After that, walk more, bike a little bit, just keep pushing yourself. I even started doing some P90X stuff at home and you would be amazed at how much 30 minutes of their Yoga program will transform your day.  TL;DR: Disregard bad food, acquire health and happiness. (Sorry for the digression) "
"A degree is a tool, not a weapon.  Use it wisely.","I am a college dropout who's been developing business applications for almost 12 years now.  I've been at my current company for 5 years, and in my current position (Senior Developer/Software Architect) for 4 years and 9 months.  I started here about 6 months after my coworker (we'll call him R), who has a Master's Degree in Computer Science.  After 3 months, one of our two senior developers left the company, and I was picked to take his place, over R.  There are two very simple reasons for this.  The first is that I was able to demonstrate an ability to not only solve my own problems, but to contribute to the solution of other team members' problems willingly and without complaining.  The second is that R uses his degree to tell other people why things are done incorrectly, and how things should be done.  As the meme goes, If you use your degree as a tool to bludgeon others, you're gonna have a bad time.  I'm not trying to discourage you seeking an advanced degree, but I am saying that your degree should give you the ability to adapt to new situations with an awesome working knowledge of your field.  Your degree should not be used to try to keep you in one place doing only one thing.  Unless, of course, you happen to be the guy who gets to use a CompSci degree to develop operating systems, machine code, new languages, etc.  tl;dr: A degree is a tool, not a weapon.  Use it wisely. "
"I use jQuery for medium sized projects. Zepto is shiny, but pointless.","jQuery removes a  lot  of the bullshit from JavaScript. No messing about with IE8, no need to have three different versions of Firefox installed for testing. It also makes a lot of stuff a lot easier (such as selecting elements). It's big, but if you use the Google CDN then the user almost certainly has it in either cache already. If that goes down, cool, local fallback. It will add a few milliseconds parsing time to  every single  request, though.  I use jQuery for medium-sized projects. For large projects, it adds too much overhead and I don't have enough control, so I write something myself. For small project, what the hell stop being lazy.  Zepto looks quite nice, but I'm not really sure what the point in it is - it only supports modern browsers (so not IE8), which is half the point in jQuery. I do like the animations stuff, but it would be nice if there were a fallback available for older browsers.  I haven't really used many other frameworks. I tried YUI, and thought it was disgusting. Should probably try something like MooTools.  tl;dr: I use jQuery for medium sized projects. Zepto is shiny, but pointless. "
"Unit Tests =/= User Acceptance Tests, also: Unit Tests are good.","First and foremost, you use unit-tests to make your software reliable. They are not about end-user-experience. Unit tests are there to let you be sure that your software works as expected in any possible state and can neither be broken by malicious input nor by introducing a code-change.  You do unit testing to avoid having to test each part of your software all over every time you change something.  Ever had the situation when you made just a minor change to a file which was intended to affect a very specific part of your software but after a while discovered, it had effects on a seemingly unrelated part? Well, unit tests would have had that covered.  Ever had some seemingly random fuck-up occur with just one user's input data and couldn't recreate the issue because the user just complained about an error but did not provide you with sufficient data on the issue? Well, unit tests are intended to catch these kinds of unexpected quirks, if written correctly.  It seems as if either you never write software more complex than what you can scribble on one piece of paper, or that you never have worked on mission-critical software, even less with a team.  Please do not let your ignorance be an argument against educating yourself. Just because you don't understand it, doesn't mean it's bad.  TL;DR: Unit Tests =/= User Acceptance Tests, also: Unit Tests are good. "
"that is, the plotting syntax is too verbose; the author didn't bother reading existing research about visualization.","I think you're just plain wrong about ggplot2. Off-the-shelf visualizations are only good if one exists that does exactly what you want. The minute you need something more, you're stuck with drawing primitives and obnoxiously verbose code. Most of the time people simply give up and do the visualization that was easy, rather than the visualization that tells the right story.  By contrast Ggplot and the other ""grammar"" based visualization systems (ggplot for python, Bokeh, Gadfly, Vega are the ones I know of) occupy the middle ground between rigid, off-the-shelf visualizations, and build it all from scratch.  I think a lot of people get frustrated with ggplot, because they see a pretty plot, and want to reproduce it, all the while thinking that ggplot2 will work exactly like their old-world plotting package. They don't take an hour to read about the first principles, and then they spend 2 hours cursing at the software because its unfamiliar.  In my humble opinion, ggplot2 is what you get when an academic with solid software engineering skills actually does the lit review before getting started. I can point to dozens of half-baked plotting libraries in a whole lot of languages, and I can sum up the problem with almost all of them as ""tl;dr""---that is, the plotting syntax is too verbose; the author didn't bother reading existing research about visualization. "
The optimum solution in my mind is so to include decoders for both formats and let the market decide which is superior in overall value.,"> JPEG2000 is an abomination in terms of quality, that's why no one's calling for it.  [Lies.]( Sometimes JPEG2000 is a bit better than JPEG, sometimes it's a bit worse. Overall the actual improvement is so slight if there is one at all that no one really cares enough to bother with it. That and there are lots of fears about submarine patents around it that delayed adoption significantly.  > Using superior formats is not about saving space (although that's one of their many pluses) - it's about quality.  And? That's not a reason to only provide patent encumbered formats to users. They already let you chose between low quality, high quality and HD on YouTube. What's one more transcode between friends?  > No, they care more about having a good video format.  Theora is not a bad video format. The main problem it has is that things sometimes turn out quite blocky in places. It is still pretty good at low bitrates and for many clips the blockiness isn't that apparent anyway. People do seem to make out like Theora is some sort of Tetrisification filter which really isn't fair at all.  Theora is not H.264. It is not as good as H.264. It might never be as good as H.264. So what? Yeah, people who pay money for patented stuff have a competitive advantage when it comes to quality. People who use open formats have a competitive advantage when it comes to collaboration and innovation.  TL;DR — The optimum solution in my mind is so to include decoders for both formats and let the market decide which is superior in overall value. "
"Yo Dawg, I put a list in your list so you could zip while you ++'d.","> fibs = 0 : 1 : zipWith (+) fibs (tail fibs)  This floored me.  Edit: For people who don't know haskell; or FP, I thought I might explain why the hell that works:  Haskell allows lazy lists, so fibs is only evaluated as long as something calls it (think 'yield' in Python/C#).  [Note the ' 0 : 1 :  myList ' syntax -- analogous to ' 0 + (1 +  myList ) ' in python]  zipWith, in this case will just add two lists element-wise. Which isn't really that much to get your head around. But when I first saw it, I couldn't believe you could add the list you were currently making to the tail (tail just says I want everything of this list but the first element) of itself and have that as part of the list.  tl;dr; Yo Dawg, I put a list in your list so you could zip while you ++'d. "
"gnaritas is right, it's in the article and the [kb entry](",">Both answers cannot be correct. How do I know which of you is right?  Simple: RTFA!  OK, I know the article is a bit technical and might be hard to follow for  some people. So, here's an excerpt FTA:  >In several places where earlier Windows versions report the amount of RAM that the kernel recognises as usable, Windows Vista SP1 instead reports the total amount of RAM that is installed. For instance, if the original Windows Vista sees only 3069MB of RAM on your machine that has 8GB installed, then the System Properties in Windows Vista SP1 will likely say that you have 8.00GB of RAM. This does not mean, of course, that Windows Vista SP1 actually regards 8GB as usable.  RAM that is overridden for hardware support is as lost to Windows Vista SP1 as to the original. RAM in excess of the license limits is discarded by Windows Vista SP1 as by the original.  Windows Vista SP1 just doesn’t let these losses show as obviously.  ...  >  the System Information program (MSINFO32.EXE), is known to contrast the amount of installed RAM with the amount that the operating system believes is usable.  There's also a reference to a KB article by Microsoft:  [ Windows Vista SP1 includes reporting of Installed System Memory (RAM)](   tl;dr: gnaritas is right, it's in the article and the [kb entry]( "
"History major gets feelings hurt, whines about it 
 EDIT: Removed the stupid.","It really bothers me when math and science types go after the humanities for not being problem oriented or call them bullshit.  We're all on the same side here. Interdisciplinary dick-waving contests are some of the worst wastes of intellectual power in the history of thought. I don't see why there's a need to criticize literature or history professors for their method of learning.  I know that reddit has a lot of programmers, so allow me to present the humanities side of the argument. Just because there aren't rigid, definite answers, doesn't mean that we don't spend hours and hours and hours thinking critically about issues of history, philosophy and literature. You may not think this is valuable, which is fine, but try and be polite. We are not your enemy. Idiots are your enemies. That is not to say that there aren't idiots who coast on memorization and the sluggishness of an ancient system in academia, but be fair. Many of us are bright, intelligent, critical thinkers who happen to suck at math.  And yes, math does have big scary teeth. I moved around a lot in high school, and every state has different requirements, which means that I actually didn't have TIME for anything past Algebra II. I was too far behind for a science or math based major, so I went with what I was good at.  TLDR  History major gets feelings hurt, whines about it  EDIT: Removed the stupid. "
"being intellectual is awesome, whether you know math or not. It's just that intellectual people usually are drawn towards math/sciences.","I didn't express myself very clearly. I indeed think that people like yourself are no less ""mathematical"" or ""scientific"" than a guy who can prove E=mc2 or whatnot... A good example is a lawyer I know who is stands out of the rest of the lawyers because she actually is a problem solver and not just a database front-end. I always tell her that she has a scientific / mathematical brain and if she would ever learn math it would be a no-brainer for her. I actually once put her through an IQ test and she got a very high score. I was quite impressed with someone who didn't even know what x = 1 means to do so well.  TLDR being intellectual is awesome, whether you know math or not. It's just that intellectual people usually are drawn towards math/sciences. "
everything that could possibly exist already does and we are just patterns observing ourselves,"Here's a thought experiment. Imagine if you could capture the physical state of a human brain and then model it in a computer - all of the physical processes being modelled 100% accurately inside a computer in real time, and being connected to the outside world so that you could communicate with the brain.  Then imagine leaving your computer on to calculate exactly what was happening in the brain in real time, but without examining the output, and the only input to the brain program is another deterministic program you wrote to simulate a virtual environment for it to live in, but both of the programs were completely deterministic and you gave no input to them.  Then imagine that you slowed down the program so that it was no longer running in real time. Then you interleaved the program with instructions to do other things not related to simulating the brain. Then imagine that you just wrote a program to calculate the Champernowne constant and do nothing else, and since any state of your brain program can be represented by a single finite integer, you are actually simulating your brain as a side effect.  tldr: everything that could possibly exist already does and we are just patterns observing ourselves "
"Stop making your webpages lag and jump around with this God-awful gimmick. Build a mobile version that is faster to load, easier to develop, and gives users a choice.","Responsive web design is one of the stupidest gimmicks I have seen in a while. Not only does it take longer to implement than a dedicated mobile version and constrict your design, but it does not even work well. Why would you force users to download two sets of CSS, JS, images or anything else? That is slow as fuck to download and to render. Make a mean and lean mobile version with nothing but the bare essentials. And in terms of usability, do you really think any desktop user resizing their window down a bit suddenly wants to see a mobile version? (Hint: not one of them does.) And do you really think that nobody accessing your website on their phone wants to access the desktop version? (Hint: many of them do.)  TL;DR: Stop making your webpages lag and jump around with this God-awful gimmick. Build a mobile version that is faster to load, easier to develop, and gives users a choice. "
"Yeah, you aren't going to have quantum money in your wallet any time soon, but that doesn't mean that it isn't worth investigating.","This is not being proposed as a currency scheme that will be practical in the near future.  No concrete physical implementation has even been proposed, so it is counterproductive to say ""it's going to be too expensive, so why bother researching it?""  Rather, a more productive response would be more along the lines of ""let's try to find a way of making this [or other quantum protocols] practical.""  I mean, of course it's not practical right now!  It's a protocol for a quantum computer!  Furthermore, since the cost of making a bill would be entirely independent of the value of the bill, it is kind of silly to say that each bill would cost hundreds of times more to make than it would be worth.  Just make the bill worth a thousand times more.  Unless you are saying that it would cost more than, say, a billion dollars to make one bill, then you are not actually ruling out the protocol's usefulness for business transactions.  As I pointed out before, trying to name a per bill cost at this point is pretty premature.  The cost lies in figuring out how to make the money in the first place, not in making each bill.  Since a practical implementation of this protocol is technologically pretty much equivalent to a practical implementation of large scale quantum computation, it's kind of funny to count the price of doing the basic research towards a hypothetical unit cost of the money.  Finally, you do not have to remake the money each time you verify a bill.  That is one of the main advantages of this proposed scheme over schemes that have been proposed before.  TL;DR: Yeah, you aren't going to have quantum money in your wallet any time soon, but that doesn't mean that it isn't worth investigating. "
"in python you have 3 choices (imo): slowness, terrible graphics, or foreign libraries","well, for curiosity's sake, I'll enlighten you, but if you're using pygame, you're fine.  I'm guessing you know about Dwarf Fortress (I don't know much, my friend showed me), well, you can have tilesets (e.g. make stuff not ascii) but with tkinter, it's extremely difficult. (actually I forgot about the python image library cause I'm doing 3.1.2 but that's a lib too) because tkinter SAYS it supports about 3 image formats for both color and on/offs but it really only works with .xbm's and those are stupid.  that is the problem with tilesets, but even if you just did it in command prompt (and I noticed this with a tetris game I made) it isn't made for stuff like that, and flickers really weirdly when you print multiple lines at once.  pretty much, python is a bitch with graphics unless you use foreign libraries because tkinter is inconvenient and slow with pictures (it updates them very slowly unless it's about 150x150 or smaller)  TL;DR in python you have 3 choices (imo): slowness, terrible graphics, or foreign libraries "
The correct answer for this question is returning Booleans and acting appropriately to them.,"The answer to your question lies in the details of what you are wanting to make. The package is defined as an input validation library that is used to validate information given to it and notify the system calling the library as to the result of this validation.  Because I see exceptions are recommended multiple times in this thread, I will start with them. Exceptions are used for handling an unexpected occurrence. In the case of validation, it is expected that validation will either say the item is valid or invalid. These two states are expected and normal in the execution, so they are not exceptional to the system. InvalidArgumentException is designed for a different purpose as well, that being if the argument is not of the correct type. It is also a Logic Exception, which is defined as an exception that leads to a direct fix in the code. An acceptable use of exceptions would be to throw InvalidArgumentException if you are validating a number is between 0 and 2000, but the comparison is only safe for integers and the given number is not an integer, but a string with a number. Whether or not the number is between 0 and 2000 is not an exception.  Assert should not be used because it does not match the definition of how to use it.> Assertions should be used as a debugging feature only. You may use them for sanity-checks that test for conditions that should always be TRUE and that indicate some programming errors if not or to check for the presence of certain features like extension functions or certain system limits and features.  This leaves your last option: returning a Boolean. This option is most likely what you will be using for input validation. Most validations are checking if it is something or is not something.  tl;dr: The correct answer for this question is returning Booleans and acting appropriately to them. "
"if you're new, copy-paste away, although try to understand what it is you're borrowing. If you're learning, you'll find yourself doing it less as if by magic.","I accept that the author's original statement is overbroad and will probably cause her a wince of embarrassment a year or two down the line. But there is a truth to it. By her own account, she has only been programming for about a year. When I was in that spot, I was copy-pasting a lot of stuff too; it's what's required to get stuff done, and if you're learning something practical, it's a good psychological boost to get something working, even if you have to 'cheat' on things you don't yet understand.  Nowadays, when I google something, it's typically to find out if there's a standard library function in $LANGUAGE that does what I need, or remember the damn name of one I already know exists - I've gotten to the point where picking up the basics of a new language is easy, but nobody is born with an exhaustive list of rust crates wired into their brain. I do try to keep in mind how rapidly I would come unstuck as a n00b, though; and the author is right to highlight the things we come to forget are a tedious pain in the ass to begin with (environment setup on a virgin development machine etc).  tl/dr - if you're new, copy-paste away, although try to understand what it is you're borrowing. If you're learning, you'll find yourself doing it less as if by magic. "
"Prove your abilities, dedication, and interest and the work will come.","How long have you been working there?  Giving you the small stuff at first is more to make familiar with the code base and the established coding standards.  Whenever you hear about anything more complicated, either volunteer to do it, or ask the other developers if you can help/sit with them while the locate and fix the bug.  If you have a bug tracker, ocasionally do some of the little ones to help clean up the code base and lower technical debt.  If not, try finding the little broken or hacky things in the CSS, javascript, or html and fix them here and there.  TL;DR: Prove your abilities, dedication, and interest and the work will come. "
There is nothing inherently unsafe about PHP/MySQL but its culture and legacy is full of bad habits.,"You're correct They're not limited to the PHP/MySQL stack. The PHP/MySQL combo is as safe as anything else. But many years of sloppy coding and missing features in the PHP/MySQL world resulted in a lax culture that to this day reverberates in Google.  For example, if you do a google search for [php how to insert data into a database]( you will get [this page]( as a first result. That page includes this gem:  $sql=""INSERT INTO Persons (FirstName, LastName, Age)       VALUES       ('$_POST[firstname]','$_POST[lastname]','$_POST[age]')"";  Yup. The first  six  search results are examples that are open to sql injection. On the other hand, doing the same search but replacing PHP with Django, Rails and ASP.NET give you safer examples.  tldr: There is nothing inherently unsafe about PHP/MySQL but its culture and legacy is full of bad habits. "
"retarded headline. 
 Then I read the rest of the article and it turned out to be retarded whatever headline you gave it.","> The projects that VisionMobile analyzed include Android, Eclipse, the Linux kernel, MeeGo, Firefox, Qt, Symbian (based on the governance model of the Symbian Foundation prior to the the platform's transition back to a closed model), and WebKit.  Which of these is a mobile platform? Not so many - Android, MeeGo and Symbian. Which of these isn't dead or unheard of? Android. Of course due to the complicated nature of the world it's not as open source as it possibly could do, but it's a hell of a lot better than iOS, windows mobile or anything else that's actually being used by people.  tl;dr: retarded headline.  Then I read the rest of the article and it turned out to be retarded whatever headline you gave it. "
don't make the country field a mandatory entry. And don't force a ###-###-#### format on phone numbers.,"Two other typical US-american errors in web-site forms:  (Please note that I use german examples, but this applies to (all?) many european countries as well).   assume all of the world has your fixed phone number scheme (in Germany, he can have numbers like 089/1234567 or 06003/123. The number before the slash is the prefix, 089 for München, 06003 for Rosbach). And no, there is no such town as ""Munich"", similar to the fact that there is no such town as Neujork (""New York"" germanized).  assume that in all of the world the state is of important. No one cares if some town is in Hessen, Rheinland-Pfalz or Nordrhein-Westfalen. After all the ZIP is unique for the whole country.   TL;DR: don't make the country field a mandatory entry. And don't force a ###-###-#### format on phone numbers. "
Don't quit. Approach boss and try to refine expectations to be reasonable.,"Take a step back and remember that of the 3 (Quality, Speed, Cost) you may select 2.  You have to make sure your company is aware of this as well.  I don't know of one developer who isn't required to deploy code that they are not quite happy with due to a client demanding a release or their company forcing them release the product.  I am with you in the fact that I strive for high quality code and do not like any bugs in the UX.  However it is unrealistic to think you will ever deploy a 100% bug free system, even in just the UX, let alone the entire system.  Demand the best work from yourself, but don't let it get to you when you create bugs.  Just strive to be able to detect the cause of bugs and patch them with increased responsiveness. I can say that my clients are more appreciative with my response time in fixing buts and generally don't even seem to be bothered with the fact the bug was there.  In regards to your company not having a Q/A department, this is unfortunately normal.  You should tell your boss that you are stressed, but don't just go to your boss with words.  Take documentation of all you have been doing, as well as bring some documentation that can help you to plan a better approach.  If your boss is not willing to work with you to reduce your stresses, then look for another job.  A good boss can appreciate that you are willing to admit you're stressed out, especially when you help them to identify what needs to be done to fix the problem.  TLDR; Don't quit. Approach boss and try to refine expectations to be reasonable. "
"Do something fun. Realize that you can't be perfect, but you can fix things at a later date.","I'll echo the others. Sounds like burnout. I'm currently going through a phase that's similar but not quite burnout (more like apathy than stress).  My solution? Program something fun on the side that gets me excited about new techniques or approaches. Doing this makes me think about problems in a different manner I often wind up solving problems in my work-related projects that I didn't even know I had.  For instance, I'm currently building a clone of Tradwars (the old BBS game) in PHP, but in the process, I came up with a simple process to render views from controllers that I think works better than what we have at work. I plan on refining it a bit and then showing my boss. We'll see what happens.  As far as bugs go, I don't get too concerned about those. I tend to be happy when something works. Edge cases are... well, they're edge cases. I can fix those later. For larger projects, I spend a ton of time bug testing because, like you, I don't want to submit buggy code, but at the same time, I'm not anal about it. If I miss something, I miss something. So does everyone else.  For smaller projects, testing is easier so I'm more confident about releasing solid code. But again, you can always fix stuff later.  At my job we have a QA person who screens and reproduces all bugs before passing them to a developer (usually me). She's pretty good at finding those weird edge cases, so that helps me a lot.  tl;dr Do something fun. Realize that you can't be perfect, but you can fix things at a later date. "
Linux users didn't care one way or the other about the Mono project. They didn't want it forced onto their desktops.,"> the Linux community never liked it from the start  That's a common myth. And also completely untrue.  Mono was installed by default on a bunch of popular distros. Users (including me) asked why. They were given a long feature list. The users then said, ""Yeah, but why is it installed by default? We're not going to write apps that need Mono."" It turned out that a single application (that sometimes worked right) relied on it and required a complete Mono installation.  Some Linux users were concerned about things like patents. That's a different story though. There was no good reason for it to be installed by default. Mono was never removed from the repos. It was trivial to install, and nobody was ever prevented from using it.  de Icaza was upset that he didn't have the power to force his new project onto everyone's desktop, so he took his ball and went home. They sold it as, ""Look at those hippies that are freaking out about anything Microsoft. So unreasonable."" That argument would have made sense if it was about more than whether it should be installed by default.  tl;dr: Linux users didn't care one way or the other about the Mono project. They didn't want it forced onto their desktops. "
They need to give us way more technical details before we are willing to put money into the platform.,"One of the major differences is that here you have no idea what you are really getting into. You can get the generally gist of it, but the rest seems to be behind a pay wall. If some of the documentation was available first it would be nice.  On Steam games have to go through somewhat of a screening process and on Kickstarter enough people have to believe in an idea in order for them to take your money. There is no guarantee of quality or service here. Which makes it understandable why people would be skeptical.  I really want this platform to be awesome. Though without more information than just this platform deploys to everything and allows you to re use your code for both client and server, it is not worth the money. When dealing with technical people and asking them to put money into your platform you need more concrete numbers. Like if they just told me how efficient their platform is compared to native on each platform I would be swayed. However all they are doing is throwing around buzz words. Technical people do not want market speak.  You are also going to want to use different UIs on each device. An Android UI is different than an iOS UI or Windows UI. Even Windows 7 to 8 is different. Do they have tools that help to use different UIs with each platform or do we have to come up with our own like using git sub modules?  TL:DR They need to give us way more technical details before we are willing to put money into the platform. "
"I think a recursively improving computer system is plausible in the sufficiently distant future, although it would probably be immensely complex and far more specific.","Not that I disagree with you at all, I think the whole AI apocolypse fear is pretty silly, but the article writer did preface that with the starting point of a human-level general intelligence AI. If we had a general/strong AI, and tasked it with ""getting smarter,"" we might just see such exponential results. However, that might require leaps in computer science that are so far ahead of where we are now that we cannot yet entirely conceive of them, hence why the EVE learning curve esque cliff of advancement probably is an exaggeration.  I don't think it's entirely unreasonable to expect for programs to optimize programs or programming in an intelligent manner in the future however. I think we're starting to see some of the first inklings of that in various cutting edge research that's being done, like work on proof writing programs.  tl;dr I think a recursively improving computer system is plausible in the sufficiently distant future, although it would probably be immensely complex and far more specific. "
"In order to write software, you need to know what the software is supposed to achieve.","That's the thing.> It's strictly larger than zero.  is false. The methods we use for ANI aren't getting us any closer to AGI or ASI, because these methods are, by the nature,  incapable  of doing anything like AGI or ASI. You need some  computable  information on the fitness of your machine model, and something like ""intelligence"" is not a computable criteria. Worse Siri to Better Siri is just an improvement on statistical methods. In fact, we are so far away from AGI and ASI in practice, because not only do we not know the processes necessary for human-like intelligence, we don't even know how to evaluate or compare intelligence in a computable, effective way.  tldr; In order to write software, you need to know what the software is supposed to achieve. "
One person's incompetence is not a compelling reason to literally invite MitM attacks. I believe we're done here.,"> I guess we should ban proxies entirely then. Even the HTTPS sort where you install the proxy's root certificate. All websites should be pinned.  That's a  trusted  proxy. A different thing entirely and not the subject here.  > Which failure modes? Have you examined all of them?  Let me put it another way: you propose to weaken a protocol already known to be fragile. I cannot see how literally inviting monkeys into the middle is a good idea, and we have both agreed that it strips away real features.  In practical terms, the failure modes look like the failure modes of code signing. Meaning someone gets to MitM you. Especially nasty when  you are inviting random people to do that .  > Not according to people who actually use the stuff. The benefits are marginal for you maybe, which doesn't mean they're marginal for everyone. The fact that one person actually was relying on HTTP caching already proves that.  One person being terrible at scripting it in no way, shape, form, or manner the same. Your argument on his behalf comes down to ""Some people are terrible at handling data in an organized manner, so we need to enable random third parties to mount attacks in order to solve this problem"".  I really wish I could say that was a caricature.  tl;dr: One person's incompetence is not a compelling reason to literally invite MitM attacks. I believe we're done here. "
floats are architecture dependent and many systems can't actually do bit wise on a float in hardware.,"Because bitwise operations on a float imply that you know the size and location of the mantissa and the exponent, not just the size of the variable (which you can determine anyways with various operations like sizeof() in C). Most languages do not specify the way the way floating points are stored as it is quite often hardware dependent (they are stored however the FPU likes to store floats which varies by architecture). A quick example is an 386 system, floats are stored as 80-bit numbers and you can't do bitwise operations on those numbers with the FPU, thus a float on a 386 needs to be moved as a two word number and the bitwise operations have to actually be performed as an integer.  TL;DR floats are architecture dependent and many systems can't actually do bit wise on a float in hardware. "
I want to know how the finished thing looks and functions. I have to make mine if you show me anything or not.,As i said i am doing this myself one way or a nother. I already have a few strings i can use and we did everything in school already and i can send an email to my teacher anytime and ask for help if i need it. I am just trying to see how other people do this. We did just the most basic thing in school but i think that people that have more knowledge about the program or have learned the basics and everyhing by themselves usually think more outside the box. I have a pattern i can follow and with my limited knowledge i have gained in the last week i cant really just write the code and finish in an hour.. i have to go trough all the excercises we did and make this thing work. I am going to do that but i want to see how other people do it.. my teacher is going to know.. she knows what we learned and how my thing should look so turning in something someone else made is not an option..TL;DR I want to know how the finished thing looks and functions. I have to make mine if you show me anything or not. 
Java has  new . Memory management is still the programmer's responsibility.,"Actually, both C and C++ also have enforced memory management. Or, what do you think  malloc  or  new  do? They only leave calling  free  up to the user.  Java uses a different memory management scheme that only eliminates needing to call  free . Thus, it is only an ""enforcement"" at the lowest level of abstraction, where it provides marginal gain. The programmer must absolutely manage memory in Java, ensuring that references are appropriately unbound, otherwise in the hands of idiots Java programs can get amazing memory leaks just like C++ programs. So much for any kind of ""real"" enforcement.  If you still have to manage memory though application policy, I would prefer the C++ RAII idiom, which coupled with C++'s deterministic invocation of destructors, while requiring a little discipline to write in the RAII style, leads to code that is easier to reason about.  TL;DR - Java has  new . Memory management is still the programmer's responsibility. "
"Programmers, 99% of the time, are exempt meaning no OT for you...(and me)","(non professional opinion here)Technically, an exempt worker is a professional position which determines how best to do a job and the time it will take to do it.  In reality, this means  any  most any job where your boss/manager is not telling you pretty much exactly what to do and how to do it, where to do it, etc. Also education and salary are determinants. You earn more and you went to college to get your job, most likely exempt.  So traditional labor like construction, store workers, etc. where the person can't just decide to build a room with 2x6's instead of 2x4's or to stock goods a certain way is non-exempt, i.e. they get overtime. Same for cashiers, phone workers, etc. Anyone in a union has their own set of rules.  If you want the really long explanation, see here ->  TL;DR Programmers, 99% of the time, are exempt meaning no OT for you...(and me) "
"not the author, agree with you (both) on structured interprocess format.","lol so many misunderstandings.   I quoted my blog somewhere else on reddit today, reading your comment made me feel like a boss for a second, because I might mentioned the idea of using structured data as a canonical interprocess format (json would be a nice candidate). Being a fp/lisp/clojure/ajax fan I'd love to see the map/reduce/filter being used everywhere. It will, when the ecosystem is ready. ps: recent articles talking about GNU coreutils LoC made me look at  ls . 30% of the source is about formatting output, that's not even easy to parse for other tools.. useless to me, but hey.. legacy and compatibility is real. I was almost tempted to fork and rewrite some part of coreutils on my own just for the thrill of it.   When I meant that I should have edited the title, I meant the reddit topic, not the blog article, which I found on HackerNews, with a longer and clearer title, by now you inferred that I'm not the blog author.    tl;dr : not the author, agree with you (both) on structured interprocess format. "
"He asked 1000% more questions than the other applicants, and now I'm late for lunch.  Ugh.""","This is a solid list, but you should probably prioritize it if not pair it down substantially.  It would take a lot of time to get through this list.  Possibly more time than would be permitted.  A lot of places I've worked and interviewed, the techinical and management interviews were usually scheduled for an hour each, and were often scheduled back-to-back with lunch and/or other meetings.  Another thing to consider is that this docket of questions looks an awful lot like a survey.  Anyone with a mild inclination to think analytically is going to wonder if you're there to just covertly gather data, with no real aim at being hired.  tl;dr: ""He asked 1000% more questions than the other applicants, and now I'm late for lunch.  Ugh."" "
When people think of escaping they usually think about the text concatenation paradigm.,"Parameterization does not necessarily be an abstraction of escaping. It might be a circumvention of the entire text abstraction altogether, in some instances (another call flow path).  The distinction might seem trivial or obvious but the fact that not everyone understands parameterisation stands as a counter-example to that. I have to work with PHP where a lot of people seem to think escaping means just that, and that the ultimate goal is parameterisation is often lost on the PHP community. I have seen so many bugs because of character encoding errors when talking to the database (also a parameterisation issue), people missing the call to the escaping function, people escaping data before using a parameterised interface, and the fact that a lot of PHP interfaces have parameterisation as an afterthought. I have been hacking PHP for 12 years now and still only learned yesterday that the SimpleXML addChild function [is not parameterised]( apparently by design, but this is [not mentioned in the manual](  ... all jokes about PHP aside.  TL;DR: When people think of escaping they usually think about the text concatenation paradigm. "
I think gets/sets have a place in the absence of properties.,"Aren't getters and setters just Java's version of C#'s property system?  I thought the whole idea was if you want to add validation, logging, tracking, etc. to a property of an object later, you can just modify the set method that's already used instead of having to change all the code that calls set.  Similarly for get, if you wanted to change ""Weight"" from being set to being calculated based off other properties you could do it without anyone outside knowing or caring.  For immutable objects approach, how would you do something like this (I'm genuinely open to an alternative, not sure how you would do it):  Lets say you have a Dog editor where a user can make a picture of a dog by selecting breed, color, name, etc..  I would probably have a Dog class that had setters/getters for all the properties that default to something nice in the constructor (or from a file that was remembered from a previous session) but can be changed.When a new color is selected in the editor, you would do something like this:  public class DogEditor extends SomeMoreGenericEditor {    private Dog mDog = new Dog();    private DogRenderer mRenderer = new DogRenderer();//...    protected void onColorChanged(Color newColor) {        mDog.setColor(newColor);    }//...    protected void onDraw(Canvas canvas) {        mRenderer.render(mDog, canvas);        }//...}  The dog could be drawn with a DogRenderer, which would use getters on the Dog (which extends some more generic Renderer):  public class DogRenderer extends Renderer {//...    public void render(Dog dog, Canvas canvas) {        canvas.fillShape(dog.getColor(), dog.getShape());        // Draw other things based off the dog's properties here        // (Could also call them dog.color() and dog.shape(), but they would do the same thing)    }//...}  I could see you instead of having a DogRenderer and Dog, be a more generic AnimalRenderer and Animal, with an AnimalEditor.  In any case it seems like getters and setters work well here rather than constructing a new Dog whenever something about the Dog changes.  tl;dr;  I think gets/sets have a place in the absence of properties. "
"a@.fm"" = ""No. 23, London"". 
 ""a@a"" = ""No. 23, Godknowswhere Street"". 
 
 Neither of these letters will get there. ;-)","I'm not sure about a@.fm.  Possibly there's a difference between the root DNS server for a TLD domain and a normal server  on  that domain - sending something to a@.fm would be like sending a letter to ""No. 23, London"" - it's useless without a street name, as ""London"" isn't a postal address, and in addresses cities don't directly contain houses (cities contain streets/postal codes, and streets/postal codes contain houses).  Hypothetically I suppose you could reserve an entire TLD for one machine and then create user-accounts on that machine, but it's hideously wasteful and inefficient to waste an entire TLD on one machine, so I'm not surprised nobody's apparently done it.  Regarding a@a, these days by ""e-mail address"" we usually mean ""e-mail address that's accessible using the internet"", and ""@a"" isn't a valid domain on the internet (no TLD to specify a DNS root server => invalid address, or if ""a""  is  the TLD, see the previous point).  If you want to roll your own e-mail system only accessible from within its own local network then yes, you could indeed use addresses like this... however, it's useless to anyone not on your local network, so it's not what we'd usually mean by an ""e-mail address"".  TL;DR:   ""a@.fm"" = ""No. 23, London"".  ""a@a"" = ""No. 23, Godknowswhere Street"".   Neither of these letters will get there. ;-) "
"Ask questions and listen carefully, and you will avoid most of the mistakes.","I've noticed that new programmers do not ask enough questions, trying to come off as independent and not needing guidance. In some cases it is true, but it's more important to understand exactly what is expected of you rather than project an image of independence.  Also, don't ignore the advice you get from more experienced programmers (so, read this thread, there's plenty of good advice here!). They usually have a good reason to point something out. Even if your first reaction is to disagree, try to overcome that and understand the other point of view. Programmers are often not great communicators, so this may not be always easy, but you will learn from the experience.  tl;dr: Ask questions and listen carefully, and you will avoid most of the mistakes. "
threads are hard and you should analyse the problem first to make sure that you will actually see a net benefit.,"When using threads, you have to specify the semantics for the thread safety. It depends on the problem trying to be solved.  Most cases there exists global system state that has to be shared between threads. Trying to synchronise this information is hard, and may end up reducing the ability to multi-thread anyway. If the work has to take a lock on the global data to do something then you can get lock-step where only one thread can run at a time anyway. At the hardware level, threads may end up with data bouncing between cpu core caches leading to stalls and hard to diagnose performance impacts.  Actually seeing performance benefits of threads (depending on the amount of shared state) is hard and bug prone.  If you can get the size of shared state down, then you may as well fork and have seperate process space. Then it's likely that your processes won't be trashing each other or in lock step.  The other big thing, that is often neglected is that doing QA and debugging in a threaded environment is also more complicated and harder. The behaviour dependson the interactions and races with other threads.  tl;dr  threads are hard and you should analyse the problem first to make sure that you will actually see a net benefit. "
Schools forget about teaching students social skills. I love the music analogy.,"I agree. I graduated in 2008 from a college that pumped out a truck load of ""software engineers,"" about 90% of which had ZERO social skills and didn't want to do anything but sit in a corner and be a programmer.  The few of us who stood apart from the majority were all hired before we graduated. We're all in ""engineering"" positions, and I ended up consulting in the financial services industry. Some of the remainder are still looking for work.  And as for the difference between ""programmer"" and ""software engineer,"" IMO it's the engineer who is asked to help analyze business rules, shape the requirements of the project, design and architect each layer of the system, and implement the system with the tools best fit for the job. The programmer is asked only to do that which the title implies -- program. Personally I prefer to call it ""software development"" so that I don't have to toss the two titles around.  I've often represented the company I work for at career fairs around the state. Time and time again the ""computer science,"" ""management information systems,"" or ""programming"" majors we choose to pursue can't make it through a simple phone screen -- they choke up on basic software concepts that the ""software engineering"" majors barely even blink at.  So that's my $0.02. Sorry if it sounded a bit egotistical, it was not my intent. I simply wanted to share my observations in my limited experience as a software engineer.  Also, I love the analogy to music.  TL;DR  Schools forget about teaching students social skills. I love the music analogy. "
"Yes, C++ isn't great, but its better then C.","You are right, of course.  I could write this same program without ""ugly"" (subjective) template syntax in a dynamic-typed language like python or perl where the code would be EVEN better.  The point is that people say ""Anything that has more features than C and I would never want to do anything else and C++ is terrible"" are wrong.  C++ isn't the best language in the world.  It is even particularly bad at this particular task compared to other languages.  However, the fact that it supports so many ""complex"" and ""detrimental"" features makes general-purpose programming WAY easier than straight C.  People who are arrogant enough to think that they know better then the past 40 years of programming language development are idiots.  tl;dr: Yes, C++ isn't great, but its better then C. "
"OO often seems to be about managing complexity, not reducing it.","> OO does a great job of separating and modularizing stuff.  This statement is absolutely true. But then, so is this one:  > ""[OO] is terrible because it makes this simple project way too complex!""  (Note: I deliberately omitted the word ""overly"" there.)  Beyond the issues with inheritance that Raphael_Amiard mentions, a difficulty I have with OO itself is that it establishes a fairly high complexity floor on every project. I am an old curmudgeon of a professional programmer; I believe it is part of my job to remove as much complexity as possible from my solutions. Sometimes, not frequently but irritating when it happens, the complexity floor of OO programming is higher than what I think the program requires; in other words, OO is adding complexity. At that point, my choices are to abandon the OO language I'm using, which is not often much of a choice; to go with a non-OO design, which leads to fighting the language and a poor OO design; or to accept the complexity hit.  tl;dr: OO often seems to be about managing complexity, not reducing it. "
"Some parts of technology change very fast, but the fundamentals don't change that often.",">It's estimated that half of what a CS undergrad learns in their first year will be obsolete by the time they graduate  Ugh, people repeat blindly repeat this crap all of the time. Have you even looked at undergraduate CS degree requirements?  Typical first year CS program from an accredited university:   2 semesters of calculus  Physics I (mainly classical mechanics and some basic fluid flow)  Intro to Digital Logic (gates, functional units, state machines, sequential vs. combinational)  Programming I (loops, if statements, functions, and maybe some basics of classes/objects)  General education requirements (English, social sciences, etc.)   Calculus and classical physics have been set in stone for a few hundred years and aren't changing anytime soon. Digital logic and basic procedural programming haven't changed significantly in 50 years (with the only exception being object oriented stuff). None of this is magically going to be obsolete in a few years.  Some parts of technology do change very fast (e.g. the semiconductor process technology on one end and the most popular websites on the other end).  But in the middle, there is this vast sea of things that move very slowly. For example, IPv4 is known to have problems with address space, but even attempts to force the change to IPv6 have proven to be an enormous problem. Another example is C, which will probably still be with us when the sun runs out of fuel. All major operating systems and almost all compilers, database systems, web browsers, and any other major performance-dependent system you can think of are written in C/C++. And above I mentioned the semiconductor process (which is more EE/ChemE than CS, but the point remains). The size of the circuits and details of the process might change, but for the last 50 years, this whole thing has had just one goal: making transistors out of silicon to create integrated circuits. And the fundamentals of integrated circuits (especially the things we teach undergrads) aren't going to change before one graduates.  Another important thing to note is that in the computer architecture research field, we constantly joke about how IBM actually invented everything that was possible back in the 60s and research now is just  a process of rediscovering and finding new ways to use this original work. :)  TLDR: Some parts of technology change very fast, but the fundamentals don't change that often. "
"Raise your wages which lowers the number of kid requirements.
Advertise possibly in in-game ads.","I also found the pricing confusing and thought i would give you the questions that popped in my head as i was reading them.  why is the price a range? Is the deposit taken from what i owe or extra  a few other notes why not add that $1.  without market research and such I couldn't give you a educated price but if it were offered where i live in West Virginia then I would have no problem paying $1000 a week and would consider $1200.  As far as expanding goes have you considered franchising might be a good way to expand like you want as well as greatly increased revenues for a successful one, and if you enforce criterion you can get rid of the ones that are not abiding by your standards.  Also advertising is crucial, considering the market you are after, people who don't go outside. A good direction might possibly be in-game ads shouldn't be that expensive though I am not really sure.  Hope this helps and I really think what you are doing is awesome. I truly wish I had something here like this when I was a kid and had one here now that I have my own kid.  tldrRaise your wages which lowers the number of kid requirements.Advertise possibly in in-game ads. "
"different people have  real different actual requirements. it's not just those ""lazy uncaring"" library/framework authors","> It seems like progress is only being held up by some library maintainers/authors who find it too much work or too hard to upgrade their code.  there are dominos that need to fall in order for python3 to be adopted. The problem is, everyone has different dominos. And it's an individual decision not a community decision.  For me it's 2 things.  1 pyramid(currently in beta so this one will fall soon)  2 Appengine(now at 2.7, maybe a few years before it gets to 3, or I decide to port to heroku etc...)  Given 24 hours in a day to work on stuff, the company I work for has customers that want things other than knowing that the software they use runs on the new fangled python. They don't pay us $$ because we are using python, they pay us $$ because we continue to deliver something useful to them in a timely manner.  We just upgraded to 2.7, our customers don't care.  I'm sure there's plenty of people who work in shops where red hat is used in production and it has been my experience that the netops people that manage those servers aren't going to allow it in prod on their servers until those servers have been upgraded to a version that comes with python 3. Last time I dealt with this, it was 2.4 or nothing. Has it gotten better? Fortunately for me I don't need to care anymore.  tldr: different people have  real different actual requirements. it's not just those ""lazy uncaring"" library/framework authors "
"Sun didn't fail, it succeeded beyond its wildest (and very limited) imaginations with the JVM.","While the JVM is biased towards Java and static typing, even before JDK7 and  invokedynamic , it is still flexible enough to host JRuby, which even with quite a bit of hoop-jumping actually performs better than Ruby on its native VM (when running in an app server, thus discounting JVM startup time).  A VM biased ""against"" Ruby performs better than a VM tailor-made for Ruby. This is because the hoop-jumping pays off by letting JRuby take advantage of all of the centralized optimization work that has been done on the JVM over decades, as opposed to haphazard and uncoordinated optimization efforts spread out across numerous language-specific VMs. Not to mention that JRuby has access to JVM tooling that doesn't exist on the native Ruby VM, giving a productivity win as well as a performance win.  I think history has shown that while bias is present, for the most part it can be rendered irrelevant. As long as bytecode  flexible enough , it can host a wide variety of different languages, and can take advantage of performance optimizations of the ""host"" language. Rising tide lifts all boats, etc.  TL;DR - Sun didn't fail, it succeeded beyond its wildest (and very limited) imaginations with the JVM. "
"I will never respect the word feminism as long as: 
 
 feminism - supposed to mean ""equality"" 
 while 
 
 masculism is synonymous with chauvinism.","That is the crux of the issue, most who use the word don't either.  feminism  should  mean equality for women.  In such a case then  masculism  should mean equal rights for men.  Although the existence of the words are counter intuitive by enforcing the segregation.  what I find funniest Is that feminism has devolved into  ""we want all benefits that we are entitled to, but none of the associated costs""  which is exactly the mindset men historically had that led to the mistreating of women in the first place.  All of this is human nature. we are all, regardless of gender wanting all the good and none of the bad.  The problem is, some women have hijacked a legitimate concern towards sexism and exclusion in certain situations and seem to have the mindset:  Since it is globally agreed that feminism exists and it is hard to disagree with it lets change it's meaning and message to get what we want!   tl;dr  - I will never respect the word feminism as long as:   feminism - supposed to mean ""equality""  while   masculism is synonymous with chauvinism.  "
"OP is basically saying that Java-style OOP isn't perfect.  True, but, uh...we knew that already.","Let me explain:  It's really easy to draw up a crazy UML class diagram about how ""all birds are animals"", or about how Car inherits from Land Vehicle, which inherits from Vehicle.  But the real world is really complicated.  A motorcycle and a car are both land vehicles, but you can't sleep inside a motorcycle.  And we can add multiple inheritance, but that doesn't help much either.  In fact, this entire noun hierarchy [is kinda messed up](  The world doesn't work the way your Java textbook wants you to think it does.  In short, OOP (at least, the Java flavour) sort of represents and, well,  instantiates  a very artificial way of looking at the world.  That process of instantiation we could call ""reifies"", and that artificial view of the world we could call ""subject object theory"".  And so we could say that OOP reifies subject object theory, or to put it another way, it trains you to make stupid UML diagrams and then write really bad code.  And it  might  even train you think about the world in bad ways too, which would be really unfortunate.  Now, the flip side of this is:  We already know all this.  That Kingdom of Nouns blogpost is from 2006; we've been arguing about nouns and Java-style OOP for even longer.  As the blogpost makes clear, other languages don't make the mistake (IMO) of focusing on nouns.  And we have a lot of other solutions too.  Python, for example, is kinda sorta object-y, but idiomatic Python makes heavy use of duck-typing, which leads you to a very different place.  Java is obsessed with inheritance and the "" is-a "" question (is this object a instance of this class?).  Python just wants to know if you've got the methods we want.  Which, if you squint hard enough, could even seem like the entanglements OP was talking about.  :)  And we see similar ideas in Go.  In fact, I'd say in general there's a trend away from pure inheritance, because pure inheritance looks great in a textbook, but doesn't always work very well in the real world.  TL;DR:  OP is basically saying that Java-style OOP isn't perfect.  True, but, uh...we knew that already. "
People who only know high level languages are becoming the majority and they are scared of low level languages.,"Perhaps it's because far and away the most common type of programmer on here is the web stack programmer specialising in C#/Java/Python/PHP on the server and JavaScript/CSS/HTML on the browser end. Even if you're not doing public-facing websites, chances are you're working with some desktop grade technology like Java/C#.  To that, things like Assembly and (to a lesser degree) C and C++ are becoming arcane arts that these programmers either used briefly during their schooling, or have not used at all. It's perfectly possible for a programmer these days to have a professional job doing client-server stuff without having ever written a line of C. So these people are more likely to entertain the experimenting in a language they largely understand (lol JS uber alles!) while shunning experimentation in lower level languages that frighten them.  Just my theory.  TL;DR: People who only know high level languages are becoming the majority and they are scared of low level languages. "
"Your client should know nothing about what is happening on the server, treat your services as another API that you develop against.","I'm going to expand and clarify a bit of what Castas said below to hopefully help you understand a bit more of what is going on in your application.  The first is, the basis behind REST is not requesting pages, but resources. A resources is a logical thing that you expose to the world, in your case above, Contact Message is a resource.  Now that we have our resource defined, how do we do things with it? The basic flow is that a client, and this is your client, or some other client that you didn't write or have control over, and may not be a browser, makes a request to the resource. Every request contains at least 3 parts   The method the client want to perform. GET, POST, PUT, DELETE, UPDATE  The format the client wants the result in. HTML, JSON, XML, etc.  And of course the resource it wants   So, given the above, what would your services look like? You may have a /contactmessage resource that has for it a GET method that returns either JSON or HTML. This is now your API that your client interacts with  Now, once your services are written, mentally treat these as written by someone else, you have no knowledge of the internal model or methods on that internal model. Your client, the Angular code, takes the data emitted by the service, and manipulates it independently. If you want something to be done on the service side, you ask the service to do it for you, but the key is the client and the server are completely separated. Someone else could theoretically write a completely separate client that uses your services, without having to know what is happening.  So tldr; Your client should know nothing about what is happening on the server, treat your services as another API that you develop against. "
I want PHP devs to be able to use powerful MVC framework features without the messy config/setup of the big php frameworks.,"Nah man, not a joke... The title is a bit trolling, my bad... But it's a reference to the design of the framework. I broke a few ""best practices"" to offer developers using it a better experience.. or at least, that's my intent. There are only a couple of issues that might be considered ""bad"" like a single include file to contain the framework, or the global set() and get() *magic functions ... But this is done by design.  I'm generally trying to offer a framework that caters specifically to beginners, small projects, start-ups, or even experts who need a more agile framework; I feel many of the big/popular ones don't do enough to support project bootstrapping, i.e., quick/small projects.  tl;dr -- I want PHP devs to be able to use powerful MVC framework features without the messy config/setup of the big php frameworks. "
Try writing a regex that matches balanced strings of parentheses.  You can't.,"When you are matching a regular expression to a string, you basically don't need to do anything more complex than keep track of your position within the regexp.  There are only so many positions within a regular expression, so there is a limited amount of information about your matching state.  This is equivalent to saying that a program that performs a regex match only has to have a fixed amount of state to keep track of the matching.  In order to match strings in a context-free grammar there is no limit to the nested structure of the grammar, so there is no fixed limit to the amount of information you have to track in order to make sure the string matches the grammar.  You basically need an unbounded stack to track all the state.  tl; dr:  Try writing a regex that matches balanced strings of parentheses.  You can't. "
Bluetooth is a lot more flexible on Android. At least when it comes to SPP.,"One problem with iOs:  SPP support in bluetooth is lacking. A wide variety of barcode and rfid readers do not support HID mode, and so can't be used with iOS unless you use a hid bridge which is super clunky. SPP Support is whitelisted on iOS only for teletype devices used by deaf people.  If you use a HID based barcode reader with iOS, you can not use the reader with the onscreen keyboard at the same time. You must disconnect the device before using the onscreen keyboard. Multiple hid devices can be used on Android at the same time, including the on screen keyboard.  One final fact, as the SPP readers send their data over serial, you can have much finer control on how that data makes it into the app. If using HID mode, its all but impossible for a app to discriminate between data entry from typing vs the Hid device.  So basically, this is why we are using Android.  TL;DR; Bluetooth is a lot more flexible on Android. At least when it comes to SPP. "
"work on open source, SimplePie sucks so help fix it","Sweet, you can [fix all of SimplePie's issues]( for me! :D  On a serious note, find an open source project that you've used and like, and see if it has any bugs you can look at fixing (or even helping with). It'll help you out using the project, since you'll get an understanding of the internals, and you'll be able to help yourself if you need to use the library in the future. As an open source developer, I'm getting increasingly less time to work on my projects, so receiving patches makes my day. This is true for a lot of projects that don't have someone working on them full time.  tl;dr: work on open source, SimplePie sucks so help fix it "
"shitty JS developer went overboard with redundant iframes and i had to deal with the aftermath of nesting hell 
 Edit: Formatting.","There is this one JS nightmare I will  never  forget... I had just started a new job and I had taken over an application that was heavy in client side JS. The tool was pretty complex, the developer before me had worked on it for two years and never completed it. My job was to complete the tool and judging by the  past developer's progress, it seemed like a quick, simple task. I mean, the tool was overall completed, just had some quirky behavior and a vast array of bugs. Anyway, I get to looking at the code and I am far from thrilled with anything about the structure. Now despite the fact the tool was primarily JS, the past developer still chose to iframe the tool, and progess through the tool's steps by submitting and reloading the frame. Guess he never heard of AJAX... This was rather annoying, but it got to be almost unbearable when the nested frames started appearing. Certain steps of the tool had three to four nested frames within the frame and it became a nightmare. Now the codebase already had two libraries (jQuery, Protoype) so '$' was already reserved and jQuery had to be called by it's full name. Now here I am, trying to decipher the poorly written JS, which is beyond spaghetti code; it made PHP look clean. There were event bindings for the parent frames within the child frames so there were multiple lines with code like:   parent.parent.jQuery('#ele').click(function(){      parent.jQuery('.eles').hide() });  Snippets like this were all over the place. Now this wasn't the worst thing in the world, nor at all challenging, but just  super  fucking annoying. It really made me want to kill the previous developer and quit my job. Trying to keep track of what frame I'm in and which frame the second parent is and yada, yada.  Oh, I almost forgot the verdict...I ended up getting so pissed, I rewrote the tool to use AJAX submissions and no iframes (except for image uploading, but that had to get done somehow, it's hidden though...). The tool works so much smoother, and debugs in a breeze.  tl;dr  shitty JS developer went overboard with redundant iframes and i had to deal with the aftermath of nesting hell  Edit: Formatting. "
"If you ever worked with a system which had priority numbers you had to set to get things to work in the right order, the system is poorly designed.","My dirty secret is that I don't split functions because they are too big, but I do it to tokenize away layers of complexity, because I'm not smart enough to debug my programs otherwise. Methods which do coordination between various subsystems often remain large, but by scrubbing out the details through iterative refactorings, those parts read like they are coordinating things, and the program flow remains clear.  Not giving the programmer insight into program flow is actually the #1 thing that I despise about Ruby on Rails, Drupal, and Yii, and probably every other RoR-like library that I would ever get around to trying.  Incidentally, this is another subtle reason that programming languages are converging to Lisp. Imagine being able to macroexpand your declarations. Imagine if the machinery which translates your declarations into actions could be polled directly to ask what those actions are. Instead, we settle for systems which give us silence or generic, ""something is broken"" errors.  tldr: If you ever worked with a system which had priority numbers you had to set to get things to work in the right order, the system is poorly designed. "
More FP features are awesome. I care less about the language. I'd rather ensue more competition to advance the field of programming in general than have more pointless flamewars.,"Well [I won't say that  no one  is using F#]( or functional programming languages in general for that matter. I think it a matter of features and not trying to provide ""an answer"" to another language.  A lot of the features F# and other FP languages provide are helpful and surely have their similarities in other languages. Honestly, in terms of developer productivity, Apple can go ahead and copy as many FP-style features as they want. It better productivity for developers and hopefully will bring more functional programming concepts main stream. I'm not sure of getting every developer to understand monads though XD.  tldr: More FP features are awesome. I care less about the language. I'd rather ensue more competition to advance the field of programming in general than have more pointless flamewars. "
The reaction to this post in this subreddit is pretty ridiculous...,"Wow, what a reaction ! Did you even bother to read the [very accessible introduction]( to automatic differentiation proposed in the first paragraph ?  The redditor below that assert that he took Calculus 3 and Abstract Algebra and can't even understand the beginning of this introduction... This defy belief since the first explanation of Forward-mode AD at least ought to be understandable by a first year undergrad with a modest baggage in Mathematics.  Paddy3118 definitely has an overactive imagination though it failed to entice him to try running the code that was given (I seriously doubt that any generator would include REPL interaction sample in its paper, too easy to check).  TLDR. The reaction to this post in this subreddit is pretty ridiculous... "
I think the article's point is proven pretty well here.,"OK, here are the reasons why that doesn't tell me a damn thing:  1) The code is in Haskell. This actually makes the whole thing very inaccessible.  2) Step 1: I understand ""functor"" to mean ""object that is callable like a function"". You suggest that ""polymorphic containers like lists are a prime example (of functors)"", but I have no idea what it means to call a list.  3) Step 2: I have no idea what ""component"" means in this context. Are you talking about a data member?  4) Step 2: Are you defining a function named foo_X that maps the result of calling F on X to the result of calling G on X? If so, why is it called a ""component""? If not, WTF is this?  5) Step 3: What is the dot operator? And how come you are now passing functions as input to the functors F and G? I am familiar with the concept of higher-order functions, but you didn't say anything about expecting the input to be a function before. Also, what is foo_Y? You said there exists a ""component"" foo_X, but you didn't prove there is more than one. Is this supposed to apply to any pair of ""components"" (including trivial pairs like (foo_X, foo_X))?  6) Step 4: OK, so now we have some kind of definition; except WTF is the ~> operator? And what does it mean for a function to be comprised of these component thingies?  7) OK, and how does this tell me anything that I can use? What, exactly, is the parametrically polymorphic function in the example that ""is"" a ""natural transformation""? And how does the knowledge that it ""is a natural transformation"" make a better programmer?   tl;dr I think the article's point is proven pretty well here. "
C's a great language but it's not by any means a catch all.,"Fair enough. However, I don't think those extra features really should be in the C standard library. C is a language that is pretty close to the metal without jumping to assembly. Some programmers are going to want those feature lacking functions because they don't need the overhead of a wrapper function that looks for configure files first. Those who do want those features can usually find it in an external library or write their own.  Now as you pointed out, because of the fact C doesn't come with a lot of feature full functions out of the box, I think this is why I've seen a lot more desktop applications written in Python or C#. Most desktop applications don't need to be super fast so there's no reason to write them in C. Not saying that Python or C# are slow by any means mind you.  tl;dr; C's a great language but it's not by any means a catch all. "
"Apple does blunder and does fix its blunders, as its past has shown.",">  In fact, Apple has never made a business blunder. Ever.  As a long-time Mac OS developer I beg to differ. When Apple bought NeXT they wanted to transition all Mac OS 7/8/9 development to the NeXTStep API's, the predecessor of the Cocoa frameworks. There was an immediate hue and cry from most development shops, especially Microsoft and Adobe who had extensive Mac OS-based code bases.  The following WWDC they compromised: Mac OS X would contain the Carbon API, which would be the modernization of the existing Mac OS API, as well as the Cocoa API, the successor to the NeXTStep API. The transition to Cocoa-only would be conducted over many years, and gently encouraged and not forced.  Why did they blunder? Control of the platform and where it was moving to, primarily. Many third-party frameworks had grown up around the Mac OS e.g. PowerPlant, Metrowerks etc. Same as the reasons for this current blunder.  Why did they fix their blunder? Obviously there were influential developers that made loud noises, but they had a small market and realized that the new Mac OS X needed all the help it could muster to get off the ground. So it was the developers and the market that convinced them. Same as the reasons for this current fix.  TL;DR: Apple does blunder and does fix its blunders, as its past has shown. "
"Disagree, one is voting for the american idol and one is voting in a democratic political election","Disagree. You've put it in a nice pithy way but the difference between the two is significant.  ""I refuse to support the existence of Ruby so I'll never install the command-T plugin for vim"" is a crazy statement. The goal is some kind of programming language purity that even if achieved would be harmful, a large ecosystem of tools is almost always better than a small one.  ""I refuse to support the existence of microsoft IP in mono so I'll never install Gnome Do"" is less crazy. The goal is software freedom and if achieved would actually have a societal effect.  I don't subscribe to either, but one is pure tribalism with no discernible end goal and the other is a principled stand against a political issue (the ability to copyright ideas); it might be a useless principled stand that makes an insignificant difference to the actual political struggle but it's not fair to say the two are the same.  tldr; Disagree, one is voting for the american idol and one is voting in a democratic political election "
"Make the world a better place, don't use singletons or global state.","The singleton pattern is one of my pet peeves in programming, in all its forms. Static classes, static variables holding that one instance, thread locals, global variables etc. Global state is a bad thing without an exception.  I haven't seen any use of the Singleton pattern where it actually improves the quality of the code base. It makes testing difficult and usually causes problems when people don't realize that there is a hidden dependency they should keep track off.  Unfortunately the workarounds for singletons require some effort. Singleton can be refactored with either ""pass as parameter"" (for constructors or method calls) or with ""make a class for holding the state"". Both of them make the dependency more visible, but require a bit more work.  public static xx() {    Singleton.getInstance().doSomething();  }  When exposing the dependency to the constructor, the code becomes something like this:  class MyClass {  private Something something;  public MyClass(Something s) {    something = s;  }  public xx() {      something.doSomething();  }}  Sometimes it's not a good thing to cache the singleton to the class, instead it can be given as a method parameter:  public xx(Something something) {    something.doSomething();}  Pushing the dependency to a state class works also, but it unfortunately feels a little bit more ""global"":  class MyState {  private Something something;  public Something getSomething() {    return something;  }}class MyClass {  private MyState state;  public MyClass(MyState s) {    state = s;  }  public xx() {      state.getSomething.doSomething();  }}  And the method counterpart:  public xx(MyState state) {    state.getSomething.doSomething();}  tl;dr Make the world a better place, don't use singletons or global state. "
"A hacker is the ultimate entropy creator, and great entropy creation makes success land on PG's lap.","It is my specific opinion that PG suceeded only because of PG's hacker qualities. But there is a world of differences between coders and hackers, let let expand on my definition of a hacker:  A coder is someone who codes. A great coder is a lazy coder: someone who spends the minimum amount of time to kill a problem. A great coder also need not produce the perfect piece of code. But a great coder has to finish what is on his immutable TODO list.  A hacker is someone who only follows the path of least resistance towards a very ambitious goal. A hacker is a great coder, but with no immutable TODO list. He defines the path he wants to take. Also a hacker goes well beyond being a lazy coder: he only focuses on the 80% that is done in 20% of the time. And his code's first purpose is testing reality. He has an internal pipeline of things to be tested. He build things and makes them evaluate reality fast, keeping 5% of the best ones and throwing the rest quickly.  The path he choses always is the one that reduces the greatest number of possible outcomes of reality. He turns random behavior into something he wants to see appear. If you're familiar with Computer Science or Physics, you can understand he is the ultimate entropy creator.  If you define the ambitious goal as solving search or solving identity, and follow the hacker way I described you have Sergey Brin, Larry Page and Mark Zuckerberg.  I'd like to add to my definition of a hacker that you can be a hacker and have your craftmanship be something else than producing code.  Are those qualities enough for everyone's success? No. Because you can always be a better hacker and this way is not for everyone. And there is always luck.  Did luck play a role in PG's success? Yes. But my point is that the best way to make chance land on your lap is creating entropy, and that's the hacker way.  TL;DR: A hacker is the ultimate entropy creator, and great entropy creation makes success land on PG's lap. "
"I choose my words carefully, and my point is that often, there's better uses of one's time than  canonical  documentation. You misunderstood, or choose to ignore, that.","Note that my upscase is standalone, not a string member.  Note that I argued that no explanation is needed to understand what  parameter  to upcase is.  Note that locale handling simply isn't subject of my upscale, and that is clearly visible from it's signature.  Note that you say the same yourself: ""Locale is a property of the context in which a string is displayed and transformed."" My upcase is simply orthogonal to that context. By extending your logic, the documentation for  a lot  of functions needs to contain large swaths of text explaining the said context. That can't be good either.  Note, finally, that I said that you don't need  an example  for firstName, no more, no less.  Tl;dr: I choose my words carefully, and my point is that often, there's better uses of one's time than  canonical  documentation. You misunderstood, or choose to ignore, that. "
Sometimes there's no good name for what ought to be a fundamental concept.,"In chemistry, everything has a proper and correct designation with respect to its chemical structure, but often chemicals are referred to by a shorter or more familiar name to save space and time when referring to it  Chemistry has the advantage that many chemicals come from or represent natural structures and can thus take their name from them.  e.g. Pentacyclo[4.2.0.0^(2,5).0^(3,8).0^(4,7)]octane, is better known as [Cubane](  In computer science, we often need a similarly short and descriptive name. While the article's suggestion to ""call it 'Alpha' and give it a better name later"" is the same concept, the short name should ideally be at least some sort of description.  The hard part, which Chemistry suffers from less, is either establishing the short name as accepted jargon, or being forced to rename the function with a sentence as the function name.  Now, there's the rule of thumb which says that if your function name is a sentence then the function does too much and should be split into smaller units, but that does not solve the problem for the function itself, unless it is replaced by its subunits in all places.  This kind of defeats the object of a function serving a particular purpose, especially if those subunits always follow each other in sequence. The function is a shorthand for those subunits. How do we decide which method is right?  Also consider that if the subunits are always called together, whatever parent function now calls them is apparently doing so much more than it was before, and may become the function which needs a longer name.  TL;DR Sometimes there's no good name for what ought to be a fundamental concept. "
I told on cheaters anonymously.  I come here to bare my soul (err...  pseudonimously).,"My first programming course (long time ago, in Ada no less) we had a lab assignment for which you turned in a printout of your code and a printout of the output.  I was in the CS lab the night before it was due, and  everybody  was running around with the same bug.  I don't remember the exact error, but I'm sure the problem was designed to introduce it, because literally everyone in the lab had the same problem with their output.  So then one guy starts telling people how he sent the output of his program to a file, edited the file to fix the mistake, and then printed that.  His program still had the bug, but he was only turning in the printout and was betting that the TA woudn't bother to type them in and run them (yes, we literally didn't have a system to turn in compilable source... this was the olden days...).  I stayed there in the lab all night trying to figure it out.  About 4am I found it.  By then pretty much everyone else had cheated and left.  Well, the professor had a thing called ""gripe"" set up on all the terminals so you could send anonymous feedback straight to him.  I debated a while, but just couldn't stand it (I'm like Hermione that way) and I told on everyone.  Still not sure how I feel about that, but I did it.  The next day the professor announced that anyone who confessed would get a 60% or something like that, and if they didn't and were caught they'd get the full cheating treatment.  I had a panic moment that he would make us raise our hands or something and then everyone would know who to murder, but he kept it discreet.  tl;dr: I told on cheaters anonymously.  I come here to bare my soul (err...  pseudonimously). "
"Kid stole my source code, and got caught because my prof. remembered how horrible of a speller I am.","A while ago I was running a blog of my CS experience at Penn State. I would also post project source code on a ""private"" section of the website, as a backup of my work. I stopped updating around Sophomore year and eventually forgot I ever uploaded my source files.  A few semesters later, there was a random class discussion about department rules and my CS professor pointed out (awkwardly in class, in front of 10 other students) that I was involved with getting a student kicked out of the department. No idea what he was talking about, so I went to him after class. Turns out some kid in his other class googled a project he was assigned and found my ""private"" backup of source code. Kid copied it, turned it in, and reaped the rewards for cheating after getting caught.  The best part: how did the prof. know the kid was cheating? I'm a horrible-as-fuck speller and the prof. quickly found the origin of copied source by googling my misspelled comments.  I love that professor for so many reasons; most definitely the best prof. at Penn State! His decision to not get me involved, but take care of the problem, was awesome :-)  tl;dr - Kid stole my source code, and got caught because my prof. remembered how horrible of a speller I am. "
You fatally assume my critique to be an attempt at the qualitative enumeration of Dr. Pepper where it is actually a literary affair.,">You are assuming that I think your opinion is less valid than my own  I would note the entire post you just wrote outlines why you think my opinion is less valid.  >the key point is not that they were opinions that we both have, but how they were delivered  My delivery strictly outlines how I feel about Dr. Pepper, thus it reflects my opinion, showing opinion to be the issue at heart. Do I think Dr. Pepper tastes bad? Yes. Is that an insult? Sure. That has no bearing on the validity of my opinion. ""Bush was a terrible president."" Just because I am insulting former president Bush does not mean that my opinion is invalid, or that I am saying everyone else must think Bush was a terrible president. Sure, if you read it literally, it's an absolute, but we, as humans, don't relate literally.  >it sounds more like a Glen Beck style mostly emotive insult  While I resent the ironically emotive comparison to Glenn Beck, I don't think emotion is, by any means, a bad thing in the expression of opinion. My use of the word ""shit"" denotes extreme distaste, and foreshadows the end of my consumption of Dr. Pepper.  tl;dr: You fatally assume my critique to be an attempt at the qualitative enumeration of Dr. Pepper where it is actually a literary affair. "
"abstraction is awesome when it's actually abstraction, and terrible when it's not.","Your example argues against a point I never made. I didn't say ""you should never use functions for abstraction"". I said ""you shouldn't always break things apart when you reach an arbitrary limit.""  So, yeah, of course there's points where you  should . No arguments there.  That said, your examples aren't even equivalent - you haven't shown the implementation of InnerContext1 and InnerContext2. That's where all that nasty ""innerctxelem1"" stuff is going to show up.  > IOW, up people use abstraction for everything all the time. And here, you are trying to flatten stuff out.  I absolutely agree with the proper use of abstraction. I don't believe that ""oh man, we've got three loops, time to add some abstraction!"" is the proper time. Some abstraction makes sense with a single loop. Some abstraction doesn't make sense with ten loops.  > I have this to put to you: even if it seems that going flat is simple, it's eventually bigger burden on the brain. (Is there a psychologist in the room to explain this better, please?)  I disagree with this as well. Abstraction adds mental overhead. It can also reduce mental overhead if the abstraction is reasonably self-contained and not leaky. If Algorithm A uses Algorithm B, and Algorithm A involves sixteen temporary values and needs to pass two of them to Algorithm B, then abstraction makes a hell of a lot of sense. B really doesn't rely on A. That's good! Do that! Thumbs up!  But if Algorithm A uses Algorithm B, and A involves sixteen temporary values,  all of which have to be passed to B , then we don't really have abstraction. We have a single process that's been cleaved into two separate functions for arbitrary reasons. That's not going to save you brainpower - if anything it's going to  cost  you brainpower.  tl;dr: abstraction is awesome when it's actually abstraction, and terrible when it's not. "
Checked exceptions: Seems like a good idea until you have to use them,"Often times it's inscrutable exceptions from the library you are using that cause checked exceptions to be a pain in the ass. For instance ""BadAlgorithmException"" being raised because the algorithm is passed to the library method as a string which is then parsed inside the method. This is clearly terrible design for the library: why wouldn't you want to have the type system check this for you?  But the exception is checked, so now you have to either structure your entire program to handle the ""world is coming to an end"" case where you mis-typed the string literal you are  looking at right now and can see isn't wrong  or you can catch the exception and do nothing because you know it will never be thrown. Either way, your code is much worse because someone somewhere down the line decided that some random thing needed to be a checked exception. (Or you can be super evil and add BadAlgorithmException to your method's list of exceptions!! MWAHAHA)  tl;dr Checked exceptions: Seems like a good idea until you have to use them "
"When competing for a job if you and your competition have equal skills, but he already knows the language used in the position, who gets the job?","I can agree with this, but in some cases when you want a new job they are going to say things like ""we want someone with 3 years experience in language X"".  I think it really depends on what X is, many good programmers can pick up a new language pretty quickly, especially the well designed ones in use today.   The difference is certain languages like C++ have some many nasty corners, and surrounding knowledge areas (build systems, cross platform support, real time considerations) that it can just be easier to get someone with the experience in the first place.  tl;dr - When competing for a job if you and your competition have equal skills, but he already knows the language used in the position, who gets the job? "
"Anyway,  plonk . 
 edit, sorry I'm new here, this was the first time i've tried to quote anything, let alone separate walls of text 
 edit2: fixed","I have a few problems with his response:>Modules are about name control and only secondarily, if at all, about types.  in what world is modules about name control? Modular programming is defined by the concept that all aspects of the software is separate and independent of all other parts. This is not directly related to name control in any way.  > F# is what it is: a .NET language first, ML second. If the F# > developers wanted an implementation of ML on .NET, they could have>had one,  .NET is a runtime environment, not a paradigm of languages. Which by the way is to a great degree composed of modular languages, while F# is a primarily functional language that makes use of this runtime, and it's many Modular libraries.  >perhaps by modifying the SML implementation on the JVM, >which interfaces with other JVM code only at the far end of a >convoluted stick.  I hope he realizes that the JVM and .NET frameworks are completely and totaly seperate entities. In fact, to a large degree they are competitors.  >programming, like teaching, is primarily a job  No programming is the act of using a tool to which a Computer Scientist, software developer, software engineer, etc uses to preform their job.  tl;dr?>Anyway,  plonk .  edit, sorry I'm new here, this was the first time i've tried to quote anything, let alone separate walls of text  edit2: fixed "
"In JavaScript ""Composing functions is only possible if those functions only do one very specific thing"", because doing otherwise is too verbose. This could possibly change that.","Not if you generalize the ""very specific thing"" they do. Take the function [fmap]( for example.  In an very abstract way, this function takes a function as its first argument applies it to everything that is ""inside"" the second argument -- the only requirement is that the second argument is an object for which it makes sense to talk about it ""containing"" something.  In my mind, a large part of the reason for Haskells recent success is that tries hard not to be for the programmer not to write composable code.  Javascript does not do this. Do you want to pass a way to combine two items to your function? If the operation was a simple plus, then you need to write:  some_func(function (a,b) { return a+b; })  Compare this to haskell:  some_func (+)  Do you have a function that takes two arguments, but you would like to set one of them as a constant, thereby getting a function that only takes a single argument? In javascript:  function (a) { return orig_func(CONST, a); }  In haskell:  orig_func CONST  While these examples are just superficial syntax, they (together with the type system) enable the programmer to write truely extensible code. For a more high-level example see  many1 , and arbitrary state and an arbitrary output-type and a new parser, which has a list as it's output type instead and works by applying the original parser until it fails.  While you are of course able to write JavaScript which is just as modular, nobody does this, because it is way to verbose. I believe that something like this project could change that.  TL;DR: In JavaScript ""Composing functions is only possible if those functions only do one very specific thing"", because doing otherwise is too verbose. This could possibly change that. "
"The article should have been titled, ""Introduction into DI in JavaScript"" and had my upvote.","Haha, I guess I could just be putting more faith in people than you. It just seemed like the article treated me like a child. ""What is that, you say? I can use mock objects?"" I just thought everyone knew (inherently) that JavaScript was a duck typed language and duck typed languages are made for mock objects.  There is no framework, only the same 'ol duck typing JavaScript. Move along.  The article just describes, generically, how one should make use of DI. That's all. No magic, not even really anything JS specific. The methods described should be heeded in JS as well as if one were implementing some interface in java.  As for your comments about AngularJS, I certainly don't run Java for my tests nor my applications, so your quip about its IoC methods has no bearing on anything to do with it as a JavaScript DI framework.  TL;DR: The article should have been titled, ""Introduction into DI in JavaScript"" and had my upvote. "
"It doesn't matter. Each has a strength and a weakness, and neither will go away. Pick one and use it, you'll be fine.","Neither.  It comes down to a matter of comfort and which style you like better.  symfony2  symfony2 has more 'magic' than Zend Framework 2. By this I mean things like annotations, or the need to never really specify where things are at. Things just kind of... work. There's a lot of CLI commands through their console that help with code generation and basic tasks.  The downside is that because you can do things so many different ways (annotations, XML, PHP, yaml), keeping your bundles consistent can be a pain. If you are using annotations for a bundle but can only find tutorials that use XML, it looks inconsistent.  Zend Framework 2  Config hell! Actually, it's gotten a lot better than the betas, but you still specify where a lot of things are. If you like tight control of where things are located, or being able to quickly glance at a config file to see how something is set up, it's there. There's a lot less magic in Zend Framework 2.  The documentation is a bit lax in places, but overall because the system isn't so spread out between different ways to declare code it feels more consistent. You will also lose a bunch of command line helpers that symfony2 has.  tl;dr  It doesn't matter. Each has a strength and a weakness, and neither will go away. Pick one and use it, you'll be fine. "
"Symfony2/SensioLabs are getting so much attention for those projects because they collectively appear more involved than any other project. 
 Edit: noticed another Zend user","> So why is there a need to associate these projects with Symfony? Yes, there is some overlap between the projects, but that's true for any popular open-source project.  There is a lot more overlap with Symfony2 than any other framework or project. Of the mentioned projects, [Behat]( seems to have the lowest amount of Symfony2 developers.  Even then, the top contributor is a significant user (member of FriendsOfSymfony). The next nine include a Symfony2 developer, a Symfony2 contributor, and a SensioLabs contributor and another significant Symfony2 user.  The numbers are also heavily biased by users of Git and GitHub. So its not to say that other framework users/contributors/developers aren't contributing code anywhere.  While looking at the top 10 contributors-- those using another framework were more rare. My longer-than-expected look at the contributors showed Symfony2 users/contributors/developers representing 26/40 of the top 10 contributors for the 4 projects you mentioned. Other frameworks counted up to 8/40... two of which were Silex developers).  tl;dr  Symfony2/SensioLabs are getting so much attention for those projects because they collectively appear more involved than any other project.  Edit: noticed another Zend user "
"HTML, JS, drawing apps good. Flash drawing apps bad. We have cookies.","The thing that works so well is these kinds of programs give you a lot of freedom of expression that I think by making more accessible we can popularize, and really get a future-proof, lasting community going. Current drawing apps aren't too popular as they're not accessible to the host of many devices people use, chatting's not versatile or user-friendly enough to compete with chat sites, and their designs look god-awful like it's still the 90's. So with what we're after, we've got some insane potential in this project.  Also if anybody's good with P2P networking, node.js, SQL, or server management.  tl;dr HTML, JS, drawing apps good. Flash drawing apps bad. We have cookies. "
"Information theory is awesome! You just add a bunch of redundant data, and the receiver re-builds the message. Don't mess with Shannon, he eats punks like you for breakfast.","You can't really say a channel has 100% noise. You really need to deal with SNR (signal-to-noise ratio), which describes how powerful the signal is compared to the channel noise. 3dB SNR means the signal has twice the power of the noise.  ""Okay smartypants, how do you transmit information over a channel with -20dB SNR (noise 100x the power of the signal)?"" you ask. Don't worry, Shannon's got our back.  White noise alone is an uncorrelated, random, signal. That means no sample can be predicted based on the previous sample(s). If I send a signal through that noise, even if it is much weaker than the noise, it will affect it, ever so slightly. If I hold a tone (for example) long enough, you can correlate it ""out of"" the noise, because what should be random noise will actually have small patterns.  What this boils down to, is if you need 3dB SNR (signal = 2*noise) to receive a message error-free, and you've got a -10dB SNR (signal = noise/10), you can add enough redundant data (parity bits) to achieve that. Your data rate will be 1/20th what it is in a 3dB SNR environment, but it will be just as error-free.  CDMA does this, and current uses will easily work with an SNR of -30dB (noise = 1000*signal)!  TLDR: Information theory is awesome! You just add a bunch of redundant data, and the receiver re-builds the message. Don't mess with Shannon, he eats punks like you for breakfast. "
do some research before throwing serious time and money down the pan. Don't pay attention to buzzword bullshit from professional bullshitters.,"SV is a fucking circle-jerk buzzword bubble. Failure is praised because it's just somebody else's funny money.  The OP clearly shows he has the talent and drive to get something up and running. The payments idea was crap (a bit of prior research would have told him that, but never mind). He can go and do something else rather than have to support a dead business, go back to work in a real company, whatever.  The whole startup/SV/Techcrunch/Y combinator/Hacker News bullshit is beginning to grate. Go to FeedMyApp.com and you see 200 Twitter clients, dozens of Basecamp clones, social network crap, map my fucking this or tweet my fuck knows what. One or two genuinely interesting, original ideas in there, the rest is dross but not only that, a waste of hard work, talent and money, just copying someone else's idea without any original twist.  tl;dr; do some research before throwing serious time and money down the pan. Don't pay attention to buzzword bullshit from professional bullshitters. "
Coding styles are a personal matter. There's really no right and wrong here.,"Quoting the article:  > You might notice that in this article I’m not telling you should be semicolon-free. I’m just laying out concrete evidence that you can be. The choice should always be yours.  As you see, he isn't suggesting people adopt or not semicolon-less style. He's just suggesting that people who don't like it because fear or ""Cockford said so"" should instead learn how the language is parsed, and decide for themselves whether semicolons suit their style or not.  You can actually write JavaScript quite similar to Python, in whitespace and indentation. If it actually helps the ones that will be working on the program, why should you force everyone to adopt an alien coding style that just gets on their way?  tl;dr; Coding styles are a personal matter. There's really no right and wrong here. "
PHP Developers make a pretty penny. A solid portfolio trumps a degree.,"As opposed to the majority of advice here, I say you can be incredibly successful as a PHP Developer. I see about 1 or 2 job postings for that exact title coming at me every month with pay ranges between $85k-$125k.  The absolute most important thing you can be doing with your time right now is developing a very strong portfolio that showcases your ability. When you interview for a developer position the first thing they're going to quiz you on is the software you've used. The next thing they're going to want to know is: ""What have you done?""  Being able to back up your resume with physical examples of your work will be far more important than an education. No company that I've ever worked for has ever asked me ""Where did you go to school?""  TL;DR : PHP Developers make a pretty penny. A solid portfolio trumps a degree. "
"PHP is not a ""bad language"", just no language i would use as an example for a programming language concept.","Ah, come on. That whiney article again. PHP had never a real concept, that's true, but that doesn't make it unusable, nor less productive than other languages. You get stuff done in PHP and you can write rather good code in PHP too (circumventing the quirks). There are truly worse languages than PHP. I have to admit, i tend to call Python the ""better PHP"" - same level of productivity, more features, same speed, same if not better learning curve and most importantly a language with a inherent concept - but if you know PHP well, it's just fine to get stuff done.  The core problems of PHP are the sprawled - not grown - parser and the ""need"" to keep a lot of bad stuff in and the mindset that you have to please everyone, so you have operators from C and Pascal and at least 4 different syntaxes for a simple ""if"". If they would be as hardcore as Python was with Python 3, a lot of problems would be gone by now.  The biggest advantage PHP has over Python, is that companies find a crapload of people who ""can do"" PHP. Or are thinking that they can do.  Disclaimer: Doing PHP is my job, but i'm having an affair with Python. I just like her. Sorry.  TL;DR: PHP is not a ""bad language"", just no language i would use as an example for a programming language concept. "
"Some tools may not be literally useless, but they are practically useless and pointless.","> but that every tool has its use  Bullshit.  No one in their right mind would use a manual drill for any reason other than to say they did it, unless they were forced to by external influences.  Electric drills have superseded them in every way.  In the programming world, that would be like VB Classic (pre-VB.net) or straight ""sh"" shell scripting (as opposed to bash or other modern shells).  They'll still work, they can still probably get the job done just fine in most cases, but you'd be rightly considered an idiot for starting a new project with such things.  Now note that I am not speaking of PHP here as a useless tool.  I like Python more as a language, but the ability to simply slap a bit of code in the middle of a HTML page and toss it on a web server with no further thought is huge for me as someone who often needs quick-and-dirty web based programs.  tl;dr: Some tools may not be literally useless, but they are practically useless and pointless. "
I'm a linux fanboy and you're full of shit,"I'm a linux guy and my work machine is OS X, so I'm by no means a windows fanboy, but powershell is object oriented rather than text oriented, this allows it to do some things a lot better than Unix shells,   when dealing with a CSV, import-csv will allow each row to be an object and it's properties can be accessed a lot more cleanly than cuting and teeing it   allows for actions on nested properties in a set of objects that don't all have these objects quite cleanly   you can get more details of an object earlier in a pipe without having to rewrite the command to parse that extra data    If the csv is:  username, email, passwordbob, bob@microsoft.com, hunter123  You can run something like (I'm no powershell guru):  $users = import-csv $filenameforeach($user in $users){ ADPasswordchange -username ""$user.username"" -password ""$user.password"" }  I don't have a clear example for 2  the example for pipes would be  get-data-on -users | filterfor -username ""bob"" | filterfor -created ""2013"" | output  can become  get-data-on -users | filterfor -username ""bob"" | filterfor -created ""2013"" | output -verbose   with unix shells the -v has to go on get-data-on, then you have to hope this will not break your filters.  Python interpreters are not deployed everywhere and lack system integration, so while they are a powerful tool there are plenty of ways that powershell is better, not least of which that you can run windows programs from it without having to mess around with popen.  Powershell does a lot of cool stuff even if windows isn't designed around scripting (in fact it's because of this that powershell was written to hook into systems deeply, they don't have 30 years of everything being text to benefit from)  tl;dr I'm a linux fanboy and you're full of shit "
outrage and non-constructive criticism are trendy phenomena on the rise.,"It's funny (in a cynical way) because all of these people screaming about bad segments of code they find act like they are incapable of mistakes. Open Source code allows the development community at large to go through and find these issues with the supposed benefit of suggesting/committing fixes for them. This concept is supposed to help  strengthen the codebase, but instead a lot of people scrutinize and lash out at fellow devs in a highly non-constructive manner. There is a difference between positive criticism and outright flaming and it appears the latter is becoming, sadly, more and more prevalent. I guarantee most of the loudest critics would have at least one line of code that would cause other experts to say ""that isn't right"", particularly more so as the quantity of their code contribution increased.  tl; dr - outrage and non-constructive criticism are trendy phenomena on the rise. "
"there are a billion better ways to seed an RNG, and few quite so creatively bad as this appears at a glance.","OpenSSL [rand] ( is the random number generator for the library. PRNGs (pseudorandom number generators) work by seeding them with some unpredictable data and running that data through an algorithm to produce a random result. The result is only ever as good as the seed is really random, so classic mistakes like providing a timestamp as a random seed are well known and frowned upon.  This little snippet uses the RSA private key to seed the PRNG if it is not already properly seeded. I really can't figure out  why  you would ever think that doing this is a good idea, so I can't help you there unfortunately. To be fair though, it is certainly more random than a timestamp, I guess?  Ok, so what you might ask, I wrote the PRNG and I know that it will not be spraying out the private key I handed it onto twitter, what's the big deal? That's where that ""pluggable random subsystem"" bit comes in. What if you had redone the RNG to take the seed and feed it through some kind of unsecured randomisation process (there's nothing about a RNG that you expect that the input to that RNG will be absolutely critical private data) to get a more high quality random output than that which could be provided by the in lib RNG? Well as soon as you feed that private key in here, it could be compromised.  TLDR; there are a billion better ways to seed an RNG, and few quite so creatively bad as this appears at a glance. "
My point was [the frustration & waste (and thus the stress & anxiety) are normally about THIS](,"> One thing to always make clear to stakeholders is that you can't be responsible for finding the most efficient solution on the planet.> > > > That's just impossible.> > > > There's ALWAYS going to be some magic bullet that none of you realized. Some goddamned 2kb jquery plugin that makes unicorns fly out of your monitor and shit gold, and nobody saw it or heard about it until the day after your hard-won two year solution went live.  True, and a valid point... but that was not the one that I was making.  The weeks/months of waste are generally NOT actually being spent on coding -- certainly not on coding the final product.  Rather they are more often fits & false starts, prototypes and test projects that get tossed, etc -- all things that are typically a sign that the person/team don't  really  understand the problem they are tasked with ""solving"".  It's the kind of thing where when they (the person/people who spend months tearing their hair out) see the final completed piece (the one that was done quickly & almost effortlessly) -- they tend to exhibit shock:  ""Oh you mean   THAT   was what the needed done?  Well, then why did they have me chasing down all those OTHER dead ends?""  In some of those cases the later team is the unwitting beneficiary of the lessons learned from the failed prior frustrations; but in others (probably the majority) it is more a matter that the original team just didn't comprehend what was actually NEEDED, and instead just took off ""running"" with what the client/managers STATED: in essence screwed up the lowest level of the ""requirements"" process, and failed to get at the CORE of the ""what is it you are REALLY trying to achieve here"".  So it isn't a matter of finding some jquery plug-in or other special object from the ""bag of tricks"" -- if anything it's the opposite: spending too much time playing around trying to find/adapt/use those ""tricks"" and ""magic silver bullets"" instead of figuring out what the client/manager actually needed.  **TL;DR  My point was [the frustration & waste (and thus the stress & anxiety) are normally about THIS]( "
"worried about being canned, taking meds for anxiety and depression, determined I lack whatever it takes to do this job well.","My job contains all of the anxiety producing elements mentioned in the article. I'm not allowed to say no to anything. If someone comes to me and says 'What are you working on right now?' No matter how I answer that question the response is always 'OK, well I need you to stop doing that and do this other thing instead.' My job roles include new development (designing, implementing), testing support,  and production support. In any given day I am expected to perform all three roles. I have to report my time in half hour increments against the task I'm working on despite the fact that most of the time I'm doing three or more things simultaneously. Management yells at us for multitasking but without it there's no way to get everything done in a day. The company has an instant messaging system that allows anyone to come and interrupt me at any time. There are also group chats and I am typically in about 5-10 of those at any given time and expected to pay attention to all of them while still working.  TL;DR worried about being canned, taking meds for anxiety and depression, determined I lack whatever it takes to do this job well. "
Good article that promotes functional programming but I'd rather stick with the basic concepts it builds on for clarity when writing code.,"In essence this is functional, polymorphic programming. I'm glad people are trying to educate PHP developers in these principles because they are quite useful (in probably any language).  However, I do not see any advantage to using ""transducers"" over the simple functional algorithms such as map, reduce, and filter which are straightforward. As an example:  // Transducers$result = t\transduce(    t\flatten()    t\operator_reducer('+'),    [[1, 2], [[3], 4]]);// Simple functional programming$result = sum(flatten([[1, 2], [[3], 4]]));// or$tmp = flatten([[1, 2], [[3], 4]]);$result = sum($tmp);// or even$tmp = flatten([[1, 2], [[3], 4]]);$result = reduce($tmp, $initial = 0, function($carry, $value) {    return $carry + $value;}));  I would never choose the ""Transducer"" route. Of course, we don't have polymorphic algorithms such as reduce, map and filter available in PHP core but they are provided in many other libraries (I have two such libraries myself).  tl;dr Good article that promotes functional programming but I'd rather stick with the basic concepts it builds on for clarity when writing code. "
I brought down Mary Kay's mexico site and cost the company $80k,"I worked for Mary Kay for about a year as an intern in College.  It was my first real IT job.  I was on the International E-Commerce team which meant a group of about 10 people was in charge of making sure that all the sites were up and running smoothly.  I think there were around 15 of them.  My first assignment was to modify a legacy ASP page to redirect mexican visitors of MarkyKay.com.mx to a portal site for buying make-up.  The task was a simple one in hindsight, but at the time I had little programming experience and it seemed like a huge challenge.  So I submitted my change to their QA team and they returned it telling me it wasn't working.  This was about an hour before the production push deadline.  They had a production push only once a week so if I didn't get it in I'd be screwed till next Wednesday.  I make a quick change, test it, it works!  Instead of sending it back to QA, I forward it directly to the webmaster to push.  I then run downstairs to get a sandwhich.  Once I get back, I'm at my desk unwrapping my lunch when I hear the person in the cubicle next to me ask someone, ""What happened to Mexico?  I can't get to it!"" I start feeling cold sweats.  My hands reach for the keyboard and I slowly type in the address and hit enter.  I get a 500 error.  I brought down Mary Kay's mexico site.  Now this wouldn't be such a big deal except for the fact that it was the week that all the make-up reps were supposed to refill their orders.  The site was down for about 2 hours and I calculated that I lost the company $80k because of my SNAFU.  My boss came over after it was sorted out and begrudgingly said, ""We can't really fire interns.  Just don't do it again.""  tl;dr I brought down Mary Kay's mexico site and cost the company $80k "
"I started text messaging the CEO once a minute, every minute at midnight because of a bug I introduced.","I was working on an app that would monitor stats in a call centre and then send text messages if something fell outside a certain range.  We had made 1 or 2 hardcoded tests but my code allowed us to create alerts for any combination of stats.  We did a brief test and everything seemed to be fine, CEO came to have a look and tried it himself.  He was delighted when he got a message that some value was below its recommended level  He was able to call the person resonsible and jokingly admonish them for something that had changed literally 30 seconds beforehand.  Eveyone was feeling great and we decided to leave the test service running overnight.  Once midnight came around the bug became apparent.  I wasn't adding up values for a single day, I was adding them cumulatively. Once we went into the second day, all the values we had alerts on effectively plummeted  And the alerts started sending. To the CEO. At midnight.  He had to use his wife's phone to phone someone ( not me ) to go to the office and have them turn off the test server.  tl;dr  I started text messaging the CEO once a minute, every minute at midnight because of a bug I introduced. "
NET isn't Windows (or Microsoft product only) but you can't always rely on everything working.,"For iPhone and Android you have to pay $$$ to get access to MonoTouch. PS3 Mono is dependent upon having OtherOS still there; not all PS3s still have that functionality. Moonlight also isn't as full featured; e.g. it doesn't support DRM, which makes e.g. Netflix not work with it. IIRC there are some issues with Silverlight OOB apps too.  Without paying for MonoTouch, not all .NET 4.0 code will work; some of it still hasn't been implemented. Even with WP7, not all .NET code can be picked up and used; IIRC not all Silverlight code is supported yet (will be with Mango I believe). Mostly issues are around synchronous Web access and the like; it forces you into async code, which is OK I guess. Silverlight has the same no-synchronous network / etc. code limitation.  TL;DR .NET isn't Windows (or Microsoft product only) but you can't always rely on everything working. "
A university degree talks about more than just passing subject matter tests -- it is evidence of thriving and enduring in unreasonable conditions. That matters a lot.,"You can learn technical skills in many ways, and a well-taught CS program is one way to do that. I agree, it's not the only way. But when I'm evaluating experience, one of the things that a degree tells me is that this person can set a distant goal, work under someone else's rules on things that may or may not be personally interesting and under conditions of stress and frustration and unreasonable people and somehow still deliver results.  I completely agree that there are also other ways to demonstrate these qualities, and I have urged people who ask me how to succeed without a degree to think of ways to demonstrate these qualities. I think a core coder on a large open source project may well evidence some of these things but I'd definitely want whoever was interviewing to tell me why that was so.  TLDR: A university degree talks about more than just passing subject matter tests -- it is evidence of thriving and enduring in unreasonable conditions. That matters a lot. "
If front end tech progressed at the pace of SQL server tech it would not be able to keep pace of the changing hardware landscape.,"I don't think you should have been so heavily down voted for this.  You are right.  Maybe just said it in a way that came off as rude.  The server side environment is pretty stable the hardware you run on and requirements don't change much over or at least don't change quickly.  Javascript is primarily targeted at front end development.  The last 5-10 years has brought dramatic and fast changes in both requirements and platforms that web developers support.  For a single product or application you may need a desktop website, mobile site, mobile/hybrid application, wearable interface, soon automobile interfaces.  Front end developers could realistically be tasked with creating all of those fronts for single application.  The only sane way to write something once that can be deployed to all of those environments is to use the HTML/JS stack.  Trying to hack mobile applications or sites together using 10 year old web technology is MUCH more painful than learning a new framework that addresses the current state of the platform in my opinion.  TL;DR: If front end tech progressed at the pace of SQL server tech it would not be able to keep pace of the changing hardware landscape. "
"The authors (Tarek Ziadé and Benoît Chesneau) suggesting running your python web application behind Nginx, and Gunicorn+Gevent, because it solves a lot of problems.","They suggest nginx, gunicorn, and gevent. For people new to python, or those coming from a apache/php background, who are wondering 'huh'?  Nginx  is just an (very quick) HTTP server. In your python web app, it serves a few purposes:   It takes care of sending the client any static files, ones that the python code shouldn't be worrying about anyway. (background images, etc).   It caches output from your python application. If the user were to ask for the same resource twice, the cached result would be sent back on the second request. This keeps your python application from being overwhelmed with requests. It also means you don't have to worry about the 'simplest case' cache, though other thoughtful caching in the application can be a good idea.    Gunicorn and Gevent  are, in this context, working together. Gunicorn is another HTTP server, in this case one that spawns a number of workers to deal with incoming requests. Gevent is a library which provides Gunicorn with workers that don't block while waiting for stuff.  So, here's what you should keep in mind when deploying your app:   Your application will run slower than fetching a cached response.  Cached responses also help with DOS attacks.   Static files should be served by a 'vanilla' (read:fast) http server.  Running more than one process is good.    I'm skipping a lot of details here (because I don't feel like I have a good enough understanding of the topic to not get something wrong).  tl;dr: The authors (Tarek Ziadé and Benoît Chesneau) suggesting running your python web application behind Nginx, and Gunicorn+Gevent, because it solves a lot of problems. "
Applies to more than just programmers (which I guess he does hint at),"my god if I could give this a 100 upvotes I would.  Even though I'm now doing sysadmin work (started as a programmer and through a series of events became the sysadmin) this still holds true, if not more so.  Now I have to deal with people having problems printing or wondering why an email with a 100mb attachment isn't going out WHEN THE ERROR SAYS THE EMAIL IS TOO BIG.  I have a need to keep a model of the entire infrastructure of the network throughout most of the day.  What's the IP of this server? this service is no longer working, what other services does it depend on and are those running?  I'm exhausted by the end of the day and feel like I've gotten nothing done until maybe the last part of the day when people don't tend to bother me as much.  Fortunately work has always been pretty lenient on hours.  A couple years back they finally put rough guidelines that you need to be in by 10:00 and then 8 hours from that.  I still do programming at home and I tend to learn / do the most saturday nights after midnight.  Just yesterday I had to get up at 4am for some server maintenance  (I try to go to bed at reasonable hours during the week) and I woke up at maybe midnight or so and just went head first into some Qt stuff I'm learning.  4am came, did maintenance, and wanted to keep looking at Qt stuff.  TL;DR:  Applies to more than just programmers (which I guess he does hint at) "
"You're right. But, so is blackyoda if you use PHP's wacky definition of GC.","> More recent versions of PHP have added a separate cycle detection GC feature (cycles will never be cleaned up with reference counting) but it's not relevant to this conversation.  Actually, the PHP documentation implies the GC cycle collector is the only ""garbage collection"" feature in PHP, even though that's retarded if you're using a more general definition of GC. For [example]( ""When the garbage collector is turned on, the cycle-finding algorithm as described above is executed whenever the root buffer runs full.""  So, I think you could say that, in PHP-land at least, the zval reference counting mechanism for freeing memory just isn't considered ""garbage collection"" (as it predates things like gc_enable), even though it clearly fits the generally accepted meaning of the term outside of php-internals.  In that case, when blackyoda says that ""unset does not trigger GC"", he's entirely correct. You can unset variables after gc_disable and they'll be immediately free'd by the refcount check, but the ""garbage collector"" will not have run at all.  Another way to look at it: if objects are free'd as soon as their refcount hits zero, then calling unset() will only free the zval it operates on (and any others that have their refcount zero'd as a consequence). So, how much is it a collector? These semantics go away if you just talk about unset removing a variable from the current scope before the scope ends and leave it at that (especially as unset's effect on static/global/passed-by-reference variables is more easily understood that way).  TL;DR: You're right. But, so is blackyoda if you use PHP's wacky definition of GC. "
"W3fools is the out of date site, not w3schools.","I'm aware of it and it's a trite site that is sensationalist and out of date.  It is not, itself, a useful resource and I sincerely think the mods of /r/javascript should really consider removing this link from the menu on the right.  > Learning key web development idioms slowly or incorrectly puts you years behind your own colleagues  Picking up the basics of javascript from w3schools is not going to put someone years behind their colleagues.  W3fools seems sensationalist and is not a good response to someone linking to w3schools.  > ASP Classic was discontinued in 2000 and replaced with C#/ASP.NET. Recommending people learn an outdated language is not a good next step.  Really, we should just remove outdated languages from the web so no one can learn them?  Brilliant.  I've never had to deal with ASP classic, fortunately, but to just remove it because it's outdated?  I guess we better get erlang, cobol, fortran, and other languages off the web too, right?  Because no one uses those languages in business anymore.  W3schools has the latest [ASP.Net]( in their line up.  > ""var t=setTimeout(""javascript statement"",milliseconds);""This is terrible advice. You never pass a string to setTimeout() unless you like using eval(). Passing a string also means, among other things, that you can't use any variables from your local scope. Also, the examples on this page use a ton of global variables. Bad.  It's not terrible advice, it's a terrible example.  Look at their code example on w3schools:  myVar=setTimeout(function(){alert(""Hello"")},3000);  With all that being said, I still think w3schools is a good site for an introduction to javascript.  Codecademy is probably a bit better.  MDN I think is a great resource and good jumping off point to other resources but I'm not a fan of their  tutorial .  TL;DR: W3fools is the out of date site, not w3schools. "
Took me 30 min to finally not be able to read the Quora answer.,"Last time I tried to read an anwser on Quora from my phone this is what happened:   Ask for a signup. I don't like it but O.K. let's go with Google login  Quora want now that I enter a password? Why? Dunno. I put a password.  Now I land on a page that wants me to download the Android app. ARRGg.g. I proceed.  After 5MB of data trough the app cannot connect with my Google account.  Back to the webpage... I manage to find a ""Use the desktop"" webpage.  Now I have to login again, but only with FB or Twitter...  30 minutes and 5MB later... I give up.   TLDR: Took me 30 min to finally not be able to read the Quora answer. "
the journey of a thousand miles begins with a single step.,"I feel like the intent of this ""DNA as code"" document is not so much to say ""this is very much like this"" as to say ""I am a programmer trying to understand DNA, and am enumerating the ways this thing I don't know anything about is like this thing I know a lot about"". Writing down what you've learnt in a new way is a good way to consolidate it in your head, and I bet this guy has a better basic grasp of how DNA works than if he hadn't made broad, sweeping generalizations like ""DNA->RNA->a protein is kinda like how you first compile code, then link it, in that there is a complex process that generates a middle state, which can hang around for a while and ultimately have another complex process happen that generates a final state"".  Especially because that thought leads to some questions: can a cell keep the generated RNA around for multiple uses, the same way a computer can keep a .o file around to link together for multiple compilations when you're working on another part of the program? Why or why not? If it does keep it around, how does it decide to do ""garbage collection"" when the cell fills up with RNA? Can I work out the algorithm it uses for this, maybe even improve the GC in a program I'm writing? Or try to improve the cell's GC if it turns out to be super naive? Alternatively, if the RNA  is  one-use, what happens if a cell  doesnt  break up the RNA and leaves it around? Can we make a cell not break up the RNA and see how it fails? Does this failure mode look especially like one particular disease?  I'm sure all those particular questions have already been answered. But keep refining the analogies and eventually you get to a place where the only answer is ""that's a good question, how can you devise an experiment to answer it"".  tl;dr: the journey of a thousand miles begins with a single step. "
I'm pissed I got fired for asking for a raise. I would be in half-Nirvana if I've made something along the lines of $1k/month.,"Argentina. And due to the exchange here, I was doing about $700/month. And I was in the lucky side of things :-/  Currently unemployed (got laid off along with 3 coworkers over a payment dispute), after serving for 7 years on the same Japanese multinational.  I've started as a trainee in QA (embedded systems, fiscal controllers). Eventually I ended up restructuring everything related to how things were done in that department (I've designed, implemented, and maintained the testing automation framework currently in use), besides my tasks as tester. I was kind of the ""QA Tech Leader"". Heck, I've introduced there half of the process they are using (as a trainee, I've said, fuck all this, I'll at least use SVN/Trac and installed/maintained that for two years). You ask for traceability? I'll give you that, along with automatically generated reports in PDF.  Having a hard time now getting a new job. I can, more or less (as you can attest) communicate in written English (orally, I kinda suck), after teaching myself to do it. I can learn new things (I've done Delphi, C/C++ in kernel space, Python) on my own.  Sadly, I've never been a freelancer, I'm bad at selling my skills, and terrible at judging how much to ask for my work.  Sorry for the rant.  TL/DR: I'm pissed I got fired for asking for a raise. I would be in half-Nirvana if I've made something along the lines of $1k/month. "
code bases written in dynamically typed languages rot faster.,"You also get the benefit of powerful tools, such as refactoring IDE's. These are crucial to maintain large code bases since they allow developers to make sweeping modifications touching hundreds of files with close to 100% guarantee that they are not breaking anything.  In the absence of such tools, developers are more hesitant introducing changes that are not absolutely crucial and as a consequence of that, the code base rots and starts accumulating layers of aging code.  Just this morning, I noticed that a method was poorly named so I asked the IDE to rename it, checked in the changes and carried on with my original task. I would never have done such a thing if I had to manually verify that this renaming didn't break anything because:   This change is not crucial.  This change is not part of my current task.  I'm not confident that the tests cover this particular method.  Even if they do, I don't have the time to run the tests since I'm working on something completely unrelated.   tl;dr: code bases written in dynamically typed languages rot faster. "
don't be a jerk. The Kivy project is very very much more important to the long term pytjon ecosysyem than python 3 arm waving.,"Wow, why are python 3 advocates always such jerks?  Kivy is very python 3 friendly, which 2 seconds of investigation would have told you.  If you want python 3 support (for  any  project) get off your high horse and  actually help  contributing to the packages (like pyinstaller) that cause this sort of packaging issue.  The Kivy project is the single most forward looking python project out there; it's about getting python on mobile devices, which the core developers dont care about, because they're so busy implementing new PIPs about packaging zip files in a pointless and non portable way.  If you want python to succeed in the future, complaining does fuck all.  Build things that are amazing. ...and yes, if that means building them using python 2, that's what you have to do. (in this case you dont though, fortunately)  tldr; don't be a jerk. The Kivy project is very very much more important to the long term pytjon ecosysyem than python 3 arm waving. "
point taken but AFAICT it seems pretty theoretical without much use in real life.,"Its an interesting point and there certainly aren't any PGO compilers that I know of that do speculative optimization, but it is hardly impossible. In order for speculative optimization to be possible, there need to be two or more possible optimizations, with some degree of a tradeoff between them that can be measured. If you can generate the possibilities at your first compilation step, you can measure the tradeoff and optimize as you normally would in the second step.  Some other areas where JITs might have a theoretical advantage is if your sampling profile run doesn't have access to the same data distribution as production data would have. Just as well, changes in data distribution can happen during runtime for long running processes, meriting a reprofiling action and a potential reoptimization. In practice, though, few JITs do things as advanced as this, and some that have in the past (such as the HotSpot, which from what I can tell does not do any deoptimization any more) have abandoned prior attempts at it because they haven't proven to be very useful in real life. That being said, they are more likely to be found useful for dynamic languages than the strict/static languages that I tend towards. I would be interested in seeing the actual use cases for speculative optimization in V8 and LuaJIT.  TL;DR point taken but AFAICT it seems pretty theoretical without much use in real life. "
"I thought I'd hate indented grammars, but once I used them I love them.","Hated it.  Tried it.  Love it.  I came from a background of C, Java (and assembler, machine code) and liked the freedom to indent the way I saw fit.  (And it was usually inconsistent with the preferences of my peers, sigh...)  After a brief but torrid love affair with Perl (and seeing Ruby on the side), I've settled on Python as my language of choice.  Fast, efficient, great libraries, clean syntax (generally), and a wonderful web framework in Django.  And I've learned to love the indentation.  Curly braces be damned.  Things look better and are more consistent (and as a side effect work better due to fewer mistakes).  The major beef I have with it is that different editors indent things differently at times (tab expansion to spaces, and such) which upset Python.  But getting your settings consistent in your editor(s) remedies that fairly quickly.  It's an issue with the tools, not the syntax.  (I'm an old ""vi"" die-hard, who also uses ActiveState's Komodo.  Both can be coaxed to agree on the indentation policy.)  I guess that a reduced ability to crank out a one-liner is a slight factor for quickie scripts, but for core programming it's not an issue.  Also, the odd time, I'll delete or add an extra tab without knowing about it, affecting the syntax of the program; but those instances are rare, and cvs points them out for me pretty quickly :)  tl;dr: I thought I'd hate indented grammars, but once I used them I love them. "
"Windows is a better development environment than command line Linux (for me),","> Secondly, why edit your scripts on windows and then run them on Unix? why not just edit them on unix like everyone else.  I'll answer that - I have very strong IDEs on windows - VS2010, Komodo, etc. I use vim on Linux, but for anything larger than a script I use the IDEs and not vim mainly because GUIs are nice and mouse support is nice too. Then I SCP the files to my Linux box, compile / package them and send it to my dev computer (all automatically).  Of course, I told my text editors to use Linux line breaks.  TL:DRWindows is a better development environment than command line Linux (for me), "
How you're thinking of implementing it isn't how it's implemented. There is very clever maths involved not just throwing distributed data stores and shit at it.,"All of your questions are covered in either the paper or the site but given as it's down I'll do my best to explain from half remembered data from months ago.  > Parallel processing (ie: botnets) and imperfect/fuzzy logic are just two examples of ways to circumvent the cryptography in it.  The length of a transaction train is variable. This means that the more cpu is in the network the more cpu is required to prove a transaction.  >DOS attacks to cripple the system  current transactions are limited to 2 decimal places with a very small operator free for passing the transaction. You'd need a lot of coins to successfully DDoS the network with transactions. DDoSing all the nodes in the network is another matter, but it's one I don't see happening once bitcoin reaches a certain size.  >50k+ servers joined the service  In order to fake transactions you need more cpu then the rest of the networking. I'm a bit fuzzy on the math but there are many crypto guys on the forums who will back it up. It's possible they aren't very good or are shills but I haven't seen anyone claim it won't work.  >it is ultimately under their control (if they can figure out how to control it).  It's open source. It doesn't rely on security through obscurity.  >corrupted all the data that they held?  Then the remainder of the data would be worth more. This isn't a distributed data store. each coin is in affect turned into a different coin through the process of transferring it. Thus you can't transfer a previously transfered coin.  TL:DR  How you're thinking of implementing it isn't how it's implemented. There is very clever maths involved not just throwing distributed data stores and shit at it. "
if you  really  care about documentation then you need to check that it's done correctly and to your standard. Whining doesn't help.,"Interesting:  >   does it check parameters?>   does it throw any exceptions?> * does the implementation look like I expected?  Documentation  usually  answers  none  of these questions with any confidence. Without documentation you are forced to look at the code to find out, and that's where the truth lies.  The times when you can trust the documentation is when documentation is considered to be an important part of the deliverables, such as docs for a widely used library. Such a requirement can be included even for small, in-house projects, but the organization or team  must take it seriously . And taking it seriously is something hardly anyone does. They merely state that the docu must be done and kept up to date and then do nothing to assure that happens.  When people take working code seriously then you find tests, QA, etc. So where is the equivalent assurance for docu? Well, it hardly ever exists. If the organization doesn't take docu seriously do not expect the coders to take it seriously and you end up with innaccurate, outdated docu that is arguably worse than nothing.  TL;DR:  if you  really  care about documentation then you need to check that it's done correctly and to your standard. Whining doesn't help. "
"My applications do not require the awesomeness of C++, and somehow I wish they did.  Yet it remains my favorite way of doing things.","My forte, and most of my professional life, is C++.  And I agree with your analogy.  The thing is, in the realm in which I work at the moment, C# is a clear winner.  I know it won't give me the flexibility of memory allocation that C++ provides, nor necessarily the speed.  However, the kind of applications I write these days do not need that raw power, and no matter how awesome one is at C++, productivity on an application is very different between the two languages.  I get paid to write an application, not to do it at such an atomic level that I can boast a 4% performance increase.  Now, if my realm was different and if I were writing games, I'd be back to C++ in an instant.  Because that is a domain it rules well.  I know about XNA and DirectX's integration with C#, but to truly push the boundaries of gaming, you'll need to turn to C++.  C advocates may disagree here and I will not argue about the differences between the two as we'll be here all day.  Needless to say, currently, there is no real need to ""drop"" to C in a computer game these days.  Especially given the kinds of optimizations C++ compilers are pushing out now.  If you have to drop to C, I am confident there is an issue with the design.  Remember, you can still write ""C-like"" implementations with the C++ compiler.  You can avoid STL if you so wish and roll your own linked-lists and vectors (though you'd have to have a fringe-case to do so...  Especially when there are STL implementations out there, such as the one from EA, which is optimized for the kinds of things games do).  Minecraft is a good example of a game that didn't  need  C++ (or C) to be successful.  But I am wondering what  could  be achieved if it had been implemented that way.  TL;DR: My applications do not require the awesomeness of C++, and somehow I wish they did.  Yet it remains my favorite way of doing things. "
I'd never P/Invoke.  The cost wouldn't be worth it.,"I wouldn't want to use P/Invoke for critical code due to the performance hit of P/Invoking.  I would write the ""engine"" in C++ with every performance-critical part, and then use a scripting language to do the rest.  This is pretty much what modern games do.  They do this for a number of reasons.  Firstly, you don't get bottle-necks on the CPU if all the script is doing is making simple decisions.  The core of the engine is still running the show, the script is merely the director.  This means that level-designers and story-writers do not have to learn C++ to direct a level; they can use whatever scripting language you-so-choose.  The other benefit is that it is simple to change scripts without having to recompile your main executable and dependencies.  LUA, Python, Squirrel (an awesome LUA-replacement, written by one of the Far-Cry team members) plug in to C++ incredibly easily, and incredibly well.  TL;DR, I'd never P/Invoke.  The cost wouldn't be worth it. "
I'd really rather not have inadvertantly clicked on another company's confidential information from my work machine.,"Not trying to piss on anyone's parade, but I just endangered my job clicking on that link.  Basically, I work for a corporation that has policies against viewing ""confidential"" information from competitors (it doesn't matter whether they are or not, it could attributed to that). Rightly or wrongly, Microsoft could be classed a competitor. While the impact of the document would have on my employer's business or marketing strategies, it still threatens my employment. (I clicked on it because I thought it was a scan of an interview)  It's not really a biggie, but I'd be surprised if there weren't others in my situation that'd appreciate it.  I'll go back to hiding in my corner now.  tl;dr - I'd really rather not have inadvertantly clicked on another company's confidential information from my work machine. "
damned if I'm bringing work home with me. :),"Fair point, although it's not quite as cut and dried as that.  I actually see r/programming as a reasonably decent resource for what I need to do in work.  I don't know if you've worked for a large-ish multinational but you frequently have a few different factors to your job:  1/ What HR want/the policies they enforce - they're not that easy to break generally. Generally speaking, they don't have an impact on my job. This is what I thought I'd run up against in my original comment.  2/ What the company is paying me to do  3/ What duties I have to my team.  It may seem that 2 and 3 are the same, but they're not. #3 is more what I need to do to get stuff done. It's the stuff that people higher up the chain don't want to know about - they only care about what the team outputs, not how efficiently it's done.  Due to the nature of #2, my work is bursty. I get bursts of activity, and then I have to occupy myself until the next burst (while I'm waiting for machines to prep, that sort of thing). What goes in that gap are the duties for #3.  Duties for #3, besides actually working to improve/streamline our process, include looking for ways to improve and expand what we do. To do so, I need a half-decent knowledge of what's in current use, and what people would say they'd use if they were starting something from scratch. Also, discussion arising from the comments tends to be informative in how the ideas are explored.  That's why I browse here: to get a feel for future areas for our team to explore and encompass or tools to improve our processes. In essence, this r/programming is relevant to my job.  tl;dr - damned if I'm bringing work home with me. :) "
Decompression is  not  the reason Windows Update takes so long.,">It is more time consuming than Mac OS, wich does not compress at all. The downloads are larger, but the update takes seconds. This is what I am talking about.  Where the hell did you hear that Mac updates aren't compressed? That would be an insane waste of bandwidth, which is worth far more to Apple than customers' CPU time.  Anyway, my experience has been the exact opposite: Macs take  forever  to update, taking a particularly long time to ""optimize system performance"".  Worse, they won't even auto-install updates on their own in the middle of the night. If they did, it wouldn't matter that they took a long time to install.  >The decompression takes time. No compression takes no time. Simple.  Another thing: no compression means the download takes longer. Much longer. Far longer than the amount of time needed to decompress.  >So you agree, it takes time.  Of course it takes time. Not nearly enough time to be responsible for what you're complaining about, however.  A single modern CPU core can decompress tens of megabytes per second. Mine, for instance,  gunzip s a 5.2MB file in 0.084 seconds.  Reading and writing  that file to/from disk takes longer than decompressing it!  tl;dr: Decompression is  not  the reason Windows Update takes so long. "
Don't use  memset  to try to initialize pointers to null.,"I'd just like to point out that a lot of people use  TYPE obj;memset(&amp;obj, 0, sizeof obj);  to try to do the same thing (one answer at SO even advocates this to set the padding bits as well for certain cases), but it does not necessarily have the same effect, even in C (or in C++ with POD types). If the type of any member/element of TYPE is a pointer, initializing it with  0  (or any integral constant that evaluates to 0) makes it a null pointer. But setting it to all-bits-zero with  memset  might not do that. There's no guarantee in C that null pointers are all-bits-zero. They could all have all-bits-one or the bit pattern equivalent to hex 0xDEADBEEF for all C cares.  tl;dr: Don't use  memset  to try to initialize pointers to null. "
"pick small projects and build successively bigger, todos -> chat -> skynet.","For a quick taster of Ember.js and a few others, there's  which implements the same Todo app across different frameworks. Not a big example, but at least shows how data is handled and updated.  From there I would probably watch mailing lists associated with projects you're interested in, and pick a small application to start with that does the kinds of things you're interested in. A good one for learning a front-end framework and node.js would perhaps be a multi-room chat kind of thing. So there are a set of rooms on the server, and each client can ask for rooms, and join some of them. You get the idea. That would take you through templating on the client, real-time communication concerns, and can be easily extended into looking at database integration and authentication. You just want some small, well defined project that touches the areas you're interested in becoming better at.  Most of the first hours/days will probably be a lot of head banging on a wall. Hanging out in related IRC channels can be a godsend, both for seeing other peoples issues and getting some help. And once you've clobbered something together that works, I would start from scratch, use the lessons learned, and build it better.  tl;dr, pick small projects and build successively bigger, todos -> chat -> skynet. "
"Enjoy what you do, but dont live to work. Life is about more than a paycheck.","I learnt this the hard way. I worked for a small startup where I was programming but also doing some operations work. I started doing on call work because I got paid a retainer (a whole $25 per day!) to be available 24x7, which at the time I was completely stoked about. After 2 years I realised just how much of an detriment this was to my life. My friends had stopped asking if I wanted to do things, I had done some insane on call shifts of 6-8 weeks where I only left my house to go to the supermarket or work, and I was physically ill from the lifestyle I had adopted due to not being able to leave the house for more than 30 minutes at a time. Shortly afterwards the startup hired another ops person, and I was taken off the on call roster. I've never been happier to take a pay cut.  tl;dr; Enjoy what you do, but dont live to work. Life is about more than a paycheck. "
It's sometimes better (and I'm using it right now for our data-intensive payroll system) to run certain PHP scripts in the command line rather than the browser,"Hi ultio,  Im currently developing a PHP & MySQL-based solution for a company's HR payroll, and one of the major problems (solved by a PHP script) is 'somewhat' similar to yours, in that it needs to process 100K+ records (biometrics time-in/time-out data) dumped in a MySQL table,  and put the computed time in another MySQL table.  Initially, I run the script thru the browser, but the running time was horrendous (1 hour+). I know that SQL query optimization can help, but orthogonal to that (and ofc the pressure of enterprise software deadlines!) I believe that somewhere in the PHP system I can increase the running time of the script, or even abandon running it in the browser itself - run it in the command line, for example.  I initially went for the first alternative, which was increase the memory limit in php.ini. I think though that this memory limit in php.ini is bound to the apache environment (server) AND the browser (client). This setup requires your browser to be on 'at all times' while the script is running (please correct me if I'm wrong on this one).  The inflexibility of having to run a script thru the browser eventually got me thinking of the command line alternative. After some searching, I found out it was possible to run my PHP script thru the command line, with ofc the following setup (I'm using Windows):   go to the folder where your script is  run  php -f <script_name>.php   (ofc, php in my Windows setup is 'found' using the PATH environment variable which can be set in the command line or (easier?) in the UI of 'My Computer')    This command line setup is what currently works for me, and I searched (and ofc proven) that there is no running time limit on this (I'm not sure on the RAM, though)  Anyhow, I believe the command line is a far more flexible tool for certain 'hard' development tasks, and I've even heard good things about Symfony 2's Console Component (haven't tried, though).  I also from your problem I can gain more insight in performance issues surrounding the running of data-intensive PHP-MySQL operations - it's part of my day to day job, anyways :)  TLDR: It's sometimes better (and I'm using it right now for our data-intensive payroll system) to run certain PHP scripts in the command line rather than the browser "
"some managers are just too ego driven to listen to reason, they believe their way is the only way.","I have seen ego get in the way!  A buddy of mine works for a school district and was tasked with creating 700 iTunes accounts for the students (quite the affluent school district, every student gets their own Macbook Pro).  Every technician was told to just do it by hand and was given a daily allotment to complete.  For instance it took about 8hrs to do about 30 with all of the clicking and typing...etc  My buddy ended up tweaking/writing an apple script to automate the entire process, allowing him to complete hundreds instead of tens.  His manager was displeased with it because he didn't ""do as he was told.""  tl;dr - some managers are just too ego driven to listen to reason, they believe their way is the only way. "
Automating things is great. Just be careful. Sometimes management won't like your automated solution.,"Back in high school I worked at an email marketing company. My job was to sift through the responses to the marketing mailings we sent out, and manually unsubscribe people or forward their questions to the client company.  It was all done in Outlook, which had some hooks for automation with VBScript. So I wrote some handy scripts to automate things like pulling the email addresses out of every selected message and putting them in the clipboard. Unsubscribing people became much quicker.  That was all fine and dandy (while very boring), but then they released a new internal webapp for doing the same thing. It was supposed to make everything better, they said.  Except this was old days, and the app was super slow. Plain HTML with a Perl CGI backend. Actions like ""unsubscribe"" or ""delete"" took 2-3 seconds to happen, and then the page had to refresh. Where I could previously multi-select a bunch of unsubscribes in Outlook, I was now forced to manually click on each one. Insanity.  So I cobbled together a really terrible HTML scraper with VBScript using none of the best practices of the day. It could grab the list of emails, identify the ones with subjects that contained ""unsubscribe"", and then follow the appropriate link.  My big mistake was letting it run one day while I went to the bathroom. When I got back, my manager was standing there watching it run. ""Mr. simcptr, what is this?"" They made me stop using it. I left shortly after.  TL;DR: Automating things is great. Just be careful. Sometimes management won't like your automated solution. "
"start small, learn while you work, and have a longterm vision that you re evaluate periodically.","I'm a software engineer / data scientist with a degree in econ and poli sci from a crunchy liberal arts college. I was a complete newb when I got out of college 5 years ago with 0 programming skill, but landed a consulting job after school. I didnt even know how to use Excel. In my first year, i learned excel and pivot tables, then VBA and MS Access. Year 2-3, I learned SQL and how to work with developers and manage projects as a business analyst at a bank. Year 4 i landed a job at a tech company as a data analyst and learned python, php, restful API's and html/css. Year 5, i learned more python, big data /hadoop and pandas. I learned mostly on the job, teaching myself at work and leveraging expert resources at each job when I had the chance. I taught myself some stuff outside of work but since i get bored easily, i prefer learning while Im gettin paid. It was too hard for me to come home after work and teach myself these radically new concepts without feeling overwhelmed, incompetent and bored. Additionally, I had an overarching visipm that I wanted to be a programmer, but I didnt know the specifics of what that meant. As time went on, the picture became more clear and I wanted to focus on data engineering vs app dev. Also - if you dont feel like there are ways to learn more technical acumen in your current role, have a talk with your manager or consider other opportunities. Hope this helps!  Tldr: start small, learn while you work, and have a longterm vision that you re evaluate periodically. "
"is unless reddit is a source, no, i don't have a reliable source
that apple was founded with ill-gotten gains.","actually, i created an account just to say this.  unfortunately, ithink that stories that prove someone was involved in an illegalactivity are hard to source.  glibly, if it were easy would we needplea bargins?  frankly, i heard it from my father.  clearly that isn'tworth a huge amount, so please have a grain of salt on hand.  that said:  *1) tenuous causal connection:[needs no explanation.  NB the comments in this link shows my point isnot unique.  of course it's not unique!  otherwise i'd have patentedit.  *2) interesting disavowal:[you'll note that he admits to building such a device but claims healways paid for the calls.  supicious?  *3) but what about...[check out ""possibly true stories about phone phreaking"".  i believe that wozniak mentioned the prank call to the vatican andalso the prank call to nixon about a toilet paper shortage in ""triumphof the nerds.""  this is not, however, easily confirmed by google so itmight as well not be true for this argument.  tl;dr is unless reddit is a source, no, i don't have a reliable sourcethat apple was founded with ill-gotten gains. "
a large quantity of unit tests is not that big of a deal if they are solid.,"I tell you what, I wrote upwards of 400 test cases for this project I'm working on. And as you might expect, that means I spend a lot of time in those test cases. I have to tweak them, usually towards a higher level of abstraction so that they become insulated from changes to signatures (yes, we use IoC - Guice will happily deal with changes to the constructor signature, but the tests, not so much).  But, let me tell you about my experience. I work on this monitoring software. The first iteration dealt with pretty simple data, and the original design ended up knowing probably more than it should have about the items being monitored. Well, we had to start monitoring some more complex objects - with parts which interact with the monitoring rules in unique ways. The original design could not cope with this without having to put in horrible kludges. The natural pattern for navigating an object graph where you want to be isolated from the internal structure is the Visitor pattern. This project has a lot of parts (which is why it has so many tests), and changing the core algorithm for object graph navigation was a fairly involved bit of work. I had to muck around with the unit tests to get them up to speed.  But , the process of updating the unit tests was  driving  the refactoring effort to move to the new algorithm! And the solidity and coverage of my unit tests made it such that the refactoring naturally unfolded, and when the effort was done and I ran the functional test scripts,  it just worked . This was not a fluke. We had to change the architecture again. As the business has developed, the volume of data to be monitored has substantially increased. The brute force algorithm was beginning to buckle. So, I rearchitected the application once again to use a more sophisticated map/reduce architecture. The experience was exactly the same. Churn through all 400+ tests, updating the production classes along the way, and at the end, the application just worked when we ran the functional tests.  So, TL;DR - a large quantity of unit tests is not that big of a deal if they are solid. "
"I was a sucky 6502 programmer, but I loved Z80.","Really? I did some ports between Spectrum, MSX, Sam Coupé and Amstrad CPC in the 80s and found Z80 to be quite pleasurable. Porting to Atari 800 and C64 on the other hand was a frigging nightmare.  To be fair, I was more familiar with the Z80, but the lack of registers on the 6502 did my head in. I also felt I could do more with less on the Z80, but then again 6502 was an overall simpler architecture. Z80s were also faster, but as I understand it (and I could be wrong, this was 25 years ago) the 6502 was supposed to be able to do more in a single cycle.  TL;DR - I was a sucky 6502 programmer, but I loved Z80. "
In Python  compile  reads a data structure  without  evaluating its contents.,"> you can't safely determine the data structure represented by the AST without some form of additional sandboxing  I honestly don't understand what you mean here.  The AST is itself a data structure (a tree). It can be manipulated as such and I linked to the ast module to illustrate that. The tree can be transformed, rewritten as well as executed. In fact, some Python templating libraries (I believe Jinja and Quixote PTL) transform as ast to rewrite Python code into templates for strings.  The data structure represented by the AST is a tree. There is nothing to determine about it, except if you mean the result of evaluating it which belongs to the evaluation stage rather than the  read  command. Python is different from Javascript in the example you give: using  compile  has no security issues whatever. Though  eval ing the result may entail security risks due the the specifics of the CPython implementation (IronPython and Jython can both use the sandboxing infrastructure available in their respective runtimes).  Here is an example:  &gt;&gt;&gt; import ast&gt;&gt;&gt; x=ast.parse('2+3*5+a')&gt;&gt;&gt; ast.dump(x)""Module(body=[Expr(value=BinOp(left=BinOp(left=Num(n=2), op=Add(), right=BinOp(left=Num(n=3), op=Mult(), right=Num(n=5))), op=Add(), right=Name(id='a', ctx=Load())))])""  Obviously, the AST is pretty hideous, but there is no evaluation, not even constant folding as you can see from the dump.  tl;dr: In Python  compile  reads a data structure  without  evaluating its contents. "
"Old content consumes CPU because it pre-dates GPU acceleration, new content with clean new interfaces performs better.","If the content provider sites were doing it for the first time today, they'd be able to trivially target the Flash APIs and codecs that support video rendering with GPU assistance.  However, the legacy codecs used by older Flash versions were devised before GPU acceleration existed and much of the video content online is in these formats to maximize market share.  Adobe (and formerly Macromedia) are obviously largely responsible for the overall situation, they can't really be blamed for not having devised a GPU-friendly formats and interfaces when none were on the market. Rather, they were the enabling technology that allowed us to have streaming video  at all  well before HTML5 and hardware acceleration.  In the same way, the new Flash Player 11 is  really  fast when doing new things like rendering 3D with textures, polygons and shaders because these are drawn into a separate 3D canvas (called Stage3D) and do not suffer from any legacy requirements from the usual 2D rendering of Flash. However, it also means that while 3D graphics suddenly become really fast to draw, old style Flash graphics and content will continue to mostly bog down the CPU.  TL;DR: Old content consumes CPU because it pre-dates GPU acceleration, new content with clean new interfaces performs better. "
"You could generate fast machine code with a Python program, but since it's an interpreted language, it would be slower than C at generating that fast machine code.","I think all the comments here missed the main point of the article.  He knows this isn't  actually  how PyPy works, and he clearly points that out.  What he is trying to do is  dispel the misconception  that because PyPy is  implemented in Python, it is necessarily slower .  Even though you don't typically run PyPy on top of CPython (you can), machine code is simply an output of a program, and in order to implement efficient machine code you use advanced compiler optimizations and fancy tricks which are easier to reason about in a higher level language.  This isn't actually how PyPy works, of course.  PyPy gives you a Python interpreter that is a replacement for the CPython interpreter.  Because it's interpreting the language in real time (and not actually generating an executable), the time to translate Python into machine code actually does matter, BUT only the first time that it gets translated.  I think this is the root of the confusion.  In practice, it's the reason PyPy's interpreter is written in RPython, which is compiled to C.  TL;DR:  You could generate fast machine code with a Python program, but since it's an interpreted language, it would be slower than C at generating that fast machine code. "
Javascript is a programming language like any other. Most people's problems with it originate [between keyboard and chair](,"I never understand the ""powerful language"" thing people talk about constantly. Javascript is a full featured programming language. There are problem domains it is good for, and problem domains where it falls short. The same can be said for almost any other language from C to Ruby, from Lisp to PHP.  Projects like node.js are not really proof that the language is somehow inherently better. These projects exist because JS is the most popular client-side language in the world. The fact that so many people are running it means that we stand to gain a lot more by optimizing the Javascript execution speed over other, less popular languages, so that is where a lot of work goes. Of course given enough optimization, there is no reason why any language can't be efficient at running on a server. This is exactly what happened with node.js.  Of course this popularity comes at a cost. The number of people working on JS projects is ridiculous, and the vast, vast majority of those people do not understand concepts like software architecture, readability, hierarchical design principles, or optimization. As a result we might get a few really nice frameworks, but a huge mass of really bad code written by self-professed geniuses. It's easy to dislike a language when every other example makes you cringe, or when the closest thing to a central documentation is amazingly painful to use, and is often a total hit or miss as far as finding actual answers.  HTML and CSS suffer from the same problem. Coffeescript, HAML and SASS certainly try to make it better (or at least cleaner), but all of those are simply stylistic patches. It's certainly nice being able to write 5 lines instead of 15, but that won't fix the fact that a lot of programmers can't really make code pleasant to read, and consistently usable regardless of if they use 5, 15, or 150 lines.  tl;dr - Javascript is a programming language like any other. Most people's problems with it originate [between keyboard and chair]( "
Chrome did best here. IE didn't do it at all ;-),"Did 1 million on Firefox. Was surprised for it to handle that many. Memory usage went to 2.5GB. Eh!? Does it mean that Windows 8 does the 3GB switch out-of-the box?  IE (64-bits) became dead slow when adding elements at 20-30000. Didn't wait ;-)  Also did 1M on Chrome. It had couple of (long) coughs starting with about 750000, but continued and went to only 1.5GB of memory use for rendering. Once there, it dropped back to some 60MB. Huh? Amazing.  There was a noticeable, but not dramatic, difference in page/line scrolling speed between between Firefox and Chrome in favor of Chrome.  tl;dr: Chrome did best here. IE didn't do it at all ;-) "
Even if the code examples make no sense to you without Haskell knowledge there is no reason to have additional boilerplate to handle errors via return values.,"Maybe allows you to implement a nullable version of any type, yes. As burtsushi explains Maybe is just the simplest example of the error handling monads though. In this case I was talking about the way the Monad instance for Maybe handles chained computations.  instance  Monad Maybe  where    (Just x) &gt;&gt;= k      = k x    Nothing  &gt;&gt;= _      = Nothing[...]  As you might be able to see the bind ( >>= ) operator of this instance treats Nothing followed by anything else as Nothing again. That means if you have a computation in the Maybe Monad it automatically aborts at the point where the first Nothing is encountered.  So instead of writing something like  foo :: a -&gt; Maybe dfoo a =  case bar a of    Nothing -&gt; Nothing    Just b -&gt;      case baz b of        Nothing -&gt; Nothing        Just c -&gt;          case quux c of            Nothing -&gt; Nothing            Just d -&gt; d  which is a way to write that if-cascade mentioned above in Haskell you can write  foo :: a -&gt; Maybe dfoo a =  (bar a) &gt;&gt;= baz &gt;&gt;= quux  or in do notation  foo :: a -&gt; Maybe dfoo a = do  b &lt;- bar a  c &lt;- baz b  quux c  (do notation is much more readable in particular in the case where each step happens to do more than call exactly one function on the previous step's result).  All of these produce exactly the same result as the cascaded version.  Another way to write it is applicative style which makes a lot of sense if all the steps produce a single value (if they succeed) and the results will be fed into one function (e.g. a constructor). This is often useful e.g. in Attoparsec or Parsec parsers (PhoneNumber is the constructor taking 3 arguments; Parser is the parser monad keeping tracking of the position in the input and failure).  parsePhoneNumber :: Parser PhoneNumberparsePhoneNumber = PhoneNumber &lt;$&gt; parseCountryCode &lt;*&gt; parseCityCode &lt;*&gt; parseLocalPhoneNumber  Either works essentially the same way as Maybe but also allows you to transport some data  along in the error case, e.g. an error code or message. Other monads can be used for more specialized error handling needs.  tl;dr: Even if the code examples make no sense to you without Haskell knowledge there is no reason to have additional boilerplate to handle errors via return values. "
"If you're looking at a framework and TRYING HARD to find fault in it to justify your preferences, you're learning wrong, and you'll never grow as a developer.","It's fairly clear that the author has a bias towards Ember and doesn't understand Angular at all.  Every negative point he makes about Angular is an  EXTREME  stretch of logic: ""Angular doesn't know when to update the view"" ... um yes it does, you just explained how it knows to update the view; ""It has performance issues"" ... performance issues in this design? For one user? Who is this user, The Flash?  I've developed large applications in Angular and I haven't had any problems. The main attraction is the dependency injection and extreme testability.  ... if you want that in Ember, you're going to have to extra diligent about grooming your JS and making sure your keeping your dependencies injectable by using  App.register  and  App.inject ... which are completely  optional  in Ember. On a large team of developers you need structure and a solid architecture, not the option to be a cowboy and put controllers wherever you please.  Ember is an awesome tool, Angular is an awesome tool. They both definitely have some strong points to take away from one another... but to pan Angular in favor of Ember is just selling yourself short. Angular has a lot of advantages over many other frameworks: Large company buy in, built from the ground up around a dependency injected architecture, complete with containers, tons of features, low bootstrap time, and it works really well with almost any other JS library... even Ember, if you wanted it to (not sure why you'd do that, but you could).  TL;DR: If you're looking at a framework and TRYING HARD to find fault in it to justify your preferences, you're learning wrong, and you'll never grow as a developer. "
"I'd highly avoid rolling your own solution with jquery and other similar frameworks, it's definitely worth using a MVC framework.","It looks like you've got a large ish client side project with a REST API, and a project in need of a proper MVC architecture.  I'd look a bit further than browserify and consider the likes of Backbone and maybe even Ember. The learning curve with Ember can be difficult but in return you could end up with binding to your HTML when your data changes, backbone doesn't have this.  If you chose Ember you could also consider creating your own REST adapter based on theirs which could do some interesting stuff like honour the caching and store it in local storage (although doing this server side may be a better option).  TL;DR I'd highly avoid rolling your own solution with jquery and other similar frameworks, it's definitely worth using a MVC framework. "
"R for prototyping of solutions and simulations, when you already have your data in usable format. Python for everything else.","So I'm using both R and Python quite regularly.  R is great for prototyping, testing different approaches or testing new methods. Because most researchers are using R, new methods are often first available in R before in other languages. But R is not a ""full"" programming language, so it is hard (and generally a bad idea) to write production code in R. The preprocessing in R is great if you already have your data in table form.  Python on the other hand is a full and very versatile programming language. I'm using Python for everything where R gets copious, e.g. web scraping, interaction with a lot of databases, transformation of raw and unstructured data. Depending on the requirements of the tasks it is mostly easier to write code that is ready for production and is better maintainable. Another reason to use Python is because there are a lot more interfaces to other programming languages and tools, for example to torch for deep learning or to openCV for image processing.  tl.dr.: R for prototyping of solutions and simulations, when you already have your data in usable format. Python for everything else. "
"businesses would be better run if managers learned management like developers learn development, or if more developers were interested in management.","Business processes are like code - they define the application that is a company.  The difference is that code is usually written by developers, who spend their lives learning how to build a variety of complex, fault-tolerant, robust, interconnecting systems, whereas business processes are set up and maintained by managers, who - to put it nicely - usually haven't.  Obviously there are good managers and terrible developers, but the crucial difference is that  you can easily tell if a program is at least basically working .  If a program fails it crashes, spits out garbage or throws an exception.  If a business process fails it just means more work, overtime and/or stress for the poor bastards involved in executing it, or an order coming down from management to ""manage yourselves more effectively"".  TL;DR: businesses would be better run if managers learned management like developers learn development, or if more developers were interested in management. "
"Useless partners suck no matter their coding ability, great partners are invaluable","I disagree...  Obviously a great programmer doesn't need a mediocre/poor product person piggybacking on them. The same way as a visionary product person doesn't want to work with a crappy developer.  But when you take a great product/marketing person and pair them up with a great developer, magic can happen.  If the next Miyamoto came to you with a game idea and wanted to work with you as the developer, even if he wouldn't write any code, would you turn him away as being useless? The next Steve Jobs?  Here are a few ways great product people can help:   Push back on your ideas   Help take the big picture view while you're buried in minutia  If a project is taking forever, help keep you motivated and structured  Bring in outside/new resources involving people/money/technologies  Do market research, talk to potential customers, research previous startups  Give new ideas for features, prioritize features  QA test like crazy  Focus on marketing, follow-up with customers to learn more about how they're using your product   Etc., etc., etc.  I understand it's hard to find that right partner, BUT it's equally hard to find great developers. While I can code I spend most my time being a product guy and there are some developers I work with who care and push back and bring ideas to the table and a lot who just do exactly what I ask them to (trouble...)  TL;DR: Useless partners suck no matter their coding ability, great partners are invaluable "
"Needed to align data in large mysql tables, got stuck due to some mysql intricacies, used Perl and it saved my ass.","Sure. So I work at place that deals with a large amount of data. I had this 600 million+ record table which I had to realign with some user information. I had 2 data sets, one was this 600 million user level data, and the 2nd was a 200 million + user id reference table. So we needed to translate this user level information table to use a new user identifier. Dealing with this size of the tables, I originally was using a multithreaded java process which would chunk out the mysql data, (but this is a problem as you reach higher records due to early row look ups). MySql's performance drops as you try to use LIMIT as you go up in records. What this means is, as you chunk out the data it gets exponentially slower unless you do some hacks like joining the table on itself on its primary key. It is still super slow and we had a time deadline.  So long story even longer. After awhile of seeing that this would take forever, instead we had dumped both the mysql tables into CSV, used a linux sort to sort the file the way we needed it, and then used a Perl script to do a merge sort to do some data merging. We then took the outputted data, and did a bulk insert into mysql into a new table with the desired relationships.  Not saying that this was the best way to do it, but after using Perl on this situation, I was just very thankful of the simplicity of the script,  the speed of the file processing, and just getting the work done. I was just flat out grateful .  Other situations were similar when processing large log files. For example, I used to work at a cell company, and processed gigs worth of log files (which represented call statistics) to do some analysis. I am definitely not a perl monk or even a perl novice, nor do I think I am an expert coder by any means, but I feel perl is one of those languages that is just powerful and is kind of like a swiss army knife.  tl;dr; Needed to align data in large mysql tables, got stuck due to some mysql intricacies, used Perl and it saved my ass. "
"Linus' rant is on-point, Gnome3 should be more readily modifiable, but does proggit really use Gnome3?","On the one hand, I'm glad Linus is so committed to the community that he uses the consumer oriented distros and desktops.  On the flipside, what are you all hacking on, because I can't imagine using Ubuntu or Fedora at this point.  All of the minimal distros seem so much more oriented towards productivity.  Whether it's Arch with Awesome or Crunchbang with Openbox.  You invest a couple days in tricking things out the way you want for maximum comfort, typically through config files, and then you port those settings across every upgrade.  Ubuntu/Gnome/Unity is trying to attain neophyte-accessible-nirvana, but for developers, I mean, our computers are our garages, we hang-up peg-board and shelving, workbenches and drawers.  For a parallel, look at finance.  My friends in finance talk about 'running' their Bloomberg terminals, meaning they have 6 LCDs set-up with different views,  hot-keys, live-data linked to their spreadsheets.  None of it's 'consumer' friendly, it's all rather elaborate, but it reflects the sophistication of the work they're doing.  TL;DR: Linus' rant is on-point, Gnome3 should be more readily modifiable, but does proggit really use Gnome3? "
"there's a reason the physics weenies I work with still prefer FORTRAN, and it's because they're trying to squeeze every FLOP/s they can out of their HPC cluster.","The worst part is that the original point of writing in C was because you could get a free compiler (once gcc first appeared) and the resulting code was so much faster than code generated by competing products (I'm harking back to my Amiga programming days, here).  But these days I've seen a number of articles that say modern C is  slower  than more structured languages because a highly structured language lets the optimizer make a number of decisions that it can't make with a weakly structured language like C - and gcc now supports front ends that let it compile many of those languages, not just C.  TL;DR - there's a reason the physics weenies I work with still prefer FORTRAN, and it's because they're trying to squeeze every FLOP/s they can out of their HPC cluster. "
DHH ought to have said OSS has a systematic advantage in ease of developer use over closed source (my interpretation).,"High quality closed-source (paid work) meets the demands of the end user where it's cost effective. High quality open source meets the demands of the user where it's wanted (given a healthy developer community). The difference is those small conveniences, here and there, which add up to something which  will  integrate into your workflow  without  asking for money.  Compare the (incomplete) pros and cons of windows and linux:   windows  it plays games well  it runs adobe well  it runs microsoft excel well    linux  it does all of those things, but not well  you have a healthy [desktop environment ecosystem]( which doesn't really have a counterpart in windows     One is not intrinsically superior to the other but if you're a developer and you choose to spend half of your day (thinking of) automating tasks to make them fit more neatly into your customised workflow, open source holds a systematic advantage if only because  you  can fork it yourself.  tldr: DHH ought to have said OSS has a systematic advantage in ease of developer use over closed source (my interpretation). "
The problem is half shitty web developers and half every single browser.,"I've come across plenty of bugs in Chrome when using more advanced css selectors where styles aren't overriden as they should be, or simply not rendered correctly, causing things to display wrong in Chrome, yet fine in other browsers (including IE9) - there's also bugs in Firefox.. and in Opera... and plenty in IE.  But what you have here is the problem that developers are working around bugs they encounter; if you develop in Chrome, Firefox and IE but not Opera, you will naturally fix the bugs that you see, and not use things that don't seem to be working as they should... and the bugs you don't see because you're not testing in Opera too... they stay there. If I only developed in Opera (which is my main browser for developing and browsing), but never tested in Firefox, IE or Chrome, then a whole lot of stuff I make would only work in Opera - but I'm not that stupid, so I test in other browsers and work around their bugs and shortcomings too.  Yet there's plenty of examples where Opera renderings something  correctly  and Chrome doesn't... if someone develops only in Chrome and uses how Chrome does things as feedback on what is right and wrong (as people have been doing with Internet Explorer for years) then they will in fact end up making plenty of mistakes.  tl;dr: The problem is half shitty web developers and half every single browser. "
"This is just the POSIX time() function, except where it got things wrong.","One fundamental flaw that has not been mentioned yet: They're getting the idea of ""time"" mixed up.  There are two types of time -- absolute time, and relative time.  Seconds, and by extension, kiloseconds, are a measure of relative time.  (So are minutes and hours.)  It would make perfect sense to speak of letting a reaction happen for 2.5 kiloseconds, for instance.  However, the time is not currently ""59.5 kiloseconds"", any more than it is ""16 hours and 32 minutes"".  It is ""59.5 kiloseconds past midnight"", even if I'm not being pedantic and including the time zone.  When we refer to the time as ""half past 4 o'clock"", the ""o'clock"" means ""of the clock"", and more particularly a standard clock with zero points at midnight and noon.  Even when the ""o'clock"" is only implied (as it usually is), it's an important part of what that time means.  And so the time that this is proposing is not actually ""kiloseconds"".  It is proposing that we measure time in ""kiloseconds past midnight local time"", which is a horrible mixture of scientific purity and nonuniversal colloquialism -- and by resetting each midnight, it inherently binds a magic 86.4 kiloseconds (or occasionally 86.401 ks, with leap seconds) into the system.  And it also binds into the system the fact that time is different depending on some remarkably arbitrary geographic divisions called time zones.  At least use kiloseconds past midnight ZMT, but better to use some reference zero time that doesn't get reset every day at non-power-of-10 intervals.  Like, oh, I don't know -- how about the beginning of the Unix Epoch; that would be reasonably convenient.  tl;dr: This is just the POSIX time() function, except where it got things wrong. "
"Based on what people take  Software Engineering  to mean these days, you're right. However, if you go by what  Software Engineering  should mean, you're wrong.","This is all really confusing semantics because most people don't know what either really are by definition.  Computer Science  is the study of the natural kinds and laws governing computing. (ex: compatibility on a Turing Machine. Halting Problem. P=?NP. etc).  Engineering  is the application of science and mathematics by which the natural kinds and laws of nature are made useful to people.  Since  Computer Engineering  was taken for the combination of hardware and software in the engineering of computers,  Software Engineering  was the next thing taken.  Based on the definitions of  Computer Science  and  Engineering,  we could conclude that the definition of  Software Engineering  should be something like:  Software Engineering  is the application of  Computer Science  to the development of useful software.  However, the definition most  Software Engineers  have is similar to what could be described as  Systems Management.  TL;DR  Based on what people take  Software Engineering  to mean these days, you're right. However, if you go by what  Software Engineering  should mean, you're wrong. "
"summarizing what the book covers in 358 pages. 
 You can [view]( the book here, but I don't think that's legit.","I can point you to a book, if that helps. See [Growing Object Oriented Software Guided by Tests]( by Freeman and Pryce. The book goes through a ""worked example"" of a sniper for an online auction which includes interfacing a GUI to an ""online action"" simulated by a chat server using XMPP.  You have a mix of functional tests, integration tests and unit tests. TDD typically works at the unit test level which is developing isolated components. At this level, you have very specific feature tests on isolated components. Acceptance tests test the entire system, connecting all the parts, end-to-end, and you have a number of ""scenario tests"" (much fewer than the unit tests, but enough to exercise the common cases). This is very TL;DR, summarizing what the book covers in 358 pages.  You can [view]( the book here, but I don't think that's legit. "
"higher level abstractions are very nice, but don't overlook the details. experience can help you figure out what you might be missing.","while I don't like much of the wording of the article, he's spot on about libraries.  I helped a student with a problem last summer, he wrote a project that would die around 40-45k web sockets. turned out he was hitting his process's open file descriptor limit(""man ulimit"" if you're wondering what i'm talking about)  not only did he not know about the built-in file descriptor(and other) per-process limits, none of the 2-3 3rd-party python frameworks he'd cobbled together for this project took the time to handle such a failure in a graceful manner. actually, none of them took the time to handle a failure like that AT ALL. no callbacks, no non-zero return codes, just... crash. his whole app. (maybe we should consider ourselves lucky that he got a stack trace?) :)  now, I have no problem with someone who's still in school not knowing some particular arcane thing like that, but if your framework is all about easily establishing and keeping network connections open, it would be nice to fail gracefully, or have mechanisms in place for similarly easy detection/recovery of failures like that.  tl;dr:  higher level abstractions are very nice, but don't overlook the details. experience can help you figure out what you might be missing. "
"don't show your photography to people who won't appreciate it, if you don't want uninformed comments.","I see both sides of this - I don't think anyone should be down-modding you, because dSLRs definitely take better snapshots than the typical compact camera.  I always end up with totally rubbish photos when I try to use a compact or a phone camera, yet equally I feel mortally embarrassed when someone tells me what a great photo I've taken, when I know it's just a good or nice snapshot from my dSLR - maybe it captures a moment or a view quite well, but I can see dozens of shortcomings in any ""snapshot"".  In photography, I'm really aware of my shortcomings - I can look at a photo and say ""I should have given it some exposure compensation, a wider aperture, done this and that differently""... then when I go out again the next day I remember to do only one of those things, I'm so involved in just framing the shot. It takes a lot of practice to get so that you remember  all  the variables instinctively.  The difference is that most people don't really  look  at photographs - they  glance  at them. If you're wielding a good camera - and care about the results - then pretty much by definition you're seeing photos differently (and trained to look at photos differently) from the average joe. Saying ""you must have a good camera"" is little different to saying ""the hi-def picture on my TV is great"" without giving any consideration to the cameras there or the director, or commenting on the fidelity of an audio system or ...  TL;DR - don't show your photography to people who won't appreciate it, if you don't want uninformed comments. "
second life is a game syntensity is a development platform for 3D internet applications.,"It allows for individual freedom, you can take your stuff anywhere and all those mmo type things. Think about the internet though, it allows for server freedom. Limiting my server (anybody can whip out the mecha they bought from another game and use it in mine) is not going to get us a metaverse that is anything more then a social mmo ever. this has a chance to actually be used for things other then socializing and selling each other shit. This has a chance of being a 3d internet (like vrml but not shite, I've looked at both) and if you think the only thing people will use 3d internet for is socializing your probably wrong.  TL:DR  second life is a game syntensity is a development platform for 3D internet applications. "
don't even bother reading the article. It doesn't add anything to the conversation.,"This is a rather uninformative and poorly done article which doesn't answer the question it sets out asking and ultimately ends up giving poor advice to the reader.  It rehashes things we all know about languages (C/C++ can have memory leaks if you're not careful, C#/Java run on a virtual machine, which introduces its own difficulties).  It essentially lies about other things (C#/Java don't have memory leaks ever...also they are slower than C/C++; most other languages are dynamic).  And it generalizes to the point of non-information (What's the most secure programming language? Whatever one is in the hands of a seasoned programmer! ; there are other languages out there...but they don't really matter. I think they're called Python or Ruby or something like that).  It finally says that the language you want is a ""type safe"" VM-based language like C# or Java. I disagree with this as ""the answer"" to the question because it looks to be entirely based on circular logic, ie: the safest programming language is either C# or Java because they are fast and safe.  tl;dr, don't even bother reading the article. It doesn't add anything to the conversation. "
I like to work on random projects sometimes just so I can learn about the problem or tech involved. Don't limit yourself to simply what you know.,"I disagree with this a bit. I mean I understand what he's saying and he definitely makes a good point. Start with simple stuff. That helps. Find projects that you can relate to and the problems will be easier to solve. But if you are comfortable writing good software then don't hesitate to branch out and learn new problems. Eventually the hard part of being a programmer isn't learning a language. This starts coming naturally. The hard part is solving the problems and this is the fun stuff. If you don't know how photoshop works then go find out. A lot of programs are written by people who are not ""masters"" at the task. You think the programmers for 3DS Max can make 3d models like no other? I doubt it. So yes I would trust a music notation program developed by a non-musician... Obviously if it sucks no one will use it. But looks like in the main question here if you ask the community for ideas you may also be able to get some good help with those ideas.  tl;dr - I like to work on random projects sometimes just so I can learn about the problem or tech involved. Don't limit yourself to simply what you know. "
Good things will grow where the ivory tower meets the dirt track.,"Unfortunately I was exposed to Haskell at an early age (for Haskell, not me) back in the days when it couldn't do I/O (this would have been the mid to late 90's I guess). It somewhat tainted my views towards it.  I  didn't  buy into the Haskell hype back then, and to some extent, I still don't now, even though all the limitations that made it so useless back then have long since disappeared.  I guess the main problem I have with it is that it's just so damn hard to get your head around.  Admittedly, Haskell is still very much an intellectual toy for me, and I've yet to use it in anger for anything non-trivial.  As a result, it takes several hours and a lot of googling for me to write even the simplest of programs.  I've never really had that kind of impenetrable ""learn barrier"" with any other language I've tackled (and I've tried most flavours over the last 30-odd years).  To be fair, I probably haven't tried hard enough and for long enough.  But that's not to say that the Haskell way (more specifically H-M type inference, which of course is not limited to Haskell) hasn't permeated my thinking.  I find myself consciously thinking about the type signature of my Perl subroutines, which as any Perl programmer will tell you is one of the most un-type-safe languages around.  I have tasted the type inference Kool Aid and it is good!  But I honestly don't believe that Haskell will ever make it into the mainstream.  I just don't think the average programmer is smart enough (which is a sad state of affairs, but a fact of life).  That said, I expect the principles of Haskell and other functional language to permeate into more mainstream languages over the years.  Perl 6 is a good example of the kind of hybrid language that is starting to blend these hi-falutin' language concepts (as Larry might say) into the pragmatic down-to-earthness of a ""scripting language"".  TL;DR:  Good things will grow where the ivory tower meets the dirt track. "
"TAKE A PILL 
 lastly
    if (!env== 0) {
        System.out.println(""that's all I have to say about that"");
    return;","SO  all of the negative venting, counter commentary and retrospective review -    comes down to a BAD TITLE?    my input,  *it is a guide more than a tutorial  *there are different schools of thought in educational writing  *there are subtle variations of the language (which doesn't help)  *bad writing does at times leach itself into our world  perhaps out of courtesy  ([this is not one of them](  *no one apart from ""James Gosling and his team"" has the right to be  inflamed, incensed or so anal about the nuances of Java or its offspring  to the point of concern of how it is evangelized over the web and the impact it has on world peace and chocolate milk   tl;dr TAKE A PILL  lastly    if (!env== 0) {        System.out.println(""that's all I have to say about that"");    return; "
loading new code is easy.  patching existing code  is hard.,"> You can extend my silly example to be truly dynamic by having it compile a DLL instead, and calling dlopen() instead of the second system()  dlopen() is the boring and easy part, dlsym() (which fetches us a function pointer by name at runtime) is the truly interesting part:  It requires that the dynamically linked library preserved some information from the source code: Function names. The compiler isn't allowed to optimize those away (a  minor optimization .  But dlsym doesn't allow you to access your own symbols dynamically by name, only those of external dlls. To allow that, your compiler is forbidden to throw away any function names at all.  Next: We need to swap the existing implementation with the newly loaded one. This means you don't only need a mapping (function-name -> function-pointer) but a mapping which includes all call sites as well (function-name -> function-pointer   [call-site]). Now the compiler cannot optimize function calls away anymore and throw call-sites away by inlining. And it may have to make every function call a  indirect function call*.  Guess what... this only covers functions and function definitions. What about datatype definitons? In C they disappear completely after compiling your code, but dynamic languages allow you to add new fields to a struct at runtime. Now your struct accesses aren't simply *(ptr + offset) anymore...  tl;dr  loading new code is easy.  patching existing code  is hard. "
"while you all are right addressing recursion, the underlying restraint is the math.","There are many comments here with excellent discussions regarding recursion in computer science. NASA's systems largely require fast numerical solvers.  Much work involves solving robust systems of non-linear systems equations, etc. working on an iterative (not recursive) set of calls to approximate existing and additional data on a model.  Here's why:   Data may be missing or incomplete.  The underlying calculations account should account for this.   New data constantly being added to the system means a system requiring a set of commands cannot be waiting for a recursive method to return.   Systems of {linear | non-linear} equations are most easily solved in tri-diagonal [and similar] form(s).  This bullet point here can be the sole focus of math-based career and there is still important work to be done in this field.   The goal is predictability in mathematical approximations to a specified accuracy...   We only need accuracy to a finite precision.  (Iterative methods can be shown to guarantee this.  Also iterative does not imply recursive...)  For information see truncation and round-off error and the important distinction between them.   This is quantitative not qualitative.    Numeric stability in ordinary differential equations is a good starting point along with the Runge-Kutta methods.  What I listed above is just a couple points that does not serve as a proper overview but rather to highlight a few of the features.  Hope this is helpful to someone wanting some insight at least with my experience doing this (horribly difficult) math myself.  TL;DR: while you all are right addressing recursion, the underlying restraint is the math. "
"Practically speaking, it is highly unlikely someone will create a complex C application that is bug free.  Theoretically speaking though, it is possible.","It is possible for a program to contain zero bugs.  Just look at a typical ""hello world"" program.  ""Sufficiently large programs"" though are highly likely to contain bugs (especially when written in an ""unprovable"" language like C).  It's an issue of complexity and there are mathematical models and theories that can estimate the number of bugs -- but it is still possible for a complex bug-free program to exist (just unlikely that we as flawed humans would be able to create it).  Some others who have responded to you have argued that one must know how to mathematically prove a large program is error free in order to declare a program error free.  I personally disagree; while it is impossible to do so for all types of code in C-like languages, that doesn't mean you can't adopt a style that is provable (like using only side-effect-free techniques).  But then again, most people who choose to program in C won't adopt these styles.  TLDR : Practically speaking, it is highly unlikely someone will create a complex C application that is bug free.  Theoretically speaking though, it is possible. "
"Drugs are like JavaScript, the user can see all of it. Maybe it's obfuscated, but hell if that's going to stop someone being paid to crack it.","That's not inherently true unless corporate paramilitary operations are allowed. You can't hide the active ingredients in a drug from another chemist that's paid to reverse engineer your shit. Any pharmaceutical that actually does any research would have maybe a few months to recoup millions of dollars in research and testing costs without patents, and that's assuming they spend time and money obfuscating their active ingredients. But each obfuscation technique will only work once (reverse engineers will remember how to break it) and could easily make the drug less reliable. Add in that for every drug that does make it to market, there are tens that failed somewhere along the way that need to be recouped by this one successful drug. Short of sending in some corporate SWAT team to destroy reverse engineering progress, patents are how pharmaceuticals can protect their ability to research new drugs without wasting time and money on obfuscation techniques.  TL;DR: Drugs are like JavaScript, the user can see all of it. Maybe it's obfuscated, but hell if that's going to stop someone being paid to crack it. "
Don't hire for what your candidate knows today. Hire for what your candidate shows they can learn tomorrow.,"One really bad way to interview is to ask narrow short-sighted questions about the knowledge the candidate has  today , rather than focusing on the foundational skills they might have to build for the future. Remember that you're not just hiring (or getting hired) to finish a single project; you're hiring/getting hired to hopefully be a long-term employee that will help direct the success of the company as a whole. That means you have to show growth potential, and that means being able to stretch beyond the ""current"" problem and show that you can handle the bigger more complicated one down the road. Most companies don't want you to be doing the same task forever. It's not good for them, and it's not good for your career, either.  tl;dr: Don't hire for what your candidate knows today. Hire for what your candidate shows they can learn tomorrow. "
"Tosh.0"" is not a part of the domain name. Whatever conceptions OP has about the rules are wrong. :)","Right.  The domain is comedycentral.com  Anything after the slash is processed by the web server.  Depending on the nature of the web server, it could be mapped to actual directories - which would work - on Windows and Linux, for sure, you can call a directory ""Tosh.0"", no problem. I assume this is also true for Mac, but I personally cannot confirm it, and am too lazy to.  It can also be that instead of being mapped to a directory, the web server software itself handles URL processing, or a script (e.g. php etc) might. In such a case, it doesn't even have to be something that would be a valid directory name.  tl;dr : ""Tosh.0"" is not a part of the domain name. Whatever conceptions OP has about the rules are wrong. :) "
is there any way to make 802.11n play nice for long-distance / directional aerials?,"How does 802.11n play into your future plans?  As I understand it 802.11n is now able to offer 600 Mbit/s fairly reliably. This would seem to be a tempting upgrade over existing 802.11a/g infrastructure, as that only offers 54 Mbit/s.  I'm thinking that in real terms  on the wLAN , 802.11n would allow the transfer of, say, a linux distro which fills a 700meg CD in a minute or so rather than (I guess) about 20 minutes on an 802.11g network.  Obviously this is of little benefit to many of the clients on the network, whose main use will be surfing the net and checking email and who will mostly only have 802.11g wireless built into their laptops. However many nodes are probably based in the homes of the network's power users (or at least nearby), and I can imagine that 600 Mbit/s would be a  massive  relief on at least some sections of backbone if it were achievable.  However, you're using directional antennas at present to achieve range, and 802.11n uses this whole MIMO thing with multiple antennas and phased (??) signals - basically, you can't replace the antennae with a Pringles can. I'm sure you could get good distance between two high points in the countryside using stock 802.11n, but I'm thinking that in town interference is going to become a problem, and there's lots of people going to be buying 802.11n for their homes in the next year or two.  TL;DR - is there any way to make 802.11n play nice for long-distance / directional aerials? "
"use an open source tool like Python's Scipy, in concert with a Python web development framework like Django.","Managed to find a [copy of the SLA](  ""4: Licensee shall not, and shall not permit any Third Party to:""""4.8: provide access (directly or indirectly) to the Programs via a web or network Application, except as permitted in Article 8 of the Deployment Addendum""  The addendum says:""WEB APPLICATIONS. Only Programs licensed under the Network Concurrent User or Designated Computer Activation Types may be called from within a web Standalone Application, provided the web Standalone Application does not provide access to the MATLAB command line, or any of the licensed Programs with code generation capabilities""  Then, see section 3.1 for license types; ""Concurrent License Option"" seems to be the one that universities use to allow institutional members to run Matlab if connected to the intranet and licensing server.  I kept following the chain of definitions for a while, but the clearest picture I have is that you could probably allow web access to people covered by an (your) institutional license, and in the same country as you.  IANAL, YMMV.  tl;dr: use an open source tool like Python's Scipy, in concert with a Python web development framework like Django. "
"Seriously, be careful if you don't have much experience and you're collecting CC numbers.","A few things to think about here:   Do you know anything about PCI compliance? If not, you probably don't want to! Just know this, don't let the CC numbers touch your server at all. Otherwise you open yourself up to lots of liability issues. Several payment processors offer javascript APIs that send the CC info directly to their servers. Here's [one]( and here's [another](  SSL. Does your site have an SSL certificate? It needs one to handle CC data. This will let you use  instead of http to encrypt CC data.  Assuming you're continuing on after the above two, what format does your API expect? XML? JSON? That's what you should send it. Read the docs.    TL;DR Seriously, be careful if you don't have much experience and you're collecting CC numbers. "
Ordinary programmers and a great project manager is better than great programmers and an ordinary project manager.,"In my experience a great programmer is at best marginally better than an ordinary one, and sometimes worse. Unless your team consists solely of great programmers(and will continue to do so in the future) they don't matter that much. Having only great programmers is also not very realistic and pretty expensive. That said, having a couple of  good  programmers is quite valuable.  Great project managers, however, is what it's all about. They are ""so much better than ordinary ones that you can't even measure the difference directly""  and  they will make your ordinary programmers perform better than great programmers under an ordinary project manager.  tl;dr Ordinary programmers and a great project manager is better than great programmers and an ordinary project manager. "
"If you think all pro-immigration arguments are for ""cheap labor"", do your own startup, get in the fray.","I think this thread's disagreements are the usual ones between "" talkers "" and the "" doers "". All the arguments from the the ones arguing against PG's points are, IMHO, armchair quarterbacks...Very easy to mouth off and pontificate if you haven't been a doer.  I with with @Yelirekim etc.  I too am a start-up founder, and have seen the challenges first-hand and alarmingly realizing the sheer scale of skills shortage. We are a machine-vision company and the talent pool gets an order of magnitude smaller in or domain (than say your stock mobile /web stack skills).  Check this link and look at the names [here]( almost to a person foreign-born and needing visa sponsorship. To top this off we are competing agains the likes of Amazon, Microsoft, Facebook & Google and you are starting to see the scale of our problem.  tl;dr: If you think all pro-immigration arguments are for ""cheap labor"", do your own startup, get in the fray. "
"I see both forms of property as highly similar, though not quite 100% of the time.","There are similarities and differences between physical-property and intellectual-property.  How ought humans behave in regards to some thing?  Property is not a 'what' but rather a 'why'?  Why ought one not trespass, steal, or destroy?  Why ought one human have right-of-way over another in a paticular scenario?  The 'why' or 'why-not' of IP and physical-property are quite similar, therefore I don't see much distinction between the two.  Both IP and physical-property fall prey to persons thinking they know better how to manage the 'thing' better than the 'owner.'  IMO, both forms of property fall prey to the same arguments (the 'why').  Squatting and piracy (for example) are similar, in that they both trespass and yet don't deprive the owner directly.  Given a ""person starving finds empty cabin scenario,"" we recognize a conflict of interests in that if a person does steal food or water to survive it may be forgivable, while stealing the big-screen TV is not.  TLDR: I see both forms of property as highly similar, though not quite 100% of the time. "
"Misleading for the purpose of self justification, self aggrandizement.","Sure thing.  I think I chose my words a bit poorly there.  It's more misleading and putting up multiple ruses than outright lying.  [This article on coding horror sums it up well](  Paul Graham is a master of self congratulatory writing, and championing ""his way"" or ""my companies way"" as  ""the way"" .  He is incredibly successful at painting pictures of things as ""the way we did it, the way it should be"", when in many cases anyone with half an imagination can see that to not be the case.  Furthermore, he is one of the stalwart microphones perpetuating ""valley culture"" - this whole ""programmers are not worth what they are paid""/""programmers should be commodity products"" spiel being one of the main tenets.  It's dehumanizing to the profession, unnecessary, and just plain  mean .  He does this because it is consistent with his other BS - advocating loudly that ""if you aren't a founder you aren't worth a shit"" and similar nonsense.  TL;DR - Misleading for the purpose of self justification, self aggrandizement. "
Op is not staying up to date on the field and just accepting the status quo of what they are learning on the job.,"As stated before *nix platform is the in-demand tool of choice.  Unless I am mistaken, you haven't diversified your programming languages and tools you know and are not staying up to date on the field.  For example, I have only 3 years of experience as a software engineer my first job I got my hands on Java doing Spring, Hibernate and Liquibase.  My second job I had an explosion of skills and technologies by using a *nix environment and program regularly in 3-5 languages. C++, Javascript, Bash, CSS, HTML and 20-30 libraries and tools.  I am currently employed as a C# developer doing exactly what you are in IIS and feel like I've taken a step back and am going to be requiring tools, technology and working with the cutting edge more slowly; however, I still stay on top of the field by learning more languages and tools. This weekend, for example, I am working on learning Ruby on Rails about amonth ago I learned Python....I get contacted for interviews even through I say I have minimal knowledge of those languages.  I even got hired as a NodeJs developer from just teaching myself the cutting edge  and I have no professional experience programming nodejs  TL;DR; Op is not staying up to date on the field and just accepting the status quo of what they are learning on the job. "
"We're all adults here, so try to act like it. You're entitled to your opinion, but so is everyone else. There are no universal truths, only relative truths.","Acting elitist and disparaging your peers for their personal or professional preferences doesn't really contribute much to the conversation. I will agree that there will always exist a purpose and demand for programmers who can use a relatively low level language, but that doesn't mean everything else is a fad. If you refused to use anything written in a language other than C or Fortran, I'm guessing you would see a lot of great stuff disappear from your life. Including Reddit, any iOS app, the majority of Android apps, etc. And asking someone to write CRUD in C, because it's 'superior' is just silly. For much of the software out there, running in 1.5 seconds instead of .5 seconds is acceptable- and much easier to get put together and shipped as well. That doesn't mean you have to use those, but unless you want to go down the [real programmers use X]( path, try to contribute to the conversation rather than issuing blanket statements.  tl;dr We're all adults here, so try to act like it. You're entitled to your opinion, but so is everyone else. There are no universal truths, only relative truths. "
keys). We just did the minimum required to regain OtherOS.,"OK, I'm not gonna pretend I know enough about this subject to decide whose contribution was more important from the two, but to me it seems pretty obvious that people suddenly were able to get a lot of things done after geohot published that hypervisor attack.  The jailbrake usb bundle got to execute code at LV2 making the game piracy possible. fail0verflow guys spent a moment in the presentation to explaining how that works so they have studied the attack in great detail. Of course it's only my guess this helped them with the next steps.  Whatever it might be, one cannot attribute all the hack 100% to geohot or fail0verflow as it has been a mutual effort and props to both of them.  In words of fail0verflow themselves:  > @rouse_y We didn't break game security, Geohot did (by releasing metldr keys). We just did the minimum required to regain OtherOS. "
"I meant people like gates and jobs are hackers.
your definition and my definition are probably completely different. :)","I took this out of context, but it's what I meant, ""A cracker is a person who attempts to break into a system by guessing or cracking passwords, by brute force. Most crackers are youth who are very malicious and get their laughs by destroying or changing the files on a computer system, and planting viruses that destroy or alter the functioning of a computer. Usually the cracker has very low knowledge of computer systems and their inner workings.On the other hand, a hacker is an individual who wants only to learn. Hackers are usually very knowledgeable, usually knowing several programming languages, and are able to use a wide variety of operating systems efficiently. Hackers keep up-to-date on security related issues involving computers, and generally have a firm understanding of the more technical aspects such as TCP/IP protocols""  tldr, I meant people like gates and jobs are hackers.your definition and my definition are probably completely different. :) "
"It's programming, Jim, but not as we know it.""","Both examples are absolutely true, some historical quibbles about the actual significance of Ada's role aside.  Both examples are also irrelevant, because they are not what TheNosferatu was talking about. The user was referring to early  commercial  programmers, who by and large were not ""programmers"" as we understand it. In that period, what we now call programming was split into two separate tasks (design and input, more or less), and the ladies in question were responsible for the task of inputting commands into computers.  That, and two people does not constitute a ""majority"" here. There's also the slight wrinkles that Ada didn't have a computer, was never a commercial programmer, and died a good half-century or so before Grace Hopper was even  born .  tl;dr: ""It's programming, Jim, but not as we know it."" "
"Use events if you aren't planning on talking between extremely deep frame/frameset hierarchies, and external windows","There is no benefit to using the messaging system over events  within the same window  This messaging system is meant to loosely couple information sent between iframes, and windows in a system, from anywhere in your window, frame, and external window hierarchy.  This is where the magic is  The library was written to alleviate issues with a legacy website that employs a ton of iframes and frames that talk to each other directly. In some cases, you would have case statements that look at the current state of which iframes are open in order to decide on  what to reference  and  what to be updating .  With this messaging library, these details don't need to be known, since i'm simply broadcasting the changes, and anyone who is interested can subscribe and perform whatever operations they desire.  The library is very minimal, and is meant as a building block.  TL;DR  Use events if you aren't planning on talking between extremely deep frame/frameset hierarchies, and external windows "
"universities are places to learn long term, broadly applicable skills, like effective programming practices, not trade schools","To be fair, going through college courses in CSCI puts you in a much better place to learn how to program effectively, as opposed to just learning through experience.  That said, why would you expect someone just out of college to know SQL? To be frank, it's just ONE technology in a sea of dozens or hundreds of data management tools which get shuffled in and out of use every handful of years. It takes a small amount of time to learn (order of days or weeks, rather than months or years), and knowing more about the general idea behind SQL and managing large sets of data in general is a lot more helpful for keeping up with the industry than knowing a narrow slice of ways to manage data.  It's fair to be skeptical if they don't even know the basics behind that (and the courses they took suggests they should have) but programming is a MASSIVE field involving dozens of nearly completely different disciplines and hundreds or thousands of main stream tools. Developing courses and teaching each of those tools (python, java, ruby, SQL, C#, C++, C, map reduce, R, matlab, perl, the handful of common shells, unix system tools, windows system tools, octave, POSIX standards, .net libraries, openGL, Direct x, nosql flavor of the month, javascript, haskel, clojure, lisp, go, CUDA, openCL as well as - apparently, according to a recent news article I read - fortran, and APIs/libraries/extensions where appropriate, as well as whatever I missed) as a prerequisite to graduation would take an obscene amount of time, most of which would be wasted throughout their career, and none of which would even start to cover theory behind using those tools (AI planning, data mining, OS structure, algorithms & data structures, machine learning, automata theory, information theory, cryptography theory, robotics theory, network theory, and whatever else I can't think of off hand).  Tl;dr: universities are places to learn long term, broadly applicable skills, like effective programming practices, not trade schools "
"NET is technically fine, but conversion of existing code bases is too difficult, limiting its adoption both within Microsoft and outside.",".NET is technically great.  But that's not surprising, after all.  They're Microsoft - they have a ton of money, they can hire top notch talent, and isolate them all in a room and they'll probably write some pretty good software, full of beautiful and twisty passages that all lead back into one other, and yet never outside the walled garden.  Technical greatness is easy for them.  So the engineers emerge triumphant, and thrust .NET upon the world, saying, ""Look what we have wrought!  But first, drink this Kool-Aid!""  And all the engineers, even Microsoft's engineers - the people writing Visual Studio, Office, Windows Media Player, Internet Explorer, Microsoft Messenger, grimaced at C++/CLI, quietly dumped the Kool-Aid down the sink and got back to writing native C++ code.  The .NET folks were left to scrounge for scraps among the likes of Windows Media Center.  If the door to your garden is big enough to fit your body but not your baggage, only the empty handed can enjoy it.  So it is with .NET.  It didn't have to be that way: even Apple, with its Johnny-come-lately garbage collection, managed to convert existing frameworks to GC and ship a fully GCed Xcode, at the same time that Microsoft sheepishly cut their .NET command shell rewrite from Vista.  So what's .NET's future re: adoption?  Maybe they'll manage to rewrite everything.  Maybe their non-.NET apps will fade away in significance, and we'll all use Silverlight apps in a simple .NET shell of a web browser.  Maybe the clean break will succeed, and in a few years, every app on Windows will be .NET.  But I don't think so, because continuity is the norm.  Intel, chained to x86's past, turned those chains into deadly weapons.  The most interesting OSes available today - OS X and Linux - have a direct line of descent from Unix (though one by adoption, not birth).  And it looks like the line will continue unbroken through smart phones, with the iPhone and Android.  This, if anything, is .NET's biggest failure.  It's Squeak.  Or pretty darn close to it.  It may forever be resigned to a niche of business software and occasional small apps.  Perhaps they should have focused less on its technical excellence and more on a path for integration with existing software.  Or maybe not; but so far I can count the .NET apps I use on zero hands.  tl;dr: .NET is technically fine, but conversion of existing code bases is too difficult, limiting its adoption both within Microsoft and outside. "
mysqli's bind syntax sucks and works by reference.  PDO's bind syntax has a slicker alternative  and   that  works by value.,"The biggest difference is in how the placeholders are bound to PHP variables.  It's a huge one, and I love explaining it because it shows how utterly stupid mysqli is compared to PDO.  Disclaimer: I am a PDO fanboy.  First similarity: Both PDO and mysqli use question marks as placeholders.  PDO also can do named placeholders, but they suck subtly.  Second similarity: Both work just like this pseudocode:  $thing = 'Hello!';$sh = $db-&gt;prepare('SELECT foo FROM bar WHERE baz = ?');$sh-&gt;bindPlaceholder($thing);$success = $sh-&gt;execute();  Here's where things fall apart for mysqli: it binds  by reference .  Here's mysqli pseudocode.  $thing = 'Hello!';$sh = $db-&gt;prepare('SELECT foo FROM bar WHERE baz = ?');$sh-&gt;bindPlaceholder($thing);$thing = 'Herp, derp.';$success = $sh-&gt;execute();// Executes ""SELECT foo FROM bar WHERE baz = 'Herp, derp.';""  That's most certainly not what you wanted.  Now, this usually isn't a problem, right?  Right?  Nope.  The second big difference between mysqli and PDO makes it a really annoying problem.  PDO does not require you to bind each placeholder one at a time.  Instead, you can pass an array to execute:  $thing = 'Hello!';$sh = $db-&gt;prepare('SELECT foo FROM bar WHERE baz = ?');$success = $sh-&gt;execute(array(    $thing ));  If you needed to give an array of values to the bind methods in mysqli, you'd need to loop over them, and looping over things that will eventually become references is perilous as fuck.  The structure  foreach($array as &amp;$ref)  can work for most modern PHP versions, but don't count on it in anything more than a few years out of date (not that you should willingly write new code for ancient PHP versions).  UPDATE:  PDO's individual  bindParam()  method  also  works by reference.  However, PDOStatement's  execute()  taking an array removes the horror that can occur in mysqli when you need to iterate over an array. /UPDATE  This matters only because most devs I've seen generally prefer not to have to do the prepare-bind-execute tapdance and usually write a wrapper for the underlying DB library with convenience methods.  Using mysqli as the base makes the convenience library severely more annoying to write.  tl;dr:  mysqli's bind syntax sucks and works by reference.  PDO's bind syntax has a slicker alternative  and   that  works by value. "
XORing the output significantly reduces whatever bias exists in the RNG.,"I was wondering this, too.  It seems that if you generate 0's and 1's with probability 0.5 +/- x, (where x is ""small"") then XOR'ing two sets of streams from the generator gives 0's and 1's with probabilities 0.5 +/- 2*x^2.  So, for example if the raw stream has P(0) = 0.499 and P(1) = 0.501 (x = 0.001), then you get P(0) = 0.500002 and P(1) = 0.499998.  It's probably possible to make x quite small by adding some negative feedback to the inverters based on ""long term"" averages of the distribution of 0's and 1's.  And since the ""conditioned"" output is only used as the seed for a software RNG, maybe that small bias doesn't matter too much?  (Damnit Jim, I'm a physicist, not a security guru!)  Fluke made a calibration multimeter in the 80's that used a similar idea for the thermal-RMS converter.  tl;dr: XORing the output significantly reduces whatever bias exists in the RNG. "
Coming up with true random number generator is really tough.,"Instead of looking at the temp. idea. Lets pretend that you want to give the location of me as I drive from Philly to LA. I am going to be driving a long distance, 9000km.  You will measure my distance down to the nearest meter.  In theory I could have 9 million different locations that I can be at.  But that assumption is wrong.  You know that I will be traveling at 100km/hr.  (That is analogous to the amount of information that the processor handles. The heat created as a result of that processing can be calculated, your processor’s heat is rather steady over long time time, and short time spans <µsec).   I can drive at 140 km/h or at 40 km/h for short distances but does not change much.  I have drive for four hours.  Where am I?  Within 100 km of Pittsburgh.  Instead of having to test 9 million locations you only need to look around southwestern PA in one of 10,000 locations.  If you have previous readings over those four hours,  my location is more well known.  Also you know that I will not retreat back east as time goes on, so that makes the range of locations even smaller.  You want a nonlinear random function.  When I check in with my location every hour it should be: I'm in Philly, Dayton, Chicago, Berkley, cornfield in Nebraska, Denver, Las Vegas, Wheeling WV, Zainsville OH, Berkley....ect at any point I could be in any city alone that path.  temperature is too predictable.  Also most cheap temp sensors are available only measure down to the nearest 0.1 degree.  tl;dr Coming up with true random number generator is really tough. "
"not a fan of the whole Metro experience taking over my computer.  Windows 8 DOES perform very nicely though, I have to give it that.","I downloaded it, and I'm personally not a fan.  I don't like how they don't initially ask you if you'd like to start up to the Metro UI or directly to the desktop, the ugly green background when installing and logging in, the new Aero frame (it's actually not bad, but I don't like the larger, centered text with the shadow), and when you press the start button it brings you BACK to the Metro UI.  At the last part I was like yeah okay I'm done for now, tried turning my computer off and I only saw the option to log off, so I logged off and while the lock screen is actually pretty nice, it's a slide-up screen (like you'd do on a mobile device).  tl;dr  not a fan of the whole Metro experience taking over my computer.  Windows 8 DOES perform very nicely though, I have to give it that. "
"standing on the shoulders of giants, analytic rigor, still left with what amounts to a mind/body problem in maths.","You're very clever, young man, very clever...but it's zeta functions all the  way down...  .  If I'm not mistaken, you're alluding to Wiles's Fermat proof, and I will agree that when taken as a whole, that a brier patch such as that can be daunting to attempt to work through.  ...but: such a thing can be done and understood, and if there exist gaps in understanding, would that not be the place to attack the validity of such a premise, in so doing allowing the presenter the opportunity to defend it?  (Which has the handy effect of producing an explanation or sending it back to the drawing board, Ala Wiles)  Should it be the case that nobody else understands it, should it not be considered (for lack of a better word) bullshit until it is the case that there exists at least one other such person that does?  Anyway down this path lies the same point beyond which there be the dragons of faith in objectivity and/or the existence of convincing, but ultimately lackluster, bullshit.  Or maybe I'm just tired and this is bullshit, or at least, solipsistic wanking of the most mediocre order.  tl/dr: standing on the shoulders of giants, analytic rigor, still left with what amounts to a mind/body problem in maths. "
"it's not about the tools, it's about a process that ensures quality.  Recognise and talk about that, not neckbeard tool wars :-).","It sounds like the merge-monkey / repository master role is no longer filled.  You need to highlight this as a risk - interpolating from what you're saying it looks like neither the developers nor management are aware of the collaborative workflow, and they're talking about ""going back to SVN"" with an essential part of what was there in the past missing, which is bound to blow up in their faces.  Now I wouldn't necessarily advocate git in this scenario, as ad-hoc management of private repositories can end up in an even worse mess if poorly managed, but the workflow definitely needs to be documented and it sounds like that's up to you.  To highlight the importance of the merging task, you might be able to pull data from histories (this would at least show how much work the repository master was doing and its critical location on the release path).  It's a tough gig to sell the importance of repo management to people who are unaware of it, and frankly the way you're talking about ""git vs svn"" instead of merge/release practice is a warning sign in my mind, but maybe you can reorient your goals in terms of practice rather than tools to make a more compelling case that doesn't sound like neckbeards arguing about the colour of the bikeshed :-).  If it comes down to it, you might have to put your hand up to be the repo master.  But even in that case, focus on having a workflow you can be confident with - even if that's everyone else committing to SVN and you merging into master via git+tailor, it's the process you're trying to protect and not the tools.  The process is what ensures quality, which is the only thing the business cares about.  You  might  be able to sell this better if you can talk up a feed in to better release tracking, automated testing and CI .. these are things the business will recognise as value, and most devs would like to have if only someone else would manage them.  But don't get carried away .. another thing people like is incremental change :-).  TL;DR: it's not about the tools, it's about a process that ensures quality.  Recognise and talk about that, not neckbeard tool wars :-). "
each one teach one and we all get smarterer (sic),"in the martial arts, part of the work involved in achieving higher belts is teaching the lower belts. so it is for computer programming. teaching what you have learned solidifies your foundation on which you can learn more.  as for stackoverflow, i was unaware that gameification was the attraction. i thought it was just an easy platform to share answers, and we computer geeks tend to like to share, especially those of us that have the appropriate level of humility that reminds us that no matter how frequently we are the smartest person in the room, that has more to do with the room than with us.  so screw game points, i'm a grown man. i will gladly teach what i know, because i want to learn  tl;dr  each one teach one and we all get smarterer (sic) "
Stack Overflow is compiling the answers you'd normally get from Google Search results. I think this is a good thing.,"> He says he has a lot of points in Java because he can google well.  To take this from another direction, it sounds like he's bitching that people on Stack Overflow are just garnering points by reposting something they got from an easy Google search.  When I Google search a problem nowadays, I usually come across Stack Overflow at the top of the search results. If people didn't provide answers that were easily Googleable, I would not be finding the answers on Stack Overflow, but rather on any number of disparate sites, many of which I'd be unfamiliar with the layout of, and which I wouldn't know whether or not to trust.  Having these exact same answers on a user-moderated site like Stack Overflow means that I'm finding the answers on a site with a familiar layout, which I know to trust, and which frequently provides multiple possible solutions.  TL;DR  Stack Overflow is compiling the answers you'd normally get from Google Search results. I think this is a good thing. "
"It's up for to team to make and support good quality code, not many large companies are interested on doing code reviews.","[95% of the ATMs in the world are running Windows XP]( and we are not talking of some retail service here. Just to give you a perspective.  That being said, there's a lot of legacy JS code in the wild and it's not near pretty as you said. In my experience, (I've been working only in JS for the past seven months) I've been enforced to follow strict coding styles and quality measures. I think that it had a huge impact on the code readability and simplicity. On the other hand i also encouraged my co-workers to drop the use of deprecated JS features as I think it's a good practice to pay attention even for the ""temporary"" code.  TL;DR; It's up for to team to make and support good quality code, not many large companies are interested on doing code reviews. "
"This RFC looks like its just about there, but would be much nicer with function overloading.","This syntax (and a similar discussion in [another thread]( are fairly nice. They even match what some other languages do. I was happy to see it included array and callable. I was also happy to see it supported regular functions and proposed additions to the reflection API.  A few concerns about this RFC, which is still far better than several RFCs I've read through:   The introduction of uncatchable fatal errors is a bit disappointing. (I'm not a fan of any fatal errors, but I guess CATCHABLE is better than anything)  It'd also be nice to see some sort of sane nullable, but we can live without it.  ReflectionType  may be better as 3 classes  ReflectionTypeCallable ,  ReflectionTypeObject  and  ReflectionTypeArray . Developers can then simply do instanceOf rather than a second method call. This removes the need for  getKind()  and allows  getName()  to be be named more clearly:  getClass() ,  getClassName() , etc.   However, return types may seem nice, but in a dynamic language like php: having some sort of  function overloading . I'm sure this isn't the first time its been mentioned, but lack of such a feature has me writing more workarounds than the lack of return types.  Example without having return types:  function find($id) {}function find(Criteria $criteria) {}function find(array $search) {}  Example with return types:  function find($id):User {}function find($id):Dog {}function find() {     $type = \func_get_type();}// calling code can determine what type it wants$user = $object-&gt;find():User;$user = $object-&gt;find():Dog;$user = $object-&gt;find():TheGreatBeyond;  Maybe I'm dreaming too big here, but this should already affect how everyone is writing their code.  tl;dr  This RFC looks like its just about there, but would be much nicer with function overloading. "
"PHP is not consistent, predictable or easy to write without having to look up the manual every minute.","I don't think PHP is a bad language, really. But there are some aspects of it which make you wonder how it got the way it did. Off the top of my head:   Some function names are snake_case, some are camelCase, some are bin2hex, some are strtolower, some arenothingatall.  Some functions return false on error, some return 0, some return -1, some throw exceptions, some return null, some return empty strings, some don't return anything at all and rely on you using another function later on to check for an error (e.g. json_decode()).  The horrible tacked-on OOP support.  Annoying automatic type conversions causing people who use == to trip up.  Error reporting in PHP is just fucked. Most core functions don't throw exceptions so you get no sort of backtrace.   I'm stopping here because my post is getting into wall of text territory. I've developed with PHP for the better part of 7 years and it's at this point that I'm realizing I need to make a move to something else before I lose my sanity.  TLDR; PHP is not consistent, predictable or easy to write without having to look up the manual every minute. "
"there's no checklist, no scoresheet, every candidate is a unique snowflake or something.","Well all that said, an interviewer needs to be able to adjust to the candidate too. Again, being strict, saying an interviewee has to conform 100% to any model is bullshit. Removing ego from the interview is tough, but your real goal is to make the company  better , sometimes that means finding someone very different than yourself.  Personally I don't ask any ""right or wrong"" questions - all of my questions have multiple solutions. Part of the interview is discussing pros and cons of the different approaches. Often times I learn new things during these discussions. I don't ask questions that refer to any algorithm or pattern by name. I only ask to see code white boarded if the discussed approach can be knocked out in say 20 lines or less, and only if the candidate seems comfortable with it. If not, next question.  I  must  see code eventually though. This is where my ego might get in the way a bit. I rarely hire someone who can't demonstrate they are an  expert  at  some  language. They can choose whatever language they want, but they must stick to that language, and they can't use pseudocode. It is interesting to see how many people screw up on this part (chose a language they're rusty in, or slip into pseudocode), so I'm pretty lenient on this now.  Anyway, I try to sniff out the strengths of an individual and focus on those. Sometimes I can pick up on them during the non-technical part of the interview. Other times it's while they're white boarding. If this approach were point based, it'd be additive, not subtractive, and there wouldn't be a score limit.  At the end of the day I'd like to think I give everyone a fair chance to display their merits - if they can't then maybe it's not a cultural fit (as opposed to a technical one).  Tl;dr: there's no checklist, no scoresheet, every candidate is a unique snowflake or something. "
Programmers should stop thinking like programmers when it comes to human-interaction.,"But I think this  does  create menu clutter.  Chrome and Firefox provide us with great tools out of the box, to look at the DOM of a web-page for those of us who are technically inclined and what a specific part of the markup.  But, as I said to  lowtolerance  your basic user will want what they clicked on, the top-most, most prevalent image.  Remember, nobody is losing out here.  We can still get at the other stuff via the DOM and being tech-savvy, that's easy to do.  Your typical mom'n'pop internet user doesn't need to know that.  We need to stop thinking like developers when it comes to these things.  And if the website in question has achieved an effect by combing an <img> tag superimposed upon a CSS-styled element with a background image where the obvious answer isn't obvious, then we'll accept that as a necessary side-effect.  We can't be right all the time, but we can be right  most  of the time.  And that's what matters in decisions like this.  And thanks to the DOM inspector, one way or another, we're right 100% of the time.  We've just made life easier for the majority of our users.  TL;DR: Programmers should stop thinking like programmers when it comes to human-interaction. "
The denominator is not zero because not being able to do one type of problem doesn't mean I have done zero work.,"Presumably we are measuring an amount of work done not type of work.  I.e. I might know nothing about algorithm design, probability, etc... and can't create a great average case algorithm to solve some hard problem.  However, I might be decent at making widgets for the UI.  Just because you solve a more difficult problem then the one I'm working on doesn't mean I have done 0 work.  I assume you don't hire programmers (or at least quickly release programmers) that can do 0% of the problems given to them.  And if someone can do only 0% of the programming tasks given to them then they are as much a programmer as a wrecking ball is.  Edit:  And I think the wrecking ball comment applies to negative performance as well.  tl;dr The denominator is not zero because not being able to do one type of problem doesn't mean I have done zero work. "
"I'm fine if you want to automate some drudge work, but do it better. The current craptastic set of ""best practices"" isn't helping.","Can't speak for the OP, but my problem with all these ""enterprise ready"" solutions that are bought and sold like so much snake oil is not that I can't grasp the challenges with large scale software development -- it's that you can't either. Or rather, you aren't any closer to solving the problem than I am.  Software hasn't gotten any better. If anything, all these enterprise tools have created software that's even worse than it ever was before. These people have fetishized tooling and reuse, to the exclusion of any other measure of software quality. Without sarcasm, a great many ""architects"" will hold the opinion that  ServiceLocator locator = new ServiceLocator();EmployeeLookupService empLkupSvc = locator.getEmployeeLookupService();EmployeeFactory factory = empLkupSvc.getEmployeeBuilder();factory.setEmployeeId(new EmployeeId(12345));Employee emp = factory.getEmployee();  is somehow preferable to  Employee emp = getEmployee(12345);  It's not. Hooray, that you can mock test objects and click buttons to generate the trivial code to initialize an Employee from an XML file. Don't get me wrong -- I'm not some luddite who things that you should always do things the old ways. But you have to weigh the costs versus the benefits, and I simply think having your code no longer be about employees, and instead be about abstract builder factory impls is way too high a price to pay for the minor benefits you're getting.  TL;DR: I'm fine if you want to automate some drudge work, but do it better. The current craptastic set of ""best practices"" isn't helping. "
"all i know is my gut tells me ""maybe""
ps. dongs","Unless my buddy was the only person to report it, I say BS.  Granted it only happened to me once, but there are a pleathora of people abusing this.  This is the first thing I have seen where it's trolling but just do a search for ""click"" and see the results.  These jacktards routinely add people to their repos as a means of spamming their links.  See:  for many repos like:huh, they go to 404s now.  if only they didn't show up in search results (also none of these users are named click anything, why do they show up?)  you (and zed) are delusional if you think this isn't a problem or that it's the most important problem in the world.  It's well known enough that most everyone I know that it's happened to just leaves it and it falls off the bottom of their list quickly  TL;DR- all i know is my gut tells me ""maybe""ps. dongs "
If you want 1000 cache pools in single run time go write something else.,"I don't think you understand how everything works, Memcached extension binds to your server daemon, using multiple instances will fail and product horrible results. Look and read up on configuration directives Memcached has.  The wrapper has static methods because A) you should not be running multiple instances of it B) it's a wrapper for Memcached, as in you call and use same methods as Memcached extension but it wraps around them and both are called statically.  So please calm down with your 2 page code reviews and suggestions that make no sense for the given application.  Your static rant is also bizzare as you fail to see how this should be used.  TLDR: If you want 1000 cache pools in single run time go write something else. "
"BASIC ages 6 to 13, VB ages 13 to 22. PHP / Javascript age 22+","I remember typing lines and lines of BASIC source code (but utter gibberish to me) into a BBC Micro at school and being amazed at the very shitty graphics produced. I must have been about 6 or 7. When I was 10 I got an amstrad cpc 464 (which was literally ten years out of date - it was built in the same year as me!) as well as some old copies of a magazine called Amstrad Action which had code snippets and cheat codes for games (back when a cheat code was actually code - POKE!). I guess thats what really started my interest in programming. After finally getting a PC I wrote several programs in the PC BBC Basic port, then gwbasic, then qbasic then finally visual basic when i was about 13. Then i got hooked on IRC and became an mIRC script ninja - i loved that language. I used VB5/6 to make various tools and programs but never really finished anything. Took a few months of computer studies at college where we used turbo pascal, but i dropped out. a few years later i decided i wanted to build websites, learnt php and javascript and now i'm a full time developer. So i've had an interest in programming for most of my life, but i've always seen it as a means to an end - i was motivated to code because of a particular problem i needed to solve, or program i wanted to be able to use. It's only in the last year or two that i've really taken an interest in the programming itself and the more advanced concepts.  That's the sum of my programming life so far, if i could do it again i'd have learnt assembler, c or java early on and have got into web development sooner. I think BASIC profoundly warped my view of software design and it took me several years to snap out of it, that said - i don't know if i'd have ever got into programming without the initial simplicity and above all promiscuity of BASIC.  tl;dr - BASIC ages 6 to 13, VB ages 13 to 22. PHP / Javascript age 22+ "
"standup nice.  pairing ok sometimes, horrible most of the time.  i think a certain balance is the key.","typically most jobs i have applied to make sure to note these things in their ad.  i have done both.. at my current company we do daily stand up meetings that take about 5 minutes to go through 5 programmers and 2 QA people to go over large issues, things we might be stuck on, and what we have finished.  very quick and simple.  pair programming at my last company got pretty horrible.. as many of the people have listed here the obvious issues with it.  the few things i did like about it was that   i learned faster because i was new to the language  i had no chance of carpal tunnel only coding for 4/8 hours  i made a really close friend and i dont think we woulda gotten to know each other as well if we hadn't paired.  working on huge things that could blow the company sky high was nice to not have the entire burden on my own shoulders  everyone knew the system   then the negatives..   i hated programming with some guys.. they would doze off, play games on their phone, bicker about every design decision  after i was all caught up on the language/system, i didn't really learn anything from pairing, and sometimes it held me back.  twice as slow for normal tasks anyone could solo on.  no one knew specific parts of the system in depth   tldr: standup nice.  pairing ok sometimes, horrible most of the time.  i think a certain balance is the key. "
"Just look at the 1-5 questions. 
 >P.S. The link is to the list of programs offered at the University I would like to attend.","edit  Thank you all for your answers in such a short period of time :) If it would Help, I'd like to say that I'm interested in game design as an end product and would like to ideally be able to make games and other stand-alone, larger scale projects in the end, but would like to assure a certain degree of job security :)  >Hey Reddit, I couldn't find a way to do a post in the form of self.programming, and I really could use the advice if anyone could provide any insight.  Here's my current issue:  >I'm applying to university and I'm totally in love with computers, but mostly the programming portion (I program in C++, Turing (programming gateway, don't hate me), and am learning C#).  Two programs come to mind when I think of computer programming, and they are Comp. Sci. and Software Engineering.  I would love some insight on how these programs differ and which would be a better fit for me.  From what I understand, Comp. Sci. is the parent of Software Engineering.  However, Comp. Sci. also includes AI, Operating Systems, Algorithms, Networking, and all the other computer related things I enjoy.  I request the answer to these questions:  >1.    What is the difference between these two programs?   Which would you recommend for me and why?  At the end of each program, what will you be able to accomplish?  What are the employment rates for Undergrad/Grad students in each respective program (link?)?  Why pick one over the other?  Can they both be made into virtually the same with electives?   >I'd be very grateful to anyone who could help me clarify this issue.  If you've got any links that would be beneficial, or are yourselves in this program, could you please assist me?  Thanks in advance Reddit, I could really use the help!  Thank in advance!  >:)  >TL;DR : Just look at the 1-5 questions.  >P.S. The link is to the list of programs offered at the University I would like to attend. "
"be nice to your underlings, we might suck but some of us try goddamnit! ;)","A guy i worked with told me this the other day. I'm not a good programmer, (i'm technically a web developer and work in php/mysql, js and as3 for the most part). I'm not good at it but i'm not awful either, i'm still wrapping my head around OOP disciplines, class structures etc. but i get better every project i take on and looking back at projects a couple months previous makes me nauseous to see the horrible route i took to get from a to b. I spend all my time reading tutorials on javascript, jquery, as3, everything i can lay my hands on and i do feel i'm getting better.  The thing that annoys me about it is that this guy has been working as a professional coder for 25 years, i've been doing it 15 months. He basically called me out on it in front of my employers and told them i was shit. They thankfully backed me up. The main reason i took the job in the first place was because i wanted to work with this 'guru' and learn everything i could but i think i've met the guy face-to-face about 10 times in the last year and a half so the whole thing fucking sucked.  Regardless this is more a rant that i've not felt to talk to anyone about as it pissed me off and i didn't want to seem weak in front of the people i work with but i appreciate your response (even if it isn't directed at me).  tl;dr: be nice to your underlings, we might suck but some of us try goddamnit! ;) "
"While you're right on the surface, it's a little more complex, and I would honestly feel better using a more ethical distro.","By being another number to add to their userbase, I'm allowing them to improve on their claims of being 'the world's favourite free operating system'. Many of the other users pulled in by the 'choose the most popular' routine  will  directly be abused by the issue.  I agree that I'm probably a net loss to them, and not directly supporting them by continuing to run ubuntu based distros, but overall indirect support is still support, at least IMO.  As for bandwidth, I'm pretty sure all of their package mirrors are on edu sites and other mirrors such where they're not directly paying for hosting anyway, so the costs aren't as clear to establish, and since pretty much every distro goes thru mirrorservice.org and the same list of mirrors, by downloading ubuntu packages I'm indirectly limiting the available bandwidth for the other, more ethical, distros.  TL;DR While you're right on the surface, it's a little more complex, and I would honestly feel better using a more ethical distro. "
"instruction execution can be statically determined, dynamically scheduling it doesn't gain you much, and is much more complex","you're describing a scheduling engine  we already have CPUs that do dynamic, out-of-order scheduling -- and it consumes amazing amounts of power and chip area to do it  CPUs and compilers can have varying levels of complexity/smartness. a dumb chip with a smart compiler is a very common, well-studied pairing -- the RISCs tend to work this way. a smart chip pretending to be a dumb chip with a smart compiler is the current state of x86 compilers -- there's some awareness that behind the scenes the CPU is doing OoOE, but it's not exposed.  The Mill aims to have a dumb chip with a clever compiler that targets it directly, removing the extra layers of complexity and inefficiency that OoOE adds.  For your specific question-- data flow graphs are directed and acyclic. having producers point at their consumers is somewhat backwards-- it's more natural for them to point at their dependencies. Instructions would say ""I need the results of #4 and #5"", and the CPU would schedule that instruction when the results are available. This is precisely how out of order execution works. The Mill's exposed pipeline simplifies this-- rather than your instruction saying it needs the results of two previous instructions, the compiler knows how long the previous instructions will take to execute and where their results will drop, so you simply say ""I need belt positions #4 and #5"", and trust that the compiler has arranged things so the data it needs is in the appropriate slots.  tl;dr: instruction execution can be statically determined, dynamically scheduling it doesn't gain you much, and is much more complex "
"there is room in agile for creativity and innovation, but ultimately we're a business that needs to make money.","> kept under the microscope of daily standups and 2-week sprints  And this can be a very good thing in some (usually larger) organisations, as it exposes the engineers who never actually do anything.  But if the project you're working on is important to your company, then your company will surely make time for you to work on it.  We're a company that uses Scrum, and we allocate 30% of our effort to our internal needs, such as upgrades, significant refactorings, tech debt etc. - business stories are the other 70%. If we had a tech need that was being poorly met by existing technologies, then we'd happily allocate that 30% to an internal project.  If that tech need was affecting the business negatively, then there'd also be business stories to work on it.  TL;DR - there is room in agile for creativity and innovation, but ultimately we're a business that needs to make money. "
the chinese government sucks ass. I think they've even blocked the wikimedia image server now. Fuckers.,"The Chinese government does all sorts of crazy shit like this with regards to the Internet. It sucks. Sites are blocked arbitrarily (for example, based on where they're hosted), or for a silly reason - Facebook, Youtube, Blogspot, thousands of others. Project Gutenberg was blocked too not too long ago. And when you try searching for certain terms (i'm afraid to give an example, I don't want to lose my Internet), or even just typing that term somewhere in a URL, you might find that the website stops working for a few minutes. Pretty much every time that I try to access a site and it doesn't work, I assume that it's blocked, because that's just so much more likely than the site being down or something.  I hear Google is getting fed up with this (particularly, the government asking for stuff from the gmail accounts of people it doesn't like, e.g. human rights activists) and is planning to pull out of China. Good for them, but not so good if the government decides to retaliate by, say, blocking Google and all its services.  tl;dr the chinese government sucks ass. I think they've even blocked the wikimedia image server now. Fuckers. "
I work in a public school all we have are Mavica cameras and flash drives are hit or miss.,"Seen a floppy disk? I use them every day at work. 3/4 of our computers are pre-2001. You dont know how much crap I go through every day with compatability issues. Typical scenario:  The network is down and I need to save this file and bring it with me. Alot of the time the computer wont read a flash drive. Oh what can I do?  Even more typical: The computer saves to a usb drive but the one I want to go to needs drivers and...  Then it hits me!  I say ""wait how big is the file? Oh yea, I can use the floppy drive!""  I can't even begin to describe how often this happens.  Also, the floppy drive serves another important function. It allows us to still use our fleet of 10 Sony Mavicas. That's right, the ubiquitous floppy drive camera from 1997. We only have three or four cameras that are any newer than that. The funny thing is that in 78% of situations the newer cameras are useless because there is no way to download the pictures. We either lack the card reader or any hope of downloading driver (firewall disallows downloads which I understand completely)  Can you guess what kind of place I work at??  If you guessed a public school system you were correct.  On a personal note I still have a large collection of floppies I took pictures on with my personal mavica. Back in the day those things were great.  tl;dr I work in a public school all we have are Mavica cameras and flash drives are hit or miss. "
"Asimov's Foundation is awesome, and things like this floppy disk abstraction is why I loved it.","This makes me think real hard about my favorite science fiction series, Asimov's Foundation.  For those who aren't aware, the series takes the reader on a short-story version of a 1,000+ year collapse and rebirth of an empire.  It kind of reads like a collection of short stories at times where every 50-100 pages may skip ahead in time to introduce the reader to a new generation of people with their own misconceptions about the past and political agendas for dealing with their own problems.  Its hard to read and not question the reality that surrounds us.  One of the things that fascinates me about the Foundation is that you get to see the meanings of events and words evolve through time.  This is exactly what comes to mind when the OP talks about the floppy disk becoming an abstraction of the idea of ""save"".  I would like to think about any one of us waking up in the year 2150 (only 140 years from now!) and seeing the ""floppy disk"" save icon on a great grandchild's equivalent of a personal computer.  It may even be a twisted and convoluted image that has become ungrounded from the physical floppy disk it originally represented.  I think it is reasonable to think that outside of historical societies, virtually no one will understand what the image is a physical representation of.  It would be an example of an information meme outliving the original physical manifestation.  I think the coolest part of all is that I can see this happening now.  Exponential growth of technology really is something that gives me cold chills when I think about it.  tldr; Asimov's Foundation is awesome, and things like this floppy disk abstraction is why I loved it. "
"I am old, and so is everyone I know.","Yo.  My mom and her business partner opened the first computer store / typesetting and word processing shop in San Juan County back in the early '80s.  I've played games on every kind of medium there was, from simple puzzles I typed in from the back of an adventure book to Commodore cartridges to 5.25"" cover floppies that came with some old DOS magazines.  The first DOS computer I used was an old dual-floppy Zenith portable; the first Mac I used was a 512k ""Fat Mac"".  As computers caught on more slowly in Japan, my wife has actually used 8"" floppies.  My dad, a civil engineer and land surveyor, still has his HP-67 with all the accessories, including (iirc) a topological analysis program on a magnetic paper card.  tl;dr: I am old, and so is everyone I know. "
it just seems a bit ambiguous until you know what's up.,"Possibly a more predominant tab across the top of the screen like ""comments, related, discussion"" tabs, ""comments"" or ""messages"" or some such. No reason to use a tiny little icon on a big giant browser. Or at least pair the mail envelope with the username, turn them both into the same link to a messaging portal. Or at least a (number) next to the envelope. That would at least give the user feedback ""oh, it's not just an orangered envelope, it appears to contain 3 items. I wonder what those three items are? ignore....2 days later  now it says 7! Hmm...  click ""  Seeing the interface separated with pipes, when I initially started using Reddit....2 years back or so? I thought username (3) | [orangered] were all linked together and when I clicked username the 3 would go away or...just didn't understand the behavior until one day I clicked on the envelope.  Now it makes perfect sense to me, but the layout/paradigm is just a bit unwieldy. I normally see a (number) as == number of items in the link.  So I guess at the end, a mail envelope icon might be fine given as you said, what else would one put there? A cheetah? A Reddit alien? (maybe the user could customize the new mail icon? That'd be a neat future feature.)  I guess tl;dr, it just seems a bit ambiguous until you know what's up. "
Customer fixed a performance problem by looking at the end of a fiber optic cable.,"Very similar to what I do on a daily basis, except for I'm debugging our own server software. Recently closed a case where the customer was experiencing some 60 second hangs, multiple times per day. Using similar debugging techniques I was able to narrow down the exact times when it was happening. This particular case didn't require a windbg dump since I was able to find that each one of the hangs ended with a timeout to the WriteFile function (using some internal counters in the server software). Told the customer that it looked like a communication problem with the SAN. Customer comes back a few hours later saying that he noticed that the transmit power was low, so he pulled the fiber cable (redundant) and couldn't see any light coming out. So he replaced the cable and boom that fixed the write timeouts.  TL;DR - Customer fixed a performance problem by looking at the end of a fiber optic cable. "
"When talking about complex software, FSMs lose some of their value because the ""space of states"" becomes too immense to comprehend.","I think that's mainly because they don't ""scale up"" well. Sure, on a conceptual level they can prove beneficial to some aspects of a complex software architecture or application design. However, the whole point of and the inherent value of FSMs are in the  states ! That all goes out the window without enforcing rigid boundaries around the sates and the transitions. In complex designs, it's just all to easy to 'break' the paradigm...essentially leaving yourself with multitudes of ""extra"" unaccounted-for states. (ie. you can't really have ""globals"" or methods with side-effects or viola, mystery state!) Worse, they tend to become extremely unwieldy when it comes to parallel programming. Although, many of the lessons one learns when thinking about FSMs, like immutability, do prove beneficial when thinking about multi-processing problems.  Taking a high-level abstract view of FSMs as they relate to programming, you can see how they can also serve as a good analytical tool...especially when it comes to dealing with complexity.  tl, dr;  When talking about complex software, FSMs lose some of their value because the ""space of states"" becomes too immense to comprehend. "
Zombies ate my brain.  Give people like me a break. ;),"I've been guilty of this myself: I don't think it's out of ignorance necessarily as much as the fact that, once you're knee deep in a problem and you've convinced yourself that you're going down the right path, you sometimes forget where you started.  To me, it's the same reason why its so hard to answer the manager's question ""what are you doing right now?"" when they interrupt you in the middle of development work.  You're nested nine levels deep in some problem solving scenario, and he's just interrupted it: and you need to come back up and remember the whole chain that brought you to that point before you can give a real reply.  That being said, it is definitely useful in both scenarios for the person to ask ""what are you really trying to solve?"" as that is quite helpful in bringing the person back to their original root goal... so I agree with the article, I just wouldn't be so hard on the people who commit it.  I don't run into many people who  don't  stumble into this problem with at least some frequency.  TL;DR Zombies ate my brain.  Give people like me a break. ;) "
Your business model isn't broken by using something like MySQL,"Um, no?  b2b businesses (even ones that provide a free service) can give you a competitive edge, while still providing them with valuable insights.  Google wants information.  They provide a free service (let's say Google Maps) so they can get more information.  If you are in the business of, let's say, providing information about movies, you should  not  be expected to go out of that business model and design a freaking geolocation application.  That's not how business works.  If Google pulls their API (or goes under, or starts sucking, or whatever), there are, or should be, competitors to replace them.  TL;DR:  Your business model isn't broken by using something like MySQL "
SMBs should be in the cloud - it's too easy and cheap.  Fortune 500 can bring solutions in-house as at that level security concerns trump the cost.,"I appreciate the point-counterpoint.  I it's not that ""you are wrong"" and ""I am right.""  I just think outside the Fortune 500 your solution makes no sense and yes, I do think you are heavily influenced by your sysadmin/IT background.  I work in a relatively stable mid-sized business.  We're probably not going to be acquired anytime soon, no investment bank is interested in fronting our IPO, but we turn a tidy little profit and we keep everybody happy.  We live in the cloud. After trying in-house solutions, it's just worked better.  Every time.  Even for something as ""simple"" as email - we don't want to spend the money on an exchange server and the IT knowledge required to maintain it.  So we went to Gmail.  Throw in FTP, video streaming, CRM and project management and it starts looking really silly.  All these things would bring our day-to-day operations to a grinding halt on a weekly basis managing, maintaining and troubleshooting.  Instead we use well known cloud solutions and never think about them.  TL;DR - SMBs should be in the cloud - it's too easy and cheap.  Fortune 500 can bring solutions in-house as at that level security concerns trump the cost. "
Not all Ruby devs are dicks. The ones who are have become self-promoted “celebrities”. That's probably why they stand out more; they force themselves upon you.,"Well, to be honest I haven't been involved with them Ruby folk for quite a while. I suppose Ruby probably got a little too popular, and a celebrity culture developed, starting with _why. Actually, _why wasn't a bad guy; he taught me a lot. But when he vanished, I guess I saw what the Ruby community had become. Sure, there were a lot of people who treated it as if a rock star had just died — they were the ones who reflected on his virtual persona and what he had contributed. John Resig was one of them.  Then there were those who were selfish and seemed to use the event to boost their own (already huge) egos. And now you have the celebrity wars of today. I think that's what pushed me away. I never really used Rails, which is what Ruby's become known for — no, I used it for small scripts (shell scripts, but more expressive). I used it for theory. I used it to teach myself how to program. And I still use it even now — I find it much better at the kind of things you would use  awk  and  sed  to do. It's  very  good at string processing, imo.  Still, the culture's become toxic, and that's a shame.  TL;DR:  Not all Ruby devs are dicks. The ones who are have become self-promoted “celebrities”. That's probably why they stand out more; they force themselves upon you. "
your experience is a minority and a PhD teaches you to think and understand rather than follow bandwagons.,"Disclaimer: I have a CS PhD.  You must definitely be in the minority. I don't think most of my professors have had industry experience, but they certainly didn't ""go to industry and  burn out and come back."" The requirements for a tenured professor position are far,  far  harder than the requirements in industry. Tenured positions all require research. Generally a lot of  research of high quality.  The majority of PhDs in CS probably don't get tenured positions and end up in industry. Certainly most people that I know who went through a top cs PhD program are leaving or have left academics for industry. These people are usually hired up by Google, Facebook, or big financial/biotech/other firms.  In my own experiences with industry and the open source scene I routinely see people reinventing the wheel over and over. Sometimes it's re-implementing a common technique in a new language like Node.js. Other times it's re-implementing known relational techniques or known in-memory databases. Sure, there is a lot of extra issues surrounding products like support, stability, features, etc. But the majority of  techniques  are not new. Many are more than 20 years old.  This is what a PhD in CS teaches you. It teaches you to see through the marketing to the basic ideas. Other things I see are people implementing solutions just because they are popular. I see people naively suggesting that the model that worked for their data works for all data.  An example ""You should use nosql instead of relational databases."" Commonly seen advice that is completely awful. Even the name nosql is aweful! It has nothing to do with sql and most nosql databases even use a form of sql for queries! What it means is that you are trading data integrity and the associated complex queries for speed, simple queries, and much less data protection. Do you want to make this trade off? It depends on the data and your requirements, not because of some popular meme that X tech is > Y tech or otherwise.  TLDR: your experience is a minority and a PhD teaches you to think and understand rather than follow bandwagons. "
This is a mobile/embedded issue. Not a desktop issue--for the moment.,"GPL is pretty damn open source. The question here is the fact it isn't the GPL 1.5 API and instead the 1.4 (timelines are fun things). ME Versions of Java require a license.  As to the GPL comments. GPL is a viral open-source. Fortunately Java 1.5+ is protected via the classpath exception allowing you to link the libraries without opening the viral open source of the GPL. Do you need to fork javax.sql  (wait, bad example)  java.lang? No, the answer is you don't.  The GPL serves its purpose. It keeps free software free. It is draconian and not my preferred license (something like Apache2 or Eclipse). Oracle is very wrong on many things, but one thing to keep in mind as a Java developer is that the SE won't be hit by this debacle. That's of key importance for those of us with thousands of hours invested in projects.  TL;DR; This is a mobile/embedded issue. Not a desktop issue--for the moment. "
Don't do load balancing on the application layer. It won't make that much of a difference.,"This question goes way beyond simply which framework you use. To answer your question though, the average business will be just fine with  any  framework. If you are an enterprise business you should be writing your own framework/comprising your own tools since all frameworks have bloat.  We are in the process of moving our application to a new server company and here are some benchmarks for our application which is written in Zend.  [BENCHMARKS](  The red and orange sets are our application at (MAX!!!) ~200 concurrent users on  our servers .  The blue and green sets are our application at  ~2000  users on Amazon and Google servers respectively.  Same application, completely different results/capacity.  TL;DR - Don't do load balancing on the application layer. It won't make that much of a difference. "
if by any chance somebody reading this who's thinking if to write a fake 'architect' position on their resume - don't do it. ;-),"Ok, so we seem to speak about two different things - I'm speaking about those who are de-facto architects, and you're speaking about those who're writing 'architect' on their resumes. And you're probably right - they do not seem to correlate well.  As for netting higher salary - maybe, but IMHO before coming to that point, they're drastically reducing their chances to be hired. Without experience to support their claims - they're out even before the interview, with fake experience - it will come out within first 10 minutes of the interview. If the guy says he was an architect in such and such project for 3 years - he should be very comfortable speaking about the project for hours (and answering questions on ""hey, and how did you deal with such and such problem?"").  For example, if somebody comes to me and says he was an architect of the system with 10M transactions per day running on Java - he'll face quite a lot of questions about 'how many servers did you need to run it', JVM memory limits, GC stop-the-world and response times, and of course, about database (which DB, partitioning, reporting is running on main DB or on replica - if the latter, then we're coming to replica mechanisms in use, isolation levels and locking, physical DB design, etc. etc.). If the guy is fake - it will be obvious and it will be over.  If the guy is an architect for boxed software rather than for a system like above - there will be a different set of questions. There is no escape - that is, if interviewer knows at least something about the field, and usually interviewer is in much better position in their own field than interviewee.  TL;DR: if by any chance somebody reading this who's thinking if to write a fake 'architect' position on their resume - don't do it. ;-) "
"lambda is not broken, its behavior is perfectly coherent with the language and its model, and with other languages in the same class.","Yeah except no, lambda implements a full closure: it closes over its creation (lexical) context, and pulls stuff from it.  Except in a procedural/OO language, that lexical context is mutable, and the mutations in the context  will  be reflected in the lambda.  In the expression  [(lambda n: i + n) for i in range(10)] ,  i  is part of the lambda's context. Not a specific value, just  i . And when what  i  refers to is changed by the iteration, the lambda reflects that change. If anything  can be argued  to be broken here it's the list comprehension, because it doesn't generate a complete brand new context on each iteration (now that doesn't mean it's broken, I don't think it is, but that's a matter of personal take/taste).  Haskell is a functional single-binding language, it therefore doesn't expose that ""feature.  But that's a  very  old feature of OO languages with closure: Smalltalk leverages it to make e.g.  while  statements unnecessary (note: in smalltalk, an expression enclosed by brackets is a  block , equivalent to a lambda,  [ 3 ]  is a block that simply returns the value  3 . Smalltalk's blocks have -- among other things -- a  whileTrue  method which takes a block parameter for the execution body). So counting from 0 to 9 in Smalltalk using a  while  looks like this:  i := 0. ""smalltalk uses := for assignment; also double quotes for comments and single for strings""[ i &lt; 10 ] whileTrue: [    Transcript show: i printString; cr.    i := i + 1.].  And this works, because  i 's modification is visible from the  [ i &lt; 10 ]  block.  TL;DR, lambda is not broken, its behavior is perfectly coherent with the language and its model, and with other languages in the same class. "
the article is wrong on every criticism regarding privacy settings.,"I agree with you on his 'Plus, it's a fucking business' comment. But the rest of what he said is true. The article  was  poorly researched, and you  can  disable all privacy killing `features'. In fact most options are set to 'only friends' by default. You can change the rest. You can even block specific people from seeing your posts like your Crazy Boss. You can also create groups of friends (I think they're called something else) and decide what each group gets to see. You can make it impossible for someone who is not a friend to know that you even  have  a profile.  TLDR; the article is wrong on every criticism regarding privacy settings. "
I play loose and hard with academic rules like that when following them can be ambiguous and potentially make me make the wrong decision.,"Strictly speaking, yeah they're the same and this was an academic setting, I'd never have said what I did.  However, when I'm using polynomial time as a factor in deciding one of many algorithms to use in actual production code, I tend to leave the coefficients in place.  Example:  You've got the the choice between two algorithms - one runs in O(n) and one runs in O(n^2 ).  The obvious choice is usually the one that runs in O(n), but if I had left the coefficients on and it turned out that O(n) was really O(10000n) and you know you're never going to have 10,000 elements - the choice isn't quite so simple any more.  Another Example:  You need to speed up a web application you developed, as it's taking way too long to render pages to clients and they're complaining.  So, you've profiled your code and noticed that the method foo() is eating 80% of your program's running time.  You analyze it and come to the conclusion that it's running in O(logn) and give up, the only other solution you can create running in O(n).  You leave the coefficients on, however, and find that your original solution runs in O(100logn) and your alternate solution runs in O(2n).  Now, the choice is a lot clearer that your alternate solution might be faster, if your set size is small enough - so you do some more profiling and find that it is, so you switch algorithms.  tl;dr: I play loose and hard with academic rules like that when following them can be ambiguous and potentially make me make the wrong decision. "
"Learn both to give yourself versatility. If you find a really good UK based shared host that does Python on the cheap, let me know.","If you have a degree in Computer Science, you should be able to work in both.  If you can't, you shouldn't really have that degree.  That aside, hosting is more readily and cheaply available with PHP, but nothing's stopping you buying a virtual server or a dedicated and putting what ever you like on it. For a quick start, stick with what ever the host allows, then when you have the capital to do so, move your clients onto a VPS and build any new sites in whatever you like.  VPS's are far more expensive than shared hosting, so if you do go down this route, you need to be able to support a large chunk of money leaving your bank account monthly. Dedicated more-so.  TL;DR. Learn both to give yourself versatility. If you find a really good UK based shared host that does Python on the cheap, let me know. "
Don't hate on Obj-C until you give it a shot in its natural habitat.,"I got my first real job since graduating last year this April doing in-house app development for a fleet tracking company.  Prior to that I had gotten one app on the app store, a game written in OpenGL, so mostly C++ with bits of Obj-C sprinkled here and there for hooks into the system.  Now having almost 4 solid months of Obj-C development under my belt (albeit ONLY for the iOS devices) I will say I'm a believer.  I haven't even meddled in iOS 5.0 yet (the beta is out right?  Heck, I'm still using XCode 3.2.6, imo if it works don't fix it) and memory management is still a cinch.  Once you do what developers are forced to repeat on stackoverflow daily in regards to exc bad accesses and sigkills, which is actually reading and absorbing Apple's documentation, specifically on MM, the development process of iOS Device + XCode + Obj-C is actually quite smooth.  Once you start to understand how the NSObject system is setup and the format of most of the messages passed to objects you go from having to look things up in the documentation to being able 'guess' the right message to send the object, and the auto complete of XCode basically states 'finishing your sentences' in a coding sense.  Honestly, the worst part was learning IB and how it hooked into everything, but once it ""clicked"" I saw the light, and regardless, that isn't really anything that can be held against Obj-C.  tl;dr Don't hate on Obj-C until you give it a shot in its natural habitat. "
function  fuck  may not be called until after 31-35; so move lines 31-35 into  fuck  or a create a new function and have  fuck  call it.,"A few more things (on top of my other comment)...   Disclaimer:  I'm not primarily a Javascript person / nor do I make a lot of Chrome Extensions.    If this is a small project and you can put it on bitbucket or github, it'd make sharing the code with us (for instance a manifest.js) a lot easier, and reduce the barrier of entry for testing/debugging.   As far as the problem with your code:   chrome.windows.create  has a  createNewWindow  callback... a few lines later we try to use the return value of this to set the value of  newTab.windowId . The problem is that  createNewWindow  or as you call it  fuck  is being called asyncronously and might actually return a value AFTER the line of code below.  The value we're assigning to  newTab.windowID  doesn't exist. If you break apart that code into it's own function (or place it in your callback) you should be good.  TLDR : function  fuck  may not be called until after 31-35; so move lines 31-35 into  fuck  or a create a new function and have  fuck  call it.  "
"If you don't use some sort of version or source control system, even as just a backup, don't expect that company to last.","I've worked at a shop where source control got  de-introduced . After complaining to his boss that working with SVN ""took too long,"" my supervisor had that server taken down and replaced with the sneakernet - we had to bring a USB stick to him with changes so that he could run the code on his personal machine.  Went from being able to fix a bug every day or so to weeks between questions and replies about whether or not my code worked with everyone else's.  Left after six months of that, maybe two weeks ahead of the collapse of the entire project.  TL;DR If you don't use some sort of version or source control system, even as just a backup, don't expect that company to last. "
"given the OP's question and code example, I deduced what the OP was really after and concluded that it was not parallel computing.","granted, but I didn't think the OP was asking about parallel computing despite misuse of the word parallel and the phrase ""do both simultaneously"". Usually in the case of polymorphic message passing (like the strategy pattern above) however, there is no check for the other values - either the desired value exists in the ""vtable"" or it doesn't. granted this is not the same as doing both checks simultaneously. Also granted, the vtable could be implemented as a simple linked list or array of { name, addr } structures, in which case the JS engine is still checking for all other values up to the one in question, but given the proclivity for JS authors to use an object as an associative array it is also reasonable to assume that member lookup is based on some sort of keyed or hashing mechanism, in which case the JS engine does not have to check for all other values, it just has to see if the desired value exists or not.  tl;dr  given the OP's question and code example, I deduced what the OP was really after and concluded that it was not parallel computing. "
My git repos contain everything related to development. Not only files needed for distribution.,"To be perfectly fair, I would need to test one of these plugins. I found the one for git. I'll give it a spin.  I tend to keep some documents in my source which does not necessarily make sense to everybody. Very often, I do scribbles on paper which I scan in. And because they are related to the project, I put them into git as well. But I don't see those in a  sdist . To me there's a difference between a source distribution and access to revision control. A  sdist  is a ""distribution"" afterall, and, in my opinion should only include ""stuff"" to make your application run. No need to clutter it up with other development related stuff.  As another example: In one project (a web-app) I added the CrystalSVG icon-set to the repository. This blows up the size of everything to somewhat around 18MB. If I want to deploy it with everything, the upload process takes  ages  (in my country we have really crappy upload speeds). If I only add the icons I really use into the manifest, then the distribution file is much smaller.  Granted, the last one is not a ""perfect"" example. I could also only add the icons I actively use into git instead of all of them. But having them all in git provides everyone on the team a ""icon-repository"" to work with.  tl,dr: My git repos contain everything related to development. Not only files needed for distribution. "
"Put the pitchfork away. It's not everything we wanted, but it's still a big step in the right direction.","They open sourced all the parts they're saying they open sourced, but many laypeople are assuming that it will allow developers to do things that it won't actually allow them to do.  There is a lot of contention over whether or not the microcode, the firmware that gets loaded directly on the graphics chip itself, needs to be open source before the hardware can be called free and open. In the case of the RPi's GPU, the microcode is responsible for a wide variety of different and important things. It doesn't simply initialize hardware--the driver has to go through the microcode to perform any hardware accelerated operation.  This is what limadriver and freedreno want to do away with when they say they are developing graphics drivers without binary blobs.  tl;dr - Put the pitchfork away. It's not everything we wanted, but it's still a big step in the right direction. "
"We made this decision because we will have to hit the database anyways, and there are solid security vulnerabilities we will be covering for very minimal development time.","Just an update:  When this problem was first being solved, we came up with a solution that provided us one-use resets without having to hit the database (involving hashing their current password hash, etc.  It's not relevant going forward).  However, I have determined this is a really bad design for a number of reasons, most notably the ability to brute force a token at that point.  So, we will definitely have to hit the database to ensure the user is in that ""flow"" for the password reset.  I still feel that it is ""secure enough"" to simply make the token expire upon use.  However, because of read-only database intrusions and the ability to brute force tokens I have made the decision to expire them (even though I do believe the use case of a user requesting a reset and then not following through is fractionally small).  I want to be clear that this has nothing to do with email security.  If a user's email is compromised, there is a lot more damage the attacker can do, including just requesting another reset.  TL;DR  We made this decision because we will have to hit the database anyways, and there are solid security vulnerabilities we will be covering for very minimal development time. "
"jQuery makes working with the DOM better. 
 Require.js helps you keep your code more organized. 
 Node.js runs outside of the browser.","Welcome to the wild world of JavaScript! Just when everything starts to make sense, you find there's a whole other section of the world you didn't know existed ;)  Thus far, you've likely been using JavaScript in the context it was originally designed for: client-side use. jQuery in particular is just a nice framework written in JavaScript with the (primary) goal of making it easier to deal with the DOM and avoid cross-browser issues.  Now your projects are getting a little larger and you want to separate out your files a bit more. But who wants to keep a manual list of dependencies and keep updating those script tags to make sure everything is in the right order? That's where Require.js comes in.  Require.js  allows you to separate your code in to separate, discrete ""modules."" In a simple employee application, this might mean that you have jQuery, Employee, ServerAPI ""modules,"" allowing you to separate out your concerns across multiple files and only load whatever modules you need when you need them. And you can say ""adios"" to having to manually keep track of what order your JavaScript files are loaded in.  Require.js is designed around [AMD]( which is a specification for loading modules. There's another specification for loading in modules known as [CommonJS]( Node.js can [load in modules]( using this style.  Finally, Node.js is what happens when you say ""you know what? JavaScript in the browser is awesome but just how badass would it be if I could use JavaScript anywhere else?"" 99% of the time, you'll see Node.js being used as a server-side language, as opposed to PHP, Java, etc. I've heard it described as a ""C++ wrapper"" and I think that's pretty fair. It allows you to do damn near anything you want on a computer using only JavaScript (and if you can't do it, you can extend it with something written in another language!).  TL;DR   jQuery makes working with the DOM better.  Require.js helps you keep your code more organized.  Node.js runs outside of the browser.  "
"I can not blame him for hating the language, he is using it just like he would use Ruby. That is a migraine recipe if I ever heard one.","It would take a counter article to refute each point. But, in virtually every case he is using the language wrong, then complaining when it does not work. JavaScript is a fundamentally different language than most, and requires you to fundamentally change the way you do things at times. I will be the first to admit this is a pain. At first it required me to think things through almost as much as when I was new to programming. But, once you get the hang of it the language is not so bad.  I will randomly select a few of his points and refute them tho, just to give you an idea of where I am coming from:  > The mixup between objects and hashes is also a very bad idea, because it defies the premise that objects can have metadata on them  That premise does not apply to JavaScript. This will make you change how you do things a bit.  > A fairly standard procedure of styling an HTML element to a certain model object for example:  &lt;div class='&lt;%= model.class %&gt;' id='&lt;%= [model.class, model.id].join %&gt;' is impossible in JS because the only types offered are 'Object', 'function' and primitives.  That is indeed how JavaScript works. Instead of complaining about it, design your app in a way that does not need type recognition. If you are too lazy to fundamentally rework your architecture, you can attach meta info to each Prototype that identifies itself. Etc.  tl;dr  I can not blame him for hating the language, he is using it just like he would use Ruby. That is a migraine recipe if I ever heard one. "
"the types are different, and C++ has passing methods that java does not, but it is C++ pass-by-value with respect to common notation.","The most commonly preferred understanding of pointers in C++ is as a type, hence why in C++ int  x; is common (in C int  x; is common); the star next to the int implies that 'int*' in itself is a type.  When passing a pointer in C++ it is passed by value as the pointer is considered the type you are dealing with. That is the same way that objects are passed in Java, their handles representing a logical address.  Passing a handle or a pointer is passing a logical reference by value (This could be a description of what Java does). The ""passing method"" is by value even if it is a value that refers to another object which is being passed.  TLDR: the types are different, and C++ has passing methods that java does not, but it is C++ pass-by-value with respect to common notation. "
learn and become very proficient in ruby so you know what the hell is going on when someones copy-paste code doesn't work,"I made the same switch in January, but have been programming in Ruby on and off for the last 3 years. The best advice I can give you is to  ACTUALLY LEARN RUBY . Rails is just a framework and too many people can not seem to realise that Ruby and Ruby on Rails are two different things.  The ease of Rails has bred a strain of ""developers"" that don't know the underlying programming language and almost entirely rely upon screencasts while copy-pasting example code.   learn ruby  learn ruby's variable scope (@var is an instance varable, var is a local variable)  learn the difference between an array and a hash.  .map is your friend.  script/console and irb are incredibly helpful.  learn how active record works, how to use it to query your tables without relying on ""magic finders"" and how to use it on tables that don't have an auto-incrementing primary key called ""id""  keep learning ruby   TL;DR learn and become very proficient in ruby so you know what the hell is going on when someones copy-paste code doesn't work "
"My company had its India development team 'upgrade' an essential analytics tool we used, and what we got was a substantial downgrade.","My company's new manager of India operations decided last year that the data tracking and reporting tool for our paid search marketing campaigns needed to be updated with a new version written by our development team in India.  No one on the development team had any experience with paid search, nor was anyone consulted who had any experience with paid search. The testing period prior to release was two weeks. Once the tool was released, they disabled use of the old version of the tool since they reasoned it was a waste of resources to maintain.  We're now six months since launch and they've  almost  gotten back to the point of having reliable data. For the first two months we lost whole weeks worth of data at a time, now we just periodically lose a day or two per month.  All of the new bells and whistles that were promised have been indefinitely shelved, as the team is working on restoring functionality to all of the features which the previous version of the tool had.  The look, feel, and navigability of the new version are  dreadful . Black background with red text. Radial buttons a few pixels wide. Changing certain metrics in reports resets all of the other settings to default. All percentage values are off by two decimal places.  tl;dr - My company had its India development team 'upgrade' an essential analytics tool we used, and what we got was a substantial downgrade. "
"PHP language is bad, but not as bad as most people who use it.","I disagree. I agree with the origin of PHP is a overblown scripting language. But current version of PHP offers enough tools and language support to create a coherent, well-engineered, enterprise-quality solutions. It is undoubtedly poorly (if at all) designed as a language, but not so much that it doesn't stop you from putting together properly designed product.  The main problem I encounter most often with PHP (which is related to the problem you describe, in my opinion) is not the quality of language but the quality of developers. Because it grew out of a glorified templating/page-scripting language, there are a lot of developers who come into PHP from a front-end design or non-engineering backgrounds. Basically, everyone who's read ""How To Write a Visitor Counter"" tutorial calls themselves a PHP programmer on their resume. When placed in a large scale project, these ""programmers"" struggle.  If you got a team of good engineers and use them on a large-scale PHP based project, they would produce something that approximates a high quality product they would have produced in any other language. If you can keep them from killing themselves, that is.  TL;DR: PHP language is bad, but not as bad as most people who use it. "
if a language is not easier than Python (or your current language of choice) you'll need a pretty good reason to use it in anger.,"I just think that computing tools have to compete with the current state-of-the-art. C isn't easier than Python, but it does have a number of other advantages. Erlang might be difficult to learn for Python hackers  because it has a slightly different paradigm, but it is much better suited to many-way parallelism than Python so it has a payoff.  Maybe I'm contradicting myself, because I'm saying easiness is not the only quality. There is also the possibility that a language like Go might teach you a different approach to things, and that's valuable too, it's just not a compelling reason to learn Go as a language for your next major application. In general, I think ease of use is an extremely important quality.  tl;dr: if a language is not easier than Python (or your current language of choice) you'll need a pretty good reason to use it in anger. "
Understand before screaming we NEED unions to save us! Sky falling. shit.,">The evidence states that the defendants agreed not to poach employees from each other or give them offers if they voluntarily applied, and to notify the current employers of any employees trying to switch between them. They also agreed not to enter into bidding wars and to limit the potential for employees to negotiate for higher salaries.  I don't believe they are trying lower salaries. They are just saying that we won't pull people out of your company. Work with any recruiter/head hunter that has morels. They won't pull you out of a company they are recruiting for.  One could argue that a bidding wage war over a employee would artificially raise salaries. In the end, the employee would probably be paid more than he/she is wroth.  While I don't approve this behavior. I understand it.  tl;dr Understand before screaming we NEED unions to save us! Sky falling. shit. "
"As a multithreaded application developer, don't worry about this shit. Just use mutexes and condition variables and you'll be fine.","It's not all that bad!  For singlethreaded C programs, just start coding and don't worry about this crap.  For multithreading, use your native synchronization primitives (mutexes, condition variables, ... in Pthreads, Win32, ....) so threads can communicate and don't access the same memory at the same time and you're usually good. Which is pretty much the same story as Python, Ruby, Java, and C#.  Instruction reordering (compiler optimizations that don't affect singlethreaded programs, but can bite you multithreaded) can happen in any language. Luckily, your major synchronization primitives  include memory fences ; or using std::atomic with the looser memory_orders; or using the barrierless Win32 primitives (InterlockedXXXNoFence).  One way in which C/C++ multithreading is tougher than Java/C#/Go is that C and C++ don't guarantee a memory model (which says which reorderings and optimizations are legal). Java/C#/Go have a language-level memory model, but in C/C++ it's determined by the architecture. In C++, this is getting better with std::memory_order, but that only gives you guarantees around your std::atomics.  tl;dr: As a multithreaded application developer, don't worry about this shit. Just use mutexes and condition variables and you'll be fine. "
"CLI commands == HTTP requests 
 What do you think?","Yes! Yes! A thousand times yes! Symfony Console is awesome but sucks so much on this.  Well except your solution is a bit confusing because of the naming:    HTTP  Symfony  Your package  Suggestion      Route  -  Command  Route    Controller  Command  -  Command     I would suggest to keep the  routing  concept of HTTP, i.e. have a  CLI router  that takes a CLI  Input  object and returns a  Route  object.  Then you would have a  CLI application  component that would (just like HttpKernel) use the router to find the route, then use a  CommandResolver  to instantiate the command, then dispatch to the controller using a  Dispatcher .  You could of course use the router alone.  Symfony HTTP components:   Symfony\Router (they should have named that HttpRouter…)  Symfony\HttpKernel uses Symfony\Router   Suggested CLI components:   CliRouter  CliApplication uses CliRouter   This allows for a very decoupled component (the router) while still providing a high level solution (the cli application) friendly with dependency injection.  Edit: also, I wouldn't do a fork. I would do a new library so that  Input  and  Output  and other classes like that can be reused without being rewritten.  TL/DR: CLI commands == HTTP requests  What do you think? "
It's just style so it doesn't matter unless it starts getting in the way. Can't we all just be friends?,"That's a debatable point. For example, if I want to represent separate quantities that are related to each other (e.g. orientation in physical space) it would make more sense to format the declarations as such:  float x, y, z; // Our coordinates within 3-space. Meters  float yaw, pitch, roll; // Our orientation. Radians  However, This isn't at all relevant to the main point of my comment. The semantics of the C language are such that the declaration syntax is symmetrical with the dereference syntax.  Building off the comment about function pointer syntax, take for example the declaration:  int (*my_func)(void);  This is a case in which the mechanics of the language syntax must be respected. Juxtaposing this declaration with another declaration as such:  int* my_int;  Creates an inconsistent style of pointer declaration. The comma-separated list declaration case is only one example of the peculiarities of the pointer  declaration syntax. While, yes, the difference boils down to style, the purpose of my comment was to explain the thought process of those belonging to that camp was.  tl;dr It's just style so it doesn't matter unless it starts getting in the way. Can't we all just be friends? "
They did the right thing by not striking back.,"As much as I'm sure everyone would enjoy seeing that it's probably better they don't for at least two distinct reasons.  First, the Chinese government will no doubt deny any involvement and any counter-attack - even an information one (""Your government is lying to you about  __ !"") - would put Github and any countries they acted out of into a very bad position diplomatically and legally, not to mention any lawyers Github employs or retains would have a stroke.  Second, that sets them against the Chinese government in an offensive way (vice the defensive stance they've taken), and Github simply isn't secured, well-funded or staffed to handle nation-level hacking that targets them.  This is actually a major issue that is being mulled over by governments and militaries around the world.  Say just for argument's sake another country physically attacked one of Github's offices in California or wherever. Github wouldn't be responsible for saving themselves or lashing back at the country, the US government and military would step in. If someone robs a bank, the police get involved, etc...  But when you translate that into the Internet, for the most part that paradigm goes out the window. The vast majority of laws simply haven't caught up to the digital age and even fewer laws have been written to handle situations like that.  The end result is that when a foreign government attacks an organization, that organization is (mostly) on their own, especially early on. That's if you can even attribute the attack and get that attribution accepted on an international stage.  And Github is a technology company, not some well-funded bank that has lawyers and CSOs and CIOs hiding in every corner of their infrastructure. They wouldn't stand a chance.  TL;DR: They did the right thing by not striking back. "
"I went to a ""Java school"" and turned out just fine.","disclaimer   I was very self-motivated in college. My Java school experience may have been different because of my self-motivation. I won't discount that as a possibility.  Technically, I went to a ""Java school"" (I threw Java school in quotes because my experience was a bit difference than what I've seen many people complain about).  The core language that CS classes were taught in was Java. I won't try to deny it; however, I felt that I was not done a disservice because my school made a significant effort to expose us to other languages/programming paradigms. For example:   I spent 2 semesters writing Common LISP code for AI-based CS classes.  I wrote Prolog for a computational linguistics class.  I wrote a ton of C throughout college for various projects.  I wrote C++ for a bioinformatics class.  I wrote MIPS and x86 assembly in various projects.  We were required to be proficient in ML or another functional language.   The above were very influential on my skills as a software engineer. But I did have some excellent classes taught in Java; for example, here are some things I had to do in Java:   I had to write a Java compiler...  in Java .  I had to implement a distributed, multithreaded Jacobian relaxation algorithm in Java.  In fact, I had to do a ton of multithreaded (java.util.concurrent based) projects in Java.  I learned about dynamic programming in Java (no, not dynamically typed languages, [dynamic programming](  I learned about web services and web programming in Java.  I learned about database programming in Java.  I had build a multiplayer, socket-based game of Risk in Java.   So I'm not as quick to trash ""Java schools"" just because they fit the standard definition of Java school. However, I do think there are a fair number Java schools that do a poor job.  tl;dr  I went to a ""Java school"" and turned out just fine. "
Java's sweet spot is an extremely simple language anyone can understand.  Please don't make it something it's not.,"Java's intent - it's leading design principal - was to be a simple enough language that it didn't have a steep learning curve so that virtually any programmer could read a piece of java code and guess right what it did.  No don't get me wrong - I love cool computer sciencey stuff like lambdas and monads and closures and self-modifying code (my favorite was a FFT which worked by modifying a ""+"" operator to a ""-"" operator in the loop).  But those really don't have a place in Java.  Better to  use Java along with other languages (either ones running in the JVM, or through JNI) if you want even slightly esoteric features.  TL/DR:  Java's sweet spot is an extremely simple language anyone can understand.  Please don't make it something it's not. "
"Lots of work, productive language, I like the ethos, many bugs the fuckers won't ever fix.  I like XAML.","> Unless you're stupid enough to latch onto one of their emerging concepts but that's the same for all technology. I don't think anyone thought XAML was going to take off in the way microsoft had hoped (flash AND silverlight competitor).  XAML was intended as a UI markup language, to help seperate visual concerns from logical application ones (the View from the View Model in MVVM parlance).  That it does, and still does.  WPF/E or silverlight was a subset of functionality that was meant to 'kill flash' and honestly, if MS had just open sourced it properly (ie helped moonlight get up to speed) I would choose it over any HTML thing anytime, hands down.  XAML is a really nice language, I'm super pissed that my current project has to use HTML not because it's a better technology, but because of needs of our clients.  No word of a lie I would be about five times more productive if this was a WPF application than doing everything in Angular or KO, desperately trying to make JavaScript serviceable with TypeScript or Dart.  However if my current venture fails, I can fall back on a rather fine day rate doing more Line Of Business WPF, heck I've one standing offer still on the table at a good rate.  TLDR:  Lots of work, productive language, I like the ethos, many bugs the fuckers won't ever fix.  I like XAML. "
"The "" Three-S's "" (State, Security, and Sanity) cannot be bolted on and must be considered in the design itself.","> The advantages are just too compelling.  There aren't any advantages tho ""bolting on"" state information -- that is "" trying to force applications through technology designed for static pages "" is simply inadequate.  There aren't any advantages to treating a  styling  solution as a  layout  solution -- which is what a great deal of CSS-usage amounts to. (The difference between the two is illustrated by how quickly CSS becomes awkward when dealing with such a simple concept as centering with dynamically sizing components... in something geared for layout like Delphi or C#'s form-builder this is trivial.)  The advantages [of web-ish apps] don't exist until you have a well-defined environment with well-defined behavior which has been designed with security in mind (rather than being bolted-on) and presents a nice/uniform interface.   TL;DR -- The "" Three-S's "" (State, Security, and Sanity) cannot be bolted on and must be considered in the design itself. "
C++ has its faults but the lack of garbage collection isn't one of them.,">The problem is that they all have some corner case  You're talking about [leaky abstractions](  As usual with C++ the devil is in the detail. Which admittedly is a problem with C++ as you mention later in your comment.  RAII is an indispensable idiom which indeed ""involves simple concepts to make complicated things"". When I have to revert to C or use another higher level language it's the C++ feature I miss the most.  The STL containers are useful on a lot of levels but they are in my opinion leaky abstractions: you need to know that if you extend a std::vector it will invalidate any iterators pointing into it.  >I'm pretty sure you never have any problems  I very rarely have any problems at all with my own C++ code. This isn't because I'm a  C++ guru by any means: it's because I tend to stay within my own safe subset of C++. I sometimes have problems venturing into the safe subsets of other developers.  >an invisible long learning curve that the programming language doesn't give you a bit of help to climb.  The learning curve need not be long: it's possible to learn a safe subset of C++ very quickly.  But long or short, the programming language doesn't help you climb the learning curve, and more importantly doesn't help you stay within your selected safe subset by enforcing parts of it. To my mind one of the more significant problems with C++ is that everyone is using a different safe subset of it.  tl;dr:  C++ has its faults but the lack of garbage collection isn't one of them. "
"apparently, restarts look good in theory, not so good in practice.","There's an interesting chapter in Stroustrup's ""Design and Evolution of C++"" about exception handling in C++. When he decided to add exceptions to C++ in the 1980's, and discussed his ideas with other language designers, the most contentious issue was whether C++ should only support unwinding semantics (as it does now) or whether it should actually support restarting semantics, like lisp and others do. Stroustrup originally preferred restarts, since they were more general and he was aware of some cases where they might be useful, but over the years others convinced him that unwinding is cleaner, more manageable and powerful enough, so he changed his opinion.  Eventually, the issue was left to be decided by the newly created ANSI C++ committe, which spent a couple of years debating it. As it seemed that plausible arguments can be made both for and against restarts, and both approaches had their staunch supporters, Stroustrup decided to invite people that had actual experience with the design and maintenance of large software systems to get some real-world input (what an idea!). It turned out that designers from DEC, Xerox PARC, Sun and IBM sided overwhelmingly with unwinding rather than restarts, some of them admitting that they had originally liked restarts and had used them in their systems, but over the years they concluded that restarts were more trouble than they were worth. On the other hand, Martin O'Riordan from Microsoft reported that Microsoft had good experience with restarts. But, as Stroustrup diplomatically puts it, ""absence of actual examples and doubts about the value of OS/2 Release 1 as a proof of technical soundness weakened his case."" In the end, the extensions working group of the committee voted 22 to 1 in favor of unwinding, and the full committee approved it with a vote of 30 to 4.  tl;dr: apparently, restarts look good in theory, not so good in practice. "
"it's a failure on the part of browsers and standards that forms served in clear text can submit to  addresses. 
 Also, a single salt for all users? Really now?","A substantial part of this article is ""how to have a non- site that doesn't refresh the page when submitting login credentials over  using ugly hacks""  That's an absurd and dangerous problem to solve. Login  forms  should always be served over  in addition to being  submitted . A malicious network can easily intercept and rewrite your plain-text login form in flight to point to submit via  in which case your app either doesn't notice or complains. In either case, they have the plain text password. It's not even easy to check such a form in your browsers' dev tools, because logic that hooks form submit may be quite elaborate or hidden.  tl;dr  it's a failure on the part of browsers and standards that forms served in clear text can submit to  addresses.  Also, a single salt for all users? Really now? "
"Letting apps crash is good for debugging, and keeping external state allows restarted applications to continue from where they left off.","In our code base, developed and in production over the last 14 years (running on Solaris), we don't bother to handle seg faults at all.  Desirable behavior for us, when encountering a seg fault, is for the process to terminate and drop a core file.  As others have pointed out, a handler might not end up running the way you want it due to the nature of the failure.  But more importantly, I don't want a handler stomping all over my pristine core image that, 99 times out of 100, will tell me exactly what went wrong and how I got there.  Core files (along with any logs our apps write out) give me all the evidence I need to root around & figure out what went wrong.  The other half of this coin is how we recover from the failure:  We make our apps as stateless (with respect to what's going on in memory) as possible.  An app that crashes & restarts will be able to continue to do what it was doing at the time of the crash, due to external state, either in the db or in local files containing checkpoint information.  This is important, as our system is a 24/7 system with extremely low tolerance for downtime.  Being able to handle a seg fault is not the only failure we need to cleanly recover from - having the hardware just crap out is another - and all the beautiful, in-memory recovery code in the world won't help you when you fail over to the standby box.  tl;dr: Letting apps crash is good for debugging, and keeping external state allows restarted applications to continue from where they left off. "
"may or may not have problems. But dont assume it works just like a 32-bit int, and test :-)","Its not necessarily a problem, but you might want to test it...  Naturally you need to be on 64-bit php.If you're using the mysql_xxx api, the mysql_insert_id() function needs testing. mysql_xxx is deprecated anyway, but doesn't mean there isnt legacy code still using it.  ""SELECT mysqli_insert_id()"" is a bit safer, but if the value exceeds MAXINT, it will return the value as a string, although I do not know what happens if you call ""SELECT LAST_INSERT_ID()"" instead, nor do I know how PDO would react to it.  Finally, PHP doesn't support unsigned integers, so if you have a unsigned BIGINT identify field, then fetch a last insert id which exceeds signed BIGINT, you may have problems. mysqli seems to return it as a string, but since this wont be obvious unless you're aware of it, most likely you'll try to cast it as an integer at some point, and this would blow up. Type casting to a native PHP float, on the other hand, would fare much better.  tl;dr - may or may not have problems. But dont assume it works just like a 32-bit int, and test :-) "
"Everybody's trying to do something ""magical"" and ""impressive."" What about ""simple"" and ""repeatable""?","Can't say I'm too impressed by Aurelia at this time. As mentioned by another commenter, two way data binding is an antipattern.  Not a fan of tightly coupling your HTML and JS using tag attributes, either. That's a pattern Angular introduced that is just plain backwards -  onclick=""doThing()""  was an old janky way to bind JS to HTML that's been deprecated in favour of the programmatic  el.addEventListener('click', doThing) .  A whole bunch of smart developers writing web specifications chose to move in this direction to avoid tight coupling and keep concerns separate. It's disheartening to see so many ambitious, talented developers not heeding these decisions and trying to couple their HTML and JavaScript as much as possible.  I get it. We need templates. You will, almost unavoidably, need to have logic in your templates no matter how separated you want to keep your concerns. The wonderful thing is that if you throw out two way data binding as a concept and require  .render()  calls to update your view, you can use engines like  doT.js .  I wish more developers would focus on the architecture and modularity of  all  layers of their application. I still don't think we have a front-end application architecture that's anywhere near what say, Rails or Django provide for the back-end --- and that's troubling to me. The success of these two projects is because they're clear, concise and follow strict, object-oriented conventions  of their respective languages  and  keep their concerns separate .  Can I use the words ""concern"" and ""separate"" any more times in a single comment? ;)  tl;dr : Everybody's trying to do something ""magical"" and ""impressive."" What about ""simple"" and ""repeatable""? "
"The army is terrible because it's hard and difficult and painful and dangerous. Oops, I mean C++.","Just because a saw doesn't have retractable teeth doesn't mean it's badly designed. It just means you have to be careful using it, and it's cheap to make.  I get tired of this argument over and over again, and I'm going to say it for the last time, so downvote away:  C++ is for people who need the program to be as fast as possible, while being complex enough to reasonably quickly write powerful constructs. While C is no doubt simpler, C++ adds power and ease of use only at the cost of complexity, not at the cost of speed (usually).  If, like me, you write games for a living, there really is little choice of language. Sure, you can write successful games in Delphi, Python, C# or even Lisp, but I refuse to believe the majority is written in C++ because of inertia or some sort of conspiracy.  Yes, so most people use a subset of C++. The thing is, every person uses a different subset, and I for one don't see anything wrong with that. If you feel the need to use exceptions, go ahead, use them, the sky won't fall down on your head. If you want to use goto and partial template specialization in the same file, be my guest, why the hell should I care what you do to get the job done?  Is C++ a terrible language? Maybe. I personally find Ruby to be a terrible language. That's why I don't write in Ruby. But I never ranted about it on the internet. It's not like everyone is forcing everybody else to use C++. If they are, then I'm sorry, my entire argument is wrong. Although I have to wonder why anyone would force you to use C++...  tldr: The army is terrible because it's hard and difficult and painful and dangerous. Oops, I mean C++. "
"I agree with you, but the author of the post wouldn't.","I think the point of the article is that they're not trying to start with  any  of those things. They want a way to start with something as close to machine code as possible, that's still human readable.  Whether that in itself is a good thing is debatable, but assuming that's your goal, Fortran might make a better choice than BASIC. You are right that it is specialty, but people only write BASIC to support programs that are already written in BASIC. People do write new programs in Fortran.  I would tend to agree with you, that as a beginner Python is probably the way to go. I find it helps when you're learning something to be able to see results quickly. A beginner could write something that actually works in Python without too much time. Then as they advance they might want to start writing C libraries, which is about as low level as anyone is likely to want to get.  tl;dr: I agree with you, but the author of the post wouldn't. "
Don't teach your kids BASIC or I will fucking kill you.,"I grew up in the 80s and was one of the kids this guy is describing. I had an Apple II and later several desktop clones that I lugged home from school garbage bins (along with whatever else I found, including a pretty cool gymnastics mat that I used to teach my younger brothers made-up karate moves on).  Yeah, I learned BASIC, and it led to a lifetime of programming that I wouldn't trade for anything. But you know why I learned BASIC? Because there was fuck all else to do on computers back in those days. You borrowed a BASIC book from somebody and made your own games. Maybe you spent 30 minutes trying to figure out how the hell to bold some text in WordPerfect and gave up. That was basically it for home computing unless you had wealthy parents.  Hell, if I had the Internet back then like kids do today, I'd probably be browsing the 80s equivalent of NeoPets or YouPorn all day (and since it's the 80s I'd be wondering the whole time which one's the man and which one's the woman), instead of making 400 games where Spider-Man has to escape the Wumpus that I pretty much have memorized because I can't find the ""Save"" command in the BASICA shell.  TL;DR: Don't teach your kids BASIC or I will fucking kill you. "
They have nothing to lose from their agreement (it protects the parts they need).,"Google is just concerned about their access to consumers. From a wireline perspective it is very unlikely their services will blocked or slowed (they own a lot of fiber, so they could make it painful for the ISPs if it came to that). For wireless, Google controls the OS with a rapidly increasing marketshare (now #3 internationally). No one will mess with them now and keeping the ability to block/slow certain traffic down helps them hinder others still trying to get a foothold in the market (i.e. MS). Why would they want to limit their or their partner's options when they're soon to be the market leader.  tl;dr: They have nothing to lose from their agreement (it protects the parts they need). "
there is a difference between javascript and the environment in which it runs.,"i code javascript in non-browser environments all the time, and it is pretty easy to tell what is javascript and what is DOM, XmlHttpRequest, and other constructs that are not part of javascript. just because someone came up with the buzzword 'AJAX' does not mean that this is a proper designation for what the technology actually is. XmlHttpRequest can also be scripted in vbscript in IE (XHR wa invented by microsoft), so in that case would you want to call it AVbAX? probably not, because it sounds dumb. XHR can also be used synchronously, but we don't call it SJAX when we use it that way.  the asynchronous aspect of XHR isn't even part of javascript, it's part of XHR. I use the .NET framework with javascript all the time, and it is very powerful, but I know where javascript ends and .NET begins.  If you want to discuss javascript, please don't confuse it with DOM or XHR or any other browser feature that isn't actually javascript.  This is exactly why javascript gets a bad name, because people confuse these things with javascript.  If browsers supported Ruby, you would still need to interface with DOM, XHR and everything else, and people would then hate ruby because they can't figure out how to call innerText on an iframe element or whatever stupid thing isn't related to the programming language.  tl;dr: there is a difference between javascript and the environment in which it runs. "
"while this script might be fine for testing this is not set up for production usage.]) 
 the title was just a random placeholder thought.  :)","that php script creates an html output (it creates the input boxes and all) but if you wanted to parse the data from another page, so long as the inputs are named intput[0] and input[1] etc, it should still work (because it will still see the $_POST is there)  (and on a side note this is a good time to mention cross site scripting vulnerabilities.  If I had that script up on a server and knew it's layout then I could post to it from any website on the internet because we didn't put any logic in to test for that. [tl;dr: while this script might be fine for testing this is not set up for production usage.])  the title was just a random placeholder thought.  :) "
"Stop adding too much widgets and if you do, use something like this or a mouse over loader for them.","As an architect for high availability platforms, people do not realize how much social widgets slow user experience down.  They also do not realize that availability of your web site will gradually go down every time you add another social widget on it.  Added Facebook? Okay, now if their API has a hiccup your website will suffer from it. Now you wanted to add Twitter? Okay now if Facebook OR Twitter stalls you will have issues and so on.  What kills me is people adding like/tweet/+1 to every content piece, and if you list it on a single page you will have to load 3 widgets per piece* 10 content pieces. = 30 requests.  So before you slap another widget, think about the consequences or use a tool like this that prevents loading of the sharing widget on every page unless you want to actually use it.  You can also use the same concept as roll over instead of double click.  TLDR: Stop adding too much widgets and if you do, use something like this or a mouse over loader for them. "
This is a very sensible and common API design pattern.,"Actually, using an interface in situations like that is pretty common and actually what I'd consider good practice. I tend to view interfaces as contracts: a way of specifying needed behavior(s). Using an abstract class (rather than an interface) forces anyone that wants to provide/implement that behavior to conform to your class hierarchy. For example, what if I want a GUI control for entering regular expressions to be a  Matcher  implementation?  public final class MyRegexField extends JTextField implements Matcher { ... }  However, there's a competing concern here: what if the library's designers need to change the  Matcher  interface/contract? Let's say they add a new  public String getDescription() method that allows  Matcher  instances to describe themselves. When they push out the release with that change, all of a sudden everyone with their own  Matcher  implementation has a compile error (unimplemented interface method). Unless, that is they're extending from  BaseMatcher , which now looks like this:  public abstract class BaseMatcher implements Matcher {  /* all the previous stuff that was there before the API change ... */  public String getDescription() { return ""some reasonable default""; }}  So, if you're implementing a  Matcher  in your own code, you now have two choices: (a) implement the interface directly in whatever class hierarchy you'd like and accept that you'll have to fix compile errors every so often when the library's API changes or (b) extend the base class, losing the ability to mesh with some pre-existing class hierarchy but gaining some forwards-compatibility. These tradeoffs will be necessary unless/until  ""defender methods""  are added to the Java language.  tl;dr  This is a very sensible and common API design pattern. "
imo Zed Shaw is one of the leading cast members of the programming world's equivalent to the Jersey Shore.,"Well he's intentionally misinterpreting the intention of K&R.  They wrote the reference in an effort to capture the essence of programming in C (and programming in general).  Everything he listed in that post is not a ""bug"" or an ""error"" but was intentionally left out in regards to the audience.  From what I remember they state multiple times that the code in the book is not production ready and that more error checking/fault finding/stringent programming would be done IRL but was left out in order that the main concept could be understood.  Zed attacking these small points while ignoring those warnings is an ""ass"" thing to do.  He's basically making an intentional misrepresentation to gain publicity and it worked pretty well.  Kinda like how he hyped his ""learnhowtocodethehardway"" (ie I like python better now), by attacking the Ruby (and specifically Rails) community.  I'm not saying he's not smart, but he has a historical pattern of saying ridiculous shit so that people pay attention to him and this is what makes him an ass.  tl;dr imo Zed Shaw is one of the leading cast members of the programming world's equivalent to the Jersey Shore. "
some compilers go out of their way to mess up benchmarks and as a side effect end up messing with other benchmarks.,"> Lets say that architecture X has a deeper pipeline than architecture Y. All of the sudden Compiler A may print out code that is optimal for architecture X but sub optimal for architecture Y.  The Intel Compiler completely avoids this. A runtime check ensures that any non Intel CPU gets non optimized code by default, guaranteeing a consistently bad experience. Officially this was passed of as a feature of AMD hardware until they took Intel to court.  Of course this feature depends on the x86 extension registry and can be bypassed by forcing the extensions. However Spending days to  configure the compiler would defeat the purpose of a compiler benchmark.  TLDR; some compilers go out of their way to mess up benchmarks and as a side effect end up messing with other benchmarks. "
"They aren't the same and you shouldn't treat them the same, even if they look the same.","Because they aren't exactly the same as the Latin characters. They only appear the same, and may share the same glyph in a bunch of fonts, but the characters have different meaning and cannot therefore be treated as equal.  Two great examples are C and С, or B and В. They probably look identical to you, but what looks like C in Cyrillic is really our S, just as the Cyrillic character that resembles our B is actually a V. Then there's Н (our N), Х (would be HA to us), and Е (IE here). And even Cyrillic letters that share pronunciation with visibly similar Latin letters have their own histories, orderings, and uses that can conflict between alphabets or even individual languages.  TL;DR: They aren't the same and you shouldn't treat them the same, even if they look the same. "
please don't dismiss something just because it's Windows (or not Linux).  Windows got a few things right from the start; async I/O and threading is one of them.,"That's unfortunate.  I spend a good twenty minutes describing how IOCP works and why it's such a good design for doing high-performance, multi-threaded I/O.  The key is Windows' support for the notion of thread-agnostic I/O, of which there is no counterpart on contemporary UNIX kernels (except for AIX's IOCP and Solaris' event ports).  It's a tough battle, but I'm trying to persuade people with Linux/BSD blinders on that there are alternate ways of doing things; if Linux/BSD kernels had thread-agnostic I/O support, everything I've implemented on Windows for PyParallel would be trivial to implement on Linux/BSD.  Unfortunately, there is no equivalent -- so I'd have to mimic it using readiness-oriented, synchronous, non-blocking I/O.  I'd rather add kernel support for thread-agnostic I/O than do that.  TL;DR please don't dismiss something just because it's Windows (or not Linux).  Windows got a few things right from the start; async I/O and threading is one of them. "
This is a custom iPhone app that runs an HTTP server that integrates with a custom PHP module. The code is  on Github .,"For people wondering how this works, allow me to explain it. This is all implemented through an Objective-C iPhone app that starts a web server built upon  CocoaHTTPServer .  To get PHP to work, I had to compile it as a static library from source to support the ARM architecture. That took a while to get working, as it was a lot of disabling and enabling of different extensions based off of what libraries the iOS SDK has, as well as just figuring out how to compile PHP with the compiler options used for iOS development. From there, you compile multiple versions of PHP to support the different ARM architectures (armv7, armv7s, arm64), as well as supporting the simulator that runs on OS X (i386). Fortunately, you can merge all those libraries into one file so you don't have to worry about managing it yourself.  So after that, it's all about writing the code that integrates PHP with CocoaHTTPServer. CocoaHTTPServer uses a connection class for generating a response to a request. We create a subclass that does that file extension checking, and invokes the PHP module if the request is asking for a PHP file. We also need to create a class the implements the protocol that describes am HTTP response (a protocol is basically the Objective-C equivalent of an interface). We need to do this as we need a response class that has a dynamic sized buffer we can write bytes to.  So, we start the server, which uses our custom connection subclass for any connections made to the server. The connection creates an instance of our response class for PHP requests. The PHP module hooks into our connection class to read data from our request (HTTP method, headers, POST data, etc), as well as write data to our response instance (the actual output from PHP, any headers set, etc). After PHP finishes executing, we send the response to the client.  TLDR: This is a custom iPhone app that runs an HTTP server that integrates with a custom PHP module. The code is  on Github . "
if you're using rails: Don't embed any application code into the framework. Delegate to your application and render the response which comes back from your application.,"It's kinda hard answering such a generic question. It boils down to keeping most of your system as free from dependencies and side-effects as possible and having one layer around all that code which is allowed to talk to things like databases or networks. The outer boundary might look like this:  MyTodoApp.new(SQLDatabaseGateway.new, SendGridMailer.new).create_todo(todo_request_object)  In your tests you should be able to test most of the units without caring about their collaborators, they're mostly just implementation details. And if you want to test the outer layer, you either replace the dependencies with dummy objects (like a mailer which writes to memory) or just test it with the real thing.  Gary Bernhardt calls this ""Imperative Shell, Functional Core"". ""Hexagonal Architecture"" is another thing which helps achieving this.  tl;dr if you're using rails: Don't embed any application code into the framework. Delegate to your application and render the response which comes back from your application. "
"Learning C is worth it for the knowledge, but do something with it.","C.  If you've never programmed in a compiled language but have stayed in scripting languages, it is worth knowing a bit about how they work.  But don't go out learning new programming languages just to have some line on a CV.  First, employers/grad schools are not impressed by that.  Second, someone is going to ask you about projects you have completed.  If all you can talk about are textbook problems that might or might not even be appropriate for the language you wrote them in, you ought to show yourself out the door.  On the other hand, if you can show me a project you've completed, and show me a bit of C code you wrote to speed up some nasty bit of analysis that was too slow in Python, that's awesome.  tl;dr Learning C is worth it for the knowledge, but do something with it. "
"We would need to do something superscalar over multiple cores"" isn't a stupid idea, although it is expressed like a layperson :)","It's not as stupid as you think. My research is based on microthreading (µthreading); splitting an algorithm into multiple independent parts at a slightly higher level than superscalar processors currently do.  Programmers currently think of threads as heavyweight. You have a ""listener thread"", a ""user interface thread"", etc.  Superscalar processors parallelise at the instruction and µop level.  The proposed 'µthreads' are somewhere in between. You take semantic information from the source language (whatever that is, or you could reverse engineer semantic information from the generated ASM using dependency and control flow analysis or somesuch) and use that to split what would normally be sequential flow over multiple microthreads, using a ""fork/wait"" semantic.  Communication latencies are Not That Bad (tm), and if you've got a problem that needed parallelising in the first place (read: don't parallelise what won't benefit from it), the communication latency of getting the result back to the originating µthread is absolutely minimal.  TL;DR: ""We would need to do something superscalar over multiple cores"" isn't a stupid idea, although it is expressed like a layperson :) "
"Expect failure, plan for redundancy = low stressed sysadmin","I have been very stressed in my IT job over the past few years. I saw early that the demands placed on me were not doable in a normal work day. I am a SysAdmin that designed and now maintains a virtualized development environment. Being a solo sysadmin to deal with all the problems in a large environment is challenging. I did away with most of my stress by implementing failover clusters, redundant networks and a medium sized SAN. If a server goes down with VMs on it, the VMs just fail over to another server. I no longer get frantic calls from the devs in the middle of the night wondering what happened to their projects. I've went from around 150 physical machines a few years ago with 75-80% uptime to 500+ mixed physical and virtual machines with 99% uptime with just a little effort.  TLDR - Expect failure, plan for redundancy = low stressed sysadmin "
There are a lot of stupid things in T-SQL that will mess you up if you are used to mysql-sql and make sure to do some reading.,"Yes, that's really all you have to do. There are a few keywords you would expect to be there that just plum aren't. One of these that caught me was UNIQUE, which in T-SQL is DISTINCT (also in Postgres). Another one is instead of LIMIT ### you use TOP ###. Other than that you will run into problems as you go along and they will all require googling MSDN until you really get them down. If you are going to be writing scripts/functions I would look up the quirks of T-SQL stored procs (I never really cared for the syntax).  [The following may or may not be true, but it happened to me.] Also as a random quirk I had was that when you do a JOIN you aren't allowed to have column names that are the same among the two tables even if you use an AS query to alias them as something else and use other columns even if you delineate it out all the way with dbo.table.colname notation. I ended up making a VIEW in MSSQL that just picked up the cols I needed with different colnames and everything worked fine.  Another thing to look up is the backup database notation. It was really confusing for me for whatever reason, and if you don't administratively add additional paths to back up to, you can't do it. (i.e. if you have two drives C: and D: you have to add a path in MSSQL to D: to be able to backup there.) The command was BACKUP DATABASE TO DISK='c:\blah.bak' but there are a million other commands littering the page for that particular query and in general coming from mysql was awfully confusing the first few times.  tl;dr: There are a lot of stupid things in T-SQL that will mess you up if you are used to mysql-sql and make sure to do some reading. "
Sometimes you use libraries that aren't yet compatible with Python 3.,"Many times it's not a matter of your code, but of a dependency that you have. For example, you have a codebase that relies on Twisted. Twisted is a huge framework that is very popular. You have thousands of man hours invested in basing your product around this. You want to move to Python 3 and that's great, but there's just one problem. Twisted doesn't yet support Python 3.  It's not that the project is dead or that they are dragging their feet. It's that the project is huge and there are tons of [high profile users]( like LucasFilm and LaunchPad and NASA using it. You don't want to leave them stranded, so you have to plan for a gradual migration. Change over pieces at a time and add backwards-compatible APIs so that they can take their time to migrate their code.  The problem is one of momentum. It's much like the Perl 5/6 problem (though in their case, they don't have a GA release of Perl 6). Too much high quality code got into very mainstream use at a time when the language was a bit immature. The language developers want to grow the language and that means breaking backwards compatibility.  In the case of Java, they choose to go the very long and slow route. They add new functionality, make it secondary, encourage it's use over the deprecated method and eventually start issuing warnings when the deprecated API is used. But that takes a very long time. There are deprecated APIs in Java from version 1.3 that they are still trying to rid themselves of.  For Python, they decided that a clean break was necessary and to maintain two code bases going forward. They have a drop-dead date for the older code base and continue to add alluring features to the new version. Now they just need to give the community time to port all of this high quality, high profile code to the new version. They knew it would end up like this.  tl;dr - Sometimes you use libraries that aren't yet compatible with Python 3. "
"He did read the manual, gave a vague implication that he didn't, and was trolled. Kudos to him for remaining civil.","It seems there are a lot of people railing on Metaleks, but I really don't think he's in the wrong here.  He came into the IRC channel asking for help, he's clearly tried several things and none of them work.  Communications break down around this point:>[tpope]: metaleks: then why’d you come in babbling about sticking it in .vim/plugin?  But really, it should be obvious why he came in babbling about that. When you try a clearly documented solution, as he did, and it doesn't work, you assume that it's the incorrect solution to his problem. So, why would he mention a solution that should logically be ruled out? Instead, he mentions the only thing that remains in his mind after he's tested and ruled out all the solutions that are documented.  Unfortunately for Metaleks, one of the solutions that he ruled out was the correct solution, and the reason it didn't work was due to other factors.  More unfortunately for Metaleks, he encountered a typical elitist troll in IRC who made assumptions and was more interested in self-gratifying flaming than in being truly helpful.  tl;dr: He did read the manual, gave a vague implication that he didn't, and was trolled. Kudos to him for remaining civil. "
Why a software or hardware divide doesn't include code that does port writes and some explanation of ports in CPU and µcu circuits.,"Short answer: No.  A bit longer answer:  Ports are software-controlled hardware devices that let the CPU send or receive data to the outside world; they have an external element, eg: the CPU presents an address on a bus, data on another (or variations), turns on select signals to indicate to another (set of) chip(s) that the data is to be sent on a port.  That's how most of the general-purpose CPUs work.  For example, you can't connect a simple LED light to most CPUs directly, there are no electrical pins on them that could perform that function.  So instead you have an extra (set of) chip(s) that the CPU can control which on one side has circuitry that connects to the CPU bus(ses) and on the other side has electrical pins that can control hardware like LED lights.  On the other hand, micro-controllers like the AVR family mentioned in this thread are a bit different though; they have integrated ports (part of the actual CPU chip) so they can be connected to external circuitry like LED lights or push-button switches directly.  This makes them much simpler to use in small projects.  Micro-controllers are designed with quick port accesses in mind so you can have code that writes to a port millions of times per second and they don't even blink, nor warm up.  A divide instructions has no hardware part, it's just a computation that takes two values, divides one by the other and returns a third value (and possibly a fourth: the remainder.)  In the case that a CPU has a divide instruction, if it gets hooked in an infinite loop it does so in its own micro-code or tightly in its own circuitry.  The the outside world the CPU just appears to be not running anymore.  In the case that a CPU doesn't have a divide instruction, then the division is done in software (ROM, Flash, RAM, etc...) but there is no reason for a software divide to access ports, so no part of the divide routine contains code that can possibly access them, the infinite loop couldn't possibly do port writes.  To the outside world the the CPU still appears to be running; it's still accessing the software- albeit a very small part.  There are complications involving pipelines and cache that aren't necessary in this scope.  TL, DR: Why a software or hardware divide doesn't include code that does port writes and some explanation of ports in CPU and µcu circuits. "
"compilers are smarter than you think, and skilled coders can turn C into ASM in their brains.","Unfortunately for compiler writers everywhere, you're vastly oversimplifying things :)  Take, as an example, this code:  int dothings(int a, int b) {  int ap = b * 2;  int bp = a * 2;  a += ap;  b += bp;  return a*a + b*b;}  This is a simple example of superscalarable code. A smart compiler can easily recognize that the += lines can be dealt with including superscalar operators, a slightly smarter compiler may even recognize the a a/b b and the a 2/b 2-and-swap. All it has to do to permit that is make sure the ADDs are next to each other, and the CPU will take care of it from there.  int addArrays(int x[], int y[]) {  for (int i = 0; i &lt; 128; i++)    x[i] += y[i];}  Here's another superscalar example. Some compilers can recognize a simple operation in a loop, and transform that loop into superscalarable behavior.  You're right that code is linear, and it's guaranteed to  act  like it executes one instruction at a time. But the compiler can do whatever crazy magic behind the scenes it wants as long as the end effect is equivalent to the shared fiction we all have about code's behavior. In reality, compilers will go so far as to reorder instructions in a function, as long as they can guarantee the end effect is the same, grouping together additions and subtractions and multiplications so they can take the biggest advantage of superscalar cores.  tl;dr: compilers are smarter than you think, and skilled coders can turn C into ASM in their brains. "
How is a hash function that has high-bit hash values useful for a hash table?,"I have a quick question about hash tables, which I've been wondering about for a long time. If you use a 64bit hash function like this one, doesn't that mean your hash table needs to have an array with 2^64 ~= 1.84x10^19 ""buckets""? If each one of those buckets has a standard 4 byte pointer to some data structure, you'd need 64  zeta bytes = 65536  exa bytes = 67108864  tera bytes of space for the array. If you use a non-array data structure (list or tree) that only has entries for the hash values that actually have assigned data, you lose the O(1) data access, which is the primary draw of a hash table. You might as well go with a tree at that point, and get O(logn) access times.  TL;DR: How is a hash function that has high-bit hash values useful for a hash table? "
"the entire thread: 
 BREAKING: web developer thinks software sucks.","> glue a substandard  ECMA-262  > incomplete  what's incomplete about it? Less crap than perl, ruby, lua, tcl... ok not lua.  > client-side language onto epoll.  there's no such thing as a client-side language.  I guess you don't like ecmascript.  That said, I agree with the idea that node.js is kinda pointless. It doesn't bring anything new or better as far as I can tell.  Yaws and stuff like [A Language-based Approach to Unifying Events and Threads]( are far more interesting. And there's at least a handful of more practical web servers (apache, nginx, maybe lig  > For example, streaming data through most web frameworks is impossible.  From nodejs.org. Amusing because I wrote some streaming stuff using web.py under Apache a couple weeks ago, granted web.py didn't actually help, but it's far from impossible.  Wow I can't believe I wrote all of that, lemme just tl;dr the entire thread:  BREAKING: web developer thinks software sucks. "
"to be pedantic, you aren't reassigning the literal '5'.","That hasn't redefined 5 (which is an  int  literal), but uses reflection to mutate the immutable  Integer  box holding an  int  field assigned to 5 because Java gives you the ability to break access permissions (if the  SecurityManager  is configured to give you those rights).  The  Integer  class caches several small numbers, so if you do multiple calls to  Integer.valueOf(5) , you get the same instance. However  int  values of 5 remain unaffected (which you actually use to math in Java) and instances of  new Integer(5)  are unaffected (because this is allocated outside of the pool).  But doing this will no doubt wreak havoc.  TL;DR - to be pedantic, you aren't reassigning the literal '5'. "
complexity won't go away. Verbosity puts it on the outside where you can keep an eye on it.,"Agreed.  Regarding Java's verbosity, my view is that the complexity is always going to be there. What's important is how it is presented. With 'simpler' languages, the complexity is often hidden within the language implementation, so you have to know what is going on under the covers. This means you need to have a clear and complete mental model of what your language impl is doing, while at the same time trying to solve your domain problems.  With Java, there is less hidden. The problem is you have to code more yourself to make up for it and that leads to the verbosity, but at least then you have control over what you produce and there is less of a mental model to carry around.  TL;DR complexity won't go away. Verbosity puts it on the outside where you can keep an eye on it. "
"fell in love with a chick, wanted to do something special for her, suck at everything else, built and dedicated webapp <3.","I fell head over heals for a girl. I thought about her way too often, missed her way more than a normal person ought to, and was pretty much in a constant state of love-induced sugar shock for weeks on end. I didn't want to seem overzealous or desperate, so I tried to play it cool -- and yet, the impulse to post embarrassing, saccharine love quotes to Facebook was overwhelming (I am but only a dork, after all).  So I did what any self-respecting geek would do in my position: I computed a SHA-2 hash of her name and put it in that weird little box thingy on the left hand side of my Facebook profile. It provided just the right balance of self expression and social tact. I was happy.  I liked the idea so much that I decided I would wrap it in a small application and dedicate it to the object of my dorkitude. As I knew it would take a few weeks to flesh out and things were (shockingly) starting to work out between this lovely individual and myself, I thought that the final product might make for a cute little way of sharing my feelings. The idea of tiny ""cupidcode"" links spreading across facebook and the twitters -- each rooted in my nerdy affection for this particular young lady -- seemed like a terribly powerful way to say ""I really think you're great"". And because I suck at music and writing, it was pretty much my only option.  Fast forward a few weeks, and cupidco.de is done. It's probably not well suited for the sorts of folks that read reddit so I hope you don't mind me sharing my link with you. My hope is that this post will kick-start the spread of a few cupidcodes 'cross the nets so that my crush (who I've been seeing now for about two weeks!) may casually stumble upon one while checking her facebook or reading a blog and discover just how special she is to me.  tl;dr -- fell in love with a chick, wanted to do something special for her, suck at everything else, built and dedicated webapp <3. "
"It's a clusterfuck of a company, and I'd prefer to stay as far away from it as possible.","I wouldn't call it hate, more of a general dislike.  My biggest problem with the iOS platform is the consistent screwing over of developers, by forcing a language, rejecting apps when they feel like it, and charging for the SDK, as if it's a goddamn privilege to get the chance to code for their magical devices, all while taking a percentage of sales.  Then there are their idiotic ""I'm a PC"" ads, which are really just pitiful.  The general Apple userbase is sucky too, but that wouldn't hold me back.  And lastly, and this goes back a long time, is their smug superiority because they do  less . They think they're better because they made an OS that has very few driver-related crashes. The only reason that's the case is because there's hardly any hardware for them to support. And again locking the OS to specific hardware, because if people could run it on the hardware they wanted, the dream would be ruined.  TL;DR: It's a clusterfuck of a company, and I'd prefer to stay as far away from it as possible. "
"Did a 4 person project on my own, the other 3 guys who didn't know how to code still passed course even though the professor knew.","I was almost ""responsible"" for 3 people failing in an upper-level required course in our CS department.  Had to work on a large project together for the last 6 weeks of the course.  I presented my idea last so I got all the volunteers that hadn't already grabbed a project.  They immediately disappeared, didn't want to meet up or do any work, claimed they were somehow coding their parts of the project without checking anything into or out of the code repository.  I knew full well what was going on, so I continued on the project coding up the entire thing myself (with many sleepless nights).  The  night  before it was due, the three of them called me and said that we should really get crunching on the project and get something out, and ""maybe the professor will give us an extension.""  I told them I had finished everything, which they took to mean I was going to give them credit.  On the assignment report, I indicated what was contributed by each member - which is to say, nothing from anyone else.  The professor told them they were all getting a zero on their projects, and they were pissed.  They called up and bitched me out for ""lying"" about the project and not giving them credit... to the point where it actually sounded like they believed they had actually done some work.  I agreed to discuss it in front of the professor with them.  They brought in all the evidence they had, which consisted of a tutorial that one guy had printed out, a couple ideas that the second had, and ""I wanted to buy a book on AI"" from the third.  They complained that they didn't understand the programming language we used (C++), etc.  For some reason, the professor gave them a tiny bit of credit for each of those things which saved them from completely failing the course (and they all graduated successfully with job offers).  I was a bit annoyed that people who made zero effort and didn't write a line of code passed a course based on programming a large-scale project, but that seems to be par for the course.  tl;dr Did a 4 person project on my own, the other 3 guys who didn't know how to code still passed course even though the professor knew. "
"Everybody is able to detect real spam, but nobody has the fucking right to decide for other people what's notable and what not.","All you wrote isn't about notability, you wrote about spam and about quality.  We all know which kind of article is spam and which is not. Article to boost a commercial web-site: Spam. Article to play out a personal grudge: Spam. Article about a niche programming language: No Spam.  Detecting if something is spam is easy to do for all people with at least average intelligence and if you fail to do it in such obvious cases, yes, then you are stupid. Or worse: You're acting in bad faith.  Things like ""spam or not"" are very hard to formalize. But that's something which is true in many areas. For example it seems impossible to come up with a clear definition what ""science fiction"" is and what not. But if you read a book you still know what it is, even if there is no such clear line you can draw between sf/no-sf. And the same is true for spam and not spam.  And as long it's no spam and people invest the time to do it, it has its place on Wikipedia. Because it's part of the worlds knowledge and it doesn't hurt anybody. Again: Wikipedia has more than enough space to store articles about every niche language, every niche hobby and every niche interest whatever.  Another thing is ""quality"". Yes, quality matters and an article should always developed in the direction to better quality. But angain a big BUT here: Even a small article of bad quality (unless it's downright wrong) is better then no article at all! If an article is bad: Tag it, remove the part which are wrong and put a big banner about it which tells it the reader. But by removing it you remove information and knowledge. And yes, it makes me sad and angry if people hide behind some rules and regulations to remove knowledge. Even if this knowledge is small and only interesting for a very small group of people.  TL;DR: Everybody is able to detect real spam, but nobody has the fucking right to decide for other people what's notable and what not. "
"unlike several of the other complaints about Python, the ""no notification at compile time"" actually has some merit, even if it's phrased misleadingly","Y'know, the compile-time thing.... I mean, first of all, python has a parsing phase, and refers to the output of that parsing phase as ""compiled bytecode"".  Consider  for example.  So it's not crazy to talk about python compile-time.  It's just a little... misleading.  Second of all, as far as I'm aware, the standard deployment of python does not include a static analysis tool to detect variable names that don't make sense.  Of course, no static analysis tool could be certain in all cases, because you can dynamically construct variables of arbitrary names, but it would certainly be possible to give warnings for obvious cases.  And this is not part of Python, right?  There are several dumber complaints in the Python section, like the  len  complaint for example.  I wrote a little counter-rant on the talk page, and if no one replies in a week, I'll go edit the article.  tl ; dr : unlike several of the other complaints about Python, the ""no notification at compile time"" actually has some merit, even if it's phrased misleadingly "
"no  amount of process, or a variant thereof, SCRUM included, saves you from poor execution.",">When I’m working on something at home and I see something that bothers me I usually fix it immediately (I, of course, finish what I’m currently working on first). At work, it’s a completely different story. When I see an “injustice”, I send an email to our product owner. He adds it to the backlog. We discuss it in meetings and prioritize it.  Beauty is in the eye of the beholder, and so is a bug. Since the beholder is the product owner, an engineer is screwed.  However, this example shows how much of everything is a team effort. Give too much power to the product owner, and the project goes sour because the team disengages over such issues and bogs the project down. Give too much power to the team, and they start being fiddly around whatever and bog the project down.  Tl;dr:  no  amount of process, or a variant thereof, SCRUM included, saves you from poor execution. "
"If a program is used as a formula, you have it reviewed as a formula, not as a program.","First you publish and get all the new algorithms you used while making the program reviewed and published. Then, a program itself will be reviewed. It should have full test coverage for every piece of logic it does. All of that is also reviewed, bit by bit. In the end, a separate ""experiment"" can even be performed by someone using all the same algorithms to see if the output matches the output of the original program. Yes, all of that is lots of hard work, but same goes for every new discovery. You have to document every piece of logic you use. Nobody just goes and tells ""hey, I've got this nice formula, it tells that e = mc^3 and I think I did not make any mistakes anywhere"".  In other words: every turing machine can be defined as a mathematical function. And every program is a turing machine. Therefore, if you test a program as if it is a huge mathematical function, and get it reviewed as a huge mathematical function - you can use it as a basis for whatever calculations you want.  TL;DR:  If a program is used as a formula, you have it reviewed as a formula, not as a program. "
recruiters' hands are usually tied when it comes to requirements and what they're allowed to disclose.,"These are all great points, but not things that recruiters have control over. They don't get to decide what the requirements are- they can advise the hiring manager to re-think their requirements, but they can't flat-out decide which skills are must-haves. Sometimes the hiring manager stubbornly insists on 5 years of java, and will reject any candidate who doesn't have it.  And recruiters know it's easier to fill a remote job than a local one, but again, they can't decide what can be done remotely and what needs to be local. They also sometimes aren't allowed to disclose the salary in the initial outreach message, even if they really want to and know it'll mean a better chance of a response, some companies are adamant about keeping that information under lock and key until the phone screen.  TL;DR recruiters' hands are usually tied when it comes to requirements and what they're allowed to disclose. "
"Barbie can't keep down a job, bitch must be crazy","Back in my college days I had a class on pop culture, aside from writing two papers about achewood and a paper about Peter Murphy's big comeback, I had to sit through horrendous presentations on what other people had interest in.  Some were awesome, don't get the wrong idea... but most were just retarded.  One girl did one on Barbie, and talked about how awesome it was that Barbie had managed so many careers, I promptly raised my hand during the questions portion and said, ""Don't you think it's a little lame that Barbie can't stick to a career?  I mean, is she fired from these jobs or does she just have a short attention span?  Shouldn't we be teaching girls that they need to dedicate themselves to something instead of being so flaky?""  tl;dr: Barbie can't keep down a job, bitch must be crazy "
"Barbie can't keep down a knob, bitch must be crazy","Back in my herion days I had a class on popcorn, aside from writing two papers about Orville Redenbacher and a paper about stag partys' big comeback, I had to sit through horrendous presentations on what other people had interest in. Some were huge, don't get the wrong idea... but most were just short and skinny. One girl did one on Barbie, and talked about how awesome it was that Barbie had managed so many movies, I promptly raised my left hand during the questions portion and said, ""Don't you think it's a little lame that Barbie can't stick to a position? I mean, is she lazy or does she just have a short attention span? Shouldn't we be teaching girls that they need to dedicate themselves to something instead of being so flaky?""  tl;dr: Barbie can't keep down a knob, bitch must be crazy "
"Javascript can make the webpage faster, impressive, nice looking, more-interactive, instant-responsive, etc etc etc.","Yes, you are basically right.  >how javascript stuff fits in the whole thing.  The primary use of JavaScript is to write functions that are embedded in or included from HTML pages and that interact with the Document Object Model (DOM) of the page. Some simple examples of this usage are:   Opening or popping up a new window with programmatic control over the size, position, and attributes of the new window (e.g. whether the menus, toolbars, etc. are visible).  Validating input values of a web form to make sure that they are acceptable before being submitted to the server.  Changing images as the mouse cursor moves over them: This effect is often used to draw the user's attention to important links displayed as graphical elements.   Because JavaScript code can run locally in a user's browser (rather than on a remote server),  the browser can respond to user actions quickly, making an application more responsive . Furthermore, JavaScript code can detect user actions which HTML alone cannot, such as individual keystrokes. Applications such as Gmail take advantage of this: much of the user-interface logic is written in JavaScript, and JavaScript dispatches requests for information (such as the content of an e-mail message) to the server.  The wider trend of [Ajax programming]( similarly exploits this strength.   TL;DR:   Javascript can make the webpage faster, impressive, nice looking, more-interactive, instant-responsive, etc etc etc.  "
"Never require more than you need; Let the user decide what they want to share, never force at gun point; Name field = ""First Name + Last Name"";","As a matter of personal principal I do not require more information that what is required to verify a user login. On sites that require a unique identity this means email address and password (Hashed of course). On sites that allow for anonymity; username / password. If the site requires any form of payment processing I require only what is needed to process the payment and always give an option to store the information for frequent users.  After that they can add what they want to their profile or leave themselves completely anonymous. A persons right to information is not contingent on the amount of information they are willing to give up. It is their choice, not mine.  To generate revenue to support the site I give people the ability to opt-in (not opt out) to market statistics and demographic information. I NEVER sell personal information as a matter of good principle. If they choose not to opt in I use limited ad placement. Nothing excessive or interfering with the users ability to use the site.  With regards to your question use the Name field. If you need to split it out write a simple method to handle it. I also never require middle name or middle initial (unless required by law).  TL;DR: Never require more than you need; Let the user decide what they want to share, never force at gun point; Name field = ""First Name + Last Name""; "
Don't rely on any single type of measurement for all cases. Capture all the timings from every run and use some common sense.,"Article needs an ""it depends"" section added to it. He has a good point when you're just trying to compare absolute execution times between 2 different pieces of fairly simple code.  If the goal is to measure a piece of software so you can tell a user how long to expect it to run - the average or max is what you'd be interested in.  Also you need to take variance into account. If the ""fastest"" time of code A is < the fastest time of code B, but the ""average"" of code B is fastest, then which one do you want to deploy?  TL;DR - Don't rely on any single type of measurement for all cases. Capture all the timings from every run and use some common sense. "
Windows NT was nothing more than a scapegoat in all this.,"Here's what happened (as does with every government IT project that fails):   The government awarded the contract to build the system to the lowest bidder.  The company who won promised too many features and too much complexity.  The company then hired the cheapest, least experienced developers they could possibly find.  When their BigImportantDoesEverything application suite was built, some incredibly stupid implementation decisions were made.   And then when things went wrong - which was inevitable, because of borderline criminal incompetence and laziness on the part of everyone involved - nobody was willing to take responsibility and instead blamed some 3rd party or defective material (in this case, the OS.)  tl;dr : Windows NT was nothing more than a scapegoat in all this. "
"security through obscurity is not secure, and debug messages should be kept private","well ... anyone could add ?debug=1 to a URL and see the SQL being issued behind the scenes in this instance, or in other instances other debug information which is not meant to be public.  You might say ""oh, but you'd need to know to put debug=1 in the URL"", but I wouldn't be surprised if automated tools such as skipfish actually include that exact check.  Going back several years I've actually done exactly the same thing with the same variable name (although I DID have the ""am I in a dev environment?"" check in there), so I'm fairly sure other people would have done too.  TL;DR security through obscurity is not secure, and debug messages should be kept private "
"HQL is a useful shorthand. It is not designed to replace SQL, considering that Hibernate makes it easy to use native SQL.","HQL is a very useful shorthand for SQL in the context of O/R mapping.   Instead of listing all the columns to needed to map to the object fields, HQL does that for you. This is especially handy if you want to map a lot of nested objects that you want to fetch at the same time through a join.  Relations are much simpler to navigate, because you can join on the property rather than the PK/FK, and it gets even shorter if you have mapping tables implementing many-to-many relationships.  HQL standardizes certain things that vary wildly in SQL between different databases through standard functions (such as dates, why did the RDMS vendors choose such incompatible and atrocious syntax?).   TL;DR - HQL is a useful shorthand. It is not designed to replace SQL, considering that Hibernate makes it easy to use native SQL. "
"the reality of the situation is much more nuanced than ""outsourcing your password hashing"".","This is, in many ways, more secure than ""password hashing"" yourself. Why? In order to explain, first, here is the flow through the various steps for a user to sign in to a site:   The user generates a public/private key pair  The user authenticates with their identity provider (ideally, their email provider, with Mozilla operating a fallback for now)  The identity provider provides them with a signed assertion saying that they own that email, signed with the user's private key  The user adds an expiration to the assertion (to prevent replay attacks), and signs that assertion with their own private key  The user presents their public key and the (doubly) signed version of that assertion to your site  Your site verifies both signatures. Note that it fetches your identity provider's public key over SSL on an independent channel (and potentially caches it locally).  If the signatures match, and it hasn't expired, you provide the user with the usual auth cookies.   Note that in the common case, you can skip a few of these steps. In particular, they will usually be authenticated with their identity provider already (Gmail, for example). They will already have the keypair. The verifying server will have already fetched common identity provider's certs.  Why is this more secure?  Actual authentication is the responsibility of fewer parties. This (somewhat counterintuitively) is more secure, due to user's tendency to reuse passwords. In effect, if a user reuses passwords, they become as secure as the least secure of all of their providers.  For developers, this obviates the need (and the responsibility) to store passwords. For user's, this centralizes their management of their authentication, and makes sign-in faster (they're never signed out of their email) . I would love to have my Google account's two factor authentication apply to a majority of sites on the internet, but relying on every site to get that right is not reasonable.  tl;dr the reality of the situation is much more nuanced than ""outsourcing your password hashing"". "
I should stop being so lazy and actually study instead of just hacking away at things until they work.,"Well now I just feel bad as you're the second person to give me an incredibly detailed explanation. I guess what I initially was looking for was just a super generic example of what he was presenting would do, not an explanation of all the terms. Like if he had said  >""[...] to flatten iterables. Take for example the following code which turns a list of lists like so  [ [ 1 , 2 , 3 ] , [ 4 , 5 , 6 ] ]  into one big list like  [ 1 , 2 , 3 , 4 , 5 , 6 ] .  [...] code snippet follows  That way people like me who know just a bit, but only in practical terms can benefit too.  TL;DR: I should stop being so lazy and actually study instead of just hacking away at things until they work. "
"If performance is not your primary concern, use MongoDB. If you need complex full text search and search facets add Elastic Search on top of any db solution.","My company just evaluated every popular datastore for our current project.  Choosing a database depends on what you want to do with it.  SQL databases force you to model your data to fit into tables. This can be time consuming and inefficient depending on how many relationships your data will have. SQL databases are hard to manage when you need massive performance for a huge application.  However, if your data needs to have many references to other pieces of data a SQL database is a good option. If you are interested in using a SQL db, Postgres has an amazing number of features that MySQL lacks. Postgres has many features of NoSQL databases with all the power of SQL. Learning Postgres is a major task.  We took a look at using MongoDB and generally liked what we saw. Using MongoDB with nodejs is super easy and using the Mongo terminal is easy.  Mongo lacks full text search until the next version. The aggregation pipeline is really weak compared to SQL. There are no relationships. If you want to relate your data, you must do that manually in your code. Many people complain about performance and scalability of Mongo when you make a mega app.  If performance and scalability are your main focus couchbase (not couchdb) might be the best answer. It lacks nearly all of the query features of Mongo but it is designed to be really fast and easy to scale. Couchbase is open source but it is not free.  If you must have great aggregation and searching performance you must use ElasticSearch in addition to MongoDB or Couchbase. Some people use ElasticSearch as their primary datastore and skip using another db. ElasticSearch is completely insecure. If you want any kind of security you have to proxy all ElasticSearch requests to sanitize the requests. I found the node helper class for ElasticSearch to be worse than useless. You can make all requests to the ElasticSearch db using a rest API and there is an awesome chrome extension ""sense"" that helps you form your API requests.  TL;DR;If performance is not your primary concern, use MongoDB. If you need complex full text search and search facets add Elastic Search on top of any db solution. "
Python should be able to do everything you want and OOP is king.,"I think there is no reason  not  to use Python. I'm no expert on R (have been using it for a few weeks a couple of years ago), but from what I hear Python can do everything R can. Also,  rpy2  is able to fill the gap if necessary.  I found that OOP programming for data analyisis is a huge advantage. As a student I've been taught Matlab and have used it for about six years. After learning Python and switching to  numpy ,  scipy ,  matplotlib  and  pandas  my productivity went up by at least 200%. Right now I'm revising some Python code I wrote about a year ago and I'm amazed about how well structured my code is compared to something I would write two years ago in Matlab, even though my skills in Python have improved manifold of the last year.  TL;DR; Python should be able to do everything you want and OOP is king. "
stop looking for excuses to not take math classes.,"I think this is only true if you limit yourself to rather inane mathematics.  Mathematics is one of the most well developed systems of metaphors and abstraction in existence, and is particularly well suited for proving things to be true. It lets us think about plenty of things for which it is extremely difficult to develop an intuitive understanding.  Examples:  *Number theory. If you say you intuitively understand of prime numbers you are lying.  *Quantum Theory. Supremely anti intuitive but relatively easy on a math level.  *Financial Derivative Pricing. They use devices that you can prove are probability distribution but which have no associated events.  I won't even get into true higher mathematics which can contain so many layers of abstraction that it is really only approachable formally.  The idea that math should be understood intuitively before it is explored formally ( ""discovering math"" style ) is a relatively recent one mostly advocated by Education types and not real quantitative scientists.  It will put you at a serious disadvantage when it comes time to really crunch some numbers.  TLDR; stop looking for excuses to not take math classes. "
It doesn't relate to programming. It relates to programmers.,"It doesn't have much to do with programming.  But it is something that is relevant for programmers who are the primary inhabitants of this subreddit.  Turing is recognized by programmers as a pioneer in many facets of programming which are directly relevant today. People look upon Turing as a very important figure in the history of programming.  The message is that this hero of programming lost his livelyhood, his manhood, and ultimately his life because of a policy of the government against homosexuality.  Arguably something that was out of his control.The way it is relevant now is that this policy which had such a negative affect on such a figure is still in place and only now coming under scrutiny.  As someone who understands his contribution to programming and how horrible what happened to Turing was, some would want to honor his memory by fighting against that which was so detrimental to his life.  TL/DR; It doesn't relate to programming. It relates to programmers. "
"If you invent something, working for someone other than yourself, you can end up with your name on a patent regardless of your view on the patent system.",">> Also, stop harassing the guy whose name is on the patent: he’s just a programmer, not the management or lawyers responsible for filing the patent.>> Unfortunately he's the one that submitted the idea to both management and the lawyer.  That's not neccesarily the case - it may be that another person has examined his work,  figuring out what is patentable, and filed the patent on his behalf, regardless of what his personal feelings on patents may be. The reason is that the assignee of the patent - the company the inventor works for, owns his work.  Adding insult to injury: If the patent application is novel, the inventor can't really refuse to sign it.  tl;dr: If you invent something, working for someone other than yourself, you can end up with your name on a patent regardless of your view on the patent system. "
The question have a purpose besides the actual answer. That is why they get used so damn much.,"Yeah. All those questions are really annoying to answer. However, for the interviewer, the answer some vary important things.  Question 1: shows that you have some sort of career plan and thought about moving your career forward.  Question 2: Shows that you have put thought into working at the place and did your research. If an applicant hasn't done their research the assumption is they don't want to work there. Sending in your Resume or applying is not enough.  Question 3: Shows you can admit weaknesses and that you are aware of your own.  Question 4: Critical thinking skills. The answer doesn't mater here. What matters is how you got to your answer.  Question 5: Softball question to see if you can communicate well.  Question 6: Another softball question to see how well you can communicate.  Until recently I would have agreed with most of that these questions suck. However after understanding what a good interviewer is actually looking for in the answers, it has made it very clear why they are asked so damn much.  Wide open questions are often used to test communication skills. The interviewer doesn't necessarily care about the answer but more of how the story is told. Can you and the interviewer converse well? Does it flow? If not, this is a sign the interview could be going poorly.  TL;DR - The question have a purpose besides the actual answer. That is why they get used so damn much. "
"language doesn't matter"" applies only after you learn how languages matters.","I think it's partly true, though perhaps not in the way the author intended it. After learning a few languages, you start seeing more of the commonalities, rather than the differences. You realize that "" struct O { void (*doStuff)(); }; "" and "" interface I { void doStuff(); } "" are the same thing, from a certain angle, and that functions producing iterators "" def f(): for x in range(0,10): yield g(x) "" are also simulators of non-deterministic computations.  Yes, some environments are terrible. But you realize each is terrible in some way, and pick up a huge repertoire of coping strategies.  TL;DR: ""language doesn't matter"" applies only after you learn how languages matters. "
"you feel i am too lazy to care about my own code, and that it pry has no real advantages past what already exists.","Why did you even respond..? Good job stating most of the obvious.. However some of your thoughts couldn't be further from reality. You should be part of FOX or CNN you seem to be good fabricating things out of thin air..  Fact, I do care about documenting it, that's why i posted this.. Fact, You claim I have the ability, but just to fat and lazy to do it..Fact, You've never talked to me, You've never taken any interest in my past, So how can you make these accusations? Is it because you're pulling from your own past and inserting it into mine?Fact, Your comments about caching systems tell me that you think to much inside the box to understand the concept.  TL;DR you feel i am too lazy to care about my own code, and that it pry has no real advantages past what already exists. "
"I'm not sure if I missed out on Django-community specific features. 
 Built in selenium support is totally cool, though.","As someone who's outside Django community and doesn't make a living using Django, the release note is a little interesting.  Please don't take this as trolling/flame bait. But I'm surprised with all the things Django finally did.   PBKDF2/Bcrypt hashing, if you roll your own authentication, could have been easily used.   The orm update is a little underwhelming when compared to SQLAlchemy.   Cookie-based session backend is a feature many other frameworks already implemented.   All sorts of template feature upgrade also felt pale compared to jinja2.    Once again, I'm not part of the community, so I may miss community-specific benefit of rolling with Django as opposed to stitching the web-app yourself with cherrypy/flask/bottle/tornado/even pyramid.  I asked this question not to spread hate, I asked because peeps keep asking me, ""Oh you are a Pythonista, you must do Django"". I did help a few friends rolling their Django apps in the past, but I can't claimed to be Django programmer.  TL;DR:  I'm not sure if I missed out on Django-community specific features.  Built in selenium support is totally cool, though. "
"Insecure code, don't use this. Merely provided it as an example query with passing parameters after a query.","mysql_query( sprintf(""SELECT * FROM `users` WHERE `id` = %u LIMIT %u"", $user_id, $limit), $mysql_instance);  Edit: Since I got downvoted to hell, I'll explain why I put this. In the article there is this line of code:  query('SELECT * FROM users WHERE id=? LIMIT ?', $userId, $limit)  I was providing an example on how to implement this without a custom function and I didn't explain this initially. In production, I would always recommend using PDO now. Regardless, this code isn't AS bad as everyone seems to think. It is properly escaped despite all the comments about SQL Injection. (%u typecasts to unsigned integer). It also has no example of how I defined $user_id or $limit.  TL;DR: Insecure code, don't use this. Merely provided it as an example query with passing parameters after a query. "
Just because it  could  just possibly be done doesn't mean it  should  be done. KISS.,"> I assume what you really mean is that you can't trust an electronically counted result as much as a hand-counted one.  It's not about trust, it's about  understanding . The average Joe has to be able to observe, or even participate, in the procedure, without learning the statistics involved in making your system tamper-proof beyond reasonable doubt.  > That would be more reliable and difficult to forge than standard paper voting  Paper voting is already basically impossible to tamper with and is highly reliable. It's not like there aren't recounts and double-checks.  > , and much faster.  And that matters why exactly?  What is there to gain, from a sociological POV, from the mechanisation of democracy?  Why should the 12 voters of Hallig Gröde use a machine for their ballot? They're always the first to send in final results, anyway.  Is it even cheaper than traditional ballots? I highly doubt so.  tl;dr: Just because it  could  just possibly be done doesn't mean it  should  be done. KISS. "
"Used unicode escapes to bypass blacklist. Found environment variables, that sometimes had a different user, which shows SSH connection containing server address. 
 EDIT : Removed compromising stuff","I had fun doing the test, but moving on, I've found the IP address of the server. Step one was bypassing the blacklist. This I did by using a short little python script I wrote that converted all of my code into escaped unicode characters that the blacklist did not detect:  code = r""""""println(System.getenv().toString().replace("" "", ""!!"").replace(""\t"", ""!!"").replace(""\n"", ""!!""));"""""".strip()def gethex(c):    if c == ""\n"": return c    t = hex(ord(c))[2:]    if len(t) == 1: t = '0' + t    return ""\\u00"" + tprint """".join([gethex(c) for c in code])  This allowed me to access anything banned by the blacklist. After attempting to use InetAddress and NetworkInterface, I looked at other's solutions and saw that /u/9ren used environment variables. His/her solution was flawed in that he/she didn't replace all whitespace, only spaces. I then found the complete environment variables. Now a few times I got different environment variables, from a different user, kapparate, instead of ubuntu. I assume that kapparate website is the front-end for the code grading, but isn't the actual server where the code is evaluated. You told /u/alanboy that 108.59.251.72 was not the IP, which I found in the environment variables and javascript code, and is the IP for the kapparate website. The SSH_CONNECTION environment variable was included in the alternate vars, and this had the ip address to the actual server.  The blacklist needs to be fixed, as that is a serious vulnerability. The final code I entered was:  \u0070\u0072\u0069\u006e\u0074\u006c\u006e\u0028\u0053\u0079\u0073\u0074\u0065\u006d\u002e\u0067\u0065\u0074\u0065\u006e\u0076\u0028\u0029\u002e\u0074\u006f\u0053\u0074\u0072\u0069\u006e\u0067\u0028\u0029\u002e\u0072\u0065\u0070\u006c\u0061\u0063\u0065\u0028\u0022\u0020\u0022\u002c\u0020\u0022\u0021\u0021\u0022\u0029\u002e\u0072\u0065\u0070\u006c\u0061\u0063\u0065\u0028\u0022\u005c\u0074\u0022\u002c\u0020\u0022\u0021\u0021\u0022\u0029\u002e\u0072\u0065\u0070\u006c\u0061\u0063\u0065\u0028\u0022\u005c\u006e\u0022\u002c\u0020\u0022\u0021\u0021\u0022\u0029\u0029\u003b  Also, when I ran this code, sometimes there would be a runtime error: ""java.lang.NoClassDefFoundError: java/lang/System"", sometimes it would give me the environment variables when the code was run by user ubuntu, and rarely it would give me the environment variables as run by the user kapparate.  TLDR : Used unicode escapes to bypass blacklist. Found environment variables, that sometimes had a different user, which shows SSH connection containing server address.  EDIT : Removed compromising stuff "
"IMO, refactoring is the reason PHP projects all turn in to shit.","Can't actually help you but I can shine some light on the discussion. My company is developing a proprietary PHP Framework with a very well written codebase. It has been under development for several years now. It's is very modular so some of the shitty older code can easily be replaced. It has ORM functionalities, very nice MVC structure, revision friendly config files, database versioning, OOP implementations of basic functionality such as file management.  Now the relevant part of my post. The major issue with PHP is the lack of an IDE with proper refactoring functions. In Java+Eclipse you can move classes/packages and update your entire codebase to reflect these changes in milliseconds. Renaming a class in a huge codebase takes a few seconds as opposed to PHP where renaming a class will take minutes. Refactoring a larger portion of your codebase is suicide. The only reason our code is still semi-clean is because we don't work for clients so we have no deadlines or time pressure stopping us from refactoring.  TLDR: IMO, refactoring is the reason PHP projects all turn in to shit. "
"I think we agree, and I'm not trying to make the point that you might think I'm trying to make.","> which would integrate seamlessly with the user's workflow  This is obviously good.  > the users were frustrated by the additional steps they had to take  This is obviously bad.  > eventually our project was scrapped in favor of the plugin  Obviously your actual experience has demonstrated that the plugin approach was the better choice.   But this wasn't a question of  you  producing a framework or producing a library. It was a question of you  using  a framework or libraries. Again, not the perspective from which I am asserting that ""libraries are better.""  Perhaps if Petrel were more library-like, ""independent"" libraries would be able to integrate into users' workflows more easily. But wishing that industry standards were different is just wishful thinking, not solid ground for making business decisions. I absolutely agree with you that frameworkphobia is not the answer.  Choosing to  use  a framework is often the right choice, as long as you select a framework appropriate to your domain. Choosing to  create  a framework is often the wrong choice, because using your framework might get in the way of using another framework. This can also a good reason to avoid using unnecessary framework, but is not a good reason to avoid using frameworks altogether.   n.b. I think the best approach for your geostatistical modelling software example would have been:   Provide the independent library  Provide a Petrel plugin that leverages the independent library bu provides the seamless integration without any additional steps necessary  Consider providing a non-Petrel way of leveraging the independent library, depending on industry demand for such a thing, if any   Again, I'm emphasizing how I believe the ""right decision"" is almost always ""provide a set of libraries"". In your case, it would have been to provide a core library with a Petrel integration layer. (But I don't  actually  know the details of that situation other than what you told me so of course this could be totally wrong. If there was no non-Petrel consumer base, then abstracting out the ""core library"" might have been more effort than it's worth.)  tl;dr I think we agree, and I'm not trying to make the point that you might think I'm trying to make. "
"Evolution works, but it's all a hack.  Focus on the end behaviors and don't worry so much about reinventing the way the brain does it.","It may be time to consider that our ""brilliantly complex brains"" may not be so brilliant after all.  I think what makes the brain so powerful is the absolutely massive amount of parallelism, but other than that I consider the brain one giant evolutionary hack that works pretty much by accident.  The problem with trying to model the way the brain does things precisely (like, for example, with Neural Networks) is that we're still a long way off from being able to achieve the massive parallelism that the brain enjoys.  It seems to me that this new approach by MIT is the right way to do it: don't try to emulate the CONSTRUCTION of the brain, focus only on the behavior.  It may, in fact, be possible for humans to engineer parts of brain function in a much cleaner and more efficient way, not needing quite as much parallelism.  To assume otherwise is the same as assuming the human eye is the ""pinnacle"" of evolution in optics, when it fact there are obvious deficiencies that could be corrected (such as NOT wiring the optic nerve connections directly in the light path of the photoreceptor cells, duh.)  Part of what will eventually lead to strong AI is the casting off of hidden theological biases in human thought about evolution.  tl;dr - Evolution works, but it's all a hack.  Focus on the end behaviors and don't worry so much about reinventing the way the brain does it. "
don't bash php just for the sake of it!,"I have nothing against the op, just a general rant towards the attitude of reddit regarding php.  I'm a professional php developer meaning that I put food on my table writing php code everyday. I like php, to date I still have to find a real issue with it (and I've built very large sites) sure it's easy to learn and there are millions of newbies putting up unsecure and horrible code, but being that easy means also that someone who as a deep knowledge of the language can take shortcuts especially for mokups to show to the client. When used correctly (both in code and in server configuration) php is very powerful and scalable (facebook docet) and with the MVC frameworks (symphony and code igniter) and the addition of php6 is moving towards ruby and python. On top of this there's the fact that 99% of servers runs php, a freelancer is more or less forced to use it. I can write in Django (which I like a lot) but sadly there is not a lot of market for that kind of work (at least for now).  tl;dr don't bash php just for the sake of it! "
Game ping time isn't terribly relevant to action response time,"150ms is very playable so long as it is constant. However, P(discontinuity) is generally related to mean ping time (as in general the farther your packets go the more routes they have, and the more chances your packets are going to be dropped, or arrive out of order (which in a game environment is more or less the same thing).  Also consider that 150ms is the 0 effort round trip time. Actually doing things will increase that. Of course this is further complicated in that just about no game actually runs raw, all of them have lag damping systems of some sort or another.  Also just to add another variable, you have to consider your games processing and display time. Running at 60hz(most LCD computer monitors do, effectively adds up to ~17ms, and because of the way graphics cards work you'll have another delay of your framerate (up to about 30ms assuming it is a busy scene and your instant framerate is 30ish, plus the game engine parsing and calculating the updated positions. Plus about 5ms for you monitor to actually display the update. Plus up to 8ms for a standard mouse polling rate (plus the processing time of the mouse, and wireless transmission time)  So yeah there are a lot of slowing in even a LAN game with 1ms pings.  TL;DR Game ping time isn't terribly relevant to action response time "
The initial value of the congestion window is controlled by the SERVER's TCP stack NOT the client.  This value is configurable.,"I suppose you could say Google is ""cheating"" but MS has nothing to do with this article.  Congestion window is maintained and controlled by the sender (AKA the server).  We are talking about how many maximum segements the server is initially willing to send without receiving an ack.  The client (your browser) does not come into play until the CWND is larger than the receive window advertised by the client (aka the window size advertised by the receiver in each ack).  Read RFC 2001 for a thorough overview of TCP slowstart's variables.  Windows servers typically initiate their SS value to 2 maximum segments (MSS) but this parameter is configurable for many OS's.  TL:DR; The initial value of the congestion window is controlled by the SERVER's TCP stack NOT the client.  This value is configurable. "
Misreading a paper to look for a reason to chime in with a triviality is lame.,"The paper isn't about the psychology of sports it's about programming; more specifically this section is about competitive markets and the reason why people enter them considering the low possibility of success.  Skizm was purposefully being pedantic on a small point (certainly there are young athletes whose inspiration is going pro) to move the conversation from programming to himself.  The author used a quote from Freakonomics to give three general examples of people entering highly-competitive markets that the casual reader could understand.  You are not providing a relevant ""counter example"" anymore than if you said you knew a Wisconsin farm girl who moved to Hollywood to be a waitress.  Discrediting the paper because you found a specific examples when the author was obviously speaking in generalities is more of a troll than a critique.  Especially since the quote is:> For the same reason that a high-school quarterback wakes up at 5 a.m. to lift weights...  He did not say ALL high-school quarterbacks, just an example of ""a high-school"" quarterback.  TL;DR Misreading a paper to look for a reason to chime in with a triviality is lame. "
"Use a local development environment, do commits to source control and run unit tests before deployment.","&#3232;_&#3232;  Editing live is really the wrong way to go about deployment. There is a lot of risk that you might lose some code or fuck up the live site.  You should set up a  local development enviroment , either localhost or on a virtual machine. Why? It's fast, you will be running on a beefy computer not some small VPS or shared server. It's easier to use whatever tools you want. For example you can search all the files with a file browser and find that function you were looking for.  Use repositories (git or mercurial) to manage your code. Then when you've tested (I mean unit testing) your work you can deploy to a staging site, which should be on the same server as the live website, but a different domain. For this you can log into the server and pull & checkout the code, or just check it out locally and rsync it across.  You can have three branches (develop, stage and master) in your repository which represent localhost, staging and live sites respectively.  It's a bit of learning to get used to unit tests and source control, but it gives you much, much better control over deployment and testing so your site(s) will end up a lot more robust. At the very least do it locally then ftp the changes across.  Editing files on the fly on a remote site is something I'll only bother doing if something needs an immediate fix. Seriously, it's just slack not to use version control. But hey, I am guilty of doing version control this a few years back, now I just know better and feel more comfortable with the tools.  It also helps to use a framework which can build your database based off the models (e.g. symfony or silverstripe) you define. Otherwise you might need some kind of schema migrations going on.  tl;dr  Use a local development environment, do commits to source control and run unit tests before deployment. "
Businesses need and use data. A hatred of databases because of some long-ago trauma is not healthy for a technology professional.,"This article makes me wonder when this guy last coded something himself. There are enough differences between SQL server and Sybase to confuse the bejesus out of someone, and they are based on the same codebase! Throw Oracle, PostGresql, MySql, etc. into the mix and your head starts to spin!Then again, I'm a corporate coder. I'm not trying to make software for sale, I'm building apps for internal use, as fast as possible, with a minimal footprint.Why are there DBA's? Because coders experiment. They make mistakes. And the best place to make mistakes is in a development environment. But coders can also be arrogant or in a hurry, and throw their code in against a production database without testing. Boom! You are now digging for backups, and the day's data is toast. The DBA puts a layer of safety in between the coder and the database, with the added bonus of being able to tweak queries and the database for better performance.TL;DR Businesses need and use data. A hatred of databases because of some long-ago trauma is not healthy for a technology professional. "
"maybe XmlCopyEditor, but I agree a great F/OSS tools is badly needed.","I've written a redonkulous amount of XSLT (mostly supporting Java based-dev of all sorts), initially I used XmlSpy and decided I couldn't live without it, so when I started consulting, I bought a laptop and XmlSpy.  Everything else in my tool stack was F/OSS, those the two purchases in my taxes that year.  The last time I tried Stylus/Oxygen, I felt they'd caught up for my purposes, but neither are free (though Stylus is $99).  I tried XmlCopyEditor and had a lot of trouble 'grokking' it.   I felt like I was fighting with it, but it seemed like functionally the capabilities were there.  Besides syntax, debugging XSLT with tools that can show the call stack, variable state, etc. is pretty valauble, and I generally DO NOT use debuggers when I'm in imperative development.  TL;DR: maybe XmlCopyEditor, but I agree a great F/OSS tools is badly needed. "
"No, this source in link is not the code needed to get around the DRM.","The ""DRM code"" in a game like this would be all of the server-side logic that would need to exist on the client to be able to play this without servers. That amount of code is probably a 100-200k lines of code at the least, and spread out of many files. Because this is the game industry I'd wager that it would most likely be C++ code as well. Code would also be necessary to either alter the client code to no longer need server syncing, or running the client would need to spin off an internal server that it could communicate with.  TL;DR:  No, this source in link is not the code needed to get around the DRM. "
"Executables themselves have not changed and will still run, programming interfaces for things like user interfaces have changed and are changing right now.","ELF executables that are 30 years old still run perfectly fine, the one big compatibility problem is the APIs themselves.  XOrg still works the same, Motif too, so really old GUIs still work. But once we jump to Wayland, backwards compatibility will probably be broken with things like Motif.  Windows still, after 20 years, uses the same API. Linux distros, on the other hand, switch APIs once a better alternative exists. The only thing that has really stayed the same is Motif, but it looks like utter shit and is deprecated for like 15 years already.  But if you have a 20 year old app that doesn't run because of API problems, you can just install that API yourself (it's opensource anyway).  TL;DR: Executables themselves have not changed and will still run, programming interfaces for things like user interfaces have changed and are changing right now. "
you don't seem to actually have any way of posting the selected option.,"Well, unless there's some dark magic I'm unaware of then this code doesn't do a whole lot of anything.  You are missing  &lt;form method=""POST"" action=""""&gt;  &lt;!-- your select and options go here --&gt;  &lt;input type=submit&gt;&lt;/form&gt;  wrapping around the select tag.  Then when someone clicks the submit button your form posts the selected option, which then gets read into the $picked variable.  There are other ways you could go about it to make things more dynamic.  You should be able to add an onchange=""myajaxcall()"" to your select tag, and then have in your javascript function ""myajaxcall"" read the currently selected option, POST with ajax to a file that only handles spitting out the result of the sql query, and then update a div on your page with the result of that ajax call.  tl;dr you don't seem to actually have any way of posting the selected option. "
a good programmer knows enough to solve problems. Knowing everything isn't necessary to do that.,"Yeah, this (IMO) emphasizes trivia over understanding.  For instance I can tell you what event.stopPropagation() does, but I've never used addEventListener's third parameter, so I probably wouldn't have been able to precisely predict what the code does without knowing in advance that this post is about event bubbling.  Instead of asking candidates to explain everything a piece of code is doing (Ain't nobody got time for that), you should craft a question that will quickly determine what you want to know.  For this code, I might say the following: When I click div-3, div-0 changes color.  Why does it do this, and how might we prevent it?  A candidate that doesn't understand event bubbling would get as far as determining that the foo and bar methods only modify direct parent or child elements, and then would probably blame magic.  To a candidate who understands events, the problem would be almost immediately apparent, and they would likely be able to provide a possible solution, or at least explain what's going on, even if they haven't memorized  addEventListener 's signature.  You could dispense with the example code altogether and just ask the candidate to explain the difference between the concepts of ""useCapture"" (the third param) and ""stopPropagation"" (the event method) but even that is a bit too specific to really gauge someones ability.  It just determines how closely they've read the spec.  TL;DR  a good programmer knows enough to solve problems. Knowing everything isn't necessary to do that. "
"from my experience, sometimes people use other languages to rapid prototype, but it's always c / c++ in the end","Rapid development in academic situations like these are still not entirely dead. Often times research is done in python / matlab / etc in order to do proof of concept work, and then ported to c++ for the end product in order to make it quick.  As someone in the field of AI (heuristic search, quite similar to this competition), it is assumed that your final product for any idea will be implemented in C++... also, a lot of people in the field re-use a ton of code that they have written over the years, in order to save time. Since their code has been around so long it's more likely to be in c/c++.  tl:dr: from my experience, sometimes people use other languages to rapid prototype, but it's always c / c++ in the end "
UNSW rocks in general cause we re-implemented a full tcp stack in elec-eng,"Great assignments seemed to be a common theme through UNSW engineering. I didn't have Richard Buckland, but some of the Elec-eng lecturers were pretty good too.  I remember having an Elec Eng assignment in 3rd yr (1995) where we wrote the C code to re-implement a mostly compliant TCP-IP stack on a 6800 microcontroller, including the CSMA/CD found in ethernets and the tcp/ip API on top of it with re-framing.  At the end of the lab we had to implement a chat client on top of a standard tcp/ip stack, then confirm it worked on our own microcontroller custom stack equally well.  It took months of work since no-one in the class knew C before we started, but 3 months after starting we all had an extremely intimate knowledge of TCP/IP.  I'm trying to teach staff of my own now, and I can't help but be frustrated at how poor their university experience is compared to what I  had.  tl;dr UNSW rocks in general cause we re-implemented a full tcp stack in elec-eng "
"Saying ""I am an expert programmer"" and saying ""I know all that there is to know"" are two very distant things, in my experience.","I've worked with a lot of programmers.  Either most programmers are below average, or good programmers are so far above the average so as to form a completely separate group.  Also, nothing about being an expert, and knowing that you are an expert, precludes you from learning new things.  In fact, one of the marks of an expert programmer (in my experience) is a willingness to sit down and learn new things, either through reading, discussion, or experimentation.  Shitty programmers already know a solution that will probably work, and by god, they are going to implement their shitty solution no matter what you say.  tldr: Saying ""I am an expert programmer"" and saying ""I know all that there is to know"" are two very distant things, in my experience. "
an order of magnitude in this test that was extracted from my own working code...),"Well, quickly...   std::string  All of the STL in general, particularly std::vector.  STL is highly optimized and has specific performance guarantees.  destructors and RAII and memory management.  Tons of syntactic improvements;  for example, C requires all your variables to appear at the start of your program, which is just dreadful - you should define your variables the first time you use them.  Templates - while tricky, they're 10000000 times better than macros, the whore of the C languages.   There's no intrinsic cost to using C++ over C and the STL is designed to be extremely tight and small.  Plus, since the STL is done entirely using templates, the compiler gets a chance to mega-optimize - see this example  to see how much this is really worth to you (tl;dr:  an order of magnitude in this test that was extracted from my own working code...) "
an order of magnitude in this test that was extracted from my own working code...),"Good question!  Well, quickly...   std::string  All of the STL in general, particularly std::vector.  STL is highly optimized and has specific performance guarantees.  destructors and RAII and memory management.  Tons of syntactic improvements;  for example, C requires all your variables to appear at the start of your program, which is just dreadful - you should define your variables the first time you use them.  Templates - while tricky, they're 10000000 times better than macros, the whore of the C languages.   There's no intrinsic cost to using C++ over C and the STL is designed to be extremely tight and small.  Plus, since the STL is done entirely using templates, the compiler gets a chance to mega-optimize - see this example  to see how much this is really worth to you (tl;dr:  an order of magnitude in this test that was extracted from my own working code...) "
"Program defensively or use one of the dozens of compatibility frameworks, don't be lazy.","Because Internet Explorer is still used by more than 50% of people surfing the web today. You can try and make the argument ""but I don't care about those users"" but that's just naive. Anything that can be done in Firefox, Chrome, Safari, Opera, et al can be done in an equivalent version of IE, you just have to not be lazy and program defensively.  Think about it another way, say a programmer is writing a program for windows and says its only compatible with Windows 7 because he didn't feel like running a simple check to see where your system folder is located(say C:\Windows\System32 vs C:\Winnt\system32 for NT). Sounds like a pretty lazy programmer that didn't bother properly running qa on his code.  I'm not saying I like that IE is so vastly different or that I even like it at all, but a good programmer never ignores over half of the market share because he/she doesn't want to take the time to finish the program.  tl;dr Program defensively or use one of the dozens of compatibility frameworks, don't be lazy. "
"I walked into a Lesbian bar, and complained about the lack of guys. They're really biased and discriminatory, and I really have to make a fuss about that.","Yes, there is obviously fanboy bias there: there always is when you have stupid competitions like 'best XXX'. But there is also the simple fact of a specific group being targeted.  If I go to a political convention of one side, and start asking questions about what people are interested in, then obviously my questions are going to be based on the prior expectations I have of that market. Republicans probably hate abortion and medicare. Democrats probably hate rich people. Good grief, the bias will kill me... oh wait, no it won't. I'm with like-minded people, doing an opinion survey inside my field of interest. There's simply no point in involving things not relevant for my demographic.  Python getting a higher amount of votes  in that community  than expected in the wider Linux world may be predictable; I honestly do not know LinuxJournal or the kind of people that frequent there. But that does not change the fact the  categories and their entries are chosen really awkwardly . (Insert something about statistics here.)  And that brings me to the reason I replied in the first place: Python is NOT to Linux what C# or the .NET family is to Microsoft. Making it sound is like it is: that is childish and disappointing. So please, yell at the people who created the categories. Yell at the fanboys. Don't make it a Windows vs Apple vs Linux flame-war, since it obviously has zero relevance to the subject here.  TL;DR: I walked into a Lesbian bar, and complained about the lack of guys. They're really biased and discriminatory, and I really have to make a fuss about that. "
"Assets and other works, even software are already covered by copyright. Patents cannot be used in an honest way and are harmful to human ingenuity.","There's an important distinction between patents (monopoly on  all  instances of a given idea) and copyright (exclusive rights to a particular work). Copyright serves as a way to credit people who made something and allows the producer of the work to pursue those that attempt to steal their work and pass it off as theirs. Copyright is important even in FOSS, with the GPL, MIT, BSD, and CC licenses. When code is released, it's typically released under a  license , which is a specification of rights. The GPL, for instance, grants you certain freedoms, but you  must  do certain things in order to maintain those freedoms and stay within the limits of the license. It just so happens that the GPL and other FOSS licenses give more rights than they take away.  Traditional copyright gives rights exclusively to the creator and those who (normally) pay royalties to use that copyrighted material. A good example is  Wreck It Ralph , which has a good number of video game characters in it. Each character that Disney did not make had to be approved by the content creator (Nintendo for Bowser, Sonic Team/Sega for Sonic, Namco for Pac-Man, etc) They had to pay a hefty sum of money to use those characters, and were under strict supervision by those companies to ensure that their properties were used in the correct light or tone.  For better or worse, that is how the copyright system was intended to work. Creators give selective rights in return for money (Bands and their t-shirts or other memorabilia, for instance). For software, it's usually a single authorized copy (like Windows) and limitations on how it may be used (which are normally unenforceable if it's something ridiculous like ""must not be used in the development of nuclear technology""). In return, you get to use it indefinitely. EULAs attempt to broaden copyright, but they're largely filled with language that  looks  legal and legit, but is practically beyond the limits of copyright and, if contested, would be deemed untenable.  tldr: Assets and other works, even software are already covered by copyright. Patents cannot be used in an honest way and are harmful to human ingenuity. "
Use a framework and stick with their best software designs and coding standards.,"I work with 3 environments: production, development and testing. I focus on backend coding and unit testing.  I use the development environment along with a development database to create or fix features. This isolates the work from production environment where data are created by user who really care.  I execute my tests using in the testing environment using a testing database and generally mocked services since it wipes pretty much everything on tear down. You want to keep your development data, so keep this isolated too.  The production environment has specific error handling using error pages and is more fault tolerant.  Most frameworks out there allow multiple setup based on an environment variable. I use Kohana, so that kind of approach is easy to implement in [the framework core](  Also, frameworks makes it easy to substitute services using configurations. If your app is hard to migrate, you will have a lot of problems. Technically, it shouldn't matter whether you app runs on a remote server or locally. I've dealt with app you had to dig in for hours before having anything running on a given computer.  Code as less as possible and reuse code you thrust. Using a template engine like [Twig]( is a very good idea as PHP ends up being way too verbose.  Ugly code generally comes with bad software design: if you focus on code quality and design, you will get a good application and a nice programming experience.  TLDR : Use a framework and stick with their best software designs and coding standards. "
"version it's not OS X or GTK or Windows (for this particular case) that suck, it's the author.","In this case, no matter the OS or the widget toolkit used, I'd perform the potentially long task in a background thread and continue to process the event in the usual way - in the main thread/event handing thread. This is a pretty standard division of work.  When I receive a request to stop the layout (including unrelated events, for which the program designer thought it would be good to stop or suspend the layout), I can act on the request explicitly by a number of mechanisms - setting a flag, suspending the layout thread ( SuspendThread, pthread_kill(SIGSTOP) ), canceling the layout thread ( pthread_cancel ), perhaps other mechanisms. FWIW, if I chose to set a flag, the function which check for this flag from inside the layout thread could be named  GetInputState , if you wish.  In fact, using the Win32 function  GetInputState  would be suboptimal as it checks for any mouse button or keyboard messages and it'd not said that any such event should suspend the layout.  tl;dr version it's not OS X or GTK or Windows (for this particular case) that suck, it's the author. "
"Use a library to format the text , they are easy to configure, proven reliable and robust","Most of the answers here are offtopic, they are talking about storing a whole html document in the database, which is not what you want to do.  From what I understand, you want to store comments, posts, or some kind of information along those lines, and the text should be able to be formatted with italics, bold, etc.  It is alright to store formatting tags in the database, as long as you sanitize them correctly. Basically, this means create a whitelist of tags, any tag not on that whitelist is automatically stripped of the text  As breach_of_etiquette suggested, markdown or textile are two excellent libraries that allow you to store formatted text, and they take care of the sanitizing for you (Sanitizing is REALLY hard, has a lot of edge cases, regular expressions don't do the job well enough, you have to use a stack-based automata or a true parser)  tl:dr : Use a library to format the text , they are easy to configure, proven reliable and robust "
Some graduates in the field are outstanding. Help yourself to stand out by doing side projects.,"I'm also a hiring manager, and I agree completely. When I interview younger programmers, I find some who are excellent, motivated and will make outstanding team members - it is a privilege to work with them. I also find those who are almost clueless about anything beyond the narrow confines of their degrees.  I usually find that the people who are passionate come in brimming with enthusiasm for their side projects, and enjoy talking about them with me.  Many CompSci and Software Engineering courses are, inevitably, a little narrowly based. That's right in many respects: I want them to help people to think, rather than just churn out ""programmers"", but this does mean that if you want an edge getting your career started, you'll need to work on some side projects.  Doesn't matter what the side project is (well - it should probably be something suitable for Googling at work ;-) but it gives the chance to work with others, and to learn on projects of a larger scale than Uni coursework typically allows, and Open Source in particular can be an outstanding learning vehicle.  As an example, I currently have an intern who has completed his 2nd year at Uni, and after three months with us is already comfortable making changes to the internals of a microkernel - and doing a pretty good job of it.  tl;dr  Some graduates in the field are outstanding. Help yourself to stand out by doing side projects. "
"When someone says implement custom ANYTHING in sharepoint, just start punching yourself in the junk.","I had the misfortune of working on a project to create a custom knowledge base for 800-1100 (depending on season) call center employees.  Starting with a publishing site as a base, then creating the custom site templates, custom work flows, event-handlers for site provisioning when a new site is created, Custom web parts to provide the ui elements needed... a stand alone database to store  data that we couldn't store with the inherent constraints of lists and more XSLT work than I care to remember...  Once you ""read a book on it"" as recommended by pab_guy, you still get to work around its shitty search facilities and mind boggling administration quirks that are not documented officially anywhere.  Add on top of that the overhead of setting up per developer environments (VMs in our case), where devs can unit test / live debug their own code (by attaching the debugger to the sharepoint process itself) and you've got yourself one totally awesome pain in the balls.  Then just when you thought this product was done kicking you in the balls, when you are finished you realize that the only contribution sharepoint is really making to your end product is autentication and hosting.  TLDR:  When someone says implement custom ANYTHING in sharepoint, just start punching yourself in the junk. "
Try to test people's thinking and not the on the spot puzzle solving skills.,"First off, I never said that they were too hard, I've solved all of them at one time or another.  Second, I think you don't understand exactly what I mean by puzzle.  What I mean is that they are in the category of things that if they strike you wrong you can potentially go down the wrong path and it wouldn't mean that you weren't smart, capable, or unable to solve the problem.  I prefer questions such as this three parter:   What is your favorite programming language? (answer doesn't matter, just a warm up)  What do you like about it?  What do you hate about it.   What we have tested in the above is not if someone can crank out some canned programming exercise, but if they can think about programming and how they think about it.  If I'm interviewing candidates and they can't tell me why they like a language I'm suspicious, if they've done any real coding in it and they can't tell me why they hate it, I will never hire them.  tl;dr:  Try to test people's thinking and not the on the spot puzzle solving skills. "
Write your code to minimize the cognitive load of reading it.,"A big part of this, whatever the language, is covered by using descriptive names on your variables, classes, and methods. I personally want to punch developers who name variables a, b, c, etc. If you name your variable secondsElapsed, it should be extremely clear what the variable is.  Good naming is an art-form, as overly-long names can be nearly as bad as overly short ones, particularly once strung together into a giant line of code. Generally just taking the time to think of a good name will be enough and will speed up later development as you no longer have to keep remembering whether you're adding 'a' as a child of 'b', or was it the other way around?  tl;dr Write your code to minimize the cognitive load of reading it. "
"support our 'idea guys' because you can, not because you have to.","'Open' does not necessarily mean 'no cost'. The spirit is that anyone can view the source code. Theres a great documentary on netflix about this.  The idea of paying for open source software is valid, I mean, these people put work into something that is useful so they should reap some reward.  The 'problem' (if you want to call it that) is that you cant let someone have your code without 'giving away the farm'.  I find it interesting that this, along with other content (like movies) all fall under the umbrella of 'an idea'. I do not believe an idea can be owned or held in place once it is in the wild.  tl;dr; support our 'idea guys' because you can, not because you have to. "
Just being able to view the source code is NOT open source.  What makes it open is the right to modify and redistribute.,"> The spirit is that anyone can view the source code.  No.  The spirit is that anyone can take that source code, modify it for their own needs, and distribute their modified version to others free of care for lawsuits of copyright infringement by the original author.  And preferably, free of charge like the manner they received it (but not always the case, e.g. the BSD license doesn't invoke economics, and BSD-licensed code has been used in many proprietary software sold to consumers)  Common sense would dictate if your modification is useful to you, it could also be useful to others hence you would contribute back ""upstream"".  But sometimes you've just got a niche problem and need to completely fork instead.  But you can, and will walk out of court with a smile on your dial if /u/kennethreitz tried to sue you over ""provideanupdation"" (your fork of requests).  TL;DR: Just being able to view the source code is NOT open source.  What makes it open is the right to modify and redistribute. "
This is horrible advice for 95% of the population.,"WTF?!  > If I don’t get fired, I’ve done the Right Thing for everyone.  If I do get fired, this is the wrong employer to work for in the first place.  So...  ""I'm always right, and if I get fired - fuck them, they're the ones who are wrong!""  This is along the lines of:   Be attractive   Don't be unattractive    > But greatness rarely happens by following rules, process and structure.  100% right.  Greatness rarely happens when following the rules, but neither does total disaster.  Think of the ""rules"" as the bottom line, the safety net - not the end goal.  Yes, if you are an awesome employee who's really good at what he does and contributes a lot to the company, they will certainly let you get away with bending the rules.  But if you aren't, you're out on your ass fast.  > It means approving an over-budget trip for a particular employee to a conference they’re passionate to attend.  If you have the political capital to back it up, you can survive crap like this.  But think if everyone did this?  Shit would be constantly over-budget.  Rules are in place for a reason most of the time.  Yes, there are often stupid ones, but there are often good ones as well.  Sometimes it's beneficial to break them if there is a substantial payout, but most of the time, that'll just backfire on you.   TLDR; This is horrible advice for 95% of the population. "
"The unpopular opinions that were easily tied to him and his inability to handle the backlash imply that he wasn't ""highly qualified"" to be CEO.","> 2) Is it acceptable that such pressure exists, making pushing out an otherwise highly qualified CEO the correct business decision?  I think you might be begging the question here ... Eich's actions (even ones from 6 years ago) factor into whether he was qualified or not given that CEO is a highly political position that's just as much about appearances as it is about substance. We can't separate his public opinions from our judgement of his qualification, and this episode (IMHO) is evidence of that idea.  I'd also put forth the idea that if you accept that CEO is an inherently political position, that his failure to handle the backlash in a politically expedient manner also shows his lack of polish in front of the public at large. He could have simply given a heartfelt apology and said he'd made a mistake and changed his mind (true or not), and I personally think all would have been forgiven.  TLDR; The unpopular opinions that were easily tied to him and his inability to handle the backlash imply that he wasn't ""highly qualified"" to be CEO. "
"Make sure your Selenium-testers can intelligently tweak XPath expressions, and don't let the allure of monolithic ""end-to-end"" tests contaminate (or cannibalize) your unit-testing methods.","> The next level up from direct use of Selenium's driver is to create a facade around the browser or UI itself. For example rather than duplicating the steps to log in within each test you could create a method PerfromLogin(...)  I've seen this mutate badly: A parent class test-case-class with hundreds of semi-overlapping ""convenience"" methods. Rather than supporting unit tests, they support bunches of end-to-end paths which take so long to run that they're useless for iterative development.  Perhaps the worst contributing factor was that the testers didn't seem to understand XPath, so they'd insert these suicidally-fragile expressions autogenerated from Selenium-IDE. These expressions would then break every time even the slightest thing changed in a page, and then they'd demand developers furnish an ID (which must be unique across the entire page to work) for every individual cell or button that they wanted to target, like  offering_4_subsection_6_terms .  Then their whole ""let Selenium-IDE do my thinking"" strategy really explodes when the company introduces single-page JS applications. Since they aren't really doing unit tests, they can't predict IDs very well, and the load-order/dom-order bears no real relationship to what's visible.  TLDR:  Make sure your Selenium-testers can intelligently tweak XPath expressions, and don't let the allure of monolithic ""end-to-end"" tests contaminate (or cannibalize) your unit-testing methods. "
"Check out new programming communities. Everything isn't ""wrong by default, but possible to configure correctly"" and it's much more pleasant to work with.","I agree with the complaints, specifically regarding how much work it takes just to do something trivial that's been done a thousand times, and this is actually what annoys me so much about lots of the older programming communities: there's a gigantic amount of boilerplate/configuration/setup/hidden gotchas, and nobody in these communities seems to be interested in solving the problem.  Specific examples include things like (in C++) template errors, removing an item from a vector, memory safety, etc., which are all highly unintuitive to do correctly (i.e., wrong by default). Similarly in Linux in general, everything just seems to be wrong by default (oh, you wanted your fonts to not look like 1995 in your desktop environment?! well you can totally configure it to look like a modern OS yourself, because everything is configurable!). Configuration is great; being wrong by default is terrible, and most older communities are absolutely plagued by this.  Fortunately, things are different in newer communities. Languages like Rust are pushing for the exact opposite: Rust forces you to be correct by default, and if you do something stupid, it won't compile and moreover, it will actually give you an error message that seems at least mildly related to the problem in your code. Static typing is much stronger than in C++, and you are guaranteed memory safety at compile time (by default at least).  Another example, in C/C++, when you want to do something as simple as try someone else's project, it's a gigantic pain in the ass. Oftentimes you have to go find and build the dependencies yourself (e.g., OpenCV/Boost) and before you know it, you've spent an entire day doing literally nothing. In complete contrast, in a language like Go, you simply do ""go get"" and it goes and finds all the dependencies for you. It works by default, instead of being a pain in the ass by default.  TL;DR - Check out new programming communities. Everything isn't ""wrong by default, but possible to configure correctly"" and it's much more pleasant to work with. "
Borud isn't smart enough to figure out how to use C++ and can't pick any fights in this thread other than this one.,"You're now grasping at straws here. If it's C and not C++ I couldn't care less. They both compile and I know you can write programs in C++ and then take them down to C later for implementation (which I know is a common practice for at least one Linux variant, spurring my first comment). Really I'm not sure why I'm getting sucked into a language flame war with a bitter old man (I still maintain C++ raped you at an early age due to the way you talk about it).  All I'm going to reiterate at this point is that you're a fucking retard if your entire point in this thread is ""I hate C++! This guy said one thing that was wrong about the usage of C++! I hate this guy!"" And the counter evidence you offer is relational queries are  'prettier' in relational algebra, and that one small part of a giant system is written in Erlang. Good job. I'm not going to bother posting here again. Start knocking down every other C++ argument in here to satisfy your little fetish.  Moron.  TL;DR : Borud isn't smart enough to figure out how to use C++ and can't pick any fights in this thread other than this one. "
"bitshift operations are the same regarless of endianness, however the union is not.","Maybe I understood the code wrong when I glanced at the article. I thought that one of the things he was looking at is extracting the features (sign, exponent, mantissa) from the floating point number.I recognize that the floats and ints will have the same endianness because that is defined by the architecture.  What I am referring to is extracting the features from the float value in the 'parts' struct.  On a big endian system the sign would be the first byte, followed by the exponent and then the mantissa like this:  struct {     uint32_t sign : 1;            uint32_t exponent : 8;    uint32_t mantissa : 23;         } parts;   If you want to extract the features independently without worrying about the endianness of your architecture you can use bitshift operations which work the same regardless of architecture:  float x = 6.5f;// Get the exponentunsigned int exp_mask = (1u &lt;&lt; 8) - 1;              // Get eight 1'sexp_mask &lt;&lt;= 23;                                    // Move into placeint xint = *reinterpret_cast&lt;unsigned int*&gt;(&amp;x);    // Interpret x's bits as an intint exp_bits = xint &amp; exp_mask;                     // Get the exponent bitsint exp = exp_bits &gt;&gt; 23;                           // Get the exponent as a numbercout &lt;&lt; exp-127 &lt;&lt; endl;  *note:  I don't really ever develop in c so I may be completely wrong about everything.  tldr; bitshift operations are the same regarless of endianness, however the union is not. "
I am happy to answer questions or make suggestions if you want to give more details.,"I've done perceptual research (vision) using Python. I ended up using  pyglet  and hacked together (ugly, ugly) code after trying to use psychopy and dealing with crashes. I was focused on response/reaction timing (mouse clicks) to masked stimuli eventually incorporating TMS and EEG. I can offer tips (e.g. sub-millisecond precision timing with CRTs), but it isn't clear to me that your research is heading in that direction. I found keeping data (other than EEG) in plain text allowed me to verify my statistics processing via spreadsheet. A header in the file with participant demographic info was also easier than trying to keep it separate (encrypted the drives to protect confidentiality).  tl;dr  I am happy to answer questions or make suggestions if you want to give more details. "
it's a good way to discourage those that are genuinely talented if you focus all/too-much on those that aren't (or aren't interested).,"> Meh, my personal opinion is that the overt push to encourage ""girls who code"" and wowing over minor achievements is a detriment to their goal.  You have a good point there. The effect of such a system is that boys aren't encouraged to be boys because to get any recognition/attention they have to (a) outperform the girls [who get praise for minor things], and (b) outperform other boys [who also have to do major things to get praise]. -- If they're starved for attention, or wearied by being behind some of the other boys, they'll go with being a ""bad boy"" [acting out, etc] or some other interest altogether.  TL;DR -- it's a good way to discourage those that are genuinely talented if you focus all/too-much on those that aren't (or aren't interested). "
"Pragmatism rules the day, and that applies to both providers and consumers of developer tools.","> But I still think that it is possible to build a language agnostic package manager which fits them all.  Honestly, I think politics and technology preferences would stand in the way far more readily than any technical challenges.  IMO, it can be done.  > But that's an ego problem. What do you think?  I'm inclined to agree on that point.  At one time, I was one of those people, thinking that if you're going to use technology X, it should be able to do lots of jobs better than technology Y.  So why not adapt ever more of the toolchain to technology X?  But you wind up losing out on all the other work being done out there, even if they don't use your favorite tech.  Nevermind the endless whirlpool of suck that is rebuilding Rome (perhaps poorly) from scratch. This zealotry for one tech over another at all costs really hurts more than it helps.  And nevermind that your compiler may not be written in your favorite technology: a lot of them are written in C.  Amusingly, GCC is now written in C++.  A good example of this all this is SASS.  The SASS compiler is in Ruby.  Yet it's the best way to get managable CSS into your Python web app, or Vibe.D, or PHP, or Angular, etc.  At the same time, SASS is built into the Ruby world-view such that you need to install the runtime and the SASS gem to use it (  To my knowlege, there's no ""virtualenv"" style install of it to be had.  TL;DR: Pragmatism rules the day, and that applies to both providers and consumers of developer tools. "
Many of the features mentioned are used for performance.  Other arguments don't hold water.  Some arguments (C++.net) just don't make sense.,"> Pass by value, pass a pointer, pass by reference ?  Different options needed to optimize performance.  > 4 kinds of smart pointer?  Different options needed to optimize performance.  > Potential of mixing malloc with new. Two kinds of delete [] and normal.  Don't!!! Just use smart pointers!  > Array on the stack, heap, vector array,  Different options needed to optimize performance.  > Complex multiple inheritance.  Ummm... Outside of only allowing multiple inheritance of interfaces (and only single-inheritance of non-interfaces), what language makes multiple inheritance simple?  Eiffel?  Curl? OCaml?  I will say that MI can be misused, but there are some rare cases where it is really quite nice to have.  > Templates?  Syntax is not very good.  However, they are awesome to use.  Much better flexibility than generics found in other languages.  > let's go insane and use c++.net and put the array on the managed heap?  .NET is completely unrelated to C++!  Yes, C++/CLI (and other .NET 'C++' variants) exist, but those are mainly used for .NET interop and don't reflect the purpose or general use cases of C++.  > The destructor throws an exception, okay wtf.  This is generally a very discouraged practice and can lead to undefined behavior easily.  Just don't do it!  TLDR: Many of the features mentioned are used for performance.  Other arguments don't hold water.  Some arguments (C++.net) just don't make sense. "
invoke dynamic is (mainly) for making dynamic languages run faster on the JVM.,"So if you want to call a method then you need to know which method you are calling ahead of time. You need to know it's name, it's parameters, and which class it's from.  However what happens if you call a method on an object but you don't know the class of the object? That's something that happens a lot in dynamic languages. That's what invoke dynamic allows you to do. You can call a method without knowing it's full signature. Instead it's looked up entirely at runtime and then called.  Now dynamic languages don't actually need invoke dynamic. What they do is build their own boilerplate on top which allows them to lookup and then call methods at runtime. However that's slow and makes it harder for the JVM to optimize the code. This is why the invoke dynamic was created.  It's an instruction that dynamic languages can use when they generate Java bytecode to be run on the JVM. This allows them to have a lot less boilerplate and makes it easier for the JVM to optimize dynamic languages.  That's a tad simplified and it has a few other use cases than just dynamic languages. But that's pretty much it as I understand it.  tl;dr; invoke dynamic is (mainly) for making dynamic languages run faster on the JVM. "
"having a broader group that shares common interests with you is always a good thing, unless you're a parochial backwater and a regressive moron.","> No one says that you can't be a nerd (or work in tech, or whatever) without also being a hacker.  You keep bringing this up, when it has nothing to do with what I said.  > The ""popular"" kids are coming and are pushing the outcasts aside.  Or, the so-called outcasts are trying to find their power to maintain their clique, only to find that they lack the social intelligence to make it stick.  > I am honestly not sure how a solution would look that would make everyone feel welcome without feeling crowded out  Simple. Be welcoming. Be accepting. Don't worry about change. Broaden your interests. Nobody's getting pushed out of anything. Assholes are just getting upset that people have finally started interacting with them and noticed that they're assholes. Don't be an asshole, and you won't get pushed aside.  > but bullying people because they don't fit neatly into society's stereotypical boxes is still very much a thing  Of course, bullying exists. But ""geek feminists"" aren't bullying anyone. We won't discuss ""brogrammers"", because a) they're largely a myth, and b) calling out the hypermachismo facade they build forces us to examine the hypermachismo facade in traditional nerd entertainments.  TL;DR: having a broader group that shares common interests with you is always a good thing, unless you're a parochial backwater and a regressive moron. "
A language that makes it easy to add C extensions makes it easy to use the right tool for the job.,"I disagree with your disagreement.  A viable alternative is to have a language that makes it incredibly easy to write C extensions.  Most of my work's now a mix of Ruby + C;  where the vast majority of lines of code is Ruby, and the majority of time spent in the code is in C extensions (some of mine, some other).  Use the right tool for the job.   For the performance critical parts of almost anything that's C, perhaps with tiny bits of assembly.  For non performance critical parts, use a language that optimizes for developer time rather than CPU time; where IMHO Python and Ruby and (drifting back on topic) R excel.  TL/DR - A language that makes it easy to add C extensions makes it easy to use the right tool for the job. "
"Please, don't do this.  Seriously, just write unit tests.  Thousands of lines printed out without any context doesn't help you.","I'm not a fan of completely banning techniques, but from my experience this is one that gets pretty close to that point.  I've seen this used to dump thousands of floats out to a file, and then diffed against the golden master.  What ends up happening is that some of the lines change when you do something, but since all of the unit tests on the new code pass (you are writing unit tests, right?) you assume that problem was a latent bug in the old code.  The old code was gnarly enough that you thought it wasn't worth writing unit tests for (hence why you went this different direction), so why wouldn't it have weird edge case bugs?  TL;DR:  Please, don't do this.  Seriously, just write unit tests.  Thousands of lines printed out without any context doesn't help you. "
"Your silliness amuses me, but I do not wish you ill, I just like making fun of people","> Heavily implies that repurposing a ghost-town port is unacceptable and incompetent behavior  > ""I'm just trying to share some knowledge""  Kid, we already know jaywalking is illegal. You don't have to try to evangelize this to us when we traipse across long-deserted streets in the name of minor convenience. We're aware of the letter of the law, we just have the common sense to comply with its spirit/intent instead of its letter. At some point you're just a drunk-with-power hall monitor, trying to enforce rules where they have no practical reason to be followed.  Which, to be honest, no one would give a shit about any more than iRDMI, if you didn't go out of your way to antagonize the pragmatists. I don't know if you expected everyone else to conform to your own anal-retentive worldview, or you just wanted to achieve a personal moral high ground over everyone else (downvotes be damned), but that just makes you look silly.  But even this, I would let slide without significant comment, if not for the weasely and self-righteous final comment, ""I'm just trying to share some knowledge."" That's just the icing on the entertainment cake. It's like you're trying to get the cease-fire of an apology, in the most snooty an non-apology way possible, and it's not like it's even a situation that deserves an apology in the first place, just some sort of live-and-let-live statement that doesn't have its head up its own ass.  I honestly don't dislike you, I'm just a person with a bitter streak/penchant for calling people out on their bullshit. None of this matters a jot, and that's kind of the point. Learning what to care about and what not to, is a pretty important part of being a successful engineer in any field. If you're sweating small stuff like running a temporary HTTP process on port 8000, you will die of a heart attack before your 40th birthday. And that would be sad. So try not to die.  tl;dr: Your silliness amuses me, but I do not wish you ill, I just like making fun of people "
call stack depth is a real cost (at least for some development styles),"i haven't done much scala debugging, but i encountered something similar with gtk (and groovy). gtk is a c library for building UIs with an extensive use of callbacks. gtk ""marshalled"" all the callbacks which made it easy to write bindings to other languages, including perl and python  the downside was that when working in c, instead of a stack trace being 2 or 3 functions deep, it could blow up to be 20 or 30 deep. for me, debugging was very slow and i had to change the way i developed code. i ultimately walked away - the marshalling made it easy to get something working quick and dirty, but harder to get it working perfectly (since each debugging iteration was so much slower). i played briefly with groovy and ran into similar challenges  tl;dr - call stack depth is a real cost (at least for some development styles) "
"It's not going to affect copyright, but it will make it a requirement to add the ability to recompile applications distributed with a compiled version of PHP.","I was discussing this with Hakre earlier.....so it turns out it's complicated.  You're right that it wouldn't affect distribution of PHP source, but I think it would affect people who distribute things built on PHP.  And yes just because the header files were touched during the compilation process, because the actual library isn't used, then the LGPL wouldn't apply. Currently people can disable the GMP extension (I assume), and  they are then free to compile PHP and distribute that binary as part of an embedded application. e.g. through either Docker or through something that generates an executable file.  If GMP was a required part of PHP core that would no longer be the case, as the Docker app, or executable would now be distributing and using the LGPL code, which would now make it a 'combined work' which would impose the requirement:  >You may convey a Combined Work under terms of your choice that, taken together, effectively do not restrict modification of the portions of the Library contained in the Combined Work and reverse engineering for debugging such modifications, if you also do each of the following:> ...> Convey the Minimal Corresponding Source under the terms of this License, and the Corresponding Application Code in a form suitable for, and under terms that permit, the user to recombine or relink the Application with a modified version of the Linked Version to produce a modified Combined Work, in the manner specified by section 6 of the GNU GPL for conveying Corresponding Source.  i.e. That would be required to give out the tools to rebuild the application. Even if this didn't affect the copyright status of the PHP source code, it would effectively mean that it would need to be distributed somehow.  Additionally, the GMP library is presumably going to be required to be compiled statically against PHP core? It would be really odd if it could be compiled as an extension. That would mean that any other extension that is statically linked would need to be provided as source code so that people could rebuild the PHP binary that is part of the 'Combined work'.  TL:DR  It's not going to affect copyright, but it will make it a requirement to add the ability to recompile applications distributed with a compiled version of PHP. "
we just don't know what the hell we're doing,"The real issue is that while HTTP was designed very well for its requirements (that is, as the name suggests, transferring hypertext documents), our use-cases, expectations and therefore the requirements have changed a lot since then.  Instead of articulating these new requirements and designing the required protocols (be it at any layer(s) of the OSI model) to implement solutions, we've been abusing HTTP ever since.  We're using a protocol designed for static documents to drive today's highly dynamic web; we abuse it to do hundreds of transactions for a single page, fetch client side code, transfer arbitrary data dynamically and continuously, implement ""server push"", transfer files, provide and implement arbitrary APIs and so on.  TL;DR: we just don't know what the hell we're doing "
Unit testing is not only useless but actually harmful in all but few select situations that vast majority of projects do not find applicable.,"Don't backpedal by expanding the scope.  This article is specifically about unit testing, not testing in general.  It is absurd to suggest that anyone would be against testing the code, it is unit testing in particular that is not only useless, but actually harmful in most situations.  The only time unit testing is justified is for library and simple infrastructure code: the API is finite and public, the operations simple, and results easy to enumerate.  You  cannot  test business logic using unit tests, nor can you avoid full integration testing by thinking that unit tests got you covered.  The whole paradigm gives itself to overtest the trivial stuff that most developers can shake out of their sleeve while half asleep, and skip the actually complex stuff where real doozies actually lurk.  Even if you foolishly decide to try and write unit tests for a complex business application, you're gonna be stubbing out and mocking up most of the stuff that actually breaks.  tl;dr Unit testing is not only useless but actually harmful in all but few select situations that vast majority of projects do not find applicable. "
all I'm asking for is a citation to support your claim.,"I know the logic behind what you are saying but as far as I can tell, there's no actual empirical research to back up the claims.  > The point is that finding and fixing problems gets more and more expensive the later in a project you get;  This has been demonstrated.  > it is several orders of magnitude more expensive than if it is caught and fixed at an early stage of development/testing.  The research does not support this  several orders of magnitude  statement.  Please provide a citation for this if you have one.  > Consequently, if writing a certain kind of tests costs 10% more time up-front but also reduces the number of bugs introduced by 10%, it is a massive win in terms of overall time spent on a project.  Yes.. it's a win if the costs are as large as you say, but again, I've seen nothing that would suggest that this is the case.  What I normally do is this:  Small unit tests for the objects which can operate in isolation, integration testing for objects which interact with other objects and system testing for releases.  I spend a small fraction of the time that others do and, as far as I can tell, I get the same level of coverage.  Again, if you have any sort of study which demonstrates what you claim, then please cite it.  You don't have to point me to a pdf or anything, just give me the title and, if possible, an author name.  I'm a researcher in software engineering so I can get a copy of the article.  If you do have citations, I'm more than happy to check out their methodologies and comment on the study.  tldr; all I'm asking for is a citation to support your claim. "
preferring Pepsi doesn't mean I won't ever drink coke.,I can prefer something without going all out and never using its competitor. I prefer root beer but I still drink pepsi when it's available. I do however try to avoid closed source software as much as I can without limiting my functionality too much.  Google doesn't get a pass from me I run a [yacy]( node and I'm trying to migrate my services away from google.  Reddit uses a few patent encumbered pieces of code or some spam catching code that they can't open source without being illegal or letting spammers do whatever they want (yes security through obscurity is bad). they are as open source as they can be.  TL:DR  preferring Pepsi doesn't mean I won't ever drink coke. 
Not realising your code as open source is bad but society makes us do it.,">How - how am I hurting the computer industry by providing a service >that people are willing to pay for?  Because you are forcing developers to rewrite code that already exists and is nice and functional. I'm not saying that I expect you to release any code you make for free, society doesn't let you do that. All I'm saying is that it's better to release your code open source then not to release you code open source and sadly due to circumstance we simply can't. If we could release any code we created as open source then the industry would no doubt move along faster simply because we would collectivly have to re-implement stuff less.  TL:DR  Not realising your code as open source is bad but society makes us do it. "
"my vote's for the logitech cordless k350 wave. 
 ps. excuse my comma splices, I've been up for like 30 hours now.","I just bought a logitech k350 wireless keyboard. I really like typing on it. Also, all the extra buttons are customizable, and something I really love about it is that it uses the logitech unifying receiver, which means that if you have multiple logitech products that use it, you only need one receiver, which keeps your usb ports free. It's 60 dollars, but dented box deals come through all the time on logitech's site. Or you could do what I did, which is if you buy two unifying receiver products, you get 30 percent off, which is stackable with a 20% off code that is easy enough to come by, giving you 44 percent off. That also qualifies you for free shipping. I ended up paying 61 dollars total for the 60 dollar keyboard and a 45 dollar mouse.  tl;dr- my vote's for the logitech cordless k350 wave.  ps. excuse my comma splices, I've been up for like 30 hours now. "
You need to improve on some issues but you seem on good track.,"Just skimmed quickly and I see several issues/questions:   Why do you use a static variable for the name of the file?  FileIO is not actually FileIO since it's doing several things. For instance ""searchFile"" is a lie, it actually searches a map (and there are easier ways to do it, for instance file[search]). The variable ""file"" is actually a std::map which is confusing. You see the pattern: you need to improve your naming of things. Names are important. There are other design issues which do not matter much for such a small program but are not generally good practice.   You also need to improve your control flow skill. Too much unneeded  ""continues"" and use of uncommon looping patterns. Try to avoid unconventional control flow. While sometimes it's the best option, many times it's just confusing.  Depending on the compiler, it's possible to actually blow your stack since (if I have analyzed the control flow correctly), for instance, appMenu and searchFile are mutually recursive.   On positives:   The code is ""clean"", consistently written and documented which is very important.  You generally avoid falling into thousands of nested if's or huge functions.   tl;dr.: You need to improve on some issues but you seem on good track. "
Half the people posting comments on the bug have no idea how to use a bug tracker.,"Hey, I have a great idea!  Let's all go and comment on the bug demanding that it be fixed, causing lots of emails to be sent to lots of people, thereby making their lives terrible without actually helping any.  It's perfect!  /sarcasm  And now begins the PSA portion of this comment:  FYI, developers hate random comments like ""OMG this has been broken for HOW LONG?"" and ""this really needs to be fixed"".  Making those comments rarely helps any, and it does, in fact, cause lots of emails to be sent out, at least on normal bugzillas (I don't know how Google Code works on this, it says it doesn't email everyone for everything...).  Never make a comment unless you're providing new information or changing the state of the bug.  CC'ing yourself  does not  count.  TL;DR  Half the people posting comments on the bug have no idea how to use a bug tracker. "
"yeah, knowing the physics of quartz crystals is necessary to programming the OS and software stack of a modern cell phone.","> Designs built from logic gates don't need to understand how transisters work.  I'll disagree. Example: If you're designing software to run a compute cloud, it's very helpful to understand how transitors work, so you can pick the right server technology to build the compute cloud and hence pick the right software to know when to turn it on and off.  Another example: You think writing software to run on cell phones is fairly high-level. Your CDMA cell phone receives two packets each cycle to decide whether it's receiving a call. The phone numbers are in sorted order, so if you see a number in the first packet higher than your phone number, you can turn off the receiver before you receive the second packet. But you better change the timer that tells you when to wake up, because your quartz crystal is going to not be as warm and hence will be running slower when you have only one packet instead of two, so you have to wake up ""earlier"" to be in time to hear the next cycle of notifications.  tl;dr: yeah, knowing the physics of quartz crystals is necessary to programming the OS and software stack of a modern cell phone. "
"let the repositories interface be responsible for obtaining existing entities, not the factory.","So, speaking from an adapted and abbreviated DDD set of philosophies, I'd say that I personally wouldn't use that kind of API; for a few reasons (I'll make a bulleted list as to not be long winded).   Factories are good and should be used when the workflow for creating ""brand-new"" objects is too complex than simply using  new .  Factories generally don't have knowledge of the persistence layer  Repositories generally have full knowledge of the persistence layer  Repositories could lean-on factories to produce entity objects based on entities already stored in the persistence layer  Thus, a factory could be utilized (as a dependency) in your Repository implementation to produce ready-to-go Entity Objects   It feels a little backwards to have a Factory that consumes a Repository and can produce an Entity (from the repo) based off an Entity Identity (prob a surrogate key in your case).  I'd much rather be asking the Repository (ie: the collection of entities) for a specific Entity:  $entity = $repository-&gt;findEntityById($id);// or $repository-&gt;find($criteria); where Criteria might be abstracted / non-persistence specific// Or in context:$cow = $cowRepo-&gt;findCowById($cowId);  My factories might have methods like this:  $entity = $cowFactory-&gt;createWithData($data); // where data might already be mapped// or$entity = CowFactory::createWithData($data);// or, even this (if the code base was small/trivial enough)$entity = Cow::createWithData($data); // sometimes I have generic enough factory i pull in via a trait  Basically, there's lots of places where a factory could exist, but in any case, I'd expect them to be fairly unaware of the persistence in that case.  I'd let my Repository fully understand the persistence and mapping mechanics.  tl;dr: let the repositories interface be responsible for obtaining existing entities, not the factory. "
"The DOM object is fine.  The DOM API is still pretty horrible, and for most of its existence was absolutely  abysmal .","The historically the DOM API was hideous - it's verbose, fiddly and long-winded.  Try creating an element and setting six values on it - that's seven lines of code, when it could be something like  document.createElement(""div"", { className:""thingy"", id:""whatever ""... }); .  Initially it almost completely lacked useful querying mechanisms, too - getElementById and getElementsByTagName followed by manually iterating through and filtering NodeLists of children were piss-poor alternatives to XPath, CSS-style selectors or the like.  Sure now we have querySelectorAll and the like, but they took  years  to make their way into the language. It's arguable that $("".css .selector"")  alone  was enough to propel jQuery and similar libraries to massive prominence, because without them DOM-manipulation in JS was agonisingly painful.  TL;DR: The DOM object is fine.  The DOM API is still pretty horrible, and for most of its existence was absolutely  abysmal . "
Learning lots of languages: Good. Putting languages on your resume that you don't know: Bad.,"A really good developer will know a lot of languages, it turns out, so it doesn't have to be a ""bad"" employer who feels lots of ""bullet points"" are a plus. In fact, I remember reading about a study that tried to find correlations between what's on a resume and how good the developer is, and the number of languages on the resume was a better indicator than the number of years of experience (the latter had an almost zero correlation IIRC).  On the other hand, if I'm interviewing you, you better really  know  something about those resumes, or you'll probably be eliminated in the first round of phone interviews.  tl;dr: Learning lots of languages: Good. Putting languages on your resume that you don't know: Bad. "
"The keys are used by the PS3 to check the program's from a legitimate source. Now everyone knows the key's, the PS3 will run anything from anyone.","It's to cryptographically sign the code that's been written to ensure that it came from Sony so it's OK to run.  Say you had a program:  LoadMainMenu();WaitForUserToPressStart();DoMainLoop();  You can't turn that (compile) into the hexadecimal number base and put it on a disk to load into the PS3.  You need to  encrypt  it as well - as the last step before you burn a CD - this step encodes the program using secret ""keys"" that no-one but Sony know. When the disk is put into the PS3, the PS3 also knows these keys, and is able to decode the disk, and can read the program.  So the above program, once encrypted would look like this:  ABF6387KNVSD;7JSHUP1398;LKSJDOAIUW;   (There's another system called ""signing"" -  signing  something using a key is where you don't encrypt the actual data (program), you just stick an encrypted checksum at the end of the data based on the content of the program. Then the PS3 could look at the program, calculate it's own encrypted checksum and compare it against the one on the disk - if they don't match then the program's been tampered with. Signing programs (or Emails) like this means you can read the plain-text code/message, AND confirm it's not been tampered with - BUT Sony would want the extra security of encoding the assembly code too! That way people can't reverse engineer the games to see how the graphics chip and things work - so it uses full encryption of the data instead.)   Write it to a CD, and the PS3 will load it up, and try using it's ""secret keys to decrypt the code. It will work! Therefore the PS3  wrongly assumes  that the code came from a legitimate Sony source as no one else knows the keys to unencrypt the data... so it will run the game!  TL;DR  : The keys are used by the PS3 to check the program's from a legitimate source. Now everyone knows the key's, the PS3 will run anything from anyone. "
async is a special-use tool for the exceptional cases where threads don't work.,"... What about threads? [Fiber]( or [lthread](  based coroutines? [Stackless](  [Goroutines](  Of the different tools I've tried, synchronous calls are always faster. Real programs block/await far more often than they fork, and threads/etc are optimized for blocking/awaiting. IMHO, about the only legitimate use of async is if (a) you're running on a platform with tight restrictions on/lacks threading (e.g. JavaScript, Windows 8 Market application), or (b) if you can't efficiently allocate or pool threads, or (c) you can't allocate enough threads (e.g. 32-bit Windows, or apps that need hundreds of thousands of threads). None of these apply to e.g. Go lang, or any other green threaded system, or even most applications using system threads.  tl;dr async is a special-use tool for the exceptional cases where threads don't work. "
is I've started also my own framework  which is not Python 3.3 compatible yet.,"First nice wrap up! The documentation is nice, there is a lot to read there is even something totally new to me in the documentation «key concepts»  What follows is not a critisim of Watson in particular but most python web frameworks:  they got inspiration from PHP at least it looks like PHP doings.  It's not bad to get inspiration from another world but in this case what they take from it is not good.  In particular module loading. This is in a sens handy but very disturbing «what loads before what, again ?», «put this in the right order or it won't be loaded correctly», loading dependencies is a hell. A consequence of that is the loose connections from the user code point of view of the different part of the code. App A gets a bunch of stuff loaded because there module path can be found in a particular file that is itself loaded because it was well named in a proper place in the filesystem. Reading a website code with no particular knowledge of Python webdev is not particularly easy to understand because it all relies on module loading magic even if of course you can guess, it's Python and Pythonistas...  I don't know if Watson does that based on the example app ( but thread locals are a reminiscence of PHP Globals, and those are not logical, request is not a global and it will never be, neither do a database connection. I'm not sure where thread locals can be useful, but if I can get my way away from it, I'll do.  I have other things I'm not happy with the dominant web framework, but this is already a bit long so tl:dr is I've started also my own framework  which is not Python 3.3 compatible yet. "
"paying your money into this indiegogo will probably just provide
funds for NjeriChelimo to buy herself a new laptop, a new TV, or a leather
couch.","I'm a Kenyan and I've been following this whole indiegogo crowdfunding drama.I've actually met Njeri Chelimo (or Martha Chuma) depending on which onlineprofile you look at. She's spent a lot of time at a technology co-workingspace here in Nairobi.  When she did her original indiegogo I was supporting (verbally, but I didn'tcontribute any money). Now I'm actually very relieved that I didn't contributemoney because this whole indiegogo thing seems to be just an elaborate cynicalstrategy on her part to get some quick money.  Visit the indiegogo page: Notice on the campaign that it is a ""FlexibleFunding campaign"" meaning whatever funds have been pledged will all be goingto Njeri regardless of the success of the Campaign. Also notice that theCampaign page does not even have any details on how exactly she'll be""building"" the HackerSchool or who she is partnering with.  Nairobi is a very small place and I have pretty good sources on what happenedafter first indiegogo that Njeri did (the one when she tried to get airfare to go toNew York) when the Campaign succeeded to raise over USD 5000 but Njeri failedto get a Visa to the US. Basically after she failed to get a visa Njeri wenton a bit of a spending spree (new laptop, new toys, stuff for the house),USD5000 goes a looooong way in Nairobi when you're just spending it on yourself. Thedeadline for this current Campaign is in 5 days and I doubt it will meet the50,000 target... I have no doubts however that when the Campaign ends Njeriwill have no qualms about spending the money on herself like she did with thefirst Campaign.  TL;DR... paying your money into this indiegogo will probably just providefunds for NjeriChelimo to buy herself a new laptop, a new TV, or a leathercouch. "
"your problem may not be jQuery, you may just have to rethink how you are structuring your code, and enforce some new structure.","> A framework that somewhat enforces on the devs. better code structuring in an object-oriented way.  I had a similar scenario recently. Was working on a large server-side web app where javascript was sprinkled about to add some behaviours to pages on as ad-hoc basis. This meant that JS was littered almost randomly in templates or dumped into gigantic javascript file full of functionality that was included on each page.  The code was a mess, but all of the devs already knew jQuery. The solution was not to abandon jQuery altogether, but start migrating towards some stricter javascript code structure conventions which made the code more testable / manageable. We went with modular code files, which each had test files and related to the application (but not tied to the application's server-side structure).  tl;dr your problem may not be jQuery, you may just have to rethink how you are structuring your code, and enforce some new structure. "
"I like Angular because of the extremely clean division between data manipulation and presentation that it encourages, despite the markedly increased complexity.","I started a new job about 7 months ago. The team I joined is working on a large-ish Angular application. My experience is that the app is significantly more complicated than anything I've worked on, but also ultimately more maintainable.  Dependency injection is one of the best things about Angular. It makes it much, much easier to test the code and to isolate domains. RequireJS is not a substitute - this solves a different problem. In fact, we use a JS loader as well. The wonderful thing about DI is that it encourages really clean separation. For instance - I've not written a single line of DOM-manipulating JavaScript during these 7 months, despite writing a ton of JavaScript. The native Angular directives coupled with those written before I joined have been sufficient for me. The vast majority of what I've written has been services, with some work in the controllers as well. This ends up being a great way to structure the code. Thinking of DOM-less data manipulation as the primary ""driver"" expressed declaratively through a dumb template feels really good. Prior to joining the team, I was a huge backbone fan...but for large projects I'm definitely convinced that Angular is a good choice.  Admittedly, I still don't understand large parts of Angular since I've never had to write a directive. I also don't have quite as good of a feel as to how the application as a whole glues together the way I did when working on large backbone apps. In some ways I would argue that this makes Angular even more impressive; I am able to be quite productive without understanding its murkier aspects.  tl;dr - I like Angular because of the extremely clean division between data manipulation and presentation that it encourages, despite the markedly increased complexity. "
Javascript is flexible. Classical languages are not. Javascript is not a classical language.,"My own criticisms have more to do with the way the language is constructed. Having a .prototype property of a function determine your prototype chain is weird and awkward conceptually.  It seems correct to classical OOP guys because when you Uppercase a function and say ""it's a constructor"" and put some key/value pairs on an object things start looking pretty familiar.  The thing is though, the function is still just a function and the object is still just an object. There's nothing going on there behind the scenes but a function being called  like.call(Object.create(like.prototype), ""this"")  and simple hash table that is completely dynamic.  You can call them ""constructors"" or ""classes"" all you'd like and if that makes you feel better, then I say go for it! But it's not representative of the language and actually encourages people to think of classes as static entities when they are anything but.  I don't really use classes at all but just a collection of objects that I can string together in the order I need, when I need, and if I need to modify a behavior, I do so. I may have 2 or 3 initialization function or ""constructors"" that I might use depending on who ""who"" is in your example above. And I have the flexibility in javascript to do so.  tldr; Javascript is flexible. Classical languages are not. Javascript is not a classical language. "
Zephir is less than one year old & you're recommending it for payment processing o_o,"Zephir is a terrible recommendation based on this post; I almost wanted to go as far as asking you to not make any recommendations in this subreddit based on this single recommendation post.  The question being presented has nothing to do with Zephir but instead Java; If you feel that Zephir trumps Java in terms of choice in this decision I'd ask that you please include them in your post and or explain them such as your statement:  > Its going to be quicker than Java  I could be very wrong making this assumption but I'm under the assumption you're not a core developer of Zephir or someone who uses it on a daily bases? Assuming that my assumption is correct I don't understand why you're recommending something you're lacking in experience with.  Even if my assumption is correct or incorrect doesn't matter because I'd have to question a recommendation that suggests to use something that isn't even at a major stable v1 release. Zephir is extremely young to be making these sorts of recommendation; Hell, I can go further just by pointing out the issues page.  TL;DR Zephir is less than one year old & you're recommending it for payment processing o_o "
EULA's exist to screw people out of fair use.,"Google Search's ToS can say whatever they want. They aren't legal, and are a joke if you never agreed to them. It's just ""google's rules"". You never agreed to them (you never pressed an ""accept"" button before googling something, right?).  If you agree to an EULA, then you could get in trouble. Sued even. EULA started because Sega lost to Accolade:  Software companies started putting EULA's into everything regarding reverse engineering, to prevent other companies from using their software outside of the ""user agreement"".  EULA's are not just for shutting down an account. It's definitely legal leverage to sue other companies and actually win.  tl;dr: EULA's exist to screw people out of fair use. "
they pay more for java because they want a pre-packed developer with 1+ year experience and certification in every tool they use and don't trust in learning curves.,"java salaries are so high because they usually ask for very specific profile of developers.  i have gone to interviews where they ask to pinpoint the exact version of hibernate i have experience with and get rejected when i don't use exactly the one they have installed.  usually when you go to a php interview they ask you if you know how to set up a database, install a framework and how much you would require to learn the software they use.  on java they ask you by phone if you use oracle server, tomcat, jboss or other type of server even if you will only configure it once.  i have even been asked what ide i use because 'everyone uses netbeans' or eclipse or any other and they wouldn't want to have diversity there.  tl:dr; they pay more for java because they want a pre-packed developer with 1+ year experience and certification in every tool they use and don't trust in learning curves. "
please give me an useful example of a module functor.,"> Hopefully it's obvious  it's not, because I haven't seen any examples of this (I've played with modules in ocaml and coq) and the only thing I learned is that it's really ugly - c# example I remember from 5 years ago, I haven't touched the language since and it was obvious back then, even though I couldn't code. I had to learn that sick syntax for functors for fp class exam 6 months ago and not only I can't remember it today, I couldn't remember it at the exam, after learning it the day before (do I need a ""with e = M.t""?). the only nice things about functors/modules is opening them.  tl;dr: please give me an useful example of a module functor. "
"stfu, it's a concrete rule and you accomplish nothing by attacking it, let alone antagonizing people who help others out by correcting their grammar with the term ""grammar nazi"".","Pick your battles, I was merely trying to help someone out with a grammatical error. It doesn't matter if the inconsistency of an apostrophe's usage annoys you, the fact remains that most people know how an apostrophe works and can potentially pass negative judgment on you where it matters (ex. your resume, cover letter, apology, etc.).  Defending black & white grammatical errors is a selfish and damaging battle. Unless you can find enough people to riot with you through the streets of something more respectable than Twitter, grammatical rules are not going to change; granted some can be bent so that a sentence can read more comfortably, or to fit a writer's style, but the apostrophe's use is not one of them.  tl;dr: stfu, it's a concrete rule and you accomplish nothing by attacking it, let alone antagonizing people who help others out by correcting their grammar with the term ""grammar nazi"". "
"C++ is a very complex language, and it's difficult to learn of all of its caveats. Yet it does not make it useless by a long shot.","Okay, I need a programming language that:   Is very fast  Allows for inline assembly  Works on embedded systems  Can link directly against C libraries  Has support for pointers and OO  Does not result in boilerplate code   Is known in the industry   Now tell me of another language except C++ which allows all of this. I'd be happy to switch if it was less complex, but still had all the requirements. C++ is unique in that it is multi-paradigm and is partially low-level and partially high-level. Sure the language has its flaws, but to claim that  noone should use it ever  is just you ranting. It solves real problems for real people.  C and D aren't always good replacements. C often leads to a lot of boilerplate code, when the projects reach a certain size threshold. D is simply not an option because very few in the field can program in it.  TL;DR C++ is a very complex language, and it's difficult to learn of all of its caveats. Yet it does not make it useless by a long shot. "
"concat&lt;T&gt;  is broken, and  vector&lt;bool&gt;  is broken as well. C++0x type inference is OK.","This kind of pitfall is not specific to C++0x. With a little effort, you can run into it in C++98 as well. Make the following function template  template&lt;typename T&gt; void Evil(T t){  T fail=string(""FAIL"")+string(""!!!"");  cout&lt;&lt;fail;}  and call it with  Evil(string(""hello "")+string(""world.""));  Your C++98 compiler will happily instantiate Evil<concat<string>> and in its body it will create a variable 'fail' of type concat<string>, just as in your example with auto, and with the same consequences. Template argument deduction is, after all, just a special kind of type inference.  You should disable copying and assignment for  concat&lt;T&gt;  (as BorisTheBrave suggested elsewhere).Or perhaps you could implement copying and assignment for  concat&lt;T&gt;  that would allocate the buffer and perform the concatenation.  As for the  vector&lt;bool&gt;  specialization, it is generally accepted to be broken in many ways. It should be avoided. I think there were some proposals for the C++ committee to remove it or deprecate it,but I don't know if they were voted into C++0x.  tl;dr:  concat&lt;T&gt;  is broken, and  vector&lt;bool&gt;  is broken as well. C++0x type inference is OK. "
"In S2, no form logic in your controllers, no html in your form class.","> (you were creating views from inside a controller, but supposed to load and validate those views in your model?)  That's one of the things I like the most about Symfony 2, and it's one of the things that took me the longest to ""get"".  In S2, your model layer is  not  your ORM classes (Entities). Your entities make up a portion of your model layer. Your forms are another portion of your model layer. When you think about it, conceptually, forms are objects; they have properties, validations, etc.; they need to be rendered and re-rendered.  In S2, you create a form class that describes all of the properties and validations of a form. Then, in your view, you tell the form how to render itself. This fixes the biggest issue with the form implementation of CodeIgniter, the last framework I used thoroughly.  tl;dr  In S2, no form logic in your controllers, no html in your form class. "
"there is huge room for improvement, and people have barely started to pick the lowest performance hanging fruit.","Go is not really much slower in any meaningful benchmarks.  The main two issues is the garbage collector that is just a 'dummy' placeholder until the new concurrent high performance GC is finished, and the libraries (for example the regexp lib) that have had no performance work done at all until recently (a few days ago rob committed a change to the regexp lib that produced a 20x speedup, and that was just the lowest of the low hanging fruit).  And of course the compilers do very little optimization at the moment because the goal has been to stay flexible as the language has been changing and evolving.  tl;dr there is huge room for improvement, and people have barely started to pick the lowest performance hanging fruit. "
I'm a good programmer because I write good code.,"You don't ask a poet why he is a good poet and expect him to recite some checklist.  If anything he would express that he  is  a poet, he sees the world through the eyes of a poet, he lives, sleeps, and dreams as a poet.  Despite any claims of a free will, he can't help but be anything else.  Writing good code is no less of an art.  I'm a programmer and not a poet.  I cannot hope to articulate what makes me a good programmer, but I do know that there is no single practice or set of patterns that will ensure code is  good .  Writing code at all is an NP-Complete problem, the very nature of our job relies on us constantly developing new heuristics for problems.  It is a creative work the requires the right kind of intuition and disposition.  But you were expecting some sort of checklist weren't you...  My poems never rhyme.  I use vague metaphors.  I write all my poems test first.  And I only write haikus.  tldr; I'm a good programmer because I write good code. "
I really hope you are not a Mormon and that you can touch type!,"Since you are using windows you could try  PyScripter  features but it was slow on my below average work computer.  Now, you already noticed that a lot of people are suggesting vim. At first they might look like those annoying door to door preachers (Mormons, Jehovah's witnesses and whatever they are called) but actually the vim preachers are really trying to help you.  Learning vim takes time and in the beginning your performance will drop. However, once your vim skills improve it is really a second nature. Just like typing without looking at the keyboard, at first it looks impossible but soon it's quite easy.  If you choose a path to master vim may I also suggest  pentadactyl  browser and it helps quite a lot to stop using the mouse and learn the shortcuts.  TL,DR: I really hope you are not a Mormon and that you can touch type! "
"SE is about technics, CS is about research. Don't mix it up! Concern their relationship!","Don't underrate one discipline because you mixed up or doesn't unterstand the domain!  Software engineering is in the domain of ENGINEERING like (mechanics, electronics, civil ...)Computer science is in the domain of SCIENCE like (math, physics, chemestriy, biology, ...)  The disciplines of the science domain build the fundation for the disciplines of the engineering domain. In science RESEARCH is undertaken which results are exploided by TECHNICS of engineering domain.  If you think programming and computer are the main field of CS you are wrong. Computers are just tools. CS asks about what is posible to calculate and how fast can it be done... It's the same problem wenn you think history is about books, no, it's about what happend in the past.  If you want to know how the programm a software or how to create a website, then software engineering the choise for you. But if you want to understand the basics and the fundamentals, then your investigation is called computer science.  TL;DR SE is about technics, CS is about research. Don't mix it up! Concern their relationship! "
An unawareness of newer features or an unwillingness to utilize some features of the language added after you created your own way to accomplish the same things.,"I can offer a trivial example, but one that might lend itself to a broader explanation.  My boss is not a classically trained programmer but he has written a hefty amount of PHP (and Basic type stuff prior).  I am not sure which version he started on (I believe it was '06ish when he began)  but I, as a new PHP dev, will often show him functions (array_map is one example), constructs (ternary operators, single line if statements with no curly braces), or other aspects of the language that he has not encountered before.  His reply is often (albeit at times borne from pride) that when he started, the language didn't have some aspects.  Now before someone shits down my throat with ""PHP HAS ALWAYS HAD TERNARY OPERATORS"" or whatever, that's fine but it's not the point.  The point is, my boss is unaware of newer PHP features or unwilling to utilize them (such as OOP) and thus writes code that could be replaced with newer features.  I am also not saying that this is the case for every developer who learned an older version of a language.  This is just the case for my boss and could be the case the OP is talking about.  TL;DR: An unawareness of newer features or an unwillingness to utilize some features of the language added after you created your own way to accomplish the same things. "
"Miguel is obviously sucking up to Apple now, that he's over Microsoft. /s","I always laugh at ""year of Linux desktop"" japes and I've been using Linux as my primary OS for years now, single boot, baby. It's always particularly funny, because there is no such thing as ""linux desktop"", there are only attempts and patch-up-jobs, no offence to Ubuntu and Fedora communities. I'm here for the freedom, but because of this freedom we will never be mainstream -- there is still no ""Joe Sixpack experience"" guaranteed anywhere. We lost that niche some time ago and now Linux is catering to industries (TVs, set top boxes, phones, tosters, cars, post-big-iron servers), so nobody cares about the freeloaders... erm... desktop people. And yet if you want freedom then you can only go to one of free BSDs, which will not give better desktop experience, will it.  tl;dr Miguel is obviously sucking up to Apple now, that he's over Microsoft. /s "
Linux on the Desktop is not user friendly to maintain. Whether that kills it or not is TBD.,"I agree with most of the observations on effect in the article. The explanations for cause seem relevant but I do not know how serious.  My own personal experience as a Linux desktop user is that every distro upgrade path I have ever taken has been caused by installing X for features X.1 and X.2 which broke A, B, and C and took away some core functionality. Forced into an upgrade features X, A, B, and C worked but D, E, and F were now obsolete, had config files moved/changed, or just did not work. Author's sound system comment rang true for me. Recent Bluetooth issues had the same problem. Many times seeking help from the community results in one of two solutions: 1) why are you using/doing THAT, use/do THIS instead (not helpful) or 2) try upgrading to svn (also not helpful).  TL;DR Linux on the Desktop is not user friendly to maintain. Whether that kills it or not is TBD. "
"No, a pure function that processes user input does not have side-effects.","I am afraid you are simply factually incorrect. Whether or not a pure function deals with user input has no effect on its purity. Pure(wholly defined, not partial functions such as, say, head in Haskell) functions are referentially transparent for all possible inputs. Barring a barrage of solar radiation or some other catastrophic system error(in which case this argument in its entirety is moot), a pure function will behave as you expect it. I'm not an expert in formally proving stuff like that, but as I understand, at least in some cases, you can even formally prove that a function will do what is intended.  TLDR; No, a pure function that processes user input does not have side-effects. "
One perspective is that the meaning of code is a function of which compiler you run it through.,"> this code claims to display the result of 1.1*1.1/1.21 according to a set of rules defined by the language  The language spec defines an abstraction. The compiler either does or does not support the abstraction as defined.  I think there are really two ways to look at this:   The code for the compiler and the code for the application can be considered one codebase. Does the code in the codebase do what it claims to? Yes, if you read it, it claims to violate the language spec.  You can compartmentalize the analysis, treating the compiler code and the application code separately. In this case, the compiler does what its code claims to (namely, provides a different abstraction than the language spec), and the application code either has no meaning (since you analyze it in terms of the spec and the spec doesn't apply), or you create a new abstraction that describes what the compiler actually does, and the code's meaning shifts to be interpreted in light of this abstraction. You can make an argument that all the code is doing what it claims to here, since if you say the application code has no meaning, then it makes no claims, or if you say its meaning exists in terms of the modified abstraction (actual compiler behavior), then it does what it claims because the claims are evaluated in those terms.   TL;DR: One perspective is that the meaning of code is a function of which compiler you run it through. "
Unit tests should only take a few minutes to write but can save you countless hours of time,"Unit tests are supposed to be fairly simple and quick to write.  Their main purpose is to  simplify  your testing and to catch regressions.  For instance, let's say you write a ripple-carry adder (which is just a very low-level way of doing math that I had to do as one of my first CS projects in university).  The input and output are absurdly simple, and your unit tests would simple be something like:  adder( 2, 2 ).should.equal( 4 )adder( -2, -2 ).should.equal( -4 )..etc  You then go on to add test cases to catch things that are more ""edge cases"", e.g.:  adder( 2^32, 2^32 ).should.equal( 2^33 )  which might test your 4 byte overflow, or:  adder( 2^128, 2^128 ).should.equal( ""1.16E77"" )  If you go on to make this a full ALU and add multiplication, division, etc then your unit tests help to ensure that you're not breaking anything by introducing changes.  I personally have my unit tests running  all the time .  As soon as I save a relevant file, the appropriate test runs against that file.  If the test fails, I get a little notification that pops up on my screen saying exactly which test(s) failed.  Think about it -- You're looking at code you haven't touched in 6 months, or perhaps you never have since someone else wrote it.  You make a seemingly innocent fix for a bug, but your fix breaks another case  that you didn't even know about .  As soon as you save the file you get an alert saying ""Hey, you broke something else"".  The alternative is that you push the code to dev, the regression possibly gets missed, it goes to production, and you get a bug report a week later on something you just ""fixed"".  tl;dr -- Unit tests should only take a few minutes to write but can save you countless hours of time "
What was opcode ==> decode table ==> circuits is now opcode ==> decode table ==> decode table ==> circuits.,">  It's not code that is doing this but transistors  On a facile level, this was true of Intel's 4004, as well. There was a decode table in the CPU that mapped individual opcodes to particular digital circuits within the CPU. The decode table grew as the the number of instructions and the width of registers grew.  The article's point is that there is no longer a decode table that maps x86 instructions to digital circuits. Instead, opcodes are translated to microcode, and somewhere in the bowels of the CPU, there is a decode table that translates from microcode opcodes to individual digital circuits.  TL;DR:  What was opcode ==> decode table ==> circuits is now opcode ==> decode table ==> decode table ==> circuits. "
being incompetent is fine. Being an incompetent dick isn't.,"People are really really adverse against getting people fired. It's not something any social human being would want to accomplish. It's not something we do for fun. This kind of directions are only taken if there are no other options at all, or if that person is incompetent and a dick at the same time.  I had personal experience with a guy like this. He was completely incompetent. He probably would not have been able to do FizzBuzz let alone any algorithmic question. His way of 'programming' was copy-pasting 'similar' code from other parts of the application, rolling his head on the keyboard until it kinda worked, and then submitting it for review.  This review process is what started to tip the scale. Before who did what to the code was more or less invisible but when the review process was implemented (poorly, but it's better than not reviewing anything) it started to become so very obvious how bad he was. It took experienced devs longer to fix the stuff he did than it would've taken them to implement it themselves.  What ultimately cost his his job wasn't just being incompetent. If he would've been a nice person he would still have been working there and people would've probably worked around him (I stopped caring about that company after 2 months and left after 6). But he was the opposite of nice; he was a complete dick to anyone and he in fact believed he was at least as competent as any developer there.  TL;DR: being incompetent is fine. Being an incompetent dick isn't. "
"Flyd is more functional, simpler and more modular than existing FRP libraries for JS.","Thank you for your comment. I've tried to highlight some of the differences in the readme. But I can do a more direct comparison (all of these arguments apply to both Bacon, Kefir and RxJS – Flyd really is quite different):   Other JS FRP libraries a object oriented. Flyd is more functional. This leads to a variety of differences:  Instead of doing something like this  someStream.filter(pred).map(fn)  you'd do this:  map(fn, filter(pred, someStream)) . This might seem odd if one is only used to OO libraries. But this makes it possible to combine and abstract functions easier. It also makes the library easier to extend because you don't have to modify any  prototype -objects.  Functions are curried, meaning they return partially applied instances of themselves if invoked with too few arguments. To create a function that takes a stream of numbers and squares them you can do this:  var squareStream = map(n =&gt; n*n)    Minimalism. Even Kefir is a pretty huge library. Flyd's core is only about 200 SLOC. This makes the library transparent. Meaning that end users can realistically get a full understanding of how the core works.  Flyd is extensible, modular and the core of the library is documented. This makes it easy for users of the library to create new abstractions if existing ones do now exist. For an example of this consider  this blogpost  and thanks to the above I could easily implement it as a Flyd extension and as a result create a more natural implementation than the one possible with Rx.   Tldr; Flyd is more functional, simpler and more modular than existing FRP libraries for JS. "
"Firefox is rendering like it should, their web developer can't code for shit.","Okay, I'll try to explain the reasoning behind this, mainly because I have some time to waste before I'll go to my job.  First, let me say that the code is ugly as hell. Instead of using backgrounds, they are using images, which are then positioned so that they can be USED as backgrounds. In normal webdesign, such a thing would be unacceptable. This is also the main source of problems.  At the top, there are two images - the ""Manage my direct loan"" one (which also acts as a background for the text and the ""Sign In"" button) and the one with the laughing students. The ""Sign In"" button is right behind them (in the HTML of the page), so when rendered, it should be positioned on the right of them (like this:  Here's the main problem. The button has a position: absolute set to it. Under Firefox, completely nothing is changed, but Webkit moves it into the next line.  The correct behavior, I think, is what Firefox does. The box is moved out of the way for every following element, but since it's top/right/bottom/left values are set to ""auto"" (the computer style, when the developer doesn't set anything), it shouldn't be positioned in any other way, than it would be without the absolute positioning (here, another scenario happens - when setting position absolute, the ""vertical-align"" of the element is ignored, that's why the button moves up, instead of staying at the bottom of the box).  So in short, without any margins, but with position: absolute, Firefox sees the page like this -  -  and Webkit like this -  - the margin positioning is history (  As far as I understand the specification, Firefox (and hence, IE) is doing it right. Webkit is pushing the element down below, and to fix it, they should set a margin of -185px 0 0 5px to the button, but still, that wouldn't make sense, since the layout is completely hacked and ugly as hell. winampman below me said they ""had some issues with getting everything in the right place"" - as far as I see, they employed somebody who didn't know how to code and he set everything ""so it'll just work"", just like people did in the old times when IE6 ruled the world.  tl;dr Firefox is rendering like it should, their web developer can't code for shit. "
The simplest interim remedy is the run a 32-bit kernel until a patched 64-bit kernel becomes available.,"A quote: ""The tool is only necessary on 64-bit systems, because 32-bit systems are immune to the vulnerability.""  This is misleading. The described tool is (1) not necessary, (2) is a commercial product, and (3) the link is a commercial promotion not unlike what one regularly sees for nearly useless Windows malware detection products. Caveat Emptor.  Here are the facts:   A remedy is not necessary on 64-bit systems  unless they are running a 64-bit kernel that is supporting 32-bit applications . That's why the above is misleading -- it makes people think the fact that they have a 64-bit system automatically places them at risk.    The flaw exists in the kernel layer responsible for supporting 32-bit applications running under a 64-bit kernel.   Concerned users should upgrade their 64-bit kernels as that becomes possible, and apply this [temporary fix in the interim](   While waiting for a coherent response to this bug, users should temporarily downgrade to a 32-bit kernel.    Because attackers can exploit this vulnerability in any number of ways, ksplice's claim to have a single, one-size-fits-all solution to prior attacks is misleading.  My advice to Linux users is to intelligently assess this bug, apply a fix as required, and avoid being taken in by commercial interests who would like to pose as the only solution to the problem.  tl;dr:  The simplest interim remedy is the run a 32-bit kernel until a patched 64-bit kernel becomes available. "
"If you are a kernel developer, don't use typedefs. All other folks continue enjoying coding however they like.","To sum it up: typedefs can be a good thing for an application or for library code (although the latter case sometimes leads to a lot of casts if my code doesn't use the exact same types the library uses), but for a kernel that is developed in C by thousands of developers, they are a bad choice.  The reason Linus gives is that they make coding more implicit (in the sense of higher level), while kernel-level programming is inherently low-level, and thus should be explicit. This mismatch is causing a lot of trouble to thousands of other developers, which is why Linus asks kernel developers not to use typedefs.  TL;DR: If you are a kernel developer, don't use typedefs. All other folks continue enjoying coding however they like. "
"Not ""broken"" or ""shitty"" code, so much as a misleading, potentially confusing and inconvenient way to represent data in memory.  But not, technically, actually  wrong .","> Fuck that. If Don Knuth writes some shitty code, it needs to be pointed out to him  For those unfamiliar with PHP, the problem is that the language allows (hell - upon checking the PHP docs on  array()  - even  intends ) for developers to be able to make use of ordered hashes, so it's only ""shitty code"" if you assume hashes shouldn't be ordered.  The problem is that most competent developers  don't  rely on hash tables having a natural, persistent order to their keys in most languages (at least: unless they're specifically using an  Ordered HashMap class, or equivalent), so such code looks dodgy and counter-intuitive to a lot of developers, even if it (apparently) works consistently in PHP.  Moreover, although you can create ordered hashes in PHP the language lacks convenient tools to adequately manipulate them after creation, so they're... sub-optimal when it comes to  convenience .  TL;DR: Not ""broken"" or ""shitty"" code, so much as a misleading, potentially confusing and inconvenient way to represent data in memory.  But not, technically, actually  wrong . "
"drunk me had the out of the box thinking to solve it, sober me worked out the details later.","i think drunk me has access to super creative ways that sober me doesnt.  this was the experience i had. working on a semester project in college, and i was at a roadblock where i had no idea how to solve a problem in a simple way (betting in poker). decided to test the ballmer peak that night (as the scientist part of me would). on my way back, i had two strokes of genius that had out of the box thinking that solved it. wrote a separate section of code with the basic idea and left the rest for the morning. when i woke up, it took be 20 minutes to figure out what the hell i was trying to do in those 6 lines, but it turns out i had the right idea (the details changed slightly).  tl;dr: drunk me had the out of the box thinking to solve it, sober me worked out the details later. "
"it won't work, because the pool is limited, and real currencies have things of worth like other currencies, gold and/or men with guns backing them.","I hate to be the creepy old guy in the room referencing the 90s, but whenever I hear about bitcoin I can't see how it's significantly different to other failed digital currencies like flooz or beenz.  Each of the differences from these seems to makes things worse, except from an anonymity perspective.  You still have those who control significant chunks of the currency (early adopters), but they have none of the responsibilities (legal and economic) that the companies that owned the other currencies had.  I really can't see it being successful, simply because of the massive deflation (against no backing at all) that would be required to turn it into a srs currency, considering that an early adopter could easily legitimately have 20,000 BTC (ie controlling a measurable amount [0.1%] of the market).  While not all theirs, Mt Gox moved 430,000 BTC to protect it [[ - the fact that this is 2% of the total currency that can ever exist (and a much larger portion of the currency that exists at the moment) is equally concerning (it also might not be the entirety of Mt Gox's bitcoins).  It doesn't make sense to use them as a currency at current, instead it encourages people treat them like blue chip stocks (because of their potential to deflate significantly if bitcoin ever becomes a widely used currency) and hoard them.  Their (already inflated) value is only based on this supposition that it will become a currency, and so the prices will tank when people realize that what they're hanging on to numbers that don't have any intrinsic value when people don't want to buy them (or, through extension accept them).  TL;DR: it won't work, because the pool is limited, and real currencies have things of worth like other currencies, gold and/or men with guns backing them. "
"the loop does the same thing every time, adds a constant to sum, so can be optimized to sum = sum + constant.  g++ will do this.","It's awesome that Luajit can generate code like this, however I'm not convinced this particular test case is as beautiful as advertised.  First of all, what are these test vectors initialized to?  Zero? It hardly matters, because in the loop, their value is constant (except for the register r of course).  This allows the JIT complier to completely skip over the branching that could occur in the min/max calls, since the args are the same every time.  Notice that no instructions are generated that can compute min/max.  Second, if you try implementing this in C++ with zero-initialized vectors and compiling with optimizations, g++ actually recognizes that no computation needs to be done and generates no code corresponding to ""distance_squared_to_segment"".  If you instead compile just the test loop as a function of the input vectors (so that g++ doesn't get to assume that the vectors are all zero) you get very similar assembly to the linked article, except that it is actually correct for all inputs.  TL;DR - the loop does the same thing every time, adds a constant to sum, so can be optimized to sum = sum + constant.  g++ will do this. "
"God bless John Carmack.  He makes intense games.  Rage looks a lot like Borderlands in many aspects, but it's dungeon sections are often a very intense Doom-like experience.","Just played quake 1 through -- that was an odd experience!  Seemed very easy even on the hardest level, even those shamblers.  Just don't get hit by them.  Anyhow, that was my main thing in the mid 90's, it was sure fun to play again.  On another note: for the first 3 hours I was mildly disappointed as Rage appeared to be a ""prettier borderlands"" with no bloodwing.  Then I discovered the boomerang thing.  Then it was the twisty little halls, all different.  Then monsters.  Then big fucking monsters.  Then what sounded like Nine Inch Nails.  And.  Fuck me.  It was FREAKING INTENSE.  Not like Borderlands anymore.  Then I went and hid and cried a little.  TL;DR.  God bless John Carmack.  He makes intense games.  Rage looks a lot like Borderlands in many aspects, but it's dungeon sections are often a very intense Doom-like experience. "
"Plan9 is/was awesome (as is 9front & Inferno), but I think it's a bit oversimplified to say the license was the  sole  reason for it's obscurity.","Well, it was released under a strange license initially, but I agree, it's pretty unconvincing. As someone who has used Plan9 & Inferno, I'd say:   License: the initial license & subsequent relicense didn't make any friends initially  The library system is different from Unix; libc & company are very different under Plan9.  although creating file systems is relatively easy, it seems counter intuitive at first to check which file systems are bound where & make sure that everything is where it is expected to be (like, when I first started, I forgot to bind a bin directory for a new server I was running in any of the startup scripts. d'oh).  Very few of the tools that people know from Unix directly translate over to Plan9; there are ports of Python & some other tools, but you either have to spend a large amount of time futzing with tools in order for them to work, or you just bite the bullet and do it properly.  Authentication is quite different; from Factotum to the notion that ""groups are users with other users attached"" to building out FS+AUTH+CPU servers.  Speaking of which, for a long time, the best tutorial on building a stand-alone FS+AUTH+CPU server was written in Japanese with very little English. The documentation has become infinitely better due to Uriel (who collated many sources together) and others.  This all points to another issue: very little of systems administration knowledge is directly transferable to Plan9 (save for things like Shell scripting (kinda sorta), awk, & sed). Even simple things like network setups are different (not more complicated; different).    tl;dr : Plan9 is/was awesome (as is 9front & Inferno), but I think it's a bit oversimplified to say the license was the  sole  reason for it's obscurity. "
"this is a rational idea, but it doesn't scale.  It's not worth my time to clean up litter on the street, but if everybody litters, we'll have a problem.","Yes and no.  This discussion depends on knowing your scale and the context of your application.  I've worked on a number of cloud scale distributed systems that had this seemingly benign excuse made at their creation time.  The application got written by somebody who'd rather be focused on adding new features or who can't be bothered to make it efficient. (all optimization is ""premature"" when you don't feel like doing it!)  While your time might be more valuable than the computer's time at small scale, that logic erodes when your code is contributed to a large scale system.  Your code might be one of a half dozen APIs used in between metal and the user, and it might be executed millions of times daily.  Your code might not even be THAT inefficient.  But in a stack of fair to middling quality APIs, you are contributing to bottlenecks and sloppiness, and so are the other developers in your ecosystem if you allow it to happen.  Performance data of end to end scenarios needs to be the determinant of whether or not your code is inefficient.  I recommend setting performance standards and formally defining what's efficient enough for your team and scenario.  tl;dr: this is a rational idea, but it doesn't scale.  It's not worth my time to clean up litter on the street, but if everybody litters, we'll have a problem. "
"Move the MMU logic for determining real addresses into your program, and have the OS inform the process of where its memory segments are after loading/paging.","I went looking for more information on this since it wasn't clear to me how you'd circumvent the lack of MMU for fork() and process isolation.  Apparently, it really is being done right now in the embedded space (uClinux):  > There are three primary consequences of running Linux without virtual memory. One is that processes which are loaded by the kernel must be able to run independently of their position in memory. One way to achieve this is to ""fix up"" address references in a program once it is loaded into RAM. The other is to generate code that uses only relative addressing (referred to as PIC, or Position Independent Code) - uClinux supports both of these methods.  The PIC approach seems like the right ticket.  So, as you say, ""a fancier function call.""  The process would have some ""segment"" or base pointer values that represent where heap, stack, text areas exist in the universal memory space.  (I suppose those would live in .text and could be manipulated by the OS if paging or relocation occurs.) Then the PIC code adds an offset to that value to get the final address.  This is key since it avoid the need for a fixup table, and other cumbersome mechanisms.  TL;DR: Move the MMU logic for determining real addresses into your program, and have the OS inform the process of where its memory segments are after loading/paging. "
"don't  not  buy into the document-db ""fad"" if you don't understand it either.","However, if you ever want to scale past one DB server it's nice if your data store is scalable. By scalable I mean you spin up a node (VPS, dedicated, whatever), run the DB server, add it to the cluster, and then walk away as your data is automatically shifted around to use the new node. All nodes should be equal. If a node goes down you shouldn't have to bother with finding out which node it was, just add another one.  Sharding doesn't give you any of those benefits, and is just one reason why people are moving to k/v stores from relational stores. Of course if you have a relational data set that you need to query all the time and fits on a single box (easily backed up), maybe relational is the way to go.  Yes Google and Wikipedia use MySQL. If you don't have their resources then perhaps you should go for a system that's easier to scale. Comparing the average company to Google is not really a productive thing to do.  edit: tl;dr don't  not  buy into the document-db ""fad"" if you don't understand it either. "
If married guys with kids are out-earning you it's your fault.  You have all the advantages.,"I don't really notice that at MS - most of the really senior guys there I know are married.  I repeat though - if you are single and whining that your company is giving the married guys a break then you should examine your situation.  You should be able to completely pwn those guys technically if you devote the cycles they are devoting to kids soccer to learning new tech.  Even if your current employer doesn't value this your next one will.  Or, if you are really good, start consulting.  This is much easier to do when you're single because it's a lot easier to live cheap when you don't have kids and a mortgage to worry about.  You have a more flexible living situation because you don't have to worry about yanking your kids around school districts and whatnot. Get established in the right field and you can suck down $200k/year.  I know dozens of guys that did this with kids; it's even easier without them.  TL-DR: If married guys with kids are out-earning you it's your fault.  You have all the advantages. "
Doing static code analysis of javascript is freaky hard.,"Yes that point could be made but a guy named Kurt Gödel already [proved 80 years ago that this cant be done](  also, for JS we have the problem that the language is dynamic and the whole language isnt ensured to be loaded when your compiler runs, try to even figure out a way to start enumerating the states the follwing code can be  function foo() {    someStandardFunc = aDynamicImplementation     // our implementation uses some standard funcs not used at compile time}function foo() {    eval ( $(""some-input-field"").html );}  now you can run both these examples infinity amount of times and you still wont cover all states that the application can be in, to write a test that covers all these infinite states you would need an application that can predict another applications output without executing the code with all possible states, this is considered impossible.  So, how do other languages handle this?In C/C++ you give the application a list of what methods/objects/resources you want to use in the link stage and everything outside this will give you an error if you try to call it, we dont want this as we dont control the compiler-stage, and it would be kinda weird for most web developers used to interpreted/JITted languages.  In Dart/python/ruby/perl the whole standard library is always available to import and run and wont give errors even when you havent told what packages you intended to import before you execute the code.  We could make an argument that there should be some kind of import/include function that both works as linker and as a promise from the programmer not to use methods outside the imports. but Dart allows by design a function not to be compiled until it is called the first time, this would at best case senario trade a big file download for several smaller file downloads and worst case senario is that you trigger a download when a user acts on a UI element for the first time.  That being said, i'm not at the least impressed by Darts output, its still very bloat.  Tl;dr - Doing static code analysis of javascript is freaky hard. "
The guy is a complete joke and should not be in the position he is in.,"I remember actually trying to convey how the Tiger tank's top speed is not accurate. In fact, it's 25% slower. I stated how historically speaking the Tiger had strength in the mobility and range of the tank, and that without the mobility strength, it's ability as a sniper is dropped enough to make some situations impossible to manage. Situations such as moving quickly after being spotted on a hill.  The reply? Very similar to what you said. In fact, I even got a lecture on how balancing a game works. I understand how balance works but once that tank is not properly balanced it throws the balance of other tanks off as well. Now instead of one imbalanced tank, we have more than one, maybe even five or more, all because of a change from historical accuracy to downright stupidity.  I understand the game is not a simulator. It's where I can chill out, let lose some stress and enjoy myself, it's an arcade game. However, crippling a feature of a tank is by no means an enjoyable experience, and I love the Tiger. (Historically speaking)  The guy is a joke and any player of World of Tanks can understand concerning the most recent (EDIT: The up-and-coming one, sorry for miswording!) patch. The guy listens to the lesser skilled players and now an entire tank CLASS (Artillery a.k.a SPGs) must suffer.  TL;DR: The guy is a complete joke and should not be in the position he is in. "
"TDD is not about mindless procedure, but rather about thinking about what makes sense to test, and how should that thing be tested.","Unit testing should be the most important part of your testing suite. The problem is, Rails community (and probably some others) have skewed TDD in a way its painfully useless. The trouble is when people follow this bad TDD mantra without actually thinking about what are they trying to achieve with it.  Unit tests should be behavior tests. Unit tests that are bounded to implementation details are good for REPL, but after that should be deleted from the suite. The only things that need to be tested is the API, the core code at its 'ports' where its interacting with other components. eg. A crypto library only needs table test covering correct/error cases on  encyrpt()  and  decrypt()  methods. We don't care about tests on implementation details, since if we break some for instance some block cipher padding method, the behavior tests of public API will break too. So its already covered.  On the other hand, high-level, e2e testing isn't really providing sufficient coverage and is in fact implementation testing on other spectrum. It is not useful as Unit testing. What if one day you swap out web forms for SPA. All of the sudden you've got 0 useful tests. e2e (system) tests are good for testing common use flows, but should never be a replacement for unit tests. Its an addition.  Also, please don't isolate code you're unit testing. Thats retarded. Whats the point of writing a million mocks? Yes, sometimes you want to mock things, but mocking everything is just over engineered pointless mess. The idea behind TDD is that TESTS run in isolation, not the actual code we are testing. Somehow Ruby community gets this all wrong.  TL:DR; TDD is not about mindless procedure, but rather about thinking about what makes sense to test, and how should that thing be tested. "
comparing the two in the light of immutability benefits seems like a silly idea.,"Meh.  The two approaches are different in ""style"", which is being discussed here, but they are also  wildly  different in abstraction levels.  The first one is very high level, it just looks like when a random Joe sends an email.  Except that the process fuses one-time operation of setting up the ""connection"" (server, authentication, sender address) with the actual sending of the email.  The second one is more elaborate in abstractions (postman, MIME, envelope, stamp, enclosure).  If I just want to send an email, first it is.  If, however, I want to write an email client or some such, things change.  Now, about immutability: the first can trivially be made ""immutability-friendly"", e.g.:  new SimpleEmail( ""Host"", 456, // Should be optional ""Sender"", new ToRecipients(...), ""Subject"", ""Msg"", .Send();  tl;dr: comparing the two in the light of immutability benefits seems like a silly idea. "
The keyboard shape can also affect the sound if it causes echoes.,"For what it's worth, I had a [CM Storm Quickfire Pro]( I got it because it was cheap and I didn't have much money at the time. It's kind of a shitty mech to be honest. Horrible media-key layout and just super-bulky.  I just replaced it with an [Ergodox]( They both have MX Browns but the Ergodox is  much  quieter. Turns out that the sound from the CM Storm was mainly because the giant bulky plastic case was serving as an echo chamber of sorts. The Ergodox is solid layered acrylic all the way through and has no real bezel to speak of, so it's much quieter.  tl;dr: The keyboard shape can also affect the sound if it causes echoes. "
"I once added some features and fixed some insanity in Firefox, they weren't happy.","Once upon a time I sent patches to Mozilla, it was fun because:   First you argue with some idiot who has no idea about programming, but doesn't want to admit that, so they just keep pushing your bugs down   Then you hit on a political or ideological nerve and then someone dev says they don't like your patch   Then you try and get them to discuss it, they don't reply   You track them down on irc, and deliberately and painstaking setup logical arguments, and explain to them they you would just like to weigh the appropriate arguments for and against such a feature, in a clear minded and non-political way - and they kick ban you from irc without any explanation, despite extremely conscientious efforts to ignore them baiting you to say something that they are just waiting to kick-ban you for.    tl;dr I once added some features and fixed some insanity in Firefox, they weren't happy. "
"your ""nope"" comes from a place of ignorance (possibly stubborn ignorance), methinks.","Nothing is ""just a tool"" (well, maybe you for making such a silly assertion).  The Hubble Telescope opened my mind to more possibilities about the vastness and make up of space.  An electron scanning microscope has opened my mind to more about the make up of the natural world at very small scale.  Learning French, German, and Spanish opened my mind up so that I thought about English and human communication differently.  Learning a pure functional language opened up my mind to a different way of programming than OO.  OO opened up my mind to a different way of programming than simple methods/scripts.  None of these experiences has made me any smarter, but they all made me think differently, opened my mind to a different way of looking at a problem or the world.  TL;DR, your ""nope"" comes from a place of ignorance (possibly stubborn ignorance), methinks. "
Method syntax sucks for SelectMany unless you are using the single argument version that is basically only good for flattening trees.,"For anything complex, SelectMany gets really nasty without LINQ:  Here's a contrived example (for simplicity.)  I have these sets:  var people = new[] {""mr. green"", ""mrs. white"", ""miss scarlet""};var weapons = new[] {""knife"", ""pistol"", ""candlestick""};var rooms = new[] {""kitchen"", ""conservatory"", ""library""};  What I'd like  string[][]  giving every permutation of these:  {{""mr. green"", ""knife"", ""kitchen""},  {""mr. green"", ""knife"", ""conservatory""},  {""mr. green"", ""knife"", ""library""},   // etc...  So 27 possibilities.  Now here's the query with LINQ:  var res = from person in people          from weapon in weapons          from room in rooms          select new[] {person, weapon, room};  Pretty understandable and pretty intuitive.  Here's the same query using method syntax (and this is with me trying to make it legibly formatted):  var res2  = people.SelectMany(                        person =&gt; weapons,                         (person, weapon) =&gt; new { person, weapon } )                  .SelectMany(                        personWeapon =&gt; rooms,                         (personWeapon, room) =&gt;                             new [] {personWeapon.person, personWeapon.weapon, room});  TL;DR - Method syntax sucks for SelectMany unless you are using the single argument version that is basically only good for flattening trees. "
"please, don't tell me how great Haskell is,  show  it to me instead",">Some problems are more suited to imperative programming, some to functional programming.  And which is which? The proportion of evangelists to programmers in Haskell is way too high. All I see is a bunch of articles showing how different it is, but I have yet to see an article showing how it's  better  than imperative languages.  If not better, at least show me a project that actually works and uses Haskell. That's how I started using every language I use on a day to day basis, I found a project that interested me and it was written in that language.  Where could I find some examples? The best I could find by googling it were sites like [this]( and [this]( which is absolutely nothing, compared to all the code you can find in imperative languages.  TL;DR; please, don't tell me how great Haskell is,  show  it to me instead "
"Competency matters a lot, don't put all the blame on the tool.  That said, not all tools are equal.","The analogy is exaggerated to help zoom in on a specific point, but it's still a somewhat useful analogy.  I've worked in PCI / HIPAA environments where your choice of software is pretty restricted (because everything has to be pre-authorized before you can use it).  The lesson I learned — Even if a tool seems  really  bad at first, if you use it constantly for a year, you can find ways to be much more productive than your coworkers who are using the same tool.  Nonetheless, maximum efficiency with one tool can be very different than maximum efficiency with another tool.  TLDR — Competency matters a lot, don't put all the blame on the tool.  That said, not all tools are equal. "
"It's not that I don't believe these points, but show me the data. Data is king.","All four points seem valid, but what I'm missing from this is data. Actual numbers.   Memory usage The easiest to account for in terms of bytes and memory bandwidth (but not easy). More work probably needs to be done in Java to reduce number of objects (and fight permgen/nursery stuff. This can and has filled books). Using more memory also means you'll run out of NUMA local RAM, and using non-local RAM has measurable impact (I only have numbers I can't share, sorry).   GC There are plenty of anecdotal evidence to exactly what you say. Less actionable data though. If you have a fragmentation problem you may in C++ have to work with allocators for some of your data (or some similar but less generic solution to the same problem).   Language spec Is there actual data on how much speed is gained in actual compilers? (not the ""sufficiently smart compiler"" you also mentioned).   Execution model (and dynamic recompilation) Same here. Any actual data with actual applications? A/B testing? C++ applications also do have some form of dynamic optimisation, but it's from the CPU branch prediction. Not as advanced to be sure, but measurable.    Let's say I'm going to write an application and have the choice between C++ and Java. Now let's say  all else being equal , what language suits me best in terms of performance for my application? Yes, there are other factors, but what data can I lean on to say C++ or Java  for performance ?  Google had one (a bit criticized) study. But it was actual data. Something that can actually be used to reason about for what it is.  tl;dr:  It's not that I don't believe these points, but show me the data. Data is king. "
Stop worrying about the nitty gritty specific details of what students learn and make sure they acquire skills for life long learning.,"Can we keep the number of concepts taught to new programmers down to a minimum please?  IMHO Scheme is about as high level of a ""first programming class"" as should be taught. And even then, it is easy to mess up teaching Scheme, elitist functional programmers who think ""everything is obvious"" including lambdas and closures, for a programming 101 course, can turn students off of functional programming for life!  (This nearly happened to me, thankfully during the last week of class I had an epiphany about WTF was going on and recovered, I can't say the same for the majority of other students in the class...)  Throwing ""my favorite language feature"" into the mix doesn't help things.  Now all that said, it'd have been nice if Pattern Matching had been taught someplace in my curriculum. But going to college is about learning how to learn, not necessarily about what you learn.  tl;dr: Stop worrying about the nitty gritty specific details of what students learn and make sure they acquire skills for life long learning. "
"There are no ""standards"" in software development. There are only solutions which seem best (or least bad) for the project you are working on.","No. You get me wrong. I'm not advocating the use of a singleton, i just give some downsides of the ""passing-instances-around"" method.  See, that's a little wrong with software development today. Everybody reads books, reads blogs where some people who do mainly write blog articles advocating the use of ""the way"". ""The way"" changes with time, but it's always ""the right way"".  Software development knows no definitive solutions. Every solution has pros and cons and the biggest anti-pattern of all anti-patterns is believing that there is one right way. Using method a or method b depends heavily on your project and your workforce, and every solution has to be discussed with all aspects in mind. Most PHP ""standards"" and ""best practices"" come from projects like frameworks, or libraries do do $thing. In short: Software which is used in many other projects.  However, things like SaaS projects are a entire different thing and you often write non-reusable code there, because you ain't gonna need it elsewhere. It's a different mindset. Entirely.  TL;DR: There are no ""standards"" in software development. There are only solutions which seem best (or least bad) for the project you are working on. "
"There's more that goes into hash storage than just crypttext, so you cannot query on modern hashes.","Querying a database based on a hash precludes effective secure hashing. In modern hashing algorithms, the output of the process is a string that contains, in addition to the crypttext, an identifier for the algorithm used, the salt used, and the number of rounds used. The former is to allow you to progressively upgrade your hashes over time, without having to store an additional field indicating what algorithm is used. The salt is so that you can have individual salts per hash. The rounds is so that you can increase the computational difficulty of hashes over time as needed.  When you wish to verify a cleartext password against a stored password hash, you query the hash from the database using an unencrypted identifier (usually a username or email), and then pass the hash data and the cleartext into a verification function which reads the algorithm, salt and rounds from the hash, and hashes the cleartext using the same parameters, then compares the output to the crypttext.  tl;dr: There's more that goes into hash storage than just crypttext, so you cannot query on modern hashes. "
"From a programming perspective, speed and [parallel multi-core execution.](","What's New in CPU's is a bit vague since the assumption might be x86 but a lot of things that x86 couldn't do in the 80's was being done with other processors.  Pre-emptive multitasking is one huge game changer which x86 was incapable of but processors from the likes of Motorola (6809, 68000 (Amiga), etc) were. Also obviously mainframe processors.  Multi-Core and [Multi-Processor]( is also new in x86. The effect will depend as well. Applications may not even be aware they are changing processor cores but the OS programmers certainly need to change the way they write their task schedulers.  RISC isn't really new (eg. ARM) but it is to x86 with the introduction of [CISC]( on [RISC]( Compilers may optimise for CISC instruction only to have the processor decoder convert them into RISC instructions before execution. Not quite sure how well a poem would rhyme written in English only to be converted to Japanese to be read...  But most things, from the application programmer's perspective is just speed. The old processors were Turing Complete just as the new processors are. They can achieve the same outcome regardless of the technology differences.  I suppose another major change which be virtualization (hypervisors) and sandboxing where an operating system can run inside another operating system ""believing"" it is the only system running and has full access to the hardware. The sandboxing aspects is where programs know they are part of a bigger system but have their access to the rest of the system (RAM, execution, etc) is limited by hardware controlled by the OS. [MMU]( come into play here as well since the memory that an application thinks it is addressing may not be the memory it is actually addressing.  TLDR; From a programming perspective, speed and [parallel multi-core execution.]( "
the myth that a sufficiently smart compiler is a requirement or would even make much of a difference today (?),"> It's not really feasible to measure it across billions of different scenarios.  You don't have billions of scenarios. And assuming that you do your optimizing compiler can't have much of an effect anyway, at least according to the presentation.  > You know, if it is free.  I think the point is that it's not free. It's not even close to free. It only appears to be free because you can ignore the costs that this has on the infrastructure, and particularly on the [optimizing] compiler. If you think about the complexity of the system holistically, there are actually mountains of [unnecessary] complexity here that aren't necessarily worth paying for any more.  That's an interesting idea.  After all -  ""Simplicity is prerequisite for reliability."" - Edsger Wybe Dijkstra  As well as portability, usability, scalability (down and in as well as up and out) and a whole family of other *ilities  tl;dr: the myth that a sufficiently smart compiler is a requirement or would even make much of a difference today (?) "
"How do proper sync app know when things get deleted, they have been deleted as opposed to never there, and what happens on the first sync?","So, I've been setting up a  Horde  for Hordecalled Trean. Trean is a bookmark manager for Horde, but is relatively new and as a result seems to have no browser support, other that a bookmarklet.  Figuring it would be a nice chance to work on my Python skills, I figured I could write a simple sync extension for Firefox (in Python, thanks to  Trying to do things the 'right' way, first I coded up a mini abstraction class for the data stores, and at the momenthave only been working with a temporary one that is merely a dictionary. You can see that code [here](  With the back end taken care of, I started to work on the sync algorithm itself. Here my problems came, Googling for [Synchronization algorithms](yielded results that are not quite useful for me, so I tried to figure it out.  The first issue I had with the stateless design I first used was deletion. The code treated deleted items like items which had simply been added in one store, but not in the other. As a way of working aroundthis, I implemented deltas. The code will accumulate all the changes in a delta, save the current store contents as 'previous' and then update them. By doing this I now know when things get deleted as opposedto added. However, this still leaves me with a problem. For the first sync, I have no 'previous' to work with, therefore if two stores were in sync, and something failed, and an initial sync had to be performed,items that had been deleted in one would reappear in the other. I really can't think of a way around this, does anyone know how SyncML or such handles it? Or more then likely, is my whole design FUBAR'ed?Code is  here .  NOTE: I'm probably abusing a few terms here, please correct me.  TL;DR:  How do proper sync app know when things get deleted, they have been deleted as opposed to never there, and what happens on the first sync? "
"Children need to be given toys not industrial tools. If you've never had to explain recursion to a 14-year-old, who just tackled the concept of functions, shut up.","Amen to that. The strength of BASIC is that it's so ... basic. The most difficult concept to grasp in the entire language is gosub. Everything else is a simple dumb operation. No pointers, no functions, no recursion.  It is possible to write in other languages while restricting yourself to BASIC's instruction set, but... Well, you can't actually. Anything more basic than adding two numbers goes through functions and classes and a ton of advanced concepts.  I won't teach BASIC to anybody over 12-13, though. At that age they can grasp all the more advanced concepts in C and even some in C++ (though I'd hold explaining templates for after they hit the legal age of consent. Otherwise it would constitute statutory mind-rape).  tl;dr: Children need to be given toys not industrial tools. If you've never had to explain recursion to a 14-year-old, who just tackled the concept of functions, shut up. "
DAG topological sort time complexity ranges between O(n) and O(n^2) depending on how interconnected the nodes are.,"Well, my initial outlook wasn't entirely accurate. There is apparently a [wiki page]( dedicated to the taskmaster. The vital sentence is near the bottom of the  Running the DAG  section:  > It's worth noting that the Jobs module calls the Taskmaster once for each node to be processed (i.e., it's O(n)) and the Taskmaster has an amortized performance of O(n) each time it's called. Thus, the overall time is O(n^2). It takes a pathological DAG to achieve the worst-case performance, but it occasionally happens in practice.  It still seems like the performance should be O(n) instead of O(n^2). I think I might write to the dev group asking if this is achievable, and how it could be done.  EDIT: Final Note:I sent an email to the scons dev list. After about ten minutes of further thought, I can guess what they'll say back. The problem with graph algorithms is that their time complexity is dependent on both the number of nodes (V) and the number of edges (E) in the graph. Thus, using one parameter (n, the number of nodes) is an inaccurate simplification. Instead, the time complexity for a topological sort of a graph is best stated as O(V+E).  Now, what is the worst case order of the number of edges in a DAG? Every node can be connected to any node before it in topological order. Hence, the time complexity is: O( sum(0..V-1) ) = O(V^2). Thus, a topological sort for a graph with many edges approaches O(n^2) and there's nothing that can be done about it.  TL;DR: DAG topological sort time complexity ranges between O(n) and O(n^2) depending on how interconnected the nodes are. "
"is that Adobe flex / air is a framework for flash, but calling it just 'flash' is incorrect","this is actually really confusing to explain, so much that adobe has a video explaining the differences haha  There are three technologies, flash, flex and AIR  you can combine the two, so you can have a AIR application written in flash (using Flash CS4 for example) you can also just have a flash program written in flash using Flash CS4  but with flex, you can't import the flex libraries into flash CS4 i believe. so with flex, you usually use something like Flash Builder 4 (or you can use notepad as they provide a command line compiler), and with that you can make Flex apps (run in the browser) or AIR apps (run on the desktop).  No matter WHAT you make however, it all generates a .swf file. Flex apps in the browser use the flash player plugin like just plain flash programs generated from Flash CS4 while AIR generates a .air file, which includes a .swf and makes a nice .exe file that calls the adobe air runtime to run the program  Its all very confusing! But i think the tl;dr is that Adobe flex / air is a framework for flash, but calling it just 'flash' is incorrect "
"To fix the problem, give scientists credit for verifying the work of other scientists.",">If a journal requires that all software used in a research article be thoroughly tested and open for verification, this problem will go away.  This problem will go away when the scientific community sees the value for verifying all research (not just computational). How many articles are published in nature in which the investigators simply verify that a previous work's results are valid and reproducable? None. You can make software (and experimental methods for that matter) as open as you want but as long as there is no incentive for verifying them, it won't be done. Subsequently, you can publish something on an obscure research topic which nobody is interested in verifying and it will remain as the valid body of knowledge until the end of time.  TL;DR: To fix the problem, give scientists credit for verifying the work of other scientists. "
Two hours of code at an on-site interview?  No dice.,"Having been both interviewer and interviewee on multiple occasions:  I'd be careful about any job interview that involved a two hour code test.  I had an all-day at one point where I was grilled by 5 different people, NONE of the sub-interviews lasted more than 90 minutes.  Well, the crazy Russian professor they had got into large dataset manipulation with me, but it wasn't a test so much a discussion.  As an interviewer I would not expect to get more than 30 minutes of concentrated effort out of a candidate.  Strange place, ridiculous pressure, etc.  Burning someone like that for two hours seems wasteful and insult.  I had a 6 (grew to 10) item questionnaire I ran them through, but if it took them more than 30 minutes they weren't getting a call-back.  tl;dr:  Two hours of code at an on-site interview?  No dice. "
You being a serious professional = you meeting committments = trust = less management oversight,"Managers want estimates because that's how they ""manage"" you. It's their craft. What they really want is a milestone that occurs within 1-3 days that they can check off. That way, they don't get too far in the project before finding out the project is off track. That's it. They are not out to get you, or make you look bad, or play CYA. They just want to have a confident answer when someone asks ""Is the project on track?"". That's their job.  Yes, it's annoying, but rather than combatting it with fat estimates and sandbagging, try building some trust instead. Give your manager some confidence that you will deliver high quality work, on time, without having to provide daily milestones.  So, guess what you need to do? You have to deliver. No fucking around. Get it done. Get it done early sometimes. Test your code thoroughly and document what you need like a big boy. Don't whine about everything that trips you up. Deliver a few times and you will build huge trust points that you can then use to make your life easier. You'll find that it's easier to control scope when people trust that you'll get things done with some extra time to spare. Estimates will be easier because people trust that if you commit to a 2 week deadline, you'll hit it.  Tl;dr: You being a serious professional = you meeting committments = trust = less management oversight "
"Laravel can  validly  call its ""Proxy"" objects Facades all it wants. It makes absolutely no objective difference.","I think you missed the general thrust of my comment.  It really doesn't matter WHAT it is called. The name makes sense within its context even if it isn't the same thing as a traditional facade. There is no central authority controlling what things can be called. The name is valid.  The problems caused by having a name which might be confused with a traditional pattern can  very easily  be overcome with a simple 5 minute conversation to return to a common understanding within the project and get on with it.  Names get called the wrong thing all the time. MVC itself in the context of web application frameworks differs WILDLY from its original intended meaning. But so what? A simple conversation resolves that problem, why then do we need to get so precious over the Facade pattern? The only reason I can see is to find some ground to attack Laravel and its author. Where I'm from we refer to this as the ""Tall Poppy Syndrome"".  tl;dr: Laravel can  validly  call its ""Proxy"" objects Facades all it wants. It makes absolutely no objective difference. "
It's not mathematics itself that's limited in describing models of computation. It's just someone's understanding of mathematics.,"When you say that something ""makes no mathematical sense"", what you are really saying is that either the right mathematical model hasn't been constructed for it yet, or that it really doesn't make any coherent sort of sense at all. I don't think programs of the latter sort would be very useful, so most of the time you probably mean the former, though in most cases what you are actually saying is that you aren't  aware  of the right mathematical model.  Mathematics is precisely the field that describes  how  things make sense, and how the implications of the sort of sense that they make play out. Mathematics, logic, and computation are all fundamentally related at their foundations. A programmer doesn't typically have to understand all the mathematical models for the tools they use, but they'd better hope that they do make mathematical sense, because the alternative is that they're most likely  wrong  in a fundamental way.  By the way, very early in the history of writing computer programs, computer theoreticians were concerned with modeling the typical programs of the day, which consisted largely of updating storage cells in-place and imperative flow control. They came up with a new branch of mathematics that models this quite well. Modern purely functional languages take advantage of the kinds of mathematical techniques that grew out of that field to model in-place update and imperative flow control.  TLDR: It's not mathematics itself that's limited in describing models of computation. It's just someone's understanding of mathematics. "
the best Agile implementations don't talk about implementing Agile.,"Here's the challenge, the type of software development that small startups use (mostly, a shut up and code, everyone talk to eachother, product manager sits right next to me type of mentality) actually works really well.  Unfortunately, because it works really well for startups and small companies, larger companies take note and try to replicate it, often with terrible results since the aforementioned mentality that makes it work produces an allergic reaction in most large enterprises.  Because it's so terrible when transitioned to enterprises, consultants must be brought in to ""Scale it up"". The problem is that consultants can't simply tell everyone to shut up and code (who pays $200/hour for that?) so they need to sell a packaged and reproducible process. Often with terrible results.  Now our friend the consultant is in a terrible position, he's got a check for $20,000 from the past two weeks but his client is on the phone complaining about how terrible this Agile business is. ""You're not following the process closely enough"" he says in desperation before moving onto the client.  Source: Former Agile consultant (by accident) turned BI Consultant.  TL;DR the best Agile implementations don't talk about implementing Agile. "
How can I integrate regular Django apps and manage their content with Wagtail?,"This was really useful.  Wagtail seems to be progressing well.  I evaluated CMS options for Django at the beginning of this year and, after narrowing it down to a choice between Django CMS and Wagtail, I ended up going with Django CMS.  The thing that made me choose Django CMS -- and I would love to hear more about Wagtail's approach for this -- is the ability to integrate regular Django apps into the CMS.  Django CMS has this concept of an app hook which allows you to attach a Django app to a CMS managed page.  The reason why I would want this is because it allows you to keep your database tables clean and well structured by using regular Django best practices in your models. Instead of adding fields to a Page object (which you can also do in Django CMS), you can integrate a separate Django app/model which allows you to loosely couple your database tables and utilise the benefits you get from the Django ORM and an MVC-like design pattern.  [This]( video gives an example of this pattern and outlines some of the benefits.  For me, a good CMS built on top of Django should allow you to utilise all the things that Django does well already.  tl;dr How can I integrate regular Django apps and manage their content with Wagtail? "
"It's impossible to ever make any argument that ""Dynamic typing is always better"" or ""Static typing is always better"". One can only compare specific implementations.","> We aren't comparing languages at all. That's where the dissonance is. We're comparing one feature of families of languages. I'm loathe to use a car analogy, but we're comparing diesel and petrol, you're comparing Porsche to Volkswagen.  The problem is that ""static typing"" is such a broad and loosely defined concept with many completely different implementations.  Cobol, C, Java, and Haskell are all statically typed languages, but are all so wildly different in implementation that there is almost nothing in common between them. It's simply not possible to debate the pros and cons of type systems in such a broad way.  To take the (much hated) car analogy further, imagine if this  was  a thread discussing Petrol vs Diesel. Somebody might say ""But Diesel's are dirty, smelly, and slow"", in which a valid response might be ""Actually, Volkswagen makes fantastic clean diesel cars that are quite fast"".  If I was in such a thread, it would not be possible to defend Diesels in the generic case - many variations  are  dirty, smelly, and slow. But this doesn't write the concept off entirely - there are good implementations of Diesel as well.  My argument is not, nor has it ever been that all forms of static typing are good - and in fact such an argument is not possible to make, since the term is so broad and loosely defined that there is almost no common denominator between such languages.  tl;dr It's impossible to ever make any argument that ""Dynamic typing is always better"" or ""Static typing is always better"". One can only compare specific implementations. "
"If you're worried about keystrokes, its time to freshen up that resume so you can wind down a bit and enjoy what little time we have here.","I'm a programmer. (By profession, not just a hobby)  Yet, I've never understood this obsession so many coders have with programs like vi and emacs, and this constant urge to reduce keystrokes.  Do a lot of coders work in an environment where they're required to churn out code as fast as their fingers will allow them?  I got into programming so I could wear dockers to work, take long lunches, and be able to tell my bosses, ""It'll get done when I'm finished with it.""  If I had to actually  work  all the time, and my efficiency was an issue, I'd just as well quit and become a security guard.  The pay wouldn't be as good, but I'm not about to worry about such minutia when I'll be lucky to get 70 years on this planet.  tl;dr: If you're worried about keystrokes, its time to freshen up that resume so you can wind down a bit and enjoy what little time we have here. "
About time they got rid of the damned Start menu.,"ITA: A lot of FUD.  Posting from my Mac  If you're a productive Windows 7 user, you probably have all but abandoned the Start menu. Why search through app folders when you can just start typing the name of what you're looking for, whether it's a file or an application, and hit enter? The Start menu convention stopped being useful the second Vista came out. All that little round button does for me now is open up the search box.  Really advanced system settings, the thing that this guy claims will make IT pros lives difficult, don't need to be in Charms or Settings or whatever they're calling it. Windows IT people will be using Powershell to automate IT tasks.  For those advanced-but-not-IT-advanced settings that he's missing may be more difficult to find in some cases, but this is something that you deal with on Mac anyway. If I want to customize Finder sufficiently I have to use a third-party app or open up a shell prompt and issue some  defaults  commands.  Regarding the pushing of MS services, when have we ever not had this? Windows XP bugged us to update MSN messenger, update Windows Media Player, upgrade to Office from Works, and don't forget the shitty weather widget on Vista, etc. People will just hide/disable/delete them as they always have in favor of installing better alternatives. In the case of the Messaging app that aims to handle a lot of social functions, the solution is to provide an app that is better and more conveniently integrates with your online services.  And does it really matter whether you use Google, Yahoo, or Bing for looking at Finance info? Really, is there some sort of massive revenue coming from these things that I'm unaware of?  TL;DR  About time they got rid of the damned Start menu. "
If you must shorten URLs use your own service otherwise you might get screwed over later. Better advice: Don't shorten URLS.,"Services like bitly are a serious problem because they really do break URLs. Their anti spam measures are a total joke. I've had numerous legitimate websites blocked by them due to some 'mistake in their filter'. The most recent 'mistake' happened after I happened to criticize their new design as being a step back in terms of UX.  Think of it this way. I have several bitly shortened links all over the Internet. Suddenly, they decide that my links are spammy (they are not btw...) and they disable them or start pointing them to a holding page. Broken...  Not only that, my previous run in with them was because they were using a 3rd party service called SURBL which has the most retarded way of marking sites as spam namely if someone (anyone..) decides to send out spam emails mentioning your domain then you get blacklisted. Not only that, but they have the most opaque whitelisting process on earth. (I had to go detective and find out who the main guy behind this list was.. Find his personal details through linked in and email him at his personal address after 20 days of no response...).  TLDR; If you must shorten URLs use your own service otherwise you might get screwed over later. Better advice: Don't shorten URLS. "
"Code will be complex if it's subject matter is complex. Ideally though, the code should avoid adding complexity as much as possible.","It certainly isn't a requirement, bit it most certainly is a worthy goal. Many of the examples of complex software mentioned in this article wouldn't be possible to develop in feasible time frames and budgets if there isn't some ""simplifying"". Software is written in teams. Teams of people who can't possibly think and abstract everything the same way. People need to be able to work on each others code. Programmers come and go, so new programmers constantly need to learn how to work with code written by programmers that might have left the organization. There a dozens of good reasons not write simple code. Simply put, simpler code is almost always better then complex code that achieves the same thing.  But that isn't really the problem. Sometimes code is complex because what it is trying to do is complex. It is one thing to learn how to use a programming language and call yourself a programmer. It is a completely different thing to apply that programming knowledge to a complex subject and write meaningful applications. Bitch about complexity vs simplicity all you want, but if you don't understand linear algebra or geometry for instance, you are not going to make the greatest 3D graphics programmer out there.  tl;dr: Code will be complex if it's subject matter is complex. Ideally though, the code should avoid adding complexity as much as possible. "
I'm paranoid and run my own infrastructure to handle my data.,"> remote server for offsite backups  I just want to throw my two cents in - from my experience cloud services are extremely temperamental. Amazon has even been known go down from time to time - rare yes - but the statistical probability of a problem with a cloud service when you are finishing your thesis or other important publication is really quite high. I'm not saying you can't use dropbox/google docs ect for collaboration but you should really invest in a RAID. I bought a 4bay readynas about 2 years ago with 2TB drives. I have yet to replace a single drive and plenty of space. Even having a 2bay drive would suffice for just documents. The statistical probability of two drives failing at the same time is extremely low. Oh and just don't go to best buy and pick up two drives - go to Amazon and read the reviews. If people are saying the drive failed in a couple of weeks - it's probably best to not buy that.  I have heard countless stories about people's hard drives failing. And having just an external drive is not good enough - as the drives that are shoved into the enclosures usually suffer from short life times.  I setup backuppc to backup all my stuff automagically - which also gives me revisions. For my technical writings I use dokuwiki with some magic plugins such as PDF export - this not only gives me the ability to collaborate on something, revisions, but since it's all flat files everything is backed up by backuppc to my NAS.  tl;dr; I'm paranoid and run my own infrastructure to handle my data. "
"No philosophy in the universe is going to counter bad coders. 
caveat: Immutable objects help. A lot.","I've been observing first-hand a massive debugging effort on a DI-structured set of web services. Turns out, testing with mocks only tests for stuff you think to test for. They  still  haven't tracked down all the bugs, and all the unit tests still pass.  Is testing necessary? Yes. Will mocks and DI solve all your problems? Oh hell no. I'll take reasoned and well-thought-out code design over mocks/DI any day.  This drank-the-coolaid approach to DI/mocking every single thing has, from my observation, mostly resulted in the most convoluted, hard to understand and debug code I've ever seen in my life.  tl;dr: No philosophy in the universe is going to counter bad coders. caveat: Immutable objects help. A lot. "
Companies have thousands of direct applications and personally I've needed some type of connection to get noticed regardless of how weak that relationship is.,"> y method for applying to jobs has always been to check software companies near my area or an area that I want to work at and apply myself through their careers page. Am I doing something wrong by avoiding recruiters? What are the advantages of going through one?  Are we talking just about 3rd party recruiters?  Most of the recruiters I've worked with are members of the company themselves.  I've always directly applied, but going through a person is the best way to get noticed.  It's a terrible state of affairs, but you have to go through a person to be competitive.  I've applied for jobs directly and gotten nothing knowing I was a good match against a job description.  When I'd try going through a person, I'd get a phone call or email soon after with an interview.  I've always hated this site, but lots of companies use linkedin internally.  My friend ""introduced"" (it's a feature) me to a recruiter at a company and she got right back to me.  Another time my friend ""introduced"" me to some guy he barely knew from some conference.  The guy gave me the phone number of a person in the HR office and emailed her saying I was a friend of a friend.  Another interview.  Both of these companies ignored my direct applications, but gave me interviews.  One flew me out.  A third time a friend introduced me to a recruiter for hulu (again all these recruiters worked directly for their respective companies--not headhunters).  My friend had failed the interview and we didn't think his recommendation would naturally carry much wait.  The recruiter, however, seemed eager to interview me.  I interviewed over the phone a couple times, flew in for an interview, and got the job.  They, too, had ignored my direct application.  From my experience, you need to have a link to the company regardless of how weak that connection is.  If you don't have one, make one.  TL;DR  Companies have thousands of direct applications and personally I've needed some type of connection to get noticed regardless of how weak that relationship is. "
India is slowly becoming the Foxconn of software development,"Worked in both India/US, head hunters and Interviewing in India is particularly a nightmare because:   India has a huge fucking market for talent. Only a miniscule of this talent is employable, but nearly every one can fake a resume.  So head hunters always deal with numbers -- treat you like shit, call you whenever they please, ask you to travel and take interviews without prior notice.   Zero fucks will be given about your privacy. Once you post your resume to any HR, you should consider that info as public. And yeah, you must give them your current salary details, without which you won't even be considered for the job.   You will need a separate cell phone -- if not expect non-stop calls when you are in office.  You will need a good spam filter -- Gmail won't cut it. You really won't have time to worry about offending emails like these.  Most of your interviewers will already know your salary and your position with your current company thanks to these fucks. So passive aggressive questions and dick measuring contests ensue in the name of interviewing.    On my first interview panel after a long gap and coming back from US to India, my manager found my very own resume. I applied to our competitor just a couple of days before!  It's not just about Indian companies -- most US companies just violate HR policies at will in India even though they are good employers in the US(I'm working for one). The situation has become really bad in India after the recession, recently there is a string of suicides by IT employees.  TL;DR India is slowly becoming the Foxconn of software development "
Don't use your root credentials when using cloud services. Create sub accounts with limited permissions and use those.,"Sorry, but this is bugging me:  > Turns out through the S3 API you can actually spin up EC2 instances  No, the S3 API cannot spin up EC2 instances . What is actually going on is that all AWS services (EC2, S3, DynamoDB, etc) use common credentials, which the author checked into source control. Unlike some other cloud providers, such as Google, Amazon doesn't require you to individually enable each and every API you want to use. So when you sign up for AWS, you can use S3, EC2, and everything else out of the box.  If you want your credentials to have limited access to AWS services (and this is a good idea, to reduce the impact of this kind of breach), then you should create a separate user account with IAM, lock down its access to just the services and resources it needs, and then use that account for everything.  TL;DR: Don't use your root credentials when using cloud services. Create sub accounts with limited permissions and use those. "
You didn't know that I have 30 years experience in personnel selection for statistics.,"Tell me please, how many job candidates you have interviewed and hired out of the Stanford MS program?  How many from other programs at the Master's and PhD level?  When you get to 500 candidates interviewed then you can be excused for telling me something that I already know, and that is immaterial to my prior comment.  When did you last conduct on campus interviews at Stanford?  For me it was near 10 years ago, because although it is a great program, their graduates are ""stickey"" to the west coast, and I can't keep my best west coast hires on the east coast for more than a few years.  How many letters of recommendation for high school students did you write in the last 5 years for Stanford, Harvard, and Princeton admissions review?  When your high school interns are gaining admission to Harvard and UPenn, and are competing for Thiel Fellowships, please let me know.  How many of your colleagues have you written letters of recommendation for as they pursue their advanced degrees at schools like Stanford?  How many of your peers from graduate school teach at schools like Stanford?   Are tenured there for life?  For Stanford, I count one of each, and they are the best in their domains, so I know where the bar is at Stanford.  In the meantime, note that I said ""Look for programs that have consulting courses"", not ""You have the qualifications for Stanford"".  TL;DR:  You didn't know that I have 30 years experience in personnel selection for statistics. "
"Network-services arent really whats beeing exploited on the consumer market, its almost always client application exploits, windows have less local security after this has happened.","Actually, no.  Most exploits nowadays, and that is discussed here are selldom against network services but against client applications, the security that matters then is not firewalls and protecting your ports, its the local security, how easy is it to gain ""root"" and run code.  Unices has the added protection of not allowing the processes to gain access to ring0/kernelmode/realmode without the proper authorisation. In windows they are trying to do this with that ACL popup that asks you if its OK to run this with administrator auth, but they arent really there yet as this popup is 1. up to the user to enable or disable and 2. Not covering all applications and dlls (for example. run32dll is whitelisted...), its more or less like adding a flag to su that makes it not ask for the root-password..  Windows further have alot of built in and thirdparty applications that requires admin rights to execute properly and thus are increasing the risk of bad code that are exploitable, combine these applications with user-interaction through UI and you have a situation that is impossible to make as secure as a system that is designed to protect different users data from eachother.  tl;dr: Network-services arent really whats beeing exploited on the consumer market, its almost always client application exploits, windows have less local security after this has happened. "
"Bar a few nit-picks, Swym rocks. Congrats. 
 Edit: Combats with Markdown, emerged victorious with the \ weapon.","Code readability and maintainability are becoming almost the  most  important factors for a programming language, so kudos for placing them first.  That said, some observations: 1.> So a-b means subtraction, as does a - b(c). But a.b-c and a-b-c and a-b(c) are function calls.  This is not just weird; It is, from my experience, is a recipe for frustrating debugging sessions. Changing the meaning of an operator if it's used more than once is quite unintuitive.  2. Making 'for' a function - wouldn't that make the for loop much slower compared to other languages (where we simply use the assembler looping instructions)? I'm not an expert here, but I'm guessing that given all the stack operations that a function call entails. Though, if you implement tail-recursive functions this inefficiency (I guess) would go away.  3.> Greeting:[""hello"",""hola"",""bonjour""];> Greeting.SortedBy{.length};  That doesn't give any output here. (Firefox 3.5.6 Windows, in case it isn't this way everywhere)  4.> And now for something completely different (different from every other language, that is). In most languages, an expression must return one and only one value. In Swym, an expression can return several values!  At first, it looked like just Perl's .. operator and similar to Python's :, but then  > writing 10 + (1..6) is like writing 10+1, 10+2, 10+3, 10+4, 10+5, 10+6.  which makes me think it's Perl 6's junctions' semantics applied to Perl's range operator. Cool and useful combination of course.  5. The method of implementing if and else is exceedingly awesome. Kudos.  tl;dr  Bar a few nit-picks, Swym rocks. Congrats.  Edit: Combats with Markdown, emerged victorious with the \ weapon. "
"It's  not  the best tool for learning programming.  Don't be afraid to get your hands dirty, and just start writing code.  Get off Reddit.","I'm pretty surprised at the amount of positive responses this is getting.  Maybe if you don't understand the  concept  of a computer program, it could be helpful.  Calling yourself an ""amateur"" implies that you're already starting to program.  So why not just keep writing programs?  This game might teach you simple logical thinking.  It's not going to help you when you're staring at a mess of code, wondering how it got so big and confusing, and trying to figure out how to redesign your huge, complicated system.  Also, this is a game.  If it were nearly as challenging or potentially discouraging as anything in real life, then no one would play it.  There are great programmers who have PH.D's in CS, great programmers who majored in business, great programmers who have only high school degrees.  The most important thing they have in common is that they buckled down, and wrote a butt-ton of code.  They trudged through poorly documented API references, and they tracked down obscure bugs.  They painfully threw away projects, and started from scratch.  TL;DR: It's  not  the best tool for learning programming.  Don't be afraid to get your hands dirty, and just start writing code.  Get off Reddit. "
Talking about motivations for creating free software is only interesting if you don't assume that people already have all the money they want/need.,"I have to conclude that you have been assuming all along that we're talking about software development  undertaken by people whose financial needs are already met .  Restricted to those conditions, of course I agree with your original statement.  It's basically equivalent to the statement ""Money is not the only form of motivation to develop software"", which is obviously true, and not very interesting.  That assumption about financial needs being met isn't necessary for the question to make sense, so you should have been explicit about making it.  Also, leaving it out simply makes the question much more interesting.  In fact I would say that that is what this whole thread is really  about  -- the conundrum that we all need money, yet some people do spend effort to create something valuable which they then release for free.  TL;DR:  Talking about motivations for creating free software is only interesting if you don't assume that people already have all the money they want/need. "
"make formats free and documented, let everyone choose OSS or closedsource or whatever for programs","For opensource: programming should be fun, others may find things useful. Also, without OSS how would I be able to play something like old screamtracker/fasttracker things now? Or how would be open some ancient document formats now? Closed source tends to do vendor lock-in (you now which companies I am talking about). I don't care that customer expects something free. Most customers are pretty stupid unless you do a high profile job like AMS modeling.  Pro closed source: when you invest in something, it may be necessary to protect that investment. I get it, but for fuck's sake, why did I have to implement stupid DRM that can be broken in a couple of days with a debugger? Especially if the cost of that it should protect is much higher than the price of the attack.  TL;DR : make formats free and documented, let everyone choose OSS or closedsource or whatever for programs "
YOU SELF-RIGHTEOUS ASSHOLES ARE WHAT'S WRONG WITH THE SOFTWARE WORLD.  BITCHING AND NEGATIVITY DOESN'T MAGICALLY BLINK PRODUCTS INTO EXISTENCE.,"WHY THIS IS RETARDED:  THE GUY MAKING THE BUG REPORT DID SO IN A CONCISE, BUT COMPLETELY UNHELPFUL MANNER.  IT IS CLEAR THAT HE HAD THE COMPETENCY TO UNDERSTAND MORE THAN THE CALIBRE AUTHORS.  HE WROTE A PERFECTLY PROFESSIONAL BUG REPORT, BUT A PROFESSIONAL BUG REPORT ISN'T WHAT'S CALLED FOR.  HE COULD HAVE EXPLAINED BETTER APPROACHES (E.G. ""INSTEAD OF SETUID, CONSIDER..."") BUT DIDN'T.  NOW THE WHOLE INTERNET IS GOING FULL-DOUCHEBAG ON THE POOR SCHMUCK CALIBRE DEVELOPERS WHO A) DIDN'T KNOW ANY BETTER AND B) MADE A SUB-OPTIMAL DECISION IN ORDER TO GET A PROJECT WORKING ON A PLATFORM THAT WOULD OTHERWISE BE WITHOUT AN E-BOOK MANAGER.  SCUMBAG USERS:  TALK ABOUT SUPPORTING OPEN-SOURCE PROJECTS; SHIT ALL OVER THEM WHEN THEY'RE IMPERFECT.  SCUMBAG USERS:  TALK ABOUT HOW BAD A PROJECT IS AND HOW THEY'RE GOING TO WRITE A BETTER VERSION; NEVER LIFT A FUCKING FINGER EXCEPT TO BITCH TO THE INTERNET.  SCUMBAG PROGRAMMERS:  TALK SHIT ABOUT DETAILS OF A PROJECT; HAVE NEVER WRITTEN SUCH A FULL-FEATURED ONE.  TL;DR:  YOU SELF-RIGHTEOUS ASSHOLES ARE WHAT'S WRONG WITH THE SOFTWARE WORLD.  BITCHING AND NEGATIVITY DOESN'T MAGICALLY BLINK PRODUCTS INTO EXISTENCE. "
"node.js is a great tool, like a well-balanced hammer... as long as you don't cut tomatoes with it.","I've already said this elsewhere, but: node.js is great as a controller, and works well out of the box with anything that would normally block a process (like IO). However, if your business logic includes heavy computation and you're doing it in node.js and not delegating it (to SQL, to a business logic daemon, to a dedicated cluster behind a web service, I don't care), you're using node.js wrong. If it doesn't occur to you to put an Apache or some other good load balancer in front of it when the traffic gets high, you're using node.js wrong. Used properly, node.js doesn't crap out. Also, being dynamic, it is not really appropriate for enterprise-level systems; use Java or something where compiler will let you know if pieces fit together on the 100000-line code base.  tl;dr:  node.js is a great tool, like a well-balanced hammer... as long as you don't cut tomatoes with it. "
Write a glossary or suffer what I call Synonym Syndrome.,"Where I last worked, there was a huge problem with vocabulary. Nobody was consistent. Our software helped people manage small businesses. Some businesses had multiple locations, so we needed a way to refer to an individual storefront. All through the code, methods and variables referred to this with different names, such as: store, location, address, unit, or building. It was confusing as hell to read the code and wonder if store and location were the same thing or if they were actually different. And of course, some parts of the code referred to ""jobs"" and others had ""tasks"". We also had the same word refer to two different things. We had issues with ""dashboard"" referring to two different parts of the interface.  TL;DR  - Write a glossary or suffer what I call Synonym Syndrome. "
"No silence, the first 5-10 mins were positive feedback + back'n forth comments about what the resume contained. Just nothing deep and nothing long.","Right, I see how what I wrote can be read as offending. It's not like we weren't discussing his resume at all though. I just didn't go in depth about any of it while we went through it together. I used those first minutes to chit-chat and make the candidate feel as comfortable as possible, by reading and making positive comments about the experience he had written down. After all, it's easy to put anything on a piece of paper. But my main objective was that I wanted to spend most of the interview time listening to what he had to say, who he was and what he knew.  I actually never looked down at candidates because each of them was unique and you never know what potential they have. Also I've always been surrounded by amazing programmers so I've learned to listen more than to talk.  TL;DR - No silence, the first 5-10 mins were positive feedback + back'n forth comments about what the resume contained. Just nothing deep and nothing long. "
C++ has no layer beneath it to abstract platform differences.,"Java and Ruby are managed languages, so platform differences are already abstracted away by the JVM (Java virtual machine) and the ruby interpreter.  The java standard library can be written once, because it interacts with the JVM, which has the same ""API"" across all platforms. Now, the JVM itself is not written in Java (Probably in C or C++), and is implemented per platform. For example Android has the Dalvik JVM. Both the JVM, and the C++ standard library have to interact directly with the operating system, and each OS has its own API.  Additionally, C++ has the number one goal of being fast. Each compiler optimizes code differently so it may be possible to write a faster implementation specific to that compiler. Then there's features of the library that require compiler intrinsics (separate topic). So the standard basically gives the method signatures, semantics and time complexity and its up to the compiler vendor to implement it. In the end you don't have to worry about it to use it across different platforms. The ""standard"" in the ""standard library"" is the fact that you the user can use it anywhere, not the fact that the implementation is identical everywhere.  TL;DR C++ has no layer beneath it to abstract platform differences. "
"Advanced math"" is simply a different, more generalized perspective on what people are already doing, and may or may not be useful to any given individual programmer.","Well, that's like saying that you can get Mac n' Cheese at one of Gordon Ramsey's restaurants, therefore Mac n' Cheese is gourmet.  A lot of this stuff is easy to grasp in the garden variety, but has a much deeper well of study for anyone who  wants  to dig into it. When you first teach shapes to children, do they know that it's a form of Euclidean geometery? Probably not, although there's always the one precocious little bastard... that doesn't mean it's  not  Euclidean geometry, just that's it's the tiny subset that's useful for kindergartners to know.  In the same way, even a crappy programmer will spend all day working in concepts that have very high-level ways of thinking about them. Good programmers may know more theory, and be able to apply it to improve their software quality. But the cost of learning always has to be justified by:   I want to learn this, it interests me, or  This will make me a better programmer, for the kinds of code I write.   If you want to write a programming language, by all means, aim high. If you have a personal interest in set theory, bring on the books. This stuff can be interesting as shit. If all you do is write login forms, advanced theory is overkill by a mile for improving your ability to do your job.  tl;dr: ""Advanced math"" is simply a different, more generalized perspective on what people are already doing, and may or may not be useful to any given individual programmer. "
MVC was a terrible idea for big apps. Developing apps with React looks ugly only at the start.,"> only popular because Facebook made it.  Nope, because React is awesome. It does look like an ugly duckling at first. My thoughts were exactly like yours before I started using it.  > HTML directly mixed in your JS  Sounds terrible right? Having HTML, CSS and JS in separate files is separation of technologies not concerns. Having everything in one place lets you see how the whole component will look like in any given state. And that's the magic of React. (more on that at the end)  > .JSX files  You don't have to use it but it is very useful. React code generates HTML. Writing down JSX makes programming faster because you can see what HTML is going to be generated. We are already used to changing layout in HTML.  React still sound like a bad idea. And  it probably is not an ideal solution. As I said two years ago about Angular, that's the best we've got. Reacts code separation into components, excellent v-dom and single directional flow (flux) support makes it one of the fastest development tools for web apps.  TL;DR MVC was a terrible idea for big apps. Developing apps with React looks ugly only at the start. "
JUST FOR YOU:  Read your emails and timesheets are important.,"I don't read emails if they are more then two sentences.  Wow, I'm sorry, but that's my biggest pet peeve.  I'm sure you're a nice guy otherwise, but I hate dealing with people who don't have the common courtesy to read the email that I spent time and effort to write.  I wouldn't have sent it if it wasn't important.  It's really a matter of basic respect.  On the flip side, I used to have your attitude towards time-sheets, like how could this possibly matter.  Then after 3 very productive years with a certain company, I was canned by people far above me because I had been billing all of my time to ""misc overhead"" instead of to the company's clients.  As far as the PHB's were concerned, I wasn't adding anything to the bottom line.  TL;DR, JUST FOR YOU:  Read your emails and timesheets are important. "
currents so strong that they might rip the very words from screen.,"> They are always trying to one up each other and prove how smart they are. They commonly engage in petty infighting and ruthless backstabbing.  Preach on, brother.  Here's my theory:  I call these types 'alpha nerds'.  These are guys[1] who are desperate to be correct.  All the time.  No matter what.  This group is comprised primarily of young, misguided males.  Like all young males, they have a deep-rooted desire to seek out conflict and win.  Why?  Because they want to be alpha and get women[2].  They're expressing a perfectly natural desire to win, but due to their strange environment it completely backfires.  Of course, women don't go for guys because of their sweet code, or because they totally won that argument in the meeting today about whether or not to overload an operator.  This, of course, causes more confusion and frustration and, hence, more angst.  And the cycle continues.   [1] the few female programmers I've met have been socially quite deft and a lot of fun to work with.  [2] yes, yes, I'm aware of the recent discrediting of 'alpha' theory, but it sure seems to be a consistent glass through which to view human social structures.   Edit:  Cut it way down to reduce tl;dr currents so strong that they might rip the very words from screen. "
"It doesn't take much to learn on your own what ""good"" coding is, and open source projects are amazing.","More people need to get involved in their own education. I absolutely love college, and learning. No matter what the subject matter, i see students around me constantly thinking their A in a subject means they know what they're doing. I'm getting my software engineering degree, and many certificates because i LOVE everything related to computers and technology. This degree program is perfect for making me well rounded when assisted with the multiple certs I want.  As for learning languages, the general lecture/lab combo is a good starting point. It helps get you set up in the right direction, and then the rest is on you. At this point in your college degree, you should be used to teaching yourself subject matter. The professors at that point are only there for connections, and theoretical questions outside of the box.  tl;dr It doesn't take much to learn on your own what ""good"" coding is, and open source projects are amazing. "
Consider a MS in CS if you want/need to work with theory on a more regular basis.,"CS BS w/minor in math & CS MS. Working in the industry for about 2+ years, but doing work as web dev for about 5+.  Some random advice for those considering a Masters in CS. Depending if you are interested in a particular area there could be some benefit for positions at the places like Google/Apple/Microsoft which weigh theory sometimes over practical experience. But if you are working or plan to be working in the general software development meaning you aren't doing cutting edge research / development then the MS would probably not be the best time/money investment.  I personally would have preferred to do an MBA over CS-MS since it gives more versatility to go into other aspects of the industry, such as being a Project Manager, Product Manager and higher. I think work experience gives a definite different value than your traditional BS/MS CS courses.  Best thing I can recommend to current CS students is to read the Code Complete book by McConnell. I wish they had 1 course just covering that book. It covers practical implementation of your software so others can read it & maintain it. Variable, function, object naming, debugging/logging methods etc.. etc. Having to debug crappy programming is not fun when you think you are ingenious writing that 2 line piece of code which can be written coherently in 4. Since more often than not it is reading efficiency vs. writing efficiency that should matter.  TL;DR; Consider a MS in CS if you want/need to work with theory on a more regular basis. "
"Don't let anyone tell you you can't get a job you want w/o a degree.  If you want it bad enough, you can't be stopped from getting it.","I thought that was the case for a long time.  It was actually the reason I never tried for a professional programming job; I didn't have a degree and was told all throughout school that you  had  to have one if you wanted any good job.  I didn't have the money for college and I wound up washing windows for a living, despite having years of experience as a programmer hobbyist.  I discovered employers care about experience  far  more than a degree, as I think many recruiters have found that even people with degrees in CS lack the passion that a good programmer has.  I hated the fact that I wasted years because I was told I could get a 'real' job without a degree.  TL;DR Don't let anyone tell you you can't get a job you want w/o a degree.  If you want it bad enough, you can't be stopped from getting it. "
"stop hating on what you don't understand. I know my industry, and it's working for me, not the other way around.","You assume ever-increasing paychecks correlates with ever-increasing project size. In all reality, my paycheck is going up, and project size is going  down . Fuck corporate work. I worked at AmEx for a year, and fuck everything about that environment.  See, what's great about being in my position, is that I can pick and choose the jobs I want. I'm not at the mercy of the market; the market is at  my  mercy. If I want a very high-scale job in a huge company, I can find one. As it is, I want more of a consulting role for companies doing smaller work, that are okay with me working remotely. Large-scale, still, but not in terms of the projects themselves.  tl;dr: stop hating on what you don't understand. I know my industry, and it's working for me, not the other way around. "
"Yes, $499 is beyond what individuals are  willing or able to shell out. Compared to the general conference landscape, it's still a sweet deal.","Absolutely. There was another article (I  thougt  it was the one by Joel, but it isn't actually) from someone organizing such conferences, and provided in great detail why the organizer doesn't have much to say about the final cost.  His/her focus was large conferences (~O(CES)), where you have nationwide only from a few venues to pick that have enough size and infrastructure - like hotels, parking, toilets. Their prices are ""bundled"" with their favorite suppliers for everything coffee, snacks, wi-fi, beamer and PA and lots more. You can of course opt out, but then you pay much more for just the venue.  I bet this is similar for small conferences, while there are much more venues for smaller audiences, many of them are either to small, to big, booked ahead for a regular events etc.  TL;DR:  Yes, $499 is beyond what individuals are  willing or able to shell out. Compared to the general conference landscape, it's still a sweet deal. "
"This has been tried by a lot of people, over many years, and found generally to indicate that what people say and what people do can be very different.","The coffee example is actually from a Malcolm Gladwell book. He took the idea from a socio-psychology book.  Also, similar experiments were conducted by David Ogilvy in the 60's.  Personally, I've done essentially the same experiment for a company in the food service industry, studying how people's ordering/buying habits change across group-size.  The condensed version: We found an indication between what people  will tell you  they want based on who's hearing to be out of step with private, solo ordering habits. The ""why"" is rather open to debate, though we did find some compelling trends.  I've also done a number of brand loyalty studies in other markets which delivered very similar findings for buying habits (though the ""buying environment"" was different so I feel uncomfortable making a total equivalence between the tests).  TL;DR  -- This has been tried by a lot of people, over many years, and found generally to indicate that what people say and what people do can be very different. "
"No, I wouldn't use Pear libs in a WordPress plugin.","As others have said, you would typically include the Pear lib with your plugin, rather than expecting the end user to have it installed. Lets say you're using Pear's Mail class, and your plugin is located at  plugins/myplugin/index.php . You would put Mail.php in  plugins/myplugin/libs/Mail.php , and simply include the file from your index.php.  However, most Pear libs use several files, and you will run into problems if they're not in your include path. For things to work correctly, you'll need to modify the include path in your index.php to something like this:  set_include_path(    __DIR__ . '/lib'    . PATH_SEPARATOR    . get_include_path());  You might have to use  dirname(__FILE__)  instead of  __DIR__  for PHP versions prior to 5.3.  Except this is a bad thing to with WordPress  There is zero isolation between plugins in WordPress. There's no namespacing what so ever. Anything your plugin does can negatively impact other plugins, and vice versa. Changing the include path could cause other plugins to crash, or other plugins changing the include path could cause your plugin to crash.  There is also the problem that another plugin might be using the same Pear lib, but perhaps a different version from what you're using. In a properly autoloaded system only one of version will be used, which may not be the version your plugin is using. Even without autoloading, you can't have two different classes with the same name. If your plugin and another plugin are both trying to include Mail.php from different locations, there is going to be a crash, because you're both trying to include different classes, but both classes have the same name.  In short, Pear doesn't adhere to the WordPress ""namespacing"" scheme of prefixing all function and class names with a unique identifier to prevent name collisions. If you're not doing it already, all your functions and classes should be prefixed with the name of your plugin. For example if your plugin is named  myplugin , all your functions would be name  myplugin_foo() ,  myplugin_bar() , etc. If you really want to use a Pear lib, you'd have to rename all the classes to match, and find all places in the Pear code where classes are being included, and modify the code. Obviously that gets very messy.  tl;dr No, I wouldn't use Pear libs in a WordPress plugin. "
"Projects have different ""standards"" and this tool provides no way to change what it thinks is the valid style.","Doesn't support short tags, which in 5.4 are much more standardized and many people use.  Also it supports only one coding ""standard"" which are actually more style preferences. The difference is that  jslint  for example checks for problems that make execution do things that aren't expected or produce undefined behavior.  For this tool to have any value for me it would have to support disabling the style checking or enforcing my own style preferences or the one with the project I'm working with. For example, PEAR has their own standard, Zend, etc. and if I fork someone's code on GitHub it would be nice to submit a pull request that requires the least work to get accepted.  TL;DR  Projects have different ""standards"" and this tool provides no way to change what it thinks is the valid style. "
People hate PM's and blame them for everything regardless of where fault may actually be.,"I can simplify this analogy.  1) The technical guys are incapable of correctly estimating the things that are going to slow them down because, (very honestly and reasonably), they simply don't know.  In addition to this, all technical people suffer from a tendency to be over optimistic about their own ability and speed in problem solving.  2)  Project managers and Business Analyst, (no matter how technical their background), are not expert enough in the actual technical problems to anticipate the things that their technical experts can't anticipate.  So, the best that they can do is to try and put a safety factor on top of their technical guys' estimates, (the smart PM's will double, triple, or even quadruple the estimated times given to them by the technical experts).  3) The sales guys responsible for 'winning the bid', don't know shit and think that they know everything.  So they see the numbers that the PM's, BA's, and estimators give them and decide that they are simply to high.  They then go back to the technical guys and get not only 'the low' estimate out of them, but they often pressure them to give a 'best case scenario' estimate to them.  The sales guys then sell something based on a delivery time closest to the technical 'best case' estimate.  4)  The PM gets blamed by everyone for everything, (tech guys blame him for committing to an impossible time, sales guys and their upper management buddies blame him for not delivering on time, the client blames him for anything he can get away with, etc...).  And that's why all technical delivery estimates are off by a factor of 2-3, minimum.  TL;DR:  People hate PM's and blame them for everything regardless of where fault may actually be. "
don't [generalize from one example]( and consider other people's use cases.,"> > > The only reason people like that breed of editor is because they've forced themselves to learn the UI (modes) and controls, and once they get past that barrier, the tools are admittedly quite strong for editing text.  Yes, I don't see how that's a bad thing.  Intuitiveness should  not  be the be-all and end-all of user interfaces. To demonstrate this is trivial; simply consider vi :)  Mode-ful thinking is now my second nature (there's a reason that a book I stumbled upon is called ""vim: editing at the speed of thought""), and I wholeheartedly disagree that they're awful to use, or clunky. Whenever I'm in a ""traditional"" editor now, I consider  that  awful to use and clunky - however, I wouldn't go as far as to say this is the case for everyone.  It's simply a matter of different use-cases. A text editor is a heavy-duty tool for me. I don't mind learning, it's something I use every day, for a lot of time. I'll gladly trade a bit of time at the beginning for a large pay of later.  Then I go home, and turn on the radio; and it's completely possible to have a radio interface that allows me to sort each radio station by average transmitting power, filter those which play rock during the afternoon and queue these for 10-minute rotation.  Would I want that? No. This is where I won't trade off a learning curve for ease of use, as I don't need UI power, or efficiency.  tl;dr: don't [generalize from one example]( and consider other people's use cases. "
It's looking  great  for many of Sun's technologies --- because Oracle is pissing off everyone enough that the major contributors to the projects are forced to innovate elsewhere.,"I think it's working out  extremely well .   The games Oracle played in 2010 with Solaris - discontinuing public updates - were a great motivator for accelerating enterprise features in Linux, some largely copied directly from Sun's R&D (dtrace, zfs).  The MySQL community which was kinda stagnant for a long time was inspired to innovate with things like [Aria](   Fujitsu's [34-core SPARC64XIfx]( is the awesomest SPARC chip ever made -- especially considering that 12 such chips fit in a single 2U chassis  --- for a total of 384 general purpose 64-bit cores; and 24 special purpose cores in a single 2U box!!!!   The only Sun technology that seems to be really suffering is Java, which it seems Oracle mostly turned into a vehicle for delivering Ask.com spyware on Windows.  [WTF Oracle!]  TL/DR:  It's looking  great  for many of Sun's technologies --- because Oracle is pissing off everyone enough that the major contributors to the projects are forced to innovate elsewhere. "
paradigms stay separate when algebraic closure is lost when combined.,"\aside maybe it's helpful to think of paradigms as ""closed"" (as in, addition and multiplication over the integers is ""closed"" in that whatever integers you add or multiply, you always get another integer - you never jump out of the system). So, in SQL, you start with tables and you end up with tables (relational algebra). This is important for allowing you to mix and match however you want, and it always works (composable, orthogonal).Functional programming is similar. [ Of course, ""paradigm"" has more to do with how you think about it, apply it and use it, than algebraic properties alone ].  But neither mix as nicely (i.e. ""closed"") with OO.  \aside2 I don't think logic programming (or its descendent, CLP Constraint Logic Programming) should count as a ""paradigm"", because it's more a bunch of libraries/frameworks than you have to tinker with to make efficient enough. You can't just code at the level of abstraction offered, but must look beneath. I suppose you could say the same of Java or Python or Ruby, it's just that modern machines are generally make it fast enough for most needs (but for high performance games you do have to). So I guess the difficulty for logic programming is that its  domain  complexity is inherently difficult - exponential time isn't solved by machines that are merely a million times faster.  tl;dr  paradigms stay separate when algebraic closure is lost when combined. "
"Found a feature that makes life much better, should've seen it much sooner.  Trying to avoid pondering time wasted in meantime.","Apologies, I probably should've been a bit less meta in my text post.  I'm a rails developer in a tiny, tiny web shop, and I've spent most of the last year working on a large system that's supposedly our company's main product.  The application was originally written in PHP by a Perl guy, was translated into Rails, has passed through a litany of programmers running the spectrum from duct-tape to purist since, and it's showing.  I've been struggling lately with trying to keep the application running somewhat efficiently whilst keeping my sanity intact, but it's been a losing battle thus far.  Then today I found scoped.  I should've known about this some time ago, but hadn't seen what I was missing til today.  I've already managed to gain some serious performance, and I've only barely scratched the surface.  It makes me feel like I've wasted the last month, but I actually see a glimmer of hope continuing on now.  tl,dr: Found a feature that makes life much better, should've seen it much sooner.  Trying to avoid pondering time wasted in meantime. "
How does that 3x slowdown actually affect runtime performance?,"This was written by developers who don't understand time. You should use whatever language you like, but to claim that the decision was made because certain operations are 3 times faster is not informative. It could be 1,000 times slower than a 1 ns operation that occurs once every minute, so what? Someone left a ridiculous comment ~""you wouldn't want Word written in C# would you?"". Well considering that I can have Word open for over an hour and it will only have to process a couple hundred keypresses at a total of about a second or more of processing time, why not?  TL;DR:  How does that 3x slowdown actually affect runtime performance? "
"It's depressingly cultural for open source to be dick-measuringly sexist 
 <singsong> 
 Someoooooone's been reading too many blogs aboouuuuut the Rails communityyyyyyyyy 
 </singsong>",">Microsoft works very hard to make developers feel wanted  I was a .NET developer for 3 years, I know this. Also, companies tend to require MS products across the board whenever they use them at all, creates a pretty big incentive to work in Microsoft disneyland when you don't want to live in Cali/NY.  >You pay some money to indicate interest  Scarcely, given MSDN + student programs.  >But, open source platforms are a community in the traditional sense of something you pretty much have to be internally motivated to join.  Hence when I respect OSS devs a great deal more.  >Joining an open source community is closer to joining a church or moving into a neighborhood  Uh? Sometimes?  >these neighborhoods are sexist boy's clubs with no facility for mentoring  That's not a statement substantiated by fact.  >no respect for design  Also false.  > mailing lists that are 50% dick-measuring contests  The Django mailing list is all business.  >My understanding is that mentoring and socialization are really important for women developers  So they have to be spoonfed? What makes them so goddamn important if they haven't proven they're willing to contribute? I have actually tutored many people before, but if they don't express an interest to learn and willingness to contribute I am not going to waste my time on them.  Only two of my 'students' have been female though, nobody else has really come through my circles expressing an interest to learn.  >This is on top of the regular gender issues: being the only woman in the group is incredibly intimidating  Is there some kind of ogling over TCP protocol I'm not aware of? Leering via Mercurial? Intimidation transaction via SVN?  >just like open source software tends to have no ability to support proper user experience design  laughingelf.jpg  >no-one wants to volunteer to enforce ""froofy"" feature-cutting front-end work  I know plenty of web designers who contribute their work to open source projects to build their portfolio, it's how I fucking met them. A lot of them hack up javascript too, but most are just designers.  >no-one wants to volunteer to discipline (and alienate) a lot of immature, sexist volunteer developers and provide a welcoming, women-oriented  This is enforced in all Ubuntu communal discussion venues and media, including IRC, mailing lists, etc. In IRC in particular they tend to get prickly really fast.  >TL;DR: It's depressingly cultural for open source to be dick-measuringly sexist  <singsong>  Someoooooone's been reading too many blogs aboouuuuut the Rails communityyyyyyyyy  </singsong> "
"Your comment proves my points. 
 No it really doesn't, you're just an over-generalizing tool with an axe to grind and no real experience with development in OSS.",">What you see as spoon-feeding is what normal men and women see as proper, instructive socialization. ""Hi, welcome to the community, here's a housewarming present, here's how things work, we could really use some help over here and I'd love to show you how to work on it, but anything you want to do, feel free and I'll be here to answer and questions and walk you through every process until you feel comfortable.""  I take you've not been in IRC much. Do you have any idea how many people presume that you exist to educate them in programming, when in fact they're simply too lazy to do their homework?  >TL;DR: Your comment proves my points.  No it really doesn't, you're just an over-generalizing tool with an axe to grind and no real experience with development in OSS. "
"It's depressingly cultural for open source to be dick-measuringly sexist, but MS makes money when they're friendly to women.","Microsoft works very hard to make developers feel wanted, welcome, and productive.  ""Developers, developers, developers"" isn't just a funny YouTube video: making  everyone  who wants to make something feel like they can, that MS will support them and help make them money, is how Microsoft has been as successful on the desktop as they have been.  It's in their best interest for them to support you as a developer. You pay some money to indicate interest, and they shower you with devkits and communities and let yourself get carried along.  But, open source platforms are a  community  in the traditional sense of something you pretty much have to be internally motivated to join.  Joining an open source community is closer to joining a church or moving into a neighborhood, and, let's be honest, these neighborhoods are sexist boy's clubs with no facility for mentoring, no respect for design, and mailing lists that are 50% dick-measuring contests.  My understanding is that mentoring and socialization are really important for women developers, there's a level of trust that has to be built, that the traditional flamebait responses to questions or broken patches are enough to turn them off completely.  This is on top of the regular gender issues: being the only woman in the group is incredibly intimidating, and you're always wondering which half of the men only see you as a sex object, and which half of the men assume you're genetically incapable of participating.  It's worse when you're a minority, on top of that.  Just like open source software tends to have no ability to support proper user experience design, because it's all volunteer and no-one wants to volunteer to enforce ""froofy"" feature-cutting front-end work (and lose a bunch of those same volunteers), no-one wants to volunteer to discipline (and alienate) a lot of immature, sexist volunteer developers and provide a welcoming, women-oriented (which is really just a socially-accepting, newbie-oriented) development and contribution environment.  TL;DR:  It's depressingly cultural for open source to be dick-measuringly sexist, but MS makes money when they're friendly to women. "
"Even if your code is well thought out and documented, it probably sucks","At my old job I coded a custom content management system for a particular contract we had gotten through the state. The project had been a nightmare without any kind of spec or direction. I eventually coded the thing using various design patterns and several frameworks. I eventually quit the job just before the product was about to go live. I spent a whole day with the programmer that was taking over the project and a manager. At first they didn't understand any of my logic for using such a ""complex"" system and framework (I was using MVC and some AOP). I went through it step-by-step and explained why I did things and there were a lot of ""oh, I see!"" and ""hmmm, yeah that would've been a problem""  About 3 months later the programmer assigned to the project IM'd me and told me that after I left, upper management assigned their own programmers, who evaluated my code, said ""IT SUCKS"" and moved to a VB and Access based system.  TL;DR Even if your code is well thought out and documented, it probably sucks "
"Govt sucks at user feedback, this advice could save lives in the military.","I work with software developed for the military really often, the process is dreadful.  They don't seem to have ""ask the users"" anywhere in the process.  I expect cutting edge awesomeness...I have to work with crappy crap crapperson.  The guys that make it, don't use it, don't really know what it is used for, and dont ask us how they can make it better.  ""We need software that does X and Y."" the gov't says.  out of three bidders, they pick the most expensive, cuz its probably the best, and then they can boost their budget for next year...  The guys that get the contract, build a piece of shit, which doesnt work.  So the gov't opens up bidding for a different company to build software that does X and Y and Z.  rinse repeat.  I have to build all of these little Perl tools to subsidize the horrors they make us use...  I wish I were developing the software.  tl;dr    Govt sucks at user feedback, this advice could save lives in the military. "
"your guess is as good as mine when it comes to writing code better, but the complexity metrics we pick now aren't any better than LOC.","I can't say if the specific case you describe was represented in any of the studies looked at. But I wouldn't call their results perscriptive; the authors of the study are describing what happens in the field, measuring the way people write code right now.  Correct me if I'm wrong: this conclusion implies that if you grab two source files of the same length and language off of github, and you want to take bets on which has more reported bugs in it, no one complexity metric will help you place your bet than you would be just flipping a coin.  They did  not  examine what happens to the results of a programmer after you tell them this result.  tl;dr your guess is as good as mine when it comes to writing code better, but the complexity metrics we pick now aren't any better than LOC. "
"you're basically right, but you're probably underestimating the lifespan of a tablet by a factor of 3, which you don't need to do to support your point.","While it is a far-fetched argument, it's also far-fetched to assume that every tablet will be replaced at the end of its warranty period. What I hear anecdotally is that most people keep their phones and tablets 2-3 years and are frequently able to pass on their used device to someone else. (Apple's assumption, if anything, seems to be that iOS devices will last 3 years, as that's been when their OS updates have stopped for a given device: a device purchased running iOS 1 could be upgraded to 3, one with 2 to 4, etc. And, of course, even if the original iPad cannot be upgraded past iOS 5, it's not as if they'll all spontaneously combust when iOS 6 is released.)  tl;dr: you're basically right, but you're probably underestimating the lifespan of a tablet by a factor of 3, which you don't need to do to support your point. "
calm the fuck down. The code police aren't going to force you to use spaces if you don't don't want to.,"I've seen a few  posts in various places now talking about this, and honestly I think people are getting excited over nothing. The PSRs are just guidelines set up so developers can easily work within multiple libraries.  Essentially, the Framework Interoperability Group is made up of authors and contributors to major frameworks and projects. The PSRs are standards they decided to adopt with the goal of making their respective projects work together. It's a great thing, and adopting similar coding styles means developers can work in code built with whatever framework they choose and know exactly how the code should behave.  These aren't standards Joe Developer has to use on his weekend project, and your team at work can choose to ignore them as well. However, for developers or teams that don't yet have their own adopted coding guidelines, they're free to use PSR-1 and/or PSR-2 if they like.  tl;dr: calm the fuck down. The code police aren't going to force you to use spaces if you don't don't want to. "
"the zone"" isn't where I come up with solutions, it's just where I write them down; the distinction is important","> Isolating yourself to ensure uninterrupted “zone” prevents you from learning from others, gaining information from others and spreading your knowledge to others  Uh... what?  I'm sorry, but in an 8-10 hour work day, letting me just sit by myself and focus for 2-3 of them is hardly ""isolating"" myself in any way that causes the effects you claim.  It does, however, tend to get more code done over the long term, if you let me actually do my job with some focus for part of the day.  > Are you sure that your domain knowledge is deep enough? While you worked in the zone and progressed did it actually move you forward in the right direction?  If I don't have enough confidence to work for 2-3 hours without getting confirmation from other people I'm on the right track, I probably shouldn't be doing it at all... and particularly distracted, where I'm likely to splice paradigms or other weirdness.  I'm against any form of sitting down and just ""hacking it out"" - in the zone, in a pair, or whatever; exploring topics is all well and good with mock-ups, prototypes, etc, but I generally produce significantly better code if I do all that, do nothing related, and come back to it a day or so later, when an actually good idea of how to solve it has me ""in the zone"".  tl;dr: ""the zone"" isn't where I come up with solutions, it's just where I write them down; the distinction is important "
Dependency Injection/Inversion of Control facilitates unit testing a single class and only a single class.,"Inversion of control makes injecting mock classes easier. When unit testing a class, you should only be testing that class. However, when developing an application, many of your classes are going to depend on the execution of other classes. If your dependencies are hard-coded into the class you're testing, then you are inevitably testing more than one class. For example:  public class FooProcessor{    public void ProcessFoos()    {        FooGetter getter = new FooGetter();        FooList foos = getter.GetFoos();        foreach(var foo in foos)        {            this.Process(foo);        }    }    public void Process(Foo foo)    {        ....    }}  As you can see in the above example, if you test FooProcessor, you are inevitably also testing FooGetter. If FooGetter pulls its Foos from an external URL, then you are making a web request every time you run a test.  If you instead define an interface for the dependency and provide the instantiated dependency at runtime, you can provide a mocked class that implements your interface. For example, instead of FooGetter.GetFoos() making a request to an external URL, your mocked object might instead just provide a list of test Foos.  public class FooProcessor{    private readonly IFooGetter _getter;    public FooProcessor(IFooGetter getter)    {        _getter = getter;    }    public void ProcessFoos()    {        FooList foos = _getter.GetFoos();        foreach(var foo in foos)        {            this.Process(foo);        }    }    public void Process(Foo foo)    {        ....    }}public class MockFooGetter : IFooGetter{    public FooList GetFoos()    {        return new FooList        {            new Foo(""test""),            ....        }    }}  tl;dr: Dependency Injection/Inversion of Control facilitates unit testing a single class and only a single class. "
"compilers don't set out to deliberately break undefined behavior, but it is a very important part of how they are able to optimize correct code.","This shows that you really don't know how an optimizing compiler works.  One if the major tasks of the optimizer is to determine what the code was actually doing, and eliminate any extra work that wasn't contributing to the end result.  The problem is that it's Really Hard in general to determine what random side effects of a piece of code are necessary to the algorithm.  The compiler has to perform reasoning along the lines of ""if the programmer did X, then he could observe the effects of this operation.""  This is where undefined behavior saves the day!  If X is undefined behavior, then the compiler is allowed to remove side effects that could only be observed by X.  Basically, UB tells the compiler that it can assume that certain things never happen.  The compiler often ends up proving a transformation safe with a statement like ""if P is not null, this transform is safe.  If P is null, the behavior was undefined and I can assume the transform is safe.""  TL;DR:  compilers don't set out to deliberately break undefined behavior, but it is a very important part of how they are able to optimize correct code. "
"Light Table looks like a pretty REPL, not to say that REPLs aren't great.","Serious question: has anybody found Light Table useful?  Full disclosure, I've never used it, but I also don't believe in it. I think it's technically impressive and interesting, but the fundamental idea seems virtually useless in practice. The tech demos I've seen have all been carefully engineered to illustrate a particular aspect of LT and, as a consequence, do not resemble actual, useful applications. Additionally, LT seems to advocate reasoning about code through exploration, which, past the point of proof-of-concepting, strikes me as a  horrible  way to reason about code -- you shouldn't be playing around with a code base to find out what it's doing, the behaviour should be explicitly specified.  tl;dr: Light Table looks like a pretty REPL, not to say that REPLs aren't great. "
"leaving dreamweaver behind is like taking the training wheels off your bike, you learn to code better - and as a result make more money.","I used to swear by dreamweaver - especially when used in conjunction with the Interakt toolkits (which later becam ADDT - Adobe Dreamweaver Development Toolset). I found that the ease with which I could create complex, dynamic frontends/backends allowed me to solve any problem I came across developing websites - and I learned a lot of PHP by modifying the code created by the plugins. However when I started a new job a few years ago I was put into a team which was developing sites using the CodeIgniter framework. After using codeigniter and the netbeans IDE my skills evolved increadibly rapidly, I was able to create better more efficient object oriented,  MVC code and it has taken my coding skills to the next level - greatly increasing my earning potential as I can now go for more advanced, higher paying jobs.  TLDR - leaving dreamweaver behind is like taking the training wheels off your bike, you learn to code better - and as a result make more money. "
What's the difference between using the API from the inside (what Google did) and using the API from the outside (what application developers do)?,"I'm really worried about that instruction. I don't really understand the way that legal precedent is established, but I'd hate for that to become one.  Any code I write against a particular API is then effectively embedding at least part of that API. (I'm doing the same thing Google did, but from the other side of the interface.) If an API is considered eligible for copyright, then doesn't that mean that every piece of software I've written against a proprietary API would now be considered a derived work of that API, thereby making it illegal for me to distribute my software without an appropriate license from the API's copyright holder?  TL;DR:  What's the difference between using the API from the inside (what Google did) and using the API from the outside (what application developers do)? "
don't do these things but you should still have a deep understanding (mastery not required) of WHY these things are legal/illegal if you want to be a great programmer,"I teach a college-level C++ class that also includes some C so I only missed a couple of questions (the bitshift ones mostly) but you are correct- the pragmatic answer is ""don't do that.""  Still, it is useful to have a deep understanding of not only the standard but also the compiler that you are using, and this is what (in my opinion) differentiates a senior/lead dev from a more junior one; this level of depth.  For example, in the MS compiler, ""volatile"" is defined  not only as ""don't optimize this"" but also gives an implicit full-fence memory barrier around reads/writes to that variable (source:  which is not defined in the standard.  If you didn't know that, then you could have some lock-free code that always worked for you in the past but suddenly stops working when you get a new job somewhere because you never really understood what was happening in the first place.  tl;dr; don't do these things but you should still have a deep understanding (mastery not required) of WHY these things are legal/illegal if you want to be a great programmer "
please do not do this. You will get fired.,"This guy has no idea what he is doing.  Recall that the difference between String and StringBuffer is that String is immutable. It can't be changed after being created, which means (among other things) it doesn't have an Append method.  You can concatenate two strings with ""+"", but that creates a third string.  std::string, in contrast, is  not  immutable.  It has an Append method -- it's called ""+="".  std::string  is  a StringBuffer.  The reason std::accumulate is so slow is that it uses ""+"", not ""+="", which makes this article a  tour de force  in demonstrating that you can get any numbers you want as long as you start optimizing from a sufficiently ridiculous baseline.  (Plus, it also helps -- as the original author did -- to move a lot of work into the  construction  of your StringBuilder class, and then not measure it).  TL;DR: please do not do this. You will get fired. "
"If you want to put all your data in RAM, buy more RAM. Don't dick around with custom memory management.","But suppose for a second that in this case, the combination of swapoff, mlock and fancy coding is faster right  now . There's no guarantee that this will remain true.  Basically this is coupling: relying on the internal behaviour of the current system. It's brittle because tomorrow the kernel developers might weaken the mlock or swap or whatever guarantees in order to speed something up.  You see similar stuff in SQL all the time. Lots of folklore has built up about what kinds of queries are faster. But it's all based on stuff from years, even decades ago. Now all you have is a pile of weirdo queries that confuse the shit out of a much smarter query planner.  tl;drIf you want to put all your data in RAM, buy more RAM. Don't dick around with custom memory management. "
"Moral of the goddamn story, tables become very limiting later on.","Well, to start, tables are for tabular data. That should be the only argument you need to hear.  Since you're on Reddit, I can safely assume you're not stupid, so I must assume you're just being stubborn. In that case, I posit a real world scenario:  A guy I know has a photo site. He used tables, roughly, to set up what became a very long page of photography services.  He asks me how he can take each service (which is comprised of a heading, some text, and an image or two) and make a tabbed layout.  I tell him how easy it is to do if you have each section in a div. Not knowing much about markup, he asked me to look. Sure enough, each one was in a div.  So where are the tables? He used a frickin table to position the divs on the page.  And now that he wants to make tabbed content, he either has to rewrite the markup, removing the tables, or do much more effort than should be necessary to do it in jquery.  tl;dr  Moral of the goddamn story, tables become very limiting later on. "
Do the simplest thing that could possibly work: Tables.,"I used to live and die by divs (for ~6 years I refused to use tables for layout). If you do enough web development, you learn sooner or later that tables are the way to go. It's the only thing that is even remotely consistent across browsers. You don't have to worry about floating, absolute positioning, clearing lines, and all of the other cruft that comes with 100% divs. Divs are great, but calling it black & white and saying you should  never  use tables for layout is short sighted and simply not accurate.  After I went back to tables it was like a giant weight had lifted off of my shoulders. It used to be a process of having to be clever and think about how I could get a series of boxes to flow together in such a way that it resembles the design I want while not getting screwed up across browsers or across different resolutions. Now it's just... great this box goes here and this box goes here and I want this box content to be vertically centered... done.  tl;dr; Do the simplest thing that could possibly work: Tables. "
Be glad you don't have to use FirstClass. Blackboard is like a walk in the park comparably.,"Back when I was in high school, the CPS (Chicago Public Schools) shifted over to using a program called FirstClass for email. My college (University of Illinois) uses Blackboard for many of its classes (thankfully not much of the CS department uses it, since they actually understand computers and hate it as much as students do). I give the crown of ""worst software ever"" to FirstClass, but fortunately most people haven't experienced the horror of it.  Some of it's glitches include a horrific user interface (the places where you would expect the Send button might be have Save, Delete, and Inbox in them. Send is the third button from the left), bugs that prevent most of the useful features from working right (like instant messaging), a glitch in the web interface that causes typing to stop working when you press backspace (I'm not kidding; I had to write my emails in vim and copy them over) combined with horiffic Linux support, lack of any POP or IMAP support for downloading, lack of support for sending email to more than 50 recipients (kind of important for a school system where teachers might want to send messages to the entire school or even all of the students in a class), multiple inboxes (oh, your email isn't in your main inbox, it's in your advisory inbox, or your announcement inbox), and completely retarded features (like ""unsend"" which deletes your email from other people's inboxes, assuming of course you sent it to another FirstClass user). Also, the software is expensive as hell when Google Apps is free for educational usage and so much better.  TL;DR: Be glad you don't have to use FirstClass. Blackboard is like a walk in the park comparably. "
"You aren't commenting what the variable is, but rather why it is the value that it is.","It's not always unnecessary to have a comment. The why is important. We know from this line that the PollingTime is stored in a value as seconds. But why is it 30 instead of 60? Maybe the network controller on the machine being polled resets if polled at an improper interval, maybe the process of a third party tool crashes if polled too soon/too late. Without the comment, we know ""the what"" (pollingtime as seconds), but not ""the why"" (world ends if not polled between 28 and 32 seconds, don't change). Then the required value gets changed (world ends if not polled between 67.4 and 939.2334 seconds).  TL;DR  - You aren't commenting what the variable is, but rather why it is the value that it is. "
"Nothing wrong with starting off on one platform, just be ready in case you do get fucked.","It not only worked for Zynga, it's worked for many other companies. Involver and Buddy Media to name two more. Look at the success of Angry Birds. It had a killer start exclusively on Apple's mobile platform, and it grew its success outside of Apple to Android, board games, plush toys, browsers, and more. Instagram also initially grew from the Apple store, and used Twitter and Facebook to help grow its own platform.  It is risky to build a company that relies on another platform, but it isn't wrong. A smart entrepreneur (or team of entrepreneurs) will figure out how to scale from that one platform and onto multiple other ones in order to minimize risk.  The example in this post failed because they didn't have a backup plan. Yes, they did solely rely on Facebook to get some initial headway and that ended up fucking them in the end. But they didn't have to fail at that point; if they had a good team, they'd have pivoted quickly and have had a plan to scale off of Facebook. They knew for a while that their idea was in turmoil, Sean Parker told them himself.  ""The hundreds of other companies that didn't make the Facebook lottery"" failed, not because they were unlucky, but because they didn't build their company in a smart way, or because they didn't know how to scale it.  tl;dr Nothing wrong with starting off on one platform, just be ready in case you do get fucked. "
"CanJS is great and easy to understand, but has a small community.","I wrote a pretty big [project]( in CanJS and now I'm doing a lot of work in Angular.  CanJS is actually very good. It provides some features for very large SPA projects that Angular is missing, like decent name spacing and a client side ORM. It is also a lot easier to learn for people familiar with server side MVC and jQuery. The controllers are basically structured jQuery plugins, so are very modular.  Now I've been working in Angular for a few months, CanJS feels very verbose in comparison with controllers often containing a lot of event binding boilerplate. I've also found the supplementary tools (minification, testing, cache busting etc) are much better with Angular, mostly thanks to Yeoman and Grunt.  The main reason I moved away from CanJS is because I feel it's fate is tied to one company ( bitovi  that they originally pushed with the framework and my project heavily depended on.  It's also just more fun working in a framework with a lot of momentum and community excitement.  tl;dr CanJS is great and easy to understand, but has a small community. "
"If you're looking for help with writing an AI, look for MC tree searches.","Monte Carlo sampling methods, probably not. What you're looking for is something called a Monte Carlo tree search.  In Chess, the state space (the space of possible moves) is large, but not so large that you can't explore it completely to a reasonable depth on each move. You have at most 16 pieces you can move to a fairly limited number of places depending on its position and piece type. It's commonly said that there are 10^40 legal chess positions.  In a game like Go, you have a much larger board 19x19 (361 spaces). This gives you a way more moves that you can make you can make, especially when you consider capturing and freeing up space. There are roughly 2x10^170 legal positions for Go. That's an enormous difference.  Monte Carlo tree searches try to explore this space by sampling randomly through different paths, whereas many Chess algorithms just do a complete tree search to a certain depth. They need to sample paths because it's infeasible to explore them all in a sufficient amount of time and memory.  TL;DR: If you're looking for help with writing an AI, look for MC tree searches. "
"Flash sucks, FF doesn't handle it well, FF bug maintainers pawn off responsibility to nspluginwrapper developer, problem eventually patched but not the best.","This happened to me with the Firefox bug maintainers regarding Flash crashing in Ubuntu x64. I reported the problem and what caused it and I ended with:  > I hope that the FF developers couldcontact the Flash developers to resolve the issue.  I had a couple of good responses from one bug maintainer, then I updated the content and another maintainer entered and said:  > sorry. if you have a problem with the plugin wrapper, please complain to them.>> we do not keep bugs open for random groups who don't cooperate with us, and they're amongst them.  I wasn't rude or anything and I didn't look at the code to find out exactly where the problem was, but I didn't need an answer like that. While he used words like ""sorry"" and ""please"", there was a condescending tone to it.  I did contact the nspluginwrapper developer and after a system update Flash would let me refresh FF instead of restarting FF. It's better, but it's not the best.  tl;dr: Flash sucks, FF doesn't handle it well, FF bug maintainers pawn off responsibility to nspluginwrapper developer, problem eventually patched but not the best. "
Put something on your site about how the security works at both high-level and in greater detail.,"I'm not sure how big the market is for storing passwords and usernames on a server somewhere else.  I'm certainly not going to do that, but I'm a bit more security conscious than the average person.  That said, how are you encrypting the passwords on the server?  You obviously can't use a one way hash, so I assume there is a key pair somewhere in there (with the private key stored on the same server as all the encrypted passwords, since they'll have to be decrypted before being transmitted over HTTPS).  I'm not sure what the gain in that is.  If your server is hacked and they get the database, they'll probably be smart enough to get the key too.  I suppose you could generate a unique private key with each PassKey bookmarklet and store the public key in the database with the log-in information.  Then you'd just store the encrypted data in the database and the client side JS would do most of the heavy lifting.  tl;dr -- Put something on your site about how the security works at both high-level and in greater detail. "
"drivers are crap quality (and not getting better over time), and this is a massive pain in the ass from the developer perspective. 
 (edited for formatting)","I develop a well used OpenGL based app which is cross platform. I consider myself a proponent of OpenGL.  However, there are serious problems which need to be addressed.  Foremost among them IMO is increasingly poor driver quality.  When Vista was released, and after that when Windows 7 was released, many new drivers were written with OpenGL support either entirely missing, or horribly broken.  Big manufacturers are very slow to update the drivers which are pre-installed on their systems, and we we're still seeing many many systems which can only get working OpenGL by taking new drivers from the manufacturer website.   This is a pain in the backside and causes about 60% of all support requests.  Hopefully the video driver architecture won't radically change in the next iteration of MS's OS or I expect to see another bout of crappy GL support in the first generation of drivers for that OS release too.  On Linux it's a damn mess - open source drivers are frequently buggy, have bad performance / no gl acceleration or at least restricted feature set (only 1.x API support, no shaders and so on).  The proprietary drivers are frequently a pain to install, and often have specific problems with ACPI.  Trying to support a user is difficult because of the variety of distros and the different ways of installing proprietary drivers.  In the long term I hope that the open drivers will get good enough to make the proprietary ones obsolete, but this is taking a very long time, and in the short and medium term I don't see much hope of things getting better.  OSX is a little better wrt drivers, what with the more restricted number of hardware configurations and more oversight from Apple.  OSX has other problems for us though.  tl;dr  drivers are crap quality (and not getting better over time), and this is a massive pain in the ass from the developer perspective.  (edited for formatting) "
"Perl 6 supports named arguments, strongly enough that I think it covers all the cases you discussed in your gist.","Simplified  I think Perl 6 does everything you mentioned in your gist.  > The debate about strongly typed vs weak/dynamically typed languages is ancient.  Perl 6 is all three.  > trying to use a variable as an integer but it's actually a string.  my Str $a = 1; # compile time error, ie Perl 6 can be strongly typed  > passing an integer that is a measure of meters into a function that expects an integer of feet  my $meters = 2;my sub yards(:$feet) { $feet / 3 } # named parameter/arg $feetyards(:$meters); # compile time error  > Python is strongly typed but dynamic which means it doesn't check anything until the very last moment.  Perl 6 type checking is done at compile time. (Except when it isn't. :))  > [need] pretty good coverage of your tests or other tooling just to catch simple spelling errors.  If you make a spelling mistake Perl 6 will generally spot that at compile time and suggest what you meant:  my $meter = 1;say $metre; # compile time error ""did you mean $meter?""  > naming all the parameters:  [@""foo"" drawAtX:10 y:4 width:6 height:8 font:@""helvetica""]sub draw($string, :$x, :$y, :$width, :$height, :$font);draw(""foo"", x =&gt; 2, y =&gt; 4, width =&gt; 10, height =&gt; 10, font =&gt; 'Times');  (I left $string as a regular positional parameter for comparison sake.)  > A problem with Objective-C is that a lot of code ends up with stuff like [@""foo"" drawAtX:x y:y width:width height:height font:font].  my      ($string,  $x,  $y,  $width,  $height,  $font) =           'foo',   2,   4,      10,       10, 'Times';sub draw($string, :$x, :$y, :$width, :$height, :$font); # define sub    draw($string, :$x, :$y, :$width, :$height, :$font); # call sub  > What if we did this with return values too?  The spec seems to suggest this should work:  sub foo() { :x, :y };say foo&lt;y&gt;;  But it generates a compile time error. So it's either not yet implemented or the spec or my interpretation of it are wrong. This works:  sub foo() { %(:x, :y) };say foo&lt;y&gt;;  But it doesn't generate an error if the wrong name is used:  sub foo() { %(:x, :y) };say foo&lt;why&gt;; # compiles OK and says undefined  TL;DR  Perl 6 supports named arguments, strongly enough that I think it covers all the cases you discussed in your gist. "
Linus is a really hot girl at the bar; people will put up with a lot if they are getting something they really want in the end.,"He is direct. That's a style of communication that's perfectly effective especially in his discipline.  Separate from that directness he is also an asshole; which is not a good thing.  Separate from both of those he is also useful; for anybody who needs his expertise that it is generally enough to look past his other traits.  He has contributed great things to the world and in all likelihood could have contributed something even more amazing if he had the ability to work well with others but he doesn't. It's fine though, not everyone can do that and he's found a niche where he has still been able to accomplish more than most people ever will.  If your not involved in this kind of work, and you don't see how useful he is I understand entirely why his poor attitude would seem so off putting.  Tl;dr Linus is a really hot girl at the bar; people will put up with a lot if they are getting something they really want in the end. "
"Git does a lot of things, but way way too many things IMHO. Mercurial won't let you fuck up as easily as Git and it actually makes sense.","I don't have much to say about Git. I used it for maybe 6 months, every time I had a question I found a lot of different answers with different effects, there are a lot of concepts that are there just because they can be and they're not extremely useful and you pretty much have to use them. There is a lot of advice out there that can make you mess things up permanently, there is a lot of default behavior which must be taken into account, I still have only a vague idea how branches work, there is no decent repository browser - at least on Linux. The terminology is also painful to absorb, there is a ton of documentation which you have to read and memorize before you can even touch Git to try and understand it. Six months later I'm still struggling to understand basic concepts because I run into them like once every week or two.  Before Git, I used mercurial for several years. I was skeptical at first, coming from SVN which I vaguely understood, but eventually I gave it a shot. Once I understood the differences between push/pull vs. commit/update and what the changeset numbers really were (numbers, not ids) and why they didn't match between clients, everything made perfect sense. It's very simple, it doesn't let you fuck up history (I used to complain about this, until I found out how it can be done on Git an what the effects are, and now I praise Mercurial's inability to edit history), and... that's about it. As long as you don't work on a behemoth - like the Linux kernel as someone here suggested - you'll be perfectly fine with Mercurial.  tl;dr  Git does a lot of things, but way way too many things IMHO. Mercurial won't let you fuck up as easily as Git and it actually makes sense. "
"One-only good, globals bad. ""Zero, one, infinity"" is true, yes, but it never hurts to have ""infinity"" available as a local code change, instead of global.","Never. You can solve all your dependency injection bloat by wrapping up your dependencies, and passing them as a complete package, said package can also implement ""one only"" constraints and everything else you can wish for. ""One only"" constraints actually make sense in a lot of cases and is the only sensible part about singletons, but you don't want that to lock you in:  Evolvability is a thing you should always optimise code for, it's measured in ""how much code do I have to change to change random stuff"", read: ""How do I manage to not code myself into a corner"". Singletons-as-globals tightly couple things, that's the exact opposite. They're both hard to cut out of code that uses them, as well as using another interface than the rest of your injected dependencies. That's  two  reasons to unify that shit.  tl;dr: One-only good, globals bad. ""Zero, one, infinity"" is true, yes, but it never hurts to have ""infinity"" available as a local code change, instead of global. "
"I saw the sign, and it opened up my eyes.","Also, such ""koans"" are meant to parody the real koans of Zen Buddhism, which are slightly opaque in meaning.  If my translation is correct, this is the joke:  When a  GUI  is clarity (Window). Your thinking (your head) is unclear (rock).""The GUI fan stated that the Guru wasn't making himself clear by just pointing at things (which is the essence of the GUI), to which the Guru replied with a gesture that means ""you're right."" (Charades uses the double-nose-tap to indicate correctness). Being done with the argument, and the GUI fan, the Guru then disposed of him.In a GUI this is usually done by dropping something in the trash.In the GUIs of the day, it was ""clever"" to have a graphic of a small dog peeing on files that were being deleted.  This action reminded the fan of his beloved GUIs, and allowed him to translate the argument of the master. Having translated it, he became ""enlightened"" (a word usually reserved for those who find intellectual and spiritual fulfillment. These koans reserve that term for becoming a fan of Unix.)  TL;DR: I saw the sign, and it opened up my eyes. "
"PHP has very little boilerplate, & is super-easy to setup & configure.","PHPs web-model (one-script == one page) and then you just call the script is so mind-numbingly simple (and impractical for large projects, that all end up using rewrite rules) that no one else uses it.  Create a file called ""foo.php"" put some magic &lt;?php ?&gt; in it and let the programming begin.  It integrates with existing static HTML pages well: you just change the extension to .php and add the aformentioned PHP tags to say integrate a PHP based ad library or hit counter.  Again, it's hard for me to argue with developers that chose the more flexible, router (usually with wildcards and such) based model for development, but the PHP deployment model is just so frickin simple to use. (as long you don't need to do anything non-trivial)  TLDR: PHP has very little boilerplate, & is super-easy to setup & configure. "
"Patents aren't copyright for manufacturers, they're supposed to reward new and non-obvious ideas.","> Still there is a problem by making an ""no math is patentable ever"" rule. The thing is that every algorithm you can implement in software, you can also implement in hardware, with no exceptions. The simplest example I can think of is to use transistors or NAND gates to make up a simple 4-bit adder, versus to use a 5-bit lookup table with 256 entries. Hardware designs themselves even love lookup tables stored in ROM.  Patents on actual hardware are supposed to be for non-obvious creations. I would argue that a simple array of nand-gates implementing an algorithm is very obvious at this point, and thus inherently unpatentable. That doesn't make computer hardware unpatentable, as this design could have energy saving properties, or heat reduction properties, etc. that are new and non-obvious, which would warrant a patent.  TL;DR: Patents aren't copyright for manufacturers, they're supposed to reward new and non-obvious ideas. "
"The Dependency Elimination principle is not beyond SOLID and does not eliminate dependencies, only the type of dependency.","How is this beyond SOLID?  OCP is still very important. Even you have Whole Values, you have algorithms that operate on those Whole Values. Are those Whole Values closed to changes in the algorithm, and is the algorithm closed to changes in the number of types of Whole Values it operates on?  LSP is still very important if you intend to have generic algorithms that can operate on different types of Whole Values. You either depend on the concrete type of the Whole Value or an interface/base class relating Whole Values.  ISP is still very important for generality. Two different algorithms might not depend on the same attributes of all Whole Values. You may not want to couple different algorithms to the same type of Whole Value, allowing the algorithm to specify which attributes are necessary and thus allowing them to operate on otherwise unrelated Whole Values. How do you do that? Interfaces.  DI is still very relevant. Whole Values are a type of dependency inversion. The algorithm is either provided the values or provided the abstract provider of the values, instead of getting those values itself. The algorithm either sends events or provided the abstract receiver of values, instead of figuring out where the values go itself. In either case, the dependency is inverted. DI means don't call me, I'll call you.  TL;DR - The Dependency Elimination principle is not beyond SOLID and does not eliminate dependencies, only the type of dependency. "
"I get how markup is prerendered and passed, but how do events and variables get passed?","Isomorphic JavaScript is a very exciting concept to me, but I have a question based on the linked article.  I get that the markup is generated on the server then delivered, then the static JavaScript is delivered, but how are the bindings and variables in memory that are dynamically created passed to the browser?  For example, say on page load my app fetches the todo list models and renders the templates. Cool, all that markup is rendered on the node side and pushed to the browser. But, what about the bindings for the todo list buttons that are bound by a promise after successful model fetch that will now not happen in the client? Or what about some token that's set as a property of an object on page load?  TL;DR: I get how markup is prerendered and passed, but how do events and variables get passed? "
"what is the actual problem that you mean by ""bloat""?","> and not only kill the ant but the colony, one of your toes and make a huge whole on the ground  No this is wrong. That part of the metaphor implies there are negative consequences.  If the overhead of a library is not an issue (and it rarely is), then there are  no downsides . You keep talking about bloat, but you need to explain what you mean by that: what is the problem (performances excluded, as said above). Is it the number of files? If so, how is that a problem?  Here is another metaphor: for going from A to B, you can take a car, a train or a plane. Sure you could go by foot! And going by foot makes sense in some cases. But most people wouldn't do that to cross a whole country. And yes a car is heavy, it has 4 seats and maybe you are alone, and it can go 200 km/h but the speed limit is 130 km/h. But that still make sense, in some situations, to use the car.  TL/DR: what is the actual problem that you mean by ""bloat""? "
"We hate Microsoft because they hate us, and they treat us with contempt and disrespect.","Because they've wasted more person-hours of human life than any other single organization in all of history?  The hours spent needlessly reinstalling the OS, and rebooting, and fighting with virus and worm infections, probably add up to millions of person-years at this point.  And none of it was necessary.  Win95 was the biggest-selling software product ever produced up to that point.  They could have reinvested that money in a proper redesign of their OS, with security and modularity as primary design goals.  Instead they released Win98 with a web browser built into the kernel, arguably the single worst software design decision ever (at least from a perspective of making the OS secure).  They never cared to deliver a quality product to the customer: they only cared to get as much money as possible.  Once they had a web browser on zillions of desktops, they actively worked to undermine web standards, making everyone's web experience poorer in order to improve their profit position.  tl;dr: We hate Microsoft because they hate us, and they treat us with contempt and disrespect. "
"PHP is an aquired taste, and in the hands of a competent and experience programmer can be just as effective/productive/buzzword-here as any other language. Don't buy into hubris.","Too much opinion, not enough fact.  > PHP leaks memory like crazy and is slow.  Solved in 5.3, but with a small hit to runtime performance. As for being slower - wtf? Can you substantiate this whatsover?  > There are actually a small number of libraries available considering the size of the userbase.  Show me a library that's available in another language that doesn't have an equivalent in PHP.  >PHP is slower to get things done than equivalent languages (ruby, python, perl) by a decent margin.  Pure subjective bullshit. Might as well argue that vanilla is more tasty than chocolate. Or that unicorns are sexier than narwhals.  >there is also a shitload of PHP fanboy-ism  Replace PHP with just about any language and your statement holds true.  >As a programmer: I think PHP is shit  Shit might be a strong word, but I otherwise agree. But opinion doesn't trump fact, and the fact is that more successful projects have been built off PHP than Python/Perl/Ruby/LanguageX.  >PHP is a good choice when quality doesn't matter much but cost matters a lot and the project itself is very simple  PHP has its fair share of problems, inconsistencies, and idioms - more so than other languages - but at the end of the day it's about getting shit done, and that's all that matters to the guy signing your paycheck. If someone can't write decent code in PHP, chances are he's not going to have any better luck in another language.  tl;dr - PHP is an aquired taste, and in the hands of a competent and experience programmer can be just as effective/productive/buzzword-here as any other language. Don't buy into hubris. "
"If you have only worked with Web Forms, and haven't played with the Microsoft MVC library, then you are missing out.","I'm assuming your talking about Web Forms (The old way to do ASP.NET). Yes it sucked, largely because of all the crap Microsoft bolted on top to try and make the development process as close to Microsoft Application Development as possible.  Take MVC though, where they ripped it back down to the bare metal and built a REAL nice framework around it, and it has become my preferred development model. You are in control of your HTML and Javascript again the way it's supposed to be!  I understand hating on Microsoft for their mistakes with Web Forms, but give credit where credit is due with how nice they made MVC.  tl;dr: If you have only worked with Web Forms, and haven't played with the Microsoft MVC library, then you are missing out. "
"No, pointers in C++ are not integers.  Counterexample is x86 ""far pointers"", which are not simple integers.","Technically, they needn't be.  In the  x86 ""real mode""  with an address value (also a 16-bit integer).  The memory address accessed is the segment value multiplied by 16 plus the address value, creating a 20-bit memory address value.  Getting back to C and C++, there is no reason that a C compiler couldn't implement a pointer as a datatype that contains these two 16-bit values, so that when you use the pointer, it emits code to set the segment register.  Not only can this be done, but it has in fact been done.  Such a pointer is called a "" far pointer .  A C compiler can use far pointers for everything and still be compliant with what the C standard allows for pointers.  You can still add integers to pointers, yielding new values that are within an allocated memory space.  (As far as I know the C standard does  not  require supporting things like having two pointers into two different arrays (or two different  malloc() ed memory areas) and doing pointer arithmetic on these two pointers.  Subtracting two pointers to different elements of the same array should yield a meaningful integer value.  Subtracting two pointers to different elements of different arrays is not, as far as I know, a defined operation.)  Anyway, these are legal C pointers that actually exist in some older C compilers, and they are not integers.  They are a linear combination of two integers!  This might seem like it doesn't matter since x86 real mode is kinda obsolete.  However, there's no reason a similar architecture couldn't be developed for performance reasons today.  You could develop a NUMA system that uses something like segments and offsets where the segments are literally different pieces of memory hardware.  And you could code for such a system in C/C++.  TL;DR:  No, pointers in C++ are not integers.  Counterexample is x86 ""far pointers"", which are not simple integers. "
PHP is one big epic fail; why do I bother?,"And I'm sure using PHP to write desktop applications is a brillant idea, but the issue here is that a weak dynamically typed effectively mimics function overloading by doing vastly different things depending on the type of the argument. This is the same language where  5 + '5' = 10  -- don't you see any conflict between those two approaches?  A sensible alternative would be requiring an error code  and  a message (annoying as most people don't need the error code and messages don't always make sense when you want an error code; PHP doesn't have keyword arguments either, so you'd have to decide which one comes first) or using different functions. Say, making  exit()  take an optional number argument and making  die()  only take strings (and return with a non-zero exit status iff an argument was passed).  tl;dr: PHP is one big epic fail; why do I bother? "
Dijkstra didn't really say anything truly useful about the matter.,"Dijkstra's argument was purely emotional:  > Adhering to convention a) yields, when starting with subscript 1, the subscript range 1 ≤ i < N+1; starting with 0, however, gives the nicer range 0 ≤  i < N. So let us let our ordinals start at zero: an element's ordinal (subscript) equals the number of elements preceding it in the sequence. And the moral of the story is that we had better regard —after all those centuries!— zero as a most natural number.  The ""nicer range""?  Are you serious?  Oh, let's make sure all our programs are ""nice"" now shall we?  I think we can write a static analyzer to evaluate programs for ""niceness"", what do you think? /s  The bottom line here is that working with zero-based vs. one-based requires working with either one of two bits of cognitive dissonance:   For zero based: always starting your counting with zeroes, which may or may not be an issue for you personally depending on your culture of origin  OR   For one based: always remembering to end the upper bounds of your loop at the maximum length + 1.    Claiming that one or the other is better for everyone is silly.  If the convention in your environment is zero or one-based, then certainly do that in the interest of reducing errors.  If you're going to design a programming language for mathematicians, guess which one you should use?  tl;dr = Dijkstra didn't really say anything truly useful about the matter. "
"interviewing people is hard, and the people who think it is easy have nothing of value to tell you.","Because nobody works that way?  Sophisticated algorithmic solutions to problems are a sober topic suitable for discussion at the beginning of a project milestone, not something you spring on people.  To put it another way, the only situation in which you literally have an hour to fix a problem is when the shit has hit the fan.  You have a bug in a live system, or a preproduction bug and the deadline is coming.  What you want is two or three obviously correct solutions to the problem, and we will pick the one with the least opportunity for blowback.  If you are trying to cook up obscure solutions at the drop of a hat that do nothing but prove how clever you are, then you're a deranged lunatic and I don't want you anywhere near my code.  Tl;dr interviewing people is hard, and the people who think it is easy have nothing of value to tell you. "
let us stop blaming opengl for crapshoot that is AMD drivers are on Linux?,"AMD's proprietary drivers perform like absolute crap when it comes to gaming. I don't think it has anything to do with OpenGL because even a under-specced NVidia card outperforms a better AMD card on Linux (when it comes to gaming).  For example, With R9 270 - I get like 20-30fps playing Dota2 on Linux. I used to get nearly 50fps with GTX 650 on otherwise same hardware. GTX 650 is slower card than R9 270 (By far infact).  Whats more, people report that, Open source AMD drivers perform better than proprietary ones. But since they don't support latest graphics cards, using them is not always possible.  TLDR; let us stop blaming opengl for crapshoot that is AMD drivers are on Linux? "
"we fired our boss for incompetence and are doing much better now, thank you.","i worked for the same dot com for eleven years.  each two to three years, the people who owned said dot com would drive the company into the ground, fire the staff down to the core four, bounce paycheques for a couple of months then find new funding, develop a new (and unbelievably unrealistic) business plan and start the whole phoenix rollercoaster yet again.  for the first four or five years, the power balance was definitely ownership-down. annual reviews were done and some folks didn't get contracts renewed. ndas and employment agreements were draconian (and unenforceable) in the extreme. employee dissatisfaction was rampant.  after about five years (and three total collapses of the company), the attitude shifted dramatically. employee contracts started to get edited by workers before being signed. nda's got sent back. new hires were mentored by existing staff to treat the three month probationary period as management being on probation. the gist of many conversations with ownership was: ""in six years, you've bounced a year's worth of my paycheques. the only agreement i'm signing is the one i write myself.""  three years ago, after the fifth collapse of the company, the original four workers (me being one of them), cut a deal with the owners to take the entire company's ip in lieu of back wages and we formed our own company. said former owners are now a client of the new company (one of many); if they have a hare-brained business scheme, they can pay to have it built like anyone else (we actually offer then a 25% discount for ""old time sake"" which i suspect is just us being suckers).  although the first eighteen months of going it alone were very difficult, since then we've been far more financially stable than under the previous ownership. we have a stable of clients that span a variety of industries and project sizes, and while managing several clients takes more overhead than just dealing with one owner, there is stability in diversity. i've not missed a paycheque in a year.  ymmv, but my experience is that a lot of bosses just take a large slice of the money pie and, in the end, contribute a disproportionately small amount of effort to the business' success. if your owner or top manager takes a massive salary and doesn't really  do  anything, they might be worth firing.  tl;dr: we fired our boss for incompetence and are doing much better now, thank you. "
This is not normal X11 behaviour. Something is actually broken. Try fixing!,"<scratches head> Well, there's certainly  something  wrong; because X11  isn't supposed to do that , and -in fact- normally doesn't.  (edit) Wait, I see you're saying there's a wireless router in between. Even if you think it's working perfectly, there might still be interference from your neighbours, reflections, or simply too much distance between you and the router.  See what happens if you:   Move closer to the router   connect the machine directly to the cable.    if your problem goes away, you'll know it's the wireless connection. (check to see if the neighbours have a router on the same/neighbouring channels, move the router around a bit, replace it, add relays,etc, depending on what the issue is)  If your problem doesn't go away, connect directly to the wire, and try use a live-CD on the laptop. If the problem goes away, you most likely have a problem with your X11 installation. If the problem doesn't go away, you have a hardware problem.  tldr; This is not normal X11 behaviour. Something is actually broken. Try fixing! "
"Google is only ""evil"" if you let them be.","> Drive past your house, taking pictures and sniffing your wireless connection in order to geo-locate you.  Yeah, they have a picture of my house.  Oh shit!  They don't, however, have a pairing of my wireless AP to my location, since the APs MAC changes daily.  > Sniff everything you type into your URL bar.  Too bad I don't use their browser.  > Track nearly every webpage you visit while building up a personal profile, via: Google Analytics, DoubleClick/Google Ads, Google Toolbar, and potentially Google Chrome(?).  Ditto for the last one and I don't use their toolbar.  I'm also not a moron, so ABP blocks both GA and Doublclick hosts from being contacted.  Even if they could be, JS from them doesn't run thanks to NoScript.  And even if I didn't have those, my house DNS server resolves all of their domains to 127.0.0.1.  And even if it didn't, their netblocks are blocked by the egress policies on my edge firewall.  > Track most of the videos you watch (via YouTube)  No Flash, no YouTube.  > Track your location on your mobile phone.  The fuck they do.  > Intercept and relay all your phone calls, performing speech conversion analysis on your voicemails (that we know of... there may be more)  For the calls that I make through Google Voice, this is true.  Unfortunately for them, that is a very, very small portion of them, and ones that I myself record and consider public.  > Store all your contacts, email messages, documents.  They do indeed have my contacts.  Well... part of them.  They have my public contacts.  Not being an idiot, I simply don't put private or potentially-incriminating contacts on their servers.  They have precisely 0 e-mail messages and documents from me.  > Remember all your search history dating back years.  Pretty impressive, considering that I haven't used their engine for 2 years, and haven't allowed cookies from their site for close to a decade.   tl,dr;  Google is only ""evil"" if you let them be. "
even if you are not unionised you can still organise.,"Many years ago I worked for a small company which had a culture of developers working long hours with no paid overtime.  The more you do it, the more management expects you to do it - resulting in a vicious cycle.  The breaking point came when a co-worker had worked all night to finish a piece of work by some arbitrary deadline and in the morning, exhausted, she sent the final product to the boss to be approved.  Where it sat in his inbox for two weeks before he got round to looking at it.  After that all the developers got together in the pub and decided we had had enough.  The next day we all went to the boss and told him that in future any overtime would be done only if paid for at an hourly rate.  If he did not agree we would work strictly to rule and only put in the hours we were contracted for.  He was not happy but he reluctantly agreed, and we had our contracts amended to  give us paid overtime.  After that the need for overtime fell off dramatically - once the company had to pay for it they were far less inclined to ask us to do it except when absolutely necessary.  TL;DR:  even if you are not unionised you can still organise. "
"is that 
 
 ITT computes nicely, but you can prove functions are equal, and  
 ETT allows you to prove functions are equal, but doesn't compute nicely.","HTT isn't a language, really. It's aim is to provide a theoretical foundation for a future proof assistant.  HTT is probably not as well suited towards working programmers, as the average programmer has no real use for the advanced features. Higher order inductive types in HTT allow you to define types which act like topological spheres and toruses. Most programmers are not interested in homotopy at all.  Conor McBride proposed a more programmer-friendly type theory called Observational Type Theory (OTT). The difference is that while HTT has many distinct kinds of equalities, OTT has just one (made from squishing all sorts of useful equalities together).  However, both of these theories are still bleeding edge research. Most languages today are based off of either ITT (intensional type theory) or ETT (extensional type theory). The tl;dr is that   ITT computes nicely, but you can prove functions are equal, and   ETT allows you to prove functions are equal, but doesn't compute nicely.  "
"no matter how much lipstick you put on that silo, it is still not a pig.","No.  There are plenty of functional, sane implementations of devops, agile, and lean (etc?) which successfully implement ideal scenarios. In fact, a big part of what defines those methodologies is their basis in real-world data and real-world implementation. It would be more akin to the government of the United States rebranding as Mao-inspired Communist without changing anything else except how the government employees and elected officials talked about the things they did. Daily activities still 100% the same, rights and responsibilities of citizens 100% the same, democratic election process with electoral college circus still the same, federation of government, actual ideals-in-practice still the same.  No matter how many times they insisted that they were Mao-inspired Communists living the Mao-inspired Communist dream, there would still be fairly little Mao-inspired Communist activity going on and a whole lot of Democratic Federated Republic action, quite a lot of Capitalism, a healthy serving of Oligarchy, turtles, etc.  tldr; no matter how much lipstick you put on that silo, it is still not a pig. "
C++ is important because of the massively widespread use of it. It's everywhere and that's really what  makes it important to understand.,"C# is largely windows/microsoft, Objective C is largely apple/iOS. Keep in mind neither of these are similar to C++.  C++ is the evolution of C making it possible to get C performance with higher levels of productivity and abstraction.  Why choose C++ above all else? Mostly because almost everything is written in C++ (or C, of which C++ is a superset) or was at one point written in C++. Everything from chunks of the linux kernel, to the windows kernel, to compilers for other languages. For example, the Python interpreter, Go compiler, D compiler etc are all written in C/C++. The reasoning for this is two-fold. First, C++ is very perfomant and gives alot of control over low-level operations which makes it ideal for building things that need to scale well (i.e. compilers etc). Second, most languages cannot compile themselves. Because of C++'s age and maturity it's one of the few languages who's compilers can compile itself making it a totally self sufficient language.  Other languages can compile themselves today but C++ was there first so many languages were originally written in C++.  TL;DR : C++ is important because of the massively widespread use of it. It's everywhere and that's really what  makes it important to understand. "
Crazy rant about how I want to code in haskell but sometimes it sucks. Is there a support group for people who have come to this realisation?,"Lazy evaluation can lead to surprising cases of terrible performance too, also profiling and especially debugging are all much harder to think about.  I'd say after 6 months coding Haskell that if I needed things to go reliably fast (say less than 2 times slower than Java) I'd choose Java or C/C++. Sure it can go fast, but high performance code really relies on you being able to reason to some extent about what the hell is running on the CPU. Lazy evaluation makes that so much harder.  I'll probably still write Haskell because programming without type inference seems medieval, and loops instead of maps/folds/zips... ugh.  Oh shit, and non-mutable data-structures.... yeah I know they can be done - but you might as well code in java or C by that point.  Anyone got suggestions for how to alleviate these issues?  tl;dr: Crazy rant about how I want to code in haskell but sometimes it sucks. Is there a support group for people who have come to this realisation? "
"I weep for my field because so many recent grads have no understand of actual computers, because colleges coddle them with walled gardens like C#.","I provide ""computer science support"" to an engineering team that is heavily involved with simulation and modeling, consisting of various mixes of hardware and software in the loop.  You might imagine, then the types of code, and languages I work with (and have to get talking to one another). Hint : buggy and ancient  When I joined the team (I am the only programmmer among about 20 engineers), the engineers wouldn't give me the time of day, until they discovered I could pointer math and algebra that will make your eyes bleed, in my sleep.  They said that all of the other programmers before me had been a waste of time, because they only knew how to work in Java, and were afraid of manual memory management, or else just wrote slow code, and suggested the team's hardware was too slow.  TL;DR I weep for my field because so many recent grads have no understand of actual computers, because colleges coddle them with walled gardens like C#. "
a different paradigm. You must think in another way to solve problems. But there is nothing that makes it a worse paradigm in general for web applications.,"I think your viewpoint is interesting, but I am afraid I must disagree with it. The notion that you need Object Orientation to harness the complexity of a web application is not a premise at all. I programming language like Haskell has many other concepts which can stand-in or replace the usual object oriented design construction.  You will have to rethink the way your application works. The  gist  is that in a language like Haskell, the data  flow  is more important than the flow of control. Hence, Haskell is very good at manipulating the flow of data. If you think about it, Object Orientation, as is practiced in most languages is about control flow. The methods you define become central in your thinking. They define the semantics of what you can do with the data inside the object. And therein lies the devil: you must think of data as a steady flow of change rather than what the program is doing to manipulate your data.  TL;DR - a different paradigm. You must think in another way to solve problems. But there is nothing that makes it a worse paradigm in general for web applications. "
"Useful alternatives but always look at your execution plan and run some tests to see which of the three (FULL OUTER JOIN, CROSS JOIN, UNION/GROUP BY) is fastest.","I remember bumping into this article a few years ago. The discussion in the comments go through the differences over FULL OUTER JOIN, CROSS JOIN, and UNION/GROUP BY. As with everything, you won't really know what is best until you look at the actual execution plan to see what works best (things will change depending on table relations/indexes/etc.). In my personal experience, I only ran into one query where a FULL OUTER JOIN ended up faster than either a CROSS JOIN or UNION/GROUP BY (I could never tell which would be faster of the latter two without an execution plan).  tl;dr;  Useful alternatives but always look at your execution plan and run some tests to see which of the three (FULL OUTER JOIN, CROSS JOIN, UNION/GROUP BY) is fastest. "
"what matters for speed are number of I/O per second and not MB per second. 
 Corollary: the HD industry fooled us around for years about performance","what thoomfish said plus the fact that while accessing sequential cells in an SSD is (mostly) irrelevant, it is not on the OS side of things: a sequential read of several megabytes can be done with a single operation on the bus, while reading a total of several megabytes in various places need multiple operations.  That's why SSD performance is not measured in (peak) MB/sec but in I/O operations per second (iops) for a given load (sequential read, sequential write, random read, random write). SSDs are not (currently) bandwidth limited (but their current limits is several orders of magnitude higher than those of mechanical HDs).  Typical desktop speed perception of storage is depending far more on random performance than sequential, that's why SSDs feels MUCH faster than HDs: their sequential speeds are only 3-5x, but their random speed are 100x  TL;DR: what matters for speed are number of I/O per second and not MB per second.  Corollary: the HD industry fooled us around for years about performance "
"you're a moron. But don't worry, the fact that you're getting upvoted means you're not unique in this instance.",">Python is sadly not an OOP language.  Yes because there is exactly one flavor of object orientation, and everyone agrees unanimously on the definition.  >A class, in essence any object, that does not know what ""self"" is, is not OOP.  I'm not even sure what this means. Without knowing your background, I can't even begin to guess what you consider to be a ""real OOP language"". In any case, Python automatically binds  self  for you when creating a new instance:  class C(object):  def foo(self):    print ""Foo func""c = C()print C.foo # unbound method C.fooprint c.foo # bound method C.foo of &lt;__main__.C object at 0xDEADBEEF&gt;C.foo() # TypeError, C.foo must be called with C instancec.foo() # ""Foo func""f = c.foof() # ""Foo func""C.foo(c) # ""Foo func""  This is quite similar to the automatic binding of the special pointer  this  in C++ during method application. I am bewildered by the sentiment that simply making self explicit somehow precludes Python from object orientation.  >It's the biggest design mistake Python did to require explicit self, followed by mandatory indent (for purity reasons too, a programming language should not care about indent per se, unless you would give it as an OPTIONAL feature to the language).  This may well be the most confused thing I've read on proggit today. How in the hell is explicit  self  in method definitions at all related to mandatory white space? If you don't like mandatory white space, don't use Python. Or use the hidden braces feature via  from __future__ import braces .  >Being able to omit ""end"" is however really cool. I'd wish Ruby would have a strict mode available by default for a very opinionated but even terser style to write.  I'm really not sure what this has to do with anything.  >Lines of code are a problem. Complexity tries to hide behind lines of code.  Wut.  Tl;dr  you're a moron. But don't worry, the fact that you're getting upvoted means you're not unique in this instance. "
"mc and double commander rocks. 
 Edit: I forgot about FAR, from the author of rar/winrar. Console browser manager for windows. I also used it, but not much.","Fucking yes.  I''ve being using Norton Commander style software since their msdos versions.  Total commander for windows was the first program I ever downloaded from internet.  In linux, mc from then start, but I was lacking a gui version. Tried krusader,  wasn't too convinced about it, but then I discovered Double Commander, and I was so so happy.  Can't go back to normal browsers. And in console at least for me mcedit is so much comfortable than any other text editor. Sure there are more powerful ones, but mcedit does everything I need in console mode, otherwise, I use a gui editor.  TL;DR; mc and double commander rocks.  Edit: I forgot about FAR, from the author of rar/winrar. Console browser manager for windows. I also used it, but not much. "
"Don't split data into historical/delete tables, learn how to use indexes.",">Why not move deleted records to an external table instead?  I've seen datbases which are used for software distribution in massive 10's of thousands of seats enterprises. The DB's were created by people who thought databases were to be designed just like spreadsheets.  When the performance of these fucking horrendous tables got bad, some clever dude decided to create historical tables and move data into them from the main table. Great, performance increased massively. What they didn't understand was that to find historical data was now a complete nightmare, and people needed historical data for billing purposes and such. Another disaster.  What they should have done was (despite learning how to database properly), was simply to create some indexes on the original table to improve performance.  People would be surprised how much performance you can squeeze out of a database, even in tables with millions of rows, with proper indexing.  TL;DR; Don't split data into historical/delete tables, learn how to use indexes. "
"even the simplest possible coding exercises for new developers can go surprisingly wrong, even when done by  professionals . 
 PS: 
 echo $name[0] . "" wrote: "" . nl2br($text[0]);","> I didn't understand why it was coded the way it was. It didn't make sense this way. But there must be a reason, because these are professionals doing this, and I'm just fresh out of college.  about 16, 17 years ago i started at a tiny webdesign company. i was hired as a designer, because it was a ""design studio"", even though i've just finished a school focused on IT (programming and sysadmin stuff). they used free guestbooks (with ads and ""get your own"" links) for really big customer websites, because nobody there knew how to code or even how to host a php script c&p'd from the web.  well, i complained that free guestbooks on a website with several thousand visitors a day looked really unprofessional, and that they should get a real one. so they hired a professional (curiously from the same school where i came from) who wore a suit and ran his own  it company  - and he coded a guestbook for them. it was expensive but also a bit buggy, so a couple days later i inspected the code:  $sql = ""select name from guestbook where id="" . $id;$res = mysql_query($sql);$name = mysql_fetch($res);$sql = ""select text from guestbook where id="" . $id;$res = mysql_query($sql);$text = mysql_fetch($text);$sql = ""select date from guestbook ...""   and so on. for  every single column  (except  id ). i though really hard why he did it this way and not just comma-separate the column names, until it hit me: he just didn't know any better.  i fixed it in a couple of minutes, told them to never hire that guy again and was 'promoted' to lead programmer (well, i was the only one - and stayed the only one long after the workload grew way too much for me. they didn't hire other devs though because they were a design studio, not a coding shop).  TL;DR: even the simplest possible coding exercises for new developers can go surprisingly wrong, even when done by  professionals .  PS:  echo $name[0] . "" wrote: "" . nl2br($text[0]); "
"They would rather pay you off, if it's cheaper.","I am not sure exactly how legal it is per se, but I do know this:  If what the squatter of the domain name is looking for is under 5k he is more likely to get the money than be brought to court. Lawyers cost money and time that nobody wants just to get the stupid thing. But if people are charging too much for a domain name they are not doing anything with, they will be brought to court and lose the domain name. In such a case how much compensation the squatter is given, if any, is unknown to me.  TL;DR: They would rather pay you off, if it's cheaper. "
the advice he offers is imminently reasonable but he chose a means of delivery that practically guarantees that it will be ignored by many and perhaps most people here.,"I think that if this guy had made his point by simply saying, ""Don't let your dislike of a language get in your way of understanding how to use it effectively when you need to use it,"" (which was really the central essence of his article) then more people would have been inclined to read this article closely and take his advice seriously.  Instead, though, he put in completely unnecessary bits in his article along the lines of, ""All languages have their faults and JavaScript is no exception, so if you don't like JavaScript them ultimately it is you and not JavaScript which has the problem,"" which is a bit offensive (though I am sure it was not intentional on his part) and turns people off from the important parts of the message.  tl;dr:  the advice he offers is imminently reasonable but he chose a means of delivery that practically guarantees that it will be ignored by many and perhaps most people here. "
"The article is about the mathematical method, and not the task of swapping variables.","I think some people are missing the point about this article, and jumping straight on the ""Don't do this ever, compilers are smarter than you"" bandwagon.  Firstly, it's an article about how to apply group theory to prove a given binary operator is suitable for your task. Using it to swap variables is one example, but you could apply it to some other two-variable task such as calculating the distance from a point, or some cryptographic hash function.  Secondly, if you are taking the article at face value, it is easy to conceive of a system that is limited in memory, registers or has a dodgy compiler where this trick is useful. If you've ever had to write a compiler for a school project on a test-bed cpu, then you'll know.  On most common machines, swapping variables like this is impractical, but it's important to know that things like this exist, and how to prove they're correct.  TL;DR: The article is about the mathematical method, and not the task of swapping variables. "
Django vs Flask depends on the project. Both are good for certain things.,"I  like  Django and I  like  Flask.  I use both of them to varying degrees and would pick one over the other depending on what I'm working on. Flask is great for getting small python projects out there really quick. Search suggestion engine powered by redis ? Flask. Tagging interface mostly doing json stuff ? Flask. Listener for certain web events ? Flask.  But if I want to build a massive social network style platform and do it quickly, I wouldn't pick Flask. I'll go with Django for that. I like the 'batteries' included because most of the batteries are needed by me for such a project. Sure, Flask can do anything that Django can - no doubt about that, but I'm happy to use Django for larger projects. It has a lot better infrastructure for horizontal scaling/i18n stuff/etc. in the event that things get big/popular.  With Flask you can build things the way you want to, customize each little piston in the engine to push maximum performance. With Django you get an entire car but you can still drive it any way you like, you can even (with a little pain) replace those pistons and do some engine mods (Pinterest/Instagram have done so.. so has Disqus).  TLDR; Django vs Flask depends on the project. Both are good for certain things. "
"Computers are pretty darn secure, no matter the creed. Users are the weakpoint.","""far more secure"" is mostly just smaller user base, users know what they are doing more often (okay maybe not true with mac, but definitely with most other OSes), and less crappy software.  Yes it's true windows did default as admin until vista, but then they implemented UAC. It's pretty darn close to the system ubuntu uses, the only difference is that every windows user turns it off as soon as they can.  But it's also true that ""viruses"" are not very common. Potentially Unwanted Programs are really the big issue, and they exploit people's laziness and stupidity which is a much bigger vulnerability.  TL;DR; Computers are pretty darn secure, no matter the creed. Users are the weakpoint. "
"referential transparency"" as a definition confers explanatory and predictive power—exactly what you want from a definition.","> This just means that someone once came up with a definition that they liked.  Even if we accept this  arguendo , it begs the question: why did they like it?  ""Referentially transparent"" programming, as a definition, has clear benefits:   It's language-independent (indeed, you can write C referentially transparently).  It's machine-checkable whether any given function in any given language is referentially transparent or not.  It's easy enough to determine whether any given language makes referential transparency nearly impossible (C), easy enough to be worth doing consistently (Scala), or nearly impossible to avoid (Haskell).  The benefits of referential transparency to concurrent programming in particular and correctness generally are relatively easy to demonstrate.   tl;dr ""referential transparency"" as a definition confers explanatory and predictive power—exactly what you want from a definition. "
"we don't need shadowy conspiracies to explain the evil of corporatist America. With ""friends"" like these-- our corporate leaders-- who the fuck needs enemies?","VCs do the same thing, and corporations outside of technology, and wealthy investors who go to invitation-only rich people events (i.e. car bomb magnets after this country gets ""Oktober'd""). When a few people have a lot of power, you get corruption inevitably.  ""Conspiracy theories"" that rely on shadowy cloak-and-dagger shit are silly. There isn't ""one conspiracy to rule them all"". But when a small number of people, with similar interests, hold all the power, you don't even need shadowy conspiracies to get self-serving, conspiratorial behavior. What would be surprising is if the boardroom elites weren't acting in concert to loot society.  TL;DR: we don't need shadowy conspiracies to explain the evil of corporatist America. With ""friends"" like these-- our corporate leaders-- who the fuck needs enemies? "
"cryptdir itself does not encrypt, it uses the very weak crypt program.","The source is really short and simple.  Basically, it just calls ""crypt"" on every file of the directory. This means, it's as secure as the ""crypt"" program in your OS is. In most OS, you shouldn't use it. Most Linux Distros don't include it by default. It's very (very!) weak, sometimes a variation of the Enigma algorithm, sometimes worse (caesar cipher, anybody?).  There are several better crypt-replacements (mcrypt, ccrypt, bcrypt) that do a decent job, cryptograohically speaking. It should be trivial to adjust the cryptdir source to use them, or even easier, on some distros they might get linked to the crypt executable.  TL;DR: cryptdir itself does not encrypt, it uses the very weak crypt program. "
"beginners and novices that are reading this sort of stuff are going to get as ""bend, frustrated, and confused"" as they would with most of the material posted here.","> Some of the experts may be getting a kick out of this (though I suspect most aren't even all that amused) but the beginners and novices don't have the knowledge yet to filter out the crap so they are just getting bent, frustrated, and confused.  How many beginners and novices hang out here on r/programming and don't feel completely bemused anyway? Same goes for hackernews. I'm not saying ""If you're a n00b, stay out"", but there tends to be an environment where new folk will lurk, and probably go away until they understand it. And chances are they're not going to follow up any blogs until they tend to stick around here for a good while.  So the new folk will wander away just as confused as if they'd read some insightful post about the underlying interrupt mechanisms of the Linux kernel.  At least with this talk like this on reddit and hackernews, people are sharing all sorts of views behind what they think is going on, and might make the new folk realise that there is no One True Paradigm To Rule Them All; and if they walk away firmly grasping that one concept, they're probably going to be better programmers for it.  tl;dr - beginners and novices that are reading this sort of stuff are going to get as ""bend, frustrated, and confused"" as they would with most of the material posted here. "
"stop being a dick to average users 
 &nbsp; 
 *and cosmotoligists, tradesmen, teachers, professors, investors, carpenters, factory workers, artists, musicians, writers, sculptors, and grandparents.","> Users who waste their time bitching at developers instead of at least submitting bug reports shoud get the blame instead. They are the ones that poison the community.  There it is! You just said it, which is illustrating what half the people on this Reddit thread are saying about the  hostile  Linux community. Experienced users have this horrible attitude toward common, average users who haven't made it their life's vocation to understand the inner workings of computers and software. Common, average users don't have the time to figure out why their computer is broken and how to submit a bug report suitable for a developer. The common users are doctors, nurses, lawyers, salespeople, self-employed, entrepreneurs, etc^*. They don't speak ""puter"" the way you do. Linux on the desktop  is supposed to mean it's accessible to the very users the community is constantly insulting.  Call them n00b, call them lazy, call them stupid, but don't cry and circlejerk wondering why Linux will never oust the Windows and OSX desktops when you insult the very users you  need  to reach critical mass.  Secondly, who the hell knows how to submit a proper bug report? Only experienced users, developers, and sysadmins. Your answer is terribly insulting. If you want Linux to gain widespread acceptance, then stop being a jerk to newbie users who don't work in the I.T. field.  TL;DR: stop being a dick to average users  &nbsp;  *and cosmotoligists, tradesmen, teachers, professors, investors, carpenters, factory workers, artists, musicians, writers, sculptors, and grandparents. "
"yes, GC is cool, allocating by bumping a pointer is cool, but any claims of “this makes it faster” need to be made very carefully.","> A GC can just allocate memory by incrementing a pointer (usually) and it will do compaction later.  [...] often a GC will give you higher speed  People often claim that bump-pointer allocation is faster, but any claim requires substantiation (and in a world where technology continually advances, that substantiation needs to be  recent ).  So, let's try to substantiate it... Part of the argument is that  malloc  is slow. Let's check that. Take a look at this code; it repeatedly  malloc s and  free s a block of 1024 integers (every integer is allocated and freed separately).  #include &lt;stdlib.h&gt;int main(){    const int SIZE = 1024;    void* allocs[SIZE];    for (int j = 0; j &lt; 100000; ++j) {        for (int i = 0; i &lt; SIZE; ++i)            allocs[i] = malloc(sizeof(int));        for (int i = 0; i &lt; SIZE; ++i)            // Free all allocated chunks in ""random"" order.            free(allocs[(i * 47) % SIZE]);    }}  This program runs in 1.86 seconds on my Linux box (Ubuntu 14.04, Intel Haswell CPU), which means each  malloc / free  pair takes  18 ns  to run.  Now, that  is  much slower than bumping a pointer (bumping a pointer is probably less than 0.5 ns, but we may also have to check against a limit and conditionally branch if it's hit), but bumping a pointer is only the allocation half the story. Factoring in the complete GC overheads is more complex and highly dependent on the GC algorithm, its implementation, and the particular program, but it's not going to be zero (in practice there will be a space overhead as well as the time overhead to trace live objects).  Finally,  even if  we're using GC that that has bump-pointer allocation and it's faster, it may not be a win. How necessary very cheap allocation is depends on the language. Some languages allocate on the heap like crazy, producing tons of short-lived objects, so allocation costs dominate. Those languages need bump-pointer allocation and generational collection. In other languages where objects are longer lived, allocation cost is amortized over a longer period and it doesn't matter nearly so much, especially when those “worse” allocation costs are still negligible.  And remember, getting good performance out of GC requires a larger memory footprint (to reduce collection frequency), which in turn can slow programs down due to cache effect.  tl;dr; yes, GC is cool, allocating by bumping a pointer is cool, but any claims of “this makes it faster” need to be made very carefully. "
what do you mean 1 server costs a lot less than 500 servers!?!,"> Typically performance is sensitive to the way computers work, and algorithms can potentially be made much quicker and using less memory using mutable state. However, most areas in a program aren't performance bound.  This is poppycock. Your program is not the only thing running on the computer and the available resources are not infinite. I've made this mistake once and I'll never make it again [1]. If you design your solution under the assumption that you can use all of the available resources then your going to find that you run into problems as soon as there's any contention. You should design your solutions to make reasonable use of the machine.  That should go without saying but it doesn't.  [1] building an IPTV system... the solution worked well in isolation... but when it was delivered to customers and was running along side other programs the performance of all the computers, as a whole, became, almost unusable... customers naturally complained... and understandably, they weren't happy with the ""just pay us tens/hundreds of thousands of monies to upgrade all your hardware"" solution the sales team put forward.  Ironically if we'd just thought about this ahead of time, and decided on reasonable limits, we could have designed a system that used a fraction of the resources and everyone would have been happy... but then there's the ""premature optimization is the root of all evil"" (bullshit). As it went we nearly killed the company dealing with issues we made.  That was the day I learned that efficiency/performance is always a concern.  Like it or not I've seen this again and again over my career. Everyone underestimates the real world cost of not thinking about this stuff. Frankly I've come to think of this attitude as irresponsible, if not wilfully negligent.  tl;dr; what do you mean 1 server costs a lot less than 500 servers!?! "
version: I don't think you'll be saying goodbye to rasterization any time soon.,"I'm not sure this is really true.  I see this asserted a lot, but it seems to be under the assumption that the raytracing is done in the best way possible, and rasterization is done in the worst way possible - i.e., the rasterization is done by simply throwing every single polygon in the scene through the rasterizer, without any spacial subdivision or visibility calculations, while the raytracer takes advantage of extensive spacial subdivision and precalculated data structures.  All the ""fast"" raytracers also tend to have limits like mistercow said, negating most of the reason for doing raytracing in the first place.  They tend to be limited to completely static scenes, and can't do most of the eyecandy that people associate with raytracing.  It's possible raytracing may never really become cheaper than ""faking it"".  Maybe it will, but people have been predicting this for a long time.  It's interesting to note that even the most high end offline renderers today generally don't use a pure raytracing approach, but a hybrid rasterization/raytracing approach, only using the raytracing for things that really need it.  In the long term, this may be where realtime stuff is headed, too.  This will probably require graphics hardware to become even more intelligent than it is now (actually current gpus may be approaching the right level of programmability, I'm not sure), or for processors to become fast/parallel enough to render gpus obsolete.  tl;dr version: I don't think you'll be saying goodbye to rasterization any time soon. "
Dijkstra is a terrible source for quotes about computer programming as opposed to computer science.,"Ugh, I hate Dijkstra sooooo much.  That quote just reminds me why. He's arguing  against  good programming languages there. He thinks that the best programming language is mathematical notation and the only reason we don't normally use it is there's no superscript/subscript button on the keyboard. But he's dead wrong about that. Mathematical notation is terrible for programming and ""anthropomorphism"" in computer is a great thing: Duh, humans are the ones who are supposed to use the damn things !  Is he against anthropomorphism in hammers too? How dare we make hammers that are easier to grip instead of perfectly balanced load-driving weights?… Etc.  TL;DR: Dijkstra is a terrible source for quotes about computer programming as opposed to computer science. "
Know PHP like the back of your hand  before  learning a framework to truly appreciate what they can do and know what you're looking for.,"Right now, you'd be doing yourself a disservice by learning Cake.  The current version of Cake is based on antiquated coding practices.  While the next major release is much better, it's still in beta.  There are [lots of frameworks to choose from](  While PHP is not a  complicated  or  complex  language, there's still lots of things to learn.  I honestly couldn't advise you to pick up a framework unless you consider yourself fluent in PHP.  Not in OO PHP, just in plain old regular PHP.  Why, you ask?  Frameworks are ""unnecessary,"" but they're damned handy once you realize all of the thing that they can do for you.  In order to both appreciate and understand all of the work involved, it's a very good idea to know all the pitfalls and annoyances of doing it by hand first.  This is doubly true for anything dealing with OO.  OO itself is a very complex subject, and doing OO on the web is a very different beast than doing OO on the desktop.  Same thing for MVC.  The traditional desktop-based MVC model translates poorly to the web, but it's adequate enough that it's worth continuing to use.  tl;dr:  Know PHP like the back of your hand  before  learning a framework to truly appreciate what they can do and know what you're looking for. "
Most people who don't like software patents are not engaged in the kind of research that produces new advances in the field.,"The RSA algorithm that secures online transactions on the internet is not obvious. The LZW algorithm is not obvious, and neither are any of the lossy algorithms used to compress audio and video data. These algorithms are the result of dedicated, time consuming research and refinement. This research is risky, and those undertaking it have not guarantees they will be successful. When they are successful, they need a mechanism to reap the rewards of their efforts. Just like other fields of human achievement, software and math are hard work, they benefit society, and those who break new ground should have both recognition and financial rewards.  The MPEG consortium is a racket, but they paid for the work that went into coming up with things like MPEG and H264. They open source community is may shortly provided codecs of matching quality, but they haven't yet. And when they do, it will likely because of fundamental, non-patentable advances made as the original patent holders (or their employees) broke new ground in compression.  There are lots and lots of bad software patents that need to be struck down, and it should be easier to invalidate bad patents. Submarine patents ala Unisys should be illegal. A fair compulsory license scheme for all sorts of tech patents and copyrighted would solve lots of problems. But there are some software patents that are legit; and killing software patents would but fundamental advances at risk.  tl;dr Most people who don't like software patents are not engaged in the kind of research that produces new advances in the field. "
Programmed an inventory system on free time-> company didn't want it-> started my own company.,"I was working for a mechanical contractor at the time when I  really  got into PHP.  I was in charge of inventorying over $100k in parts and was handed paper and pencil.  I did the inventory on paper but then created a database (at home) and started putting inventoried items in the database.  I started playing around with getting the data to spit back useful information and accept user log in and all sorts of stuff. I started with a small task and built it up from there.  I did it all on my spare time and created all the bells and whistles.  I approached the company and told them what I did on my spare time and that I would like to install it on the server and set it up so the company can track inventory properly (proper checkouts and reorders).  The said it would  complicate  things and told me to  keep the ideas coming .  Six years later: I started my own PHP based software development company and have been making progress.  I am also being contracted by the mechanical contractor I worked for to design a few systems for them.  Lesson learned? Start small, dream big.  A roadblock is just an obstacle that hasn't been broken into manageable chunks yet.  TLDR;  Programmed an inventory system on free time-> company didn't want it-> started my own company. "
the review process fails at a basic level and the reward system for reviewers does not encourage good behavior.,"> I can't even fathom why my edit got rejected.  I can, first you have to know that the review process assigns edits to reviewers completely at random. I have posted C++/Java/Python/OpenGL questions on Stackoverflow and every time I click on the review button I get to evaluate changes to Ruby,C# and other languages or frameworks I have never touched in my life. The people reviewing your edits most likely have no idea what your improvement did and sadly not everyone acknowledges that by skipping to the next edit.  Second there is ""Stackoverflow the game"" where you get badges for successfully approving, rejecting and changing edits at random (add in Careers  at Stackoverflow and these might even look important to some). Note there is no badge for not voting, which would be at least partially responsible for people not skipping an edit they have no way to judge.  TL:DR; the review process fails at a basic level and the reward system for reviewers does not encourage good behavior. "
you don't have the money). Worst you managed to nail just how little experience the people had with programming  (none).,"> Now ""I'm trying to write a huge MMO that does everything right that games like wow and everquest got wrong ... but am having problems reading this text file. Somebody please tell me what's wrong. I'm using a file reader class.""  I have been on small scale/hobbyist game dev. related forums (RPG Maker etc.) from time to time, the MMO questions have been around since at least the first day after World of Warcraft got released.  The questions got bad enough that there where sticky explanations of what you need to get an MMO running where all over the place (TLDR: you don't have the money). Worst you managed to nail just how little experience the people had with programming  (none). "
You're splitting hairs to an amount that's inappropriate to the discussion thread you're posting in.,"I truly get the technical distinction.  I'm just saying, to use your analogy, if windows only ran MSPaint, and no other program, windows would be basically MSPaint for the purposes of explaining it to someone like the person I responded to. Soon ""windows"" is going to also run ""corel draw"" but at the moment, it runs ""MS paint, and only MS paint"", and here is all the differences between MS paint and mac paint.  You took my simple explanation of what as  to what the guy cared about, and made a very fine distinction that has nothing to do with what the guy was asking about.  I wasn't writing for you, I was writing for him. Yes, I'm completely aware of the distinction between django, the djangoesque thing on GAE and fundamental technologies, but they weren't apropos to the discussion. Its like talking about basic geometry, and ""correcting"" the statement:  There exists for two points only 1 line between them.  With a post that says something like:  ""That is incorrect. There are some that have more""  Then only in later postings go into how you're talking about reimann geometry not euclidean geometry.  Sometimes simpler is better.  tl;dr You're splitting hairs to an amount that's inappropriate to the discussion thread you're posting in. "
"If knowing your password hashfail helps the hacker, then you suck at making passwords.","I think this is very clever.  From a mathematical standpoint, the function that encodes the chromatic ""hashfail"" is injective, but not surjective.  Thus, there are many inverse images of the inverse of the function, only one of which equals the original set.  To clarify, let f denote the hashfail function, let A denote the password, and let g denote the inverse of f (suppose it exists).  Thus, there are many f(g(f(A))) such that f(g(f(A))) = f(A), but many such g(f(A)) such that g(f(A)) ~= A.  The criticism that this is a breach of security is mostly unfounded.  As most password errors made by the user are repetitive, omittive or translational as opposed to substitutional, the probability that a corrupted password (assumed to have the same length as the correct version) will have the same hashfail approaches zero as the password length increases.  Since finding a password is simply a matter of time for your average devoted hacker, the usual caveat about pseudorandom alphanumeric passwords and length still apply.  tl;dr: If knowing your password hashfail helps the hacker, then you suck at making passwords. "
"your perception requires MAJOR readjustment. Either that or you're just trolling, badly.",">Why the downvotes when it is entirely true? It seems like everything anti-microsoft gets upvoted no matter what these days.  Because it isn't. The article is a humour piece and the humour has nothing to do with Microsoft being referred to as ""M$"" - which was a common throw-away back when the article was written. The joke is about the idea of actually writing a  man  page to describe a light bulb. You are being downvoted for missing the blatantly obvious in order to focus on a two-character insult of approximately zero importance in historical context in order to make a completely specious allegation about /r/programming's attitude towards Microsoft.  tl;dr: your perception requires MAJOR readjustment. Either that or you're just trolling, badly. "
I retain passion by maximizing my 'visual results of progress' to 'code time' ratio.,"> Passion comes and goes  This is so true, when I'm in the 'flow' state, I can get so much code done that it's ridiculous, the only thing that stops me is the requirement of sleep. I also love designing things such as the structure of my program and variable/function names, the only problem is that if I'm not in flow, I can't get much actual code done.  I've noticed the quickest way for me to lose momentum is if I see a massive list of things I must do before I can actually visually see something as a result of all the code. I write games, so that list can be something like: Scene loading, game mode handling, AI, input, entities, physics and media, that's quite a lot. Now those things usually aren't too hard to do to a basic level, but I have a bad habit of writing my code to be future-proof, so even if my game design only has one game mode, I'll write a proper game mode manager in case I ever need more, do this over every aspect of the engine and I've written way more than I perhaps need.  My solution right now is to write a framework that makes it such that new projects can be up and running in no time. I'm also focusing on small projects, so in theory I can complete my initial simple design within a month or so. That way I should have no problems with expanding it later on(due to an awesome framework), and so most of my time will be spent on polish, which is pretty fun.  tl;dr I retain passion by maximizing my 'visual results of progress' to 'code time' ratio. "
tight coupling between the window manager and the graphics system sucks.,"I think a lot of people are getting confused here. WM_DESTROY is an indicative signal, sent as a courtesy by the window manager to let it know a window is about to be destroyed. There is nothing inherently wrong with a userspace application sending it, the receiver can do what it wants with the message.  The confusion it seems comes with windows not having a clear separation between the responsibilities of the window manager, the graphics allocation subsystem and the application. I X11, the equivalent DestroyWindow call in this example would be to send a message to the window manager saying ""please destroy that window."" This makes it clear who you are talking too. Calling XDestroyWindow tells the graphics layer that you are done with that rectangle of graphics memory (and your window manager will hate you for it, having to detect it and try to clean up for you).  tl;dr; tight coupling between the window manager and the graphics system sucks. "
It just makes life easier for the compiler to exclude special characters (those that have other uses in the language) from identifiers.,"This can be explained, at least partially, if one considers the lexer/parser that has to ""understand"" the code: Excluding certain character from identifiers simplifies lexing and makes it more independent of the syntactical analysis.  Why is  0-9  not allowed as first character of an identifier?This way, the lexer can decide upon the first character it reads whether the following will be an  identifier  or  number  (literal). And it can do so without considering the syntactical context.  Why not allowing  ?  in identifiers?Assume that  ?  were allowed and consider the ternary operator  b ? x : y  (as found, e.g., in Java). Now, how to lex/parse  x = isTrue?-23:24;  x, assignment, and then in prefix-notation   ternary(isTrue, -23, 24), or  sub(isTrue?, 23) followed by something   Maybe this is not 100% convincing as this example can be figured out by an algorithm. If you allow other characters as well, it may become undecidable or you have to explicitly uses spaces...  tldr: It just makes life easier for the compiler to exclude special characters (those that have other uses in the language) from identifiers. "
"Sanitization' is not a technical term.  More of a metaphor, analogy, mnemonic idea.","You've got yourself so full of misconceptions I don't know where to start.  To keep it simple, I'll say this.  Sanitization includes but is no limited to escaping variables.  Validation is also part of this process.  Normalizing the data into a model for insertion into tables is a 3rd wheel of the sanitization process as well.  Sanitizing is kind of a nickname that developers have given to the process of making sure data behaves the way you intend it to behave.  Until your script processes and washes it, consider it dirty data that shouldn't be used in any critical parts of your software, and will probably end up  breaking it™  TLDR  'Sanitization' is not a technical term.  More of a metaphor, analogy, mnemonic idea. "
"The speed of computers has come to the stage where flexibility and RAD trump performance, even in performance-intensive applications where there is a need for a dynamic system.","It's a bit fuzzy and the reasons have a wide degree of merit and lack thereof, however:   I was developing a private cloud computing system a few years back and for portability's sake settled on the idea the best GUI solution would be a web browser - adding an async background process in c/c++ is trivial to me because I already have the code for it written if I really need to pull that much computing power out of the machines available.  I have a lot of servers I can throw at the problem, so it's unlikely node-based background processors won't be up to par.  My favorite language is c# for the sake of RAD, but am trying to move to open-source systems and JavaScript is the next best thing (assuming you have an IDE that works to manage code nicely and aren't staring at a wall of text hundreds of thousands of lines long - it is also the logical choice for web-based development and the portability attained by ensuring the front and back end can exchange functional objects dynamically [at least to me] far outweighs the cost of running everything in JavaScript [especially considering computing power doesn't appear to be approaching a roadblock anytime soon]).  If I ever want to surface a piece of the project on a public interface the web is the place to do it, and having the bulk of the code already in the proper format will make it easier.  c/c++ takes more time to get right, is harder to debug distributed systems of (in terms of actual debugging and simply deploying/managing deployment scripts/other maintenance bs) - JavaScript is easy and if it doesn't work nothing dies catastrophically, you just fix the issue and reload the page.   TL;DR: The speed of computers has come to the stage where flexibility and RAD trump performance, even in performance-intensive applications where there is a need for a dynamic system. "
Shared code/libraries contributes to vocabulary that can be reused.,"Actually, he didn't say that. He said ""do things in a way that's idiomatic for that environment"", which could be anything depending on his perception of what is idiomatic.  And I didn't say that frameworks were the end all to programming either, so let's keep the straw-men to a minimum. I personally use them extremely selectively, and default to getting the job done or an appropriate library if one is available. But when I have a team of 10 devs, and we say ""you know what is going to come up a lot in the lifetime of this codebase? Distributed parallel processes"" the nuts and bolts of that isn't something that we're going to code from scratch. We'll grab Hadoop, or Akka, or Hama. Whatever is appropriate for the problem at hand.  My point was that it gives you a shared language with the other people on the project. If I say Hadoop, I don't expect that my coworkers need to read the source to understand exactly what's going on. Whereas if I rolled my own, that shared language is nonexistent - now there is a set of abstractions that  only matter within this project , that everyone needs to know to be effective.  Maybe this is shaped by my experiences, where I've been on teams that owned 10+ applications at once, and where being able to transfer knowledge from one stack to another was crucial to your ability to be effective. If you're not in that situation, cool, perhaps that's why our viewpoints differ.  TL;DR: Shared code/libraries contributes to vocabulary that can be reused. "
if you find yourself reaching for  try/catch  to do operations on failure and then propagate the exception... you probably should look into tying that action to a destructor.,"It's amusing (and sad) to see a beginner struggling to get the ball running in C++. It's also unfortunate that even is final solution is so complicated, so for the sake of future readers, a C++03 solution (today you would be using  std::unique_ptr&lt;char[]&gt;  and be done with it):  template &lt;typename T&gt;class scoped_array {public:    scoped_array(): array() {}    explicit scoped_array(T* a): array(a) {}    ~scoped_array() { delete[] array; }    T* get() { return array; }    T const* get() const { return array; }private:    scoped_array(scoped_array const&amp;) /*=delete*/;    scoped_array&amp; operator=(scoped_array const&amp;) /*=delete*/;    T* array;}; // class scoped_array  And with that little utility, behold:  inline string DOMStringToStdString(DOMString const&amp; s) {    scoped_array&lt;char&gt; const array = s.transcode();    return std::string(array.get());}inline DOMString StringToDOMString(char const* str) {   scoped_array&lt;XMLCh&gt; xmlStr(XMLString::transcode( cStr ));   return DOMString(xmlStr.get());}  TL;DR: if you find yourself reaching for  try/catch  to do operations on failure and then propagate the exception... you probably should look into tying that action to a destructor. "
Programmers and managers need to find a way to share control for the cooperation to work,"This is all about perceived power and control.  Demanding an estimate is an attempt at gaining control. And control is the (much needed) lifeblood of any project or project manager. With good control, large and difficult projects can be completed.  However, programmers perform work that is close to impossible to predict, both in time, scope and quality. Given that programmers also has a real need to be in control, in order to be able to solve the complex tasks at hand, this spells trouble.  Both parties has a real need for control to get their work done. This is a source for conflict that needs to be handled. Just going ""#NoEstimates!"" won't resolve this.  Giving programmers some space to have control and to do their unpredictable work (a bit like a woman would never be micromanaged during birth, or a writer would be expected to estimate exactly how long it would take to write a saga of nine volumes) might be the solution.  tl;dr Programmers and managers need to find a way to share control for the cooperation to work "
the 'or' we use corresponds more to logical 'OR' or 'XOR' to a degree determined by the mutual independence & spanning space of their predicates.,">The English or is the English or. It's not a logical function  that's precisely what i'm arguing. or is a conjunction usually used to specify alternatives. it only has the logical property of 'exclusivity' to the extent that the 'alternatives' are independent.  >""black or white"" cannot be answered by ""yes"".  similarly ""black or white?"" can be answered ""yes"" to the extent that they are independent alternatives that also span their space. compare ""is that chess piece b or w?"" to ""should we paint the room b or w?"". the second clearly doesn't span the realm of possibilities; there are other colors, and even otherwise: who says we can't paint it some combination of the two?  tl;dr: the 'or' we use corresponds more to logical 'OR' or 'XOR' to a degree determined by the mutual independence & spanning space of their predicates. "
use frameworks/libraries if you don't have the time to implement them yourself.   Or if you like the framework and plan on using it extensively.,"As some of the others have said, stick with the language and don't tie yourself to frameworks.  Take LoDash for example.  A lot of people use it because it's fast and useful.  But... you can get by without it probably 95% of the time.  Most of us don't need to iterate 10000+ items looking for performance gains.  I use libraries not because I have to, but because I don't have the time to re-invent the wheel.  That mentality will hit you like a bag of bricks some day.  You'll start to notice  how many libraries there are, how many of them are just half-ass hacks or poorly implemented.  You'll ask yourself, do I have the time?  All I need is X, Y, Z.  Do any of my options solve these problems?  The reason Angular and React are so popular is because they solve a specific popular problem which takes a lot of time.  If you've read about Angular 2.0, they're going to scrap almost everything in favor of looking more like VanillaJs (es6).  Tl;dr - use frameworks/libraries if you don't have the time to implement them yourself.   Or if you like the framework and plan on using it extensively. "
Just cause I develop a hack for something doesn't mean I broke the law in order to do it.,"How is showing someone how to hack something illegal if you get your own equipment? I doubt they took everyone at DEF CON to Mass and said alright! Lets hack this!  Second if they ""already"" hacked it the Mass Transit Authority must provide proof. For all they know the kids could have their own equipment, duh, I mean what would they show it to DEF CON with?  Again I stand against this because this is arrest under hearsay. Which is yes perfectly legal within our system however is a waste of time for everyone. It allows the continuing trend of ""deterrent"" law enforcement just by oppressing people into not doing something that others don't like politically. If someone doesn't want me doing something they just arrest me and hold me at the proper time until that thing is over, that's the exact case here. There is no proof of crime and this is just pretty form of ""damage control"" on the part of MTA.  TLDR: Just cause I develop a hack for something doesn't mean I broke the law in order to do it. "
Supply and demand affect wage way more than unions or collective bargaining ever will as you've got to have something to bargain with (ie lack of supply over demand).,"I'm not sure about your pay theory.  Professional pay is affected by supply vs demand. With China and India (and the growing popularity of computer science in the west) there is a massive supply of programmers (putting quality issues aside because like it or lump it, most companies do) and thus programmers have become more replaceable.  You can't just simply demand more money because you're intelligent. The fact is that a growing mass of people can also do a lot of the enterprise programming that is required so the bargaining power is on the side of the organisations at the moment as tools to aid programmers get easier and easier to use.  Having said that, you can make yourself worth more by filling a niche and specialising in an area where the demand is less and thus the pay will be higher. You just need to use your smarts to position yourself in an area where the bargaining power is tilted in your direction.  For a long time computer scientists were looked upon as magicians. Business didn't understand what they did so they paid them top dollar to do their magic. As it became more mainstream, the magic disappeared and we started to be able to mass produce programmers. Sure, the defect rate is higher...but the cost is lower so it makes economic sense.  Sorry to be so blunt, I know it's probably not what you wanted to hear, but passion has much less impact on pay conditions than simple old supply and demand.  DISCLAIMER: I am one of those MBAs you speak of. Started out as a programmer, then analyst, then BA, then PM. I select the teams and deal weekly with the reality of having to please ""the man"" in order to keep anyone hired and projects rolling in.  tl;dr : Supply and demand affect wage way more than unions or collective bargaining ever will as you've got to have something to bargain with (ie lack of supply over demand). "
for both my posts I guess: I enjoy Python today and I'm looking forward to the 3.x line being the first choice.,"Oh I know, though if you're targeting 2.7 you're reasonably targeting 2.5 or at least 2.6 in most cases. Maybe in a year or so 2.7 will be widely deployed but at the moment it's not really deployed anywhere for any practical purposes. Most servers run 2.5 and most desktops 2.6. Seems Debian Squeeze will be 2.6 despite 2.7 being out for some time before Squeeze is stable, meaning 2.7 might not be deployed on many servers before maybe 2012.  Of course it's easy to get dictionary and set comprehensions with constructors and generator expressions, we can get many 2.7 and 3.x niceties in 2.6 with future-imports, and some things like ordered dicts are backported on PyPI. But as I argued in my original post, I use Python for the cleanness and sensibility; 3.x just is more clean and sensible than all these hacks.  TL;DR for both my posts I guess: I enjoy Python today and I'm looking forward to the 3.x line being the first choice. "
"at the time that document was written, idiom was a lot more important than it is today, and compilers  didn't  fix everything for you","Certainly in 1972 it was deathly important to shave clock cycles off your machine code; God forbid you had to operate on a large array in a loop that took ten instructions when it could be rigged to take five!  There may not be much difference between ten nanoseconds and five nanoseconds (or whatever) on  today's  processors, but in 1972 it could have been a difference between one and two  minutes .  Even in 1988 when I started, it could take several minutes to decode and display a JPEG image, so any time you could shave off would make you a goddamned hero.  Compiler quality varied a lot, since every vendor created its own compiler from scratch.  The compiler might or might  not  have recognized/optimized ""common idioms.""  The one on VAX/VMS was pretty damn good -- it once optimized a buddy's demo program down to a single RETURN instruction because he had neglected to code any output statements and the compiler deduced that the program,  overall, did nothing  -- but if a document such as the OP's was even  needed,  it must  not  have been the case that  all  compilers were that good.  Processors were a lot more primitive in those days, too; no pipelining, cache memory, etc.  We once received from a contractor a program whose first statement was the declaration of an enormous (100,000? 500,000?  1,000,000? elements) 2D array.  This program immediately aborted at run time because  there wasn't room for an array that big on the machine .  We had to totally restructure this array from 2D to 1D, and reorganize the code that used it so as to operate on ""one row (or column; whatever) at a time,"" in order to get it to fit/work.  This sort of thing was commonplace, and you  did  have to think about ""idiom"" as well as how machine architecture and hardware limits would interact with your data-and-code design, algorithms, etc.  So, idiom was very important and there was  no way  the compilers of the day were going to recognize that this giant array was a problem and restructure the data-and-code the way we had to.  I don't recall there even  existing  any optimizers, profilers, code analyzers, etc., unless my managers were simply too stingy to buy them...  We were glad to have a full-screen text editor, goddammit...  tl;dr - at the time that document was written, idiom was a lot more important than it is today, and compilers  didn't  fix everything for you "
unit tests were invented by consultants paid by the hour to write code.,"I've been feeling this way for a few years. The way to evaluate testing is to determine how many bugs it finds prevents with how much work. Unit tests are essentially the brute force method of program testing. Hopefully your programmers are picking representative test cases, and TDD tries to correct programmer testing bias -- we tend to focus on the ways things go wrong and forget to check that it works in the normal case.  So yea, if people are only relying on unit testing, they're probably overused. For any given budget, a diversity of testing strategies is going to outperform a single methodology. Presumably integration testing and unit testing have different ""testing surface"" (bs term I just made up) such that they'll find different classes of errors.  tl;dr -- unit tests were invented by consultants paid by the hour to write code. "
"I worked with and Traveled to Romania, the people were very nice and made me feel welcome.  The food was baffling however.","I ran an off shore group in Romania for a time, and went there (Cluj) as well.It was like the city was 1/3 old world, 1/3 soviet era, and 1/3 Americanized.Slang, t-shirts, billboards..all in English.  Even the words ""Self serve"" or ""Kiss FM"" were in English instead of Romanian.There was even a monster truck rally at the local Mall :)The people liked Americans, and there were some nice places to eat(which only the Romanians seem to know about :) I still keep in touch with the people there (from time to time) and I have nothing but nice things to say about them.I still have lots of cultural questions about the place (the deal with the Gypsies for example) and the food was baffling to me but it was a great experience.TL;DR  I worked with and Traveled to Romania, the people were very nice and made me feel welcome.  The food was baffling however. "
"You can't call yourself a JavaScript expert without also knowing the popular DOM manipulation, templating/binding frameworks, layout/styling includes, test frameworks, debugging tools, etc.","Being a good JavaScript developer means more than understanding the scoping of nested closures and understanding how to debug conflicts in the prototype chain on any browser.  JavaScript, much more so than traditional languages like C++, does not exist in a void. You'll obviously be expected to know HTML/CSS, but you need to know other bits. If all you know is pure JavaScript, it's really only appropriate to say you're an ECMAScript 5 developer.  For a lot of reasons, my day-to-day job requires writing a lot of pure JS (no libraries, frameworks, imports, etc), but I'd still be a laughingstock if I couldn't sit down and prototype a new app using Ember, jQuery, LESS and Backbone.  It really isn't hard to learn the basics of most frameworks and libraries. If you know the main MV* frameworks it's easy to pick up whatever new fad is in-style for a team's next big project.  tl;dr: You can't call yourself a JavaScript expert without also knowing the popular DOM manipulation, templating/binding frameworks, layout/styling includes, test frameworks, debugging tools, etc. "
"If you're stuck on shared hosting, no gmagick for you.  Use something else instead.","Dreamhost is shared hosting.  gmagick is a PECL extension.  You can't install PECL extensions on shared hosting.  Well, not any more.  It used to be possible to use [ dl() ]( but it was plagued with issues and was a bad idea.  If you need gmagick, then you need to be able to compile it yourself or install it from an appropriate OS-specific repository.  Both of these basically require root level access to a container, a VPS, or some other form of dedicated server.  However, in order to manage a server as root, you need to have some sysadmin chops.  If you were unable to determine that you can't install PECL extensions on shared hosting, chances are that you shouldn't let yourself touch a server quite yet.  tl;dr: If you're stuck on shared hosting, no gmagick for you.  Use something else instead. "
"The immediate mode stuff is slower, and I've transformed a pre-existing API to include ES 2.0's shaders, and it basically forced a re-write of the entire rendering pipeline.","I see your point, but the problem is that it starts out good enough, but the game has performance problems that are difficult to overcome once entrenched in a platform and loading it down with all the various aspects that a nearly complete game needs. Its a deceptive process. Become paranoid and assume by default that performance is going to almost be a release stopper, and take slight detours towards better performing tech and better algorithms early in the process. For each 10x increase in performance I get out of the box, I can implement naive slow particles and really terrible pruning schemes in my AI. This is the other side of good enough, is having the room to advance performance in a modular fashion. I can optimize my particles much easier than you can optimize the python runtime.  tl;dr; The immediate mode stuff is slower, and I've transformed a pre-existing API to include ES 2.0's shaders, and it basically forced a re-write of the entire rendering pipeline. "
"Django is a good tool for putting together a production website, but nothing's perfect.","I built my website running using Django (giftfoxx.com). A couple of the pros: Lots of built-in high-quality web tools (CSRF protection, admin page and user access control, messages framework, sessions, etc.). Template language makes it easy to access database objects. There's also a ton of great documentation.  What you trade in for all of these features is a learning curve. Sometimes I feel like I could do things faster just rolling my own—which, if I understand correctly, is more Flask-y. The advantage of making it yourself is that you understand how  your  sessions work (for example), which is fine for lightweight code but not great when you actually want your website to remain secure and in production.  A number of the features also seem very un-pythonic. Many ORM object fields are implicitly derived from class names, for example, and there are strong conventions on code layout. There is a big advantage to these conventions: they provide structure and help other people read your code. But this adds to the learning curve.  tl;dr: Django is a good tool for putting together a production website, but nothing's perfect. "
"Its not just a clear-cut ""It goes in /srv"" and is not defined in the FHS as that way.","This is certainly one way you can do this, but is a bit decieving because it is not defined as explicitly as you do. A  /srv/[protocol]  model is proposed, but not actually suggested or defined as a any  recommendation.  The entire issue is a bit distribution-specific issue. In some Debian/Ubuntu, some packages are compiled to only-allow paths under /var/www. Some other packages will automatically try to configure themselves to use /var/www. So you may want to be careful with /var/www if you install these packages.  As for what to use: Its probably best to prevent the application from reading web-server things that it doesn't need (like SSL keys and passphrases). Depending  requirements, you may organize this under one user. Access to add/update/remove the application may influence the decision.  I tend to use different users to restrict what applications can do-- so using /home (or a subdirectoy like  /home/ or /home/www`) makes sense for the way I set it up.  tl;dr  Its not just a clear-cut ""It goes in /srv"" and is not defined in the FHS as that way. "
The idea that one type of office is best is wrong. Mixed space offices would make the most sense.,"A lot of these articles feel very American to me. I've worked in open plan offices my entire career, and I've found that the stereotypes, from the weird association of open plan with startups, to it being a corporate nightmare, to it being noisy and ""collaborative"" are untrue.  I don't think open plan is as much of a devastating extrovert jungle as these articles suggest. Every office format will have detractors. Some people don't like listening to music, and don't like noise, so will hate open plan. Some people hate the idea of  sitting  in a cubicle for plenty of reasons. Some people won't like the feeling of being isolated that a closed door office can bring.  tldr; The idea that one type of office is best is wrong. Mixed space offices would make the most sense. "
I don't get why you have all that complication.,"Pretty nice but some details seem weird:   Using ""map"" as the name of a function that is just an adapter to this type of input is not so nice. That word is already used for dictionaries AND applying a function to a sequence of values already, so adding yet another meaning to the word is pretty bad.   The name ""tee"" is not explained. Is this a golf reference? Not helpful!   The try_catch case seems like it should just apply to everything instead of being a special case.    If you change validate_request to not return a dictionary with :ok/:failure but instead throw an exception you can throw away your functions map, tee AND try_catch and just use exceptions for errors.  TL;DR: I don't get why you have all that complication. "
"Your point is taken, but doesn't actually negate the argument at all. ;-)","True, but I don't think that negates the validity of the metaphor.  The reason software is so difficult to get right and make reliable is not because it's  inherently  impossible, but often because you can't really rely on the foundations it's built on.  It's like complaining that cars are less reliable than chairs - they need fixing more often and they break a lot more.  That's not because cars or car-designers are bad at their jobs - it's because ultimately the average car is a lot more complex than even the most finely-crafted average chair, and chairs don't often whiz along at a hundred miles per hour, nominally under the control of an easily-distracted and deeply unreliable user.  TL;DR: Your point is taken, but doesn't actually negate the argument at all. ;-) "
This guy is making hyperbolic arguments that ignore basic principles of multiprogramming.,"> but as we have just seen, threaded programming, which is easier to understand than callback driven programming, meets or exceeds the asynchronous model in the vast majority of cases  Is he seriously suggesting that threading is easier to get right than callbacks?  That implies that he has never used either.  Threading is easy when there's nothing shared.  I mean  nothing .  As soon as you have a  single  piece of shared data, shit gets real.  No matter what synchronizing primitive you choose (locks, mutexes, whatever), you immediately hit two major walls:  data safety and speed drops.  Screw up a lock, and you get dirty read/writes at best, and deadlocks at worst.  And synchronization is  sloooooooow , both in actual execution, and in the waiting it implies must happen on every thread that isn't currently using the synchronized resource.  There is a place for threading, and a place for event-driven callback systems.  In some cases, a mixture is necessary (the printer control software I write, for example, uses both in different places).  tl;dr:  This guy is making hyperbolic arguments that ignore basic principles of multiprogramming. "
"too context specific to matter and artificially mixing unrelated concepts.  YMMV, no reason to be excited.","Ah, the first error of novice ""OO"" programmers - bluntly transferring domain observations (square is a rectangle) into code!  Here's the deal: code should not be equivalent to the problem domain. Code models said domain, and code artefacts should be efficient in the model, not somehow equivalent to the domain.  That said, the proposed immutability solution works or not depending on the rest of the code. E.g. what if there's more than one reference to a given square/rectangle instance? Immutability makes it hard to trivially observe a change in dimensions, as you have to create a new object and change all references.  Tl;dr: too context specific to matter and artificially mixing unrelated concepts.  YMMV, no reason to be excited. "
Don't expect a Java extinction event any time soon.  Applets and J2ME were unique cases.,"> Well, as other people have mentioned, it's already been replaced in many areas. Javascript killed applets. iOS and Android killed J2ME (though to be fair Android is Java-based).  There were different forces at work there.  Applets were killed by engineered incompatibility (e.g. IE only supporting Java 1.1 well beyond it's EOL, and Sun never quite being able to build a frictionless installer to update things).  The modern HTML5-style JavaScript-driven functional websites came much later, much much later, you've got the whole rise-and-fall of Flash in between the two.  The J2ME model didn't work with smartphones, that's what put paid to that.  Perhaps it could have been if Sun had seen it coming, but it's not likely, neither Apple nor Google (although Google doesn't entirely ban it like Apple does) support heavy-weight third-party systems on their mobile OSes.  The only successful alternative SDKs for those platforms have all been ahead-of-time compilers, significantly disguising the true nature of the application, which produce platform specific code - quite a departure from the WORA model of Java.  Both of those were quite significant shifts in the technology landscape that effectively obsoleted whole classes of technology.  For the areas which Java is currently popular, that same kind of thing is unlikely to happen.  The standard big UNIX box running stuff has been the most popular place to deploy stuff for 30-odd years, and isn't likely to change soon (and is more healthy now in the world of cloud-provisioned boxes (e.g. AWS) than it's ever been; the only encroaching alternatives (e.g. PaaS like Google App Engine, Heroku, etc.) all support Java out-of-the-box.  tl;dr: Don't expect a Java extinction event any time soon.  Applets and J2ME were unique cases. "
persisting an entire program state is often not granular enough to give you what you really want / TANSTAAFL.,"CPS or continuation persistence is simple in closed-world scenarios without I/O or dependence on external state.  But as soon as you persist a program state that's not entirely self-contained, fragility is introduced because the external state can easily become out of sync with the persisted state in all sorts of ways.  Other persistent state mechanism can face similar problems, e.g. persisting sessions in web applications, but when the program has direct control over exactly what gets persisted, it tends to be a little easier.  E.g., in some OO systems you implement a persistence interface on objects that are supposed to be persistable.  With continuation persistence, what gets persisted by default is whatever's in scope in the program at the time a continuation was persisted.  Without finer manual control over this, the requirements of persistence can end up dictating how your code needs to be written, even in places which shouldn't necessarily be affected by that issue.  Of course there are ways to address this - delimited continuations, the ability to flag variables as transient, ways to reinitialize external state at load time, etc. - but that's the basic source of challenges.  Addressing this adds complexity and brings you back to a more manual style, similar to the OO example.  tl;dr: persisting an entire program state is often not granular enough to give you what you really want / TANSTAAFL. "
"the net information content of net goes up with time, so the number of searches one have to perform to find something goes down with time.","This is pure speculation..But doesn't this work by counting search requests? If that is so, then wouldn't it be possible that more successful searches result in less search requests. I mean, as time goes by, more questions get asked and answered via sites like stackoverflow. So the number of searches you need to do for finding a certain piece of information goes down with time. Also googles search may be getting smarter with time, so that it will return more relevant results the first time itself. So users don't have to try different keyword combinations to get what they want, result in a reduction in the number of searches.  tldr: the net information content of net goes up with time, so the number of searches one have to perform to find something goes down with time. "
"If your warning can't become an error, it shouldn't be thrown by your compiler, it should be thrown by a static analyzer.","I think that the problem is something completely different, it's not the errors are misused by the user, but the other way around.  Compilers shouldn't throw warnings. Linters and Static Analyzers should. If the compiler comes with one included  it should not be invoked when compiling .  When I run a program I expect me to inform me of anything related to  its main function . When I run a compiler I expect me to inform me of anything related to the state of compilation (which files it's compiling and such) and any issue that prevents it from compiling (errors) or compiling with the guarantees I expect (non-fatal errors, what most warnings become with -Werror). I am not interested in any comment on my code that may be of special note, but really don't stop the program from compiling correctly.  A static analyzer, OTOH, is one that I want to report to me a slew of comments on the quality and trustworthiness of my code. I'd probably diff the output with the last compilation, and ignore it if there isn't any change (and I'm not hunting to remove warnings). When warnings appear, I read them, consider them, and decide if they point to an error on my code or don't really have a ground, because they are warnings.  I think the error came in having compilers implicitly do the job of linters and static analyzers. These tools should have a toolchain that is separate from compilers, even if the linter/static analyzer is the compiler (with flags that make it only spew warnings but not compile anything) itself! If you want to have a build system that fails when a linter finds an error, then you add that to your make system, not your compiler.  I feel that this whole warning-in-the-compiler came from the 90s feature wars, where more integrated features meant a better program, more blades a better razor. It'd seem the only reason people didn't assume that more wheels made a better car was because they assume that more cylinders is better.  TL;DR: If your warning can't become an error, it shouldn't be thrown by your compiler, it should be thrown by a static analyzer. "
Pathfinder picks up where 3rd edition D&D left off because nobody liked 4th edition.,"[Pathfinder]( is a tabletop RPG based on Dungeons & Dragons.  When 3rd Edition D&D (and the revised 3.5ed) was released, it was under an ""open source"" license that let people freely write add-ons and variants of the rules (something that previously had  not  been allowed).  The [D20 SRD]( has most of the core game mechanics in a form that's freely quoteable.  When D&D was revised for 4th Edition, they dramatically change the feel of the game.  Many players didn't like that and wanted to keep playing 3.5 Edition.  One of the companies that had been licensed to produce the official fan magazine decided that they were going to update 3.5 and produce a new game.  This gave us Pathfinder - often jokingly referred to as ""3.75 edition"".  TL;DR  - Pathfinder picks up where 3rd edition D&D left off because nobody liked 4th edition. "
"version: extremely satisfied F5 customer, would recommend them to anyone who actually has a budget for true load balancing.","As I always say, if you have the money to buy a pair of F5 BigIPs, do it.  Using Nginx in this fashion is good if your application always behaves 100% correctly on every server.  If you need true application health monitoring, this isn't the way to go, as one app server may return a 200 response but that response is actually wrong/not what is expected.  There's no mechanism in Nginx, other than the response code, to tell whether an application is working correctly or not.  This is where the BigIP comes in; not only do you get true load balancing (much better than simple round-robin or weighted; but also the ""observed"" algorithm (which node is currently running the fastest?) and ""predictive"" algorithm (which node is currently speeding up?) are much, much better methods of distributing load to nodes.  That plus true application health monitoring (do you expect a certain keyword on a certain page), SSL session state, etc etc, and they are well worth every penny.  tl;dr version: extremely satisfied F5 customer, would recommend them to anyone who actually has a budget for true load balancing. "
We hacked out some horrible shit that we don't want to read and now we are going to do it again.,"> When a block is inefficient or behaves incorrectly, we do not read its code.> We re-implement it. Because it's faster. And because it gives us fewer headaches.  So how did you establish its inefficiency or incorrectness without reading it? How do you know where the bottlenecks are? What do you need to optimise? The best code is that which you've refactored because it takes into account it your previous mistakes.  Lots of code looks gnarly, but that's because you don't fully appreciate the complexity of the problem. Reading code is harder than writing it and the effort of comphrension makes people think that others' code is bad. It probably isn't, though. In fact, the person(often you) who wrote it probably had a better understanding of the problem.  TLDR: We hacked out some horrible shit that we don't want to read and now we are going to do it again. "
"Nice idea, but in my experience non-programmers probably shouldn't be programming.","I worked at a place that built a point and click interface for non programmers to write code.  It sounds like a good idea until you consider the support side.  It was my job to explain to these non-programming programmers why their giant, twenty deep nested if else if else if else statements did not properly validate the 120 line regexes contained within them.  I would routinely have to explain basic logic and programming concepts to the these customers, but instead would get attitude when their program didn't work as intended.  What made it worse was my fellow support ""programmers"" who did not have a programming background yet would make fun of the clients who couldn't program.  These are the same coworkers that I had to explain how a NOT operator worked.  The other problem is we were constrained to what the tool could do.  It did not allow functions or much code reuse without having to write in the core language (this being VB.NET), which the other team members did not know how to do.  It also didn't allow you to type code nor copy and paste so it took me three times as long to create code through the editor than if it had been typed.  tldr;  Nice idea, but in my experience non-programmers probably shouldn't be programming. "
Chill out.  Do a little research.  Stop giving poor tech journalism your pageviews.,"Let's clear up a few things, because the author of this article is being intentionally sensational or is grossly misinformed.  First things first:  DRM is  not  being added to the HTML standard.  What  is  being added is a standard  interface  in which DRM can be implemented.  By no means is anyone forced to have any implementation of DRM installed on their browser.  You may not be able to view certain kinds of content without the DRM, but nothing changes in that regard -- the only difference is that there is a standard interface for implementing it.  We already have flash players and Silverlight players that implement DRM in order to watch content.  TL;DR -- Chill out.  Do a little research.  Stop giving poor tech journalism your pageviews. "
"To me the PS3 security system looks like someone check boxing features, whereas the 360's looks like a single cohesive system that even once partially broken, withstands against hacking.","You should check out the Xbox360's system.  It basically did everything the PS3 did right, plus more.   The 360's hypervisor doesn't allow even the kernel to allocate arbitrary executable memory.  You have to give it the signatures for a chunk of memory that you want to make executable, make a hypercall, the hypervisor verifies the signatures removes writable permission, adds executable permission, and then returns.  And since this is basically all the hypervisor does, it's a very small attack surface.  On the contrary on the PS3 once you're in kernel space all you have to do is just ask the hypervisor to make memory executable, no questions asked.  (Which IMO kind of defeats the purpose of the hypervisor for this use case).      The 360's hypervisor never wrote plaintext to RAM.  All internal hypervisor RAM accesses are encrypted with a randomly generated (at boot time) key, and a hash is stored in an internal CPU buffer as the values leave the L2 cache.  When the memory is read back, the hash is compared, and the system freezes if it doesn't match.  There are checks by the CPU that disallow downgrades using one time burnable fuses.   What all of this combines to is an extremely secure system.  Fairly early on attackers had arbitrary write access to RAM via a game that was both signed to run off of CD-Rs, and didn't self sign it's shaders (see King Kong exploit for more information), but the only hack that became of that was closed by Microsoft in just 9 days.  Most consoles were upgraded to a non vulnerable kernel soon after, and were then incapable of ever running the exploit.  Even now, the current exploit against the 360 is crazy almost beyond belief.  You basically put a CPLD on the CPU, play with PLL signals to downclock, wait for a very specific time (when the cpu is doing a memcmp of a signature check), and pulse the reset line for a  very  short time.  Because of how the internal reset sequence for the processor works, this clears the first few registers causing the memcmp to exit it's loop and look like it's returning 0.  TL;DR To me the PS3 security system looks like someone check boxing features, whereas the 360's looks like a single cohesive system that even once partially broken, withstands against hacking. "
The summary was good. You just didn't like it.,"> Sorry, but that reaction is simplistic, knee-jerk, and juvenile.  You might think that. I think it neatly sums up the costs and benefits of DRM from a consumer's perspective. Specially, the total lack of benefits and bevy of costs. All it does is make ordinary people into criminals and inflate the cost of software.  > What if companies were mandated to make purchased content always available, even if they go under?  It'd be a good idea. DRM need not be involved.  > What if there were regulations against spying on users outside of their use of the particular content purchased?  They'd be violated six ways to Sunday in the name of collecting analytics and whoever hacked at it enough to find this out would be brought up on charges under the DMCA.  It'd be a good idea. DRM need not be involved.  > What if all particular complaints against DRM were legislated against?  Then there's no longer any point to DRM.  There are a lot of complaints about DRM. There is no form of DRM that avoids all of them. There  cannot  be any form of DRM that avoids all complaints and still be DRM. Even things as low-tech as serial keys on the jewel case can cause problems.  But hey. I'll humor you. If you somehow legislated away  all  the problems with DRM and by some magic some form of DRM still existed, I'd be only mildly disgruntled over it and its presumption of me as an incipient criminal.  Now, humor me. What form of DRM do you honestly think is possible that avoids any and all complaints about DRM while still being useful as DRM? What non-dickish uses of DRM do you think exist that still  require  DRM to function? In what way do you think DRM can be a benefit to the customer?  If you say ""Steam"", I'm going to laugh at you.  tl;dr:  The summary was good. You just didn't like it. "
"Old slow hardware brutally exposed poor coding|app design, new grunty hardware hides it.","Necessity is the mother of invention, and on old 8-bit hardware with <64KB of RAM you needed to be damn inventive. You  had  to optimise your code as memory and CPU cycles were finite and precious, if you didn't optimise your code wouldn't run. The memory and CPU cycle boundaries were always in sight and usually being rubbed in your face vigorously.  With modern hardware the boundaries are  waaayyy  over the coding horizon for most applications, to the point where the developer doesn't even need to be aware they exist 90% of the time. This in turn means that when they do bump into them, most younger devs are inexperienced with how to cope with the limits.  TL;DR: Old slow hardware brutally exposed poor coding|app design, new grunty hardware hides it. "
"OOP concepts are very usable in PHP. 
 Learn about design patterns, their strengths and weaknesses, their intent, and their common applications.","In that whole book, I only skipped 2 sections.   The very brief section early on when they talk about multi-threading.  The somewhat lengthy span of pages in the Proxy pattern chapter discussing the Remote Proxy pattern and Java's RMI (Remote Method Invocation).   However, after finishing the Proxy chapter I immediately wrote [this]( for fun, using what I learned.  After finishing the Iterator/Aggregate chapter, I then started to read up on [Spl]( and it felt like great spotlights were turned on.  I recommend this book if you think you'll ever need to..   Write a useful class, then later create a decorator for it instead of subclassing.  Refactor a class with tons of conditionals to use the state pattern instead.  Create a facade that provides a simple interface to a complicated subsystem you wrote.  Use a mediator for your controller and decouple complex communication between classes in your business logic layer.   The list goes on...  TLDR  OOP concepts are very usable in PHP.  Learn about design patterns, their strengths and weaknesses, their intent, and their common applications. "
"The guru provides the initial condition, without her nobody would ever leave.","I actually really like this problem.  The guru provides the initial condition to get the process of elimination started.  Think about the base case:  1 blue, 1 brown.  The blue guy knows that the only other person on the island (besides the guru) has brown eyes, and the brown guy knows the only other person has blue eyes.  They can't speak to each other about this, though, so neither of them has any way of knowing his own eye color (could be red, yellow, purple, whatever) until the guru speaks and says ""I see someone with blue eyes.""  The blue guy then, who can see that the only other person has brown eyes, knows she must be talking about him and leaves the island at midnight.  The same is true with 2 of each color.  Each blue guy can see 1 blue 2 brown, and each brown guy can see 2 blue 1 brown.  Again, none of them can be positive that their own eyes are either blue or brown, and none of them can be sure there aren't a total of 1 blue 3 brown or 3 blue 1 brown, and thus they are all stuck until the guru speaks.  Then, each blue guy looks at the other and thinks ""if he doesn't leave tonight, then he isn't the only one with blue eyes, so mine must be as well.""  They see each other the next morning and both confirm that they do have blue eyes, and both of them leave that night.  The same holds true for the case of having three of each color and beyond, which can be [proven by induction](  Note that in all cases, the brown guys can never leave because from their perspective, their eyes could still be any color besides blue (they only know this because they see all the blues leave the island).  Even in the 100-100 case, after the blues have left, each brown guy sees 99 other browns but can never be sure that his own eyes are brown, and thus none of them ever end up leaving.  Wow, wrote a lot more than I was expecting.  Hope it helps.  TLDR:  The guru provides the initial condition, without her nobody would ever leave. "
Don't try to out smart ass a smart ass.,"It's a perfectly good answer for an interview, but then my follow up woud be: ""except, I didn't mention what planet you are on, or even if you are on a planet. I also didn't mention what kind of egg it is... we might call it an egg, but it might not even be organic in nature. You don't know the specific gravity, the air pressure, whether the egg has been boiled, the shape of the egg, the mass of the egg, the thickness of the shell, the chemical structure of the shell, the elasticity of the shell, the rigidity of the ground, the flow of the atmosphere, the height of a 'floor' in the building, not to mention any electromagnetic field in the area as well as the electromagnetic properties of the egg that would interact with the field. You're welcome to cleverly think of ways to address these variables, as long as you can describe how you'd do so and how that would impact how you'd get at the solution.""  tl;dr: Don't try to out smart ass a smart ass. "
Ember and Angular are more full-featured than React.js. And Angular gives you the most bang per byte downloaded.,"Comparing Angular to React is ridiculous. There is no comparison. Knockout would be a better comparison for React.     React (core)  Angular (core)  Ember (core)      Two way binding  X  X  X    Reusable controls  X  X  X    Form validation  X  X  X    Templating  ^^*  X  X    Routing   X  X    Ajax handling  ^^**  X  X    IOC container   X  X    Modular code-organization   X  X    File size (minified)  ~90K  ~90K  ~350K^^***     ^^*React ^^requires ^^an ^^add-on ^^for ^^JSX ^^templating.  ^^**React ^^requires ^^Jquery ^^for ^^Ajax ^^handling)  ^^***Ember ^^(~260K) ^^requires ^^Jquery ^^(~90K)  All of this, plus a few things I also dislike:   It's going to be harder to test. What am I going to load my ""JSX"" into to unit test it? With Jasmine? I realize that I could have the code live separately, but that's not what they're proselytizing in their tutorials.  The use of text-based templating to define HTML, which is, itself, a text-based template for rendering UI... all on the client. Just use HTML for godsake. Knockout does this better.  No IOC and no code organization makes for a nightmare on larger projects.   TL;DR: Ember and Angular are more full-featured than React.js. And Angular gives you the most bang per byte downloaded. "
if you're honest ... this isn't an actual problem ...,"You can be one of two things:   Someone that remembers the order of these parameters  Someone that does not remember the order of these parameters   The following  must  therefore follow:  A) Do you remember them ?||---&gt; Yes -&gt; Not a problem.|---&gt; No  -&gt; Goto B)B) Do you look them up?||--&gt; No -&gt; The answer contradicts what we know, ie. bollocks!|--&gt; Yes -&gt; Goto C)C) Would changing one or all of the functions parameter order magically make you remember the prototype for each function ?||--&gt; Yes -&gt; The answer contradicts what we know, ie. bollocks!|--&gt; No  -&gt; Not a problem.  TL;DR if you're honest ... this isn't an actual problem ... "
it's too late to turn back now. Full power to the engines. Progress marches on.,"The last thing Google wants is a [Second Platform]( on Android devices - they can't even rein in the phone manufacturers to ship a virgin copy of the  First  platform.  Fragmentation is an inherent design flaw in open systems - someone says ""I'm doing this now"" and there's no stopping them. Usually it's good for the ecosystem, but Google's implementation has been lackluster.  Google's best bet would be to come up with some kind of ""Android (tm)"" program similar to Android Wear that enforces a unified software platform, but it's too late for that - the manufacturers would revolt and you'd get real forks instead of shitty Samsung/HTC/Amazon middleware/overlays.  And that wouldn't solve some of the real drastic problems with the platform - frankly, they don't give a damn. Developer's lives can be hell on earth as long as their apps look pretty and consistent and they continue to sell grossly overcomplicated and powerful phones.  tl;dr: it's too late to turn back now. Full power to the engines. Progress marches on. "
"The use of GOTO has a benefit and a cost. In most cases, the cost is higher than the benefits. In the remaining cases use it remorselessly.","goto has been shunned since the 70s in favor of 'structured programming', which means you no longer use GOTOs, but loops, conditions, procedures to structure program flow. This is a very good thing, because our programs have become that much more readable (so we can write bigger more complex programs without our heads exploding).  While Dijkstra has shown in his famous paper that most if not all practical use of GOTOs can be replaced by structural artifacts, there remain some cases where GOTO still makes sense (e.g. state machines).  Of course one  can emulate the program flow of a GOTO-based state machine with an enum (and perhaps a stack or two), but the resulting program is slower, more error prone and not that readable either.  TL;DR: The use of GOTO has a benefit and a cost. In most cases, the cost is higher than the benefits. In the remaining cases use it remorselessly. "
"Go read  Thinking in C++ , then  Inside the C++ Object Model , all of Scott Meyer's C++ books and finally  Multiparadigm Design for C++","You can write crap code in any language.  C++ just gives you more options for how to do it.  A lot of old-school C++ detractors tried it out 10 years ago when the compilers were still very poor. Compared to the C that they had been using for so long, it was a complicated, broken mess with a lot of potential straw man joke opportunities lying around.  A lot of C++ detractors today never went that far.  They just memorized the jokes.  I've used C++ for over a decade of console game development.  I think that a console game qualifies as a large-scale, high-performance embedded system with soft real-time and hard memory requirements that are continuously under pressure.  I'll tell you that C++ is complicated.  Sometimes it's even broken.  You can do things with it that would shame your family for seven generations.   But, it doesn't force you to.   Yes, a constructor can invisibly do evil things behind your back -but only if you write an evil constructor!  Well-written C++ can be beautiful, concise, understandable, solid and highly efficient.  I prefer C++ over C because I find that it is much easier to write good code with C++'s features (used appropriately) than without them.  Don't let inertia and obfuscated C++ jokes blind you.  Maybe if you are an old C dog like Torvalds, you can be live happily without learning any new tricks.  But otherwise, refusing to learn C++ is basically ignoring a bunch of really useful tools on the grounds that you might misuse them.  tl;dr: Go read  Thinking in C++ , then  Inside the C++ Object Model , all of Scott Meyer's C++ books and finally  Multiparadigm Design for C++ "
"I like simple tools, but there is a place for everything.","I still get on it just for the debugger. I just tend to judge tools based on how much of its functionality I have call to use. For eclipse, pretty much just the debugger and auto suggest. As to debugging, I've taken to using the jdb tool for some of the work.  ANYWAY. That being said, my philosophy is to try and have one Text Editor that I know really really well. Right now, that's Textmate. I'm acquiring skills in Vim, simply because I hear so many things about it. Forcing myself to work in it for some projects and languages helps. I'll probably give emacs a whack (again) in the near future, but I think I'll stick to vim, it fits my philosophy closer it seems. I also cultivate knowledge different IDEs and tools for different tasks. When I get deeply in Java nasty territory, I use eclipse. Not often.  tl;dr I like simple tools, but there is a place for everything. "
EVERYONE learn to ask questions correctly. EVERYONE stop being a dick when you answer questions.,"I'm no programmer but I think that this is a double-forked problem.  Firstly, those asking for help: we aren't your personal advice bureau. Try to work it out yourself plz   Figure out EXACTLY what the problem ACTUALLY is and what EXACTLY you are trying to achieve  RTFM. Start again from scratch. Google it. Chances are this problem isn't unique to you. Don't waste our time 3a. Tell us what you have tried thus far and the results. We don't want to be treading old ground. 3b. Tell us why you are doing xyz. We arent mind readers. We don't know your superspecific thought train  Don't expect a nice answer. We are essentially doing you a favour here. We like solving problems but unless you are willing to do some work yourself it isn't helping. It's doing your work for you. I have enough of my own tyvm   Secondly, those being asked the question:   Stop being a dick and fucking answer what's being asked  If you don't know the full solution, post what you have anyway. Someone might be able to finish it off  If something is Wrong. Explain why. This is documentation for future searches. Don't be a dick tho.   Don't be a dick.    Tl;dr: EVERYONE learn to ask questions correctly. EVERYONE stop being a dick when you answer questions. "
"It's not personal against you, sometimes ""hard to understand for the laymen"" is the easiest way for an expert to communicate.","Sometimes the correct answer is not possible to communicate succinctly to a layman. Also when answering a question on the internet one is not always familiar with the personal history of the asker. Furthermore, if you're writing for a general audience and not communicating privately, you run the risk of short-changing the more experienced readers if you skip over ""hard to understand"" information. ALSO the true experts on a subject are not usually the same ones that teach newcomers, they are not experienced at providing accessible answers. When you ask an expert why would you automatically expect a lay answer?  tl;dr It's not personal against you, sometimes ""hard to understand for the laymen"" is the easiest way for an expert to communicate. "
"Didn't mean to sound mean, SDL is bad, use GLUT or some other tool kit.","I'm sorry if I came off as commanding, I don't mean it like that. I listen to them because they're rules. To me, that means that you should acknowledge them at the very least. You have  two  other posts in /r/coding/. The fact that you want another in /r/programming/ upsets me.  Also, it's (code / architecture) discussion, as in code discussion. You're basically saying: ""here's my project, I'm advertising but since nobody enforces rules I can get away with it, click it!"" You're clearly demoing your application, ""interactive flocking demo."" Don't bother with my opinion though, I haven't been here long enough to know how things are run around here, and maybe you're right an nobody cares for rules.  Anyway, the major downfall here is that you're using SDL, which I hope is only for demo purposes. If you use Micro$oft Windows, SDL uses DirectX 5 (if you're using pre-1.2) or DirectX 7 (if you're using 1.2). Source: My own experience + Wikipedia. Both versions are relatively outdated, and despite SDL's easy of use I recommend against it. Check out [OpenGL]( + [GLUT]( or if you want something open source, check out [GLUT Alternatives](  tl;dr: Didn't mean to sound mean, SDL is bad, use GLUT or some other tool kit. "
you can't generate infinite strings in finite time. somebody should have told cantor.,"here's my stab in english. i'm not a huge reader of theoretical CS, but i haven't heard this proposed:  P != NP because NP-complete problems are fundamentally based on generating infinite strings and then applying some filtering algorithm to them (such as in diagonalization). since you can't generate strings of infinite length in finite time, you can't get to a P problem from an NP start without already having the infinite string available (or at least the portion that contains your solution). if you'd like to prove me wrong about the infinite strings, start writing .99999 repeating by adding more 9's on the right until you get to infinity. i'll just wait over here...  give up? hopefully you now understand why most NP algorithms don't even try to generate the base case (infinite string) and instead try to find some piece-wise iterative function on non-infinite strings (which are therefore P) that will approximate or contain the result they need to a certain level of precision. this is not equivalent to the real problem you'd be solving on the infinite string at the scale levels that make a difference, but it is usually good enough for our purposes if we have the time to wait...  tl;dr - you can't generate infinite strings in finite time. somebody should have told cantor. "
Know what HTML is and then go take a look at what the DOM is and JS should begin to make more sense in the context of browser scripting.,"From where it sounds like you are at right now, its probably best to just think of JS as a browser scripting language (yes it goes beyond that). What I mean by this is that when a browser gets an html doc it turns that doc into an object in the programming sense. That object is then made accessible to you through the browser via the document object model (DOM). So for example, suppose there is a header tag somewhere in the doc and it has an id of ""header"". You would then be able to get hold of that element via the dom using: document.getElementById('header'). Then once you have that element you can do stuff with it, i.e., hide it change it color, position, etc. This is a very simplified explanation.  Hope that helps.  TL;DR: Know what HTML is and then go take a look at what the DOM is and JS should begin to make more sense in the context of browser scripting. "
"Once you learn all the technologies, XML, XSD, and an XSD-aware serialization library (that has access to the XSD you wrote!) can automatically take care of that for you.","> If this were XML, it would have been a nightmare trying to get the data out of the strings and into floats.  You know, there is a standard XSD data type for putting IEEE [floats]( and [doubles]( into XML.  It defines exactly how a float or double should look in ASCII once it's in the XML file.  Once you write an XSD file to declare what your XML file will look like (what elements will have type  xsd:float  and  xsd:double  and so on), a good XML reading/writing implementation should just hand them to you as  float  and  double  native types in the language you're using.  For example, for Java, any JAXB-compliant XML reader/writer should be able to take an XML file and make it so that  &lt;foo&gt;2.0&lt;/foo&gt;  and be written and read with  setFoo(float f)  and  float getFoo() .  TL;DR:  Once you learn all the technologies, XML, XSD, and an XSD-aware serialization library (that has access to the XSD you wrote!) can automatically take care of that for you. "
"opinion is that ORMs make simple things simple, and complex things complicated, and I don't get paid to do simple things.","XML supports 2, 3, 4, 9 and is a pain in the ass to use.JSON supports 1, 2, 3, 4, 5, 6, 7, 8, 9 and is pleasant to use.Some S-expression format would have supported 1, 2, 3, 4, 5, 6, 7, 8, 9 and would have been even more pleasant to use than JSON, probably.  Thanks for your good post; here are my notes point by point, comparing XML and JSON:   Rules out XML.  Both ok.  Both ok. The spec can be a regular document. It doesn't have to be full of angle brackets to be acceptable and readable; in fact the fewer angle brackets it has the better.  Both ok. Any JSON or S-expression parser will reject bad syntax, and as for bad semantics, not even XSD can save you from checking from within your implementation whether something makes sense in an actual real-world use case.  Both ok, but JSON is far easier to develop with, therefore far easier to maintain, there's less code, no tools, and no XSD to update.  Rules out XML.  Rules out XML. Has no strong support in any language. Just lots of complex tools, but no language has built-in datatypes and native representation for XML. JSON works in JavaScript and Python, is very similar to many others, and you can write Lisp readers for it, and what's most important: has identical semantics to most modern programming languages built-in types.  Rules out XML :) Compact JSON is not great either, but pretty-printed JSON beats pretty-printed XML (which is an oxymoron by the way) by a long shot.  Both ok.   In addition to that, some S-expression standard would have covered all 9 points and provide a tremendously powerful tool (the LISt Processor) to implement validators, schemas and all sorts of utilities, while keeping it simple and nice.  I ran out of time and have to get my ass out of here ASAP but I'll get back and discuss ORMs later. My tl;dr opinion is that ORMs make simple things simple, and complex things complicated, and I don't get paid to do simple things. "
"Cryptography is hard, use something designed for the purpose put together by people who know what they're doing, ie. Bcrypt in PHP","Hash functions such as sha512 aren't really meant for passwords; these hashes are designed to be both secure (can't reverse) and fast -- in the case of passwords, fast is bad. Also, you're using the same salt(s) for every password, which is also incorrect.  The reason fast is bad is in the event of your database being compromised, an attacker can brute force passwords, even if you've used salts. For example, a decent [CPU cracker]( can do 100 million + hashes per second.  The solution then, is to use a slow hashing method designed for use as a password hash such as bcrypt with a changeable work factor. You can reasonably tune this to only allow, say, 50 hashes/sec on a reasonably modern computer, making all but the weakest of password virtually uncrackable.  Your method whereby you hash and rehash the results is actually LESS secure then hashing it a single time with a proper hashing method ( This is because there is a finite number of possible outputs from a hash method therefore you're reducing the possible number of eventual results ).  TLDR: Cryptography is hard, use something designed for the purpose put together by people who know what they're doing, ie. Bcrypt in PHP "
"Time is a limited resource, even when measured in credit points.","I'm an undergraduate CS student in Germany and we basically had everything from basic EE over computer engineering up to operating systems and then high-level programming (not in order).  Since I began learning how to program before university, I can say that most of us (our university) are way too bad at (applied) programming for a 4th semester... e.g. some didn't even know regexes before a PL/compilers course where they were formally introduced. Not that I use them heavily, but everyone who googles to fix an error or find an elegant solution to problems has to know them.  While I really appreciate all that broad knowledge that I acquired and that made me want to learn more, I clearly lack the time to be a guru in everything. I'll just focus on the software part, because it excites me the most. And will probably keep wondering over things I thought any CS student who has been programming for two years should know.  TLDR; Time is a limited resource, even when measured in credit points. "
Make an API. Call the API from Ajax. Call with reasonable defaults to prepopulate.,"Writing on my phone from a hotel room at a Tokyo Disney so apologies if this is terse.  Performance shouldn't be #1 consideration, maintainability should. And when you consider performance it should be with an eye to optimizing user experience, not system loads, etc.  For example, you said you're doing an Ajax call what are you returning? Ideally it should be a JSON object from an API. This can then be iterated to draw into the DOM.  A common anti-pattern (IMO) is to return prefabbed HTML. This makes your Ajax vastly bigger and slower than they need to be. This is performance that matters.  What I said about maintenance is relevant now. If you want to pre-populate the items in PHP as well as get them from an Ajax call you end up with two different languages and execution paths getting the same stuff. Smart use of routes, aliases, etc, in a framework can help here. I've done this in Laravel while moving an app from standard layouts to an EmberJS app. But all in all, it's best avoided as a needless functionality duplication.  TL;DR - Make an API. Call the API from Ajax. Call with reasonable defaults to prepopulate. "
No set can be even slightly representative of infinity.,"How would 1000 be different than 5 if your population were infinite?  Edit: I'll explain. Imagine you have a set of 10,000 people and you take various sample sizes. You see that there is really no difference between 100 and 200, and 500 still gives the same results. OK, does this hold true for an infinite population?  Absolutely not. No matter how big you initial population was, it can in no way be thought to be representative of an infinite set, because it is a member of that set. You may as well start considering the data in chunks of 10,000 members. Look at your original population - 50% are male. Look at the next 10,000 - 80% male. Next - 80% male.  Even still, the same logic could be applied. Turn the first 10k chunks into one large chunk and average it. Ad infinitum  TL;DR - No set can be even slightly representative of infinity. "
"Because they're displaying an inherently diagonal dataset, so it is the most sensical UI response to scrolling.",">Why the fuck am I scrolling down to see the timetable move sideways? That breaks so many rules in UI design.  Sorry to interrupt the bitchfest, but...because otherwise you'd be scrolling down to nothing?  They're displaying an inherently diagonal dataset, I think moving sideways to center the lower data in the view as you scroll down is a very sensible and intuitive approach, and I don't see how else you would do it.  It may be a little strange initially, but I wouldn't want to have to scroll down and then over every damn time, especially if I'm not using a mouse with horizontal scroll support. And you can't just line everything up, because the horizontal axis it time, and provides a very useful comparison.  TL;DR: Because they're displaying an inherently diagonal dataset, so it is the most sensical UI response to scrolling. "
"I read your entire post. I don't think you read mine. You're ""standing behind your claim"" even though your estimates are off by an order of magnitude.","> that this is anecdotal evidence  You're saying most design people use macs. I don't disagree, but it's irrelevant. We're talking about the number of dollars Adobe receives (cash) from Mac-using Adobe-products users. The claim was more than half, and that just doesn't pass the shake test because 20% of  mac users  aren't buying the 1500$ adobe package.  If you try to figure it out for the cheaper adobe packages, you need a higher percentage of mac users. That includes more students getting the student rates.  > 1/ Adobe Premiere & aftereffects is a LOT more expensive than photoshop (or at least when I was using it. )  CS4 Master Collection includes Premiere. If we're just talking photoshop, or simply talking premiere, then there simply aren't enough mac customers: You'd be off by almost a factor of 10(!)  > 2/ Every school...  I don't think I contradicted those numbers. I'm giving  yearly  revenue streams. 100,000 aussie macs is 30,000 yearly- and that's generous.  How many of those new 30,000 macs pays full price for Adobe CS4 Master collection  at full price ?  Remember: You need  one in five  in order for  most  of Adobe's income to be tied up in Apple.  > tl;dr  I read your entire post. I don't think you read mine. You're ""standing behind your claim"" even though your estimates are off by an order of magnitude. "
"Flash is great, but Flash is not an open, standard web technology. HTML5 can possibly help to fix that. That's why you hear so much complaining.","I'm not backing up Steve Jobs, or bashing Flash. But the argument against Flash on the web stands for very valid reasons.  The only real problem with Flash is that it's proprietary. It really isn't any deeper or more complex an issue than that. Whether it will run on your iPhone or whether it will break onmouseover events on touchscreens is a non-issue. Anyone would agree that Flash added valuable functionality to the browser when it was missing. Kudos to Macromedia for their work there. But the sad fact is that proprietary plugins aren't good for the web in the  long-term.  Web browsers, as a medium, are based  by inherent necessity  on open standards, languages and technologies. If you create a website, it goes without saying that it should be viewable and usable by as many people as possible. Likewise, the tools required to create that website should, ideally, be available to as many people as possible. So when you start to introduce proprietary tools and plugins, it really puts a stick in the gears of the entire ""open web"" concept.  In the case of web developers, specifically, it's hard to offer any significant quantity of video on your website without basing a similarly significant chunk of your code (not to mention financial investment) on top of Flash. Sure, there are alternative video codecs. But the fact is that Flash, being synonymous with web video, is basically the only format web developers can rest assured most browsers will support. And it's a self-fulfilling prophecy, because web developers continue to turn to Flash when they have to. There's no ""open standard"" there.  In earnest, and without bashing Flash, you have to agree the situation sucks for the web developer, or even the user, who really cares about open standards.  TL;DR: Flash is great, but Flash is not an open, standard web technology. HTML5 can possibly help to fix that. That's why you hear so much complaining. "
"Some useful work, but driven by very willful wishful thinking.","I've heard about de Grey before. What he says is correct in theory and principle; some techniques can already extend life, others are maybe not far from being invented. Still, there are a few loopholes:   Assuming that, as a minimum, we need to keep a brain alive. Are we assured of guaranteeing no brain tumor?  On our overpopulated planet, human life is remarkably un-precious. Artificial longevity can be a pet project for a handful of the very wealthy, but I can't see a sustainable mass market for it.  de Grey's estimates for human scientific progress are overly optimistic. Nuclear fusion is just about ready, Artificial Intelligence is just about ready, flying cars are just about ready...   While his work seems mostly respectable, I think I see in this guy the same kind of wishful fantasizing that characterizes various religious cults: Always the ""in our lifetime"" shtick, the prophecy that declares that the prophet will not die. Sarah Palin is convinced the Rapture will happen within the next 20 years, and in Alaska.  TL/DR:  Some useful work, but driven by very willful wishful thinking. "
people who claim there is no math in programming don't understand programming what programming is and/or don't understand what math is.,"Programming is math. If you can't do at least some math then you can't do programming. I wish people will stop repeating this lie. When you are working out that in some section of code, you are expecting to enter in state x or y (and throw an exception if you don't) and end in state a or b (and write unit tests to prove you are). You are doing math here. You are taking the set of initial states, defining the final states and constructing an algorithm to transform between the two. This is applied math end to end.  Yes there isn't much calculus because you are dealing with a different branch of mathematics. You are dealing with the branch that studies finite algorithms.  TL;DR people who claim there is no math in programming don't understand programming what programming is and/or don't understand what math is. "
learn to program and math will extend from it naturally.,"There are two kinds of math Algorithm math for the CSCoding math for the developer.  If you learn programing you will become better at math needed for development just as an extension of the concepts of programing. Algebra, Trig, Calculus, it all just becomes a bunch of functions.Linear, Discrete, Matrix?  Your data structures and decisions.How much you use any of it depends on what you're doing.  This will give you the foundations for CS level math if you want to go that route, but it isn't necessary for creating software. Just as it isn't necessary to know the manufacturing process of wrenches to become a mechanic.  You just need to know which wrench is best.  At some point programing, you're going to start seeing the world differently.  At that point you won't be poor at math anymore, without even attacking problems in a textbook you'll have better understanding on how math works and how to solve problems with it.  Just because you'll be better at problem solving.  You'll also have a better grasp for system analysis.  tl:dr learn to program and math will extend from it naturally. "
You don't need technical help. You need sample problems where it makes sense to use programming to solve them.,Do you want to teach programming or do you want to tell people why being able to program can enrich their classes/lessons? I assume the latter because you can't do the former at all in 45 minutes.  And if you do the latter then forget about point (3). You can think about how to implement this whole thing technically  after  you've whet their appetite. The really important and hard part here is point (2) or rather: how to find really good use cases where it actually makes sense to use programming? You should think about that very carefully. But it's not a question/problem that is related to Python in particular. The programming language and environment doesn't really matter.  TL;DR  You don't need technical help. You need sample problems where it makes sense to use programming to solve them. 
"Don't use company resources or information, and do something in an independent direction.","> > many employers actually still contractually own everything you create on and off the clock.  > *in the US  Even then, only within certain states within the US. For example, here in Washington the law is:  > #RCW 49.44.140  > (1) A provision in an employment agreement which provides that an employee shall assign or offer to assign any of the employee's rights in an invention to the employer  does not apply to  an invention for which no equipment, supplies, facilities, or trade secret information of the employer was used and which was developed entirely on the employee's own time,  unless  >  * (a) the invention relates   (i) directly to the business of the employer, or   (ii) to the employer's actual or demonstrably anticipated research or development, or   (b) the invention results from any work performed by the employee for the employer.      > Any provision which purports to apply to such an invention is to that extent against the public policy of this state and  is to that extent void and unenforceable.  TLDR:  Don't use company resources or information, and do something in an independent direction. "
"You need ""an appropriate copyright notice"" which presumably would mention the original author.","From the GPLv3:  >You may convey verbatim copies of the Program's source code as you receive it, in any medium,  provided that you conspicuously and appropriately publish on each copy an appropriate copyright notice ; keep intact all notices stating that this License and any non-permissive terms added in accord with section 7 apply to the code; keep intact all notices of the absence of any warranty; and give all recipients a copy of this License along with the Program.  >[snip]  >You may convey a work based on the Program, or the modifications to produce it from the Program, in the form of source code  under the terms of [the stuff I quoted above] , provided that you also meet all of these conditions: >[etc]  TL;DR: You need ""an appropriate copyright notice"" which presumably would mention the original author. "
"this is not a benchmark of  real world applications 
 Mind you, those results are interesting, and congrats for the good work. But numbers are numbers.","Interesting, but:  > Contrary to popular belief, hhvm is not orders of magnitude faster than PHP in  real world applications  (emphasis mine)   Wordpress  SugarCRM  login page  Drupal  homepage  Symfony  ACME app home page  (i.e. the empty demo app)  ZF  Skeleton App   The only realistic benchmark here is Wordpress. SugarCRM benchmark  could  have been interesting, but there's the ""SugarCRM anomaly"" section that can basically be summed up as: ""using the same session for all HTTP requests messed up the benchmark"".  Really? Then the solution is to  fix the benchmark , not just benchmark the login page. Use different sessions, and test real applications and real pages.  TL/DR: this is not a benchmark of  real world applications  Mind you, those results are interesting, and congrats for the good work. But numbers are numbers. "
"Thanks for sharing the link, but your headline and this comment are misleading.","This is 75% of  affiliate  revenue, Apple takes a 30% cut of  actual  revenue.  Furthermore, by default Banshee will be using the Ubuntu One store, which as far as I know had no affiliate program before now.  Canonical is offering a 25% cut of their Ubuntu One revenue to Gnome.  In comparison I think the Amazon affiliate program only offers around 3-5%.  My point is that if we assume as many people will use the Ubuntu One store as used the Amazon store previously, Gnome will likely end up making  more  money with the proposed setup than if they used Banshee's defaults.  TL;DR  Thanks for sharing the link, but your headline and this comment are misleading. "
"If non native code is fast enough for what you do then use it, for me it's really not.","I was talking about things that need to run fast. Java JIT is far from C++ in non contrived situations, and GC is an extra factor to consider which nukes the  ""oh JIT can be faster in theory"" argument even further if your using lots of memory.  Think about dealing with a gigabyte of data a second from a piece of hardware with a very small buffer. A GC would mean loosing data, in which case your program might as well segfault because it's worthless. Or think about something very numeric heavy, here Java is a terrible fit (if you don't believe me try looking for good linear algebra libraries - nothing reasonably powerful, fast, and updated in the last 4 years exists).  tl;dr. If non native code is fast enough for what you do then use it, for me it's really not. "
Don't reinvent the wheel unless you plan on making a better wheel.,"> For many programming industries, the surprise is that if you're inventing it yourself you're probably doing it wrong  It depends on whether you plan to compete on that feature. For example, there's no way I'm writing my own FTP library to transfer some reports. This is just a service within my application with no competitive differentiation. It's a giant waste of time.  However, if the app your developing has some sort of web management portal, you better believe that you're going to write most of that code.  That's because you plan to compete on that feature and the way you distinguish yourself from the competition is by doing those features better than your competitors. This is difficult to do by crudely welding large pieces of reusable code together.  Take Stackoverflow, for example. They  could  have used PHPBB and leveraged that huge amount of existing code. Forums are close enough to Q & A, right? ;). Some less experienced people might have made the fatal mistake of going down this route.  Instead they wrote their own and that allowed them to control their fit and finish build a market dominating product.  tl;dr: Don't reinvent the wheel unless you plan on making a better wheel. "
"if the team doesn't get the new workflow, maybe there is something more basic they are not getting.","That situation sounds strangely familiar. There were lots of barriers, but the biggest was attitude: VCS was  only  a way to share code. In other words, it was a glorified FTP share to upload code to. History? What's that?  This created a lovely snowball effect. Commits were messy, which made messy history, which made messy merges, so branching and merging was avoided.  Once I realized what I was up against, I realized I was not going to sell anyone on a new workflow anytime soon--no one was really taking advantage of a VCS. I finally gained traction by starting small and demonstrating the benefits of  revert , then explaining the existence of commit history.  TL;DR: if the team doesn't get the new workflow, maybe there is something more basic they are not getting. "
been there done that made the same excuses then learned better.,"As someone who almost matches the job requirements 1:1 and having worked in the industry in similar jobs: these are just excuses of the youthful.  Our industry has a strong problem with inferiority/martyr complexes that is especially present in the fresh new kids walking in.  The job needs to be done by 2+ people because, as history has proven time and time again the  fatigue  the stress induces leads to increasingly catastrophic mistakes. Plus studies have shown there is a massive amount of diminishing returns once you break the 40hr/wk barrier that start slipping into NEGATIVE productivity close to the 80hr/wk.  So what if 2 people doing the job will have an hour a day to browse /r/aww. That means your systems are stable and your IT team has the time to maintain quality and are better prepared for catastrophes.  tl;dr  been there done that made the same excuses then learned better. "
I might be getting downvotes for vocalizing the fact that I'm not be a target audience of this library.,"Which is funny, because:  > * insisting that writing code is better than using tools  I'm insisting these tools are more code you need to worry about now, that it will require you to buy into the opinions it has and might be hard/impossible to extend to perform tasks you need to perform.  > * underestimating how much code is involved in some of the tasks these tools provide.  I've been working with XLS and CSV (in sizes 2x my RAM) in Python/Clojure for years. Interfacing with data warehouses and doing control checks stuff in between. Over things like financial and HR data. But yeah, go on assuming I'm clueless. Again you're delegating responsibility for bugs to someone else, but when you trip on something you've got another layer to dig through.  > * failing to appreciate that these tools include (...)  (...) unknown amount of gotchas from  csv  + unknown amount of gotchas introduced by itself. SQLite is far better tested and can process CSV, so why not advocate SQLite? Because not shiny enough?  My CSV processing always requires map-filter-reduce logic on top of it. You're telling me that bash script will be better suited for that than Python? Because using Python's module via bash wrapper is just better than using the lib directly?  And listen -- I don't have any ulterior motives here, I'm not trying to diminish authors work and I'm not releasing my (proprietary) tools written over the years. I'm just jaded by years of leaky/culled abstractions that you need to fight as soon as you want to do something more complicated than printing to stdout. But ""oooh, shiny!""  tl;dr I might be getting downvotes for vocalizing the fact that I'm not be a target audience of this library. "
"a CA is required to prevent MITM, unless you want to do extra work to verify the self-signed certificate's authenticity.","If you can't authenticate your endpoint, then you're open to MITM attacks. Here's how CA signing prevents this:   VeriSign, or another CA, verifies an applicant's identity.  They generate a certificate for the applicant, and calculate its fingerprint.  The CA encrypts the fingerprint with their private key, creating a signature.  Applicant receives their certificate with CA signature, and uses it to set up an SSL connection.  Your client receives the certificate and recognizes the CA in its trusted certificate list.  Client decrypts the signature using the CA's public key, retrieving the fingerprint.  Client compares the decrypted fingerprint to a fingerprint it calculates from the server's certificate.  If they match, the server is authenticated.   Certificates are tied to a domain, so absent DNS attacks MITM doesn't work--the man in the middle's IP doesn't resolve to the domain on the certificate, so the certificate will be rejected.  CA-signed certificates provide authentication because the browser already has the CAs' certificates in its trusted list. Self-signed certificates can be used for authentication, but you would have to verify the certificate's fingerprint yourself.  tl;dr: a CA is required to prevent MITM, unless you want to do extra work to verify the self-signed certificate's authenticity. "
I don't see why people bash the zune.  It's a fantastic device.,"Do you happen to own a zune?  I've owned 3 ipods and a zune.  The ipods would break about 30 days after the warranty period ended.  New ipod every year, gen 3-5.  I've had my zune for 2 years and it's still going strong.  The UI is better, the case is made of some sort of unscratchable plastic.  The thing looks just like the day I bought it.  The firmware gets updated regularly with actual useful patches.  And the zune player software (Used to be a piece of crap, but it's now on v3.0) is much better than anything iTunes related.  tl;dr - I don't see why people bash the zune.  It's a fantastic device. "
"If you want to mainly do applied, learn a broad overview of the subject while you're playing around with the algorithms that someone else implemented.","Maybe this sounds like a strange question, but what, exactly, do you want to know?  Are you mostly interested in directly applying the techniques, or are you interested in learning the theory behind it?  If you're really just interested in applying the techniques, perhaps a good place to start would be just trying to learn a very vague overview of the topic (maybe wikipedia's page on  Supervised Machine Learning , and then use ML packages people have already developed.  There's packages for Matlab, if that's your cup of tea (it also has a lot of added benefits w.r.t visualizing your data). You could also look into packages like Weka.  tl;dr:  If you want to mainly do applied, learn a broad overview of the subject while you're playing around with the algorithms that someone else implemented. "
"You  have  to specialize today, there's just way too much out there.","I'm surprised nobody's mentioned just how stunningly  broad  the programming world has become in the last ten years.  Even just .NET - the environment I know best - is something like  ten times  as big as it was ten years ago.  There are something like 50,000 classes in the standard drop.  And that's just .NET - the Java world is arguably even bigger.  Yes it's good to keep your eyes open and at least dabble in other spaces (I'm starting to dabble in Android, personally) but the idea that it's even possible for a developer to be productive across a huge array of disciplines is kind of laughable.  I have been doing Windows Dev  hardcore  for 20 years and still feel like there's more on the platform I don't know than I do know.  Yes, some academic stuff like say, graph theory, sorting, Djiksta and whatnot are all about the same anywhere.  But to be truly productive means you really know how to ""think"" the right wayfor a given environment / toolset and that's not a skill you learn overnight.  In fact arguably the entire C++ community spent most of the 90s collectively learning how to 'think the right way' in C++, and I see a lot of it today in environments like Haskell.  TL-DR: You  have  to specialize today, there's just way too much out there. "
XML is a bad format because it's as readable as ascii text and as writable as a binary blob.,"I'm arguing that its readability is both a blessing and a curse.  Since it's readable as ascii text, people tend to treat it as such.  You should not be editing XML in notepad, but people do all the time, xsd be damned.  That's the problem.  It's not ascii text.  It has an explicit definition.  It needs to be validated when you edit, but that doesn't exactly happen in notepad.  You could then say the problem is that people are using the wrong editors for XML.  The editor should validate the xsd and what not.  But I say they are lured into believing they can modify it without error because it's easy to read it.  tl;dr - XML is a bad format because it's as readable as ascii text and as writable as a binary blob. "
You get more done and in less time with Python.,"I have the ZCE PHP certification.  I can confidently say that I know PHP at an expert level.  Ive been working in PHP since theres been a PHP.  That said, the more I get a chance to work in Python, the more I feel like I'm actually getting things done.As far as web frameworks go, Python is superior IMHO.  Django and Pyramid applications are so friendly to developers, you almost feel like you're cheating when you're setting one up.  And designers who typically work on Wordpress sites almost cream themselves when they see Django's templating system.If I can use only one word to describe working in Python, it would be this: clean.Everything in Python is cleaner than in most other languages.  Sure there are gotchas, but they are few and far between.  One of the most empty arguments I hear against Python is the fact that whitespace is syntactically relevant.  To me, this is more of a whine than anything else.  The benefit of this to me vastly outweighs the alleged inconvenience.  You can pick up any piece of Python code and be able to read it without having to decipher the author's individual stylistic biases.  You can't do this in PHP or Perl.  In those languages, you are at the mercy of whoever wrote the code...you hope they weren't a cowboy.  TL;DR:  You get more done and in less time with Python. "
use the best tool for the job and stop trying to peddle The One True Way because there simply isn't one.,"Why I program in various languages:  They all are good at different things. Whilst I love Erlang, it certainly sucks at different things. I'm genuinely surprised that he mentioned strings as being a good point in Erlang, when it just clearly isn't. The standard string API is kludgy and quite hard for beginners to wrap their noggins around. Especially when you start adding binary data or the  dreaded  Unicode strings. Since Erlang makes no attempt to hide the fact that strings are just integer arrays.  Erlang is  extremely  good for writing asynchronous message-based servers/protocols. Simply because it's  how  the language works. Is this the best answer to every problem? Nope. Is it good sometimes, yes!  TL;DR use the best tool for the job and stop trying to peddle The One True Way because there simply isn't one. "
the point of mockups and planning ahead is so you don't have to constantly 'throw away' code.,"This guy is trolling. I hope. This is nothing at all what the professional dev environment is like. Code does not get 'thrown away'; it gets refactored IF urgently needed. Mockups and design decisions are crucial to solidify expectations and plan structure. If you do not believe in this workflow then I seriously question whether you've ever had a time crunch, and by extension whether you've worked in a professional environment.  Amateurs reading this: if you want to be a professional in the field, do not fall under the misconception that going 'rambo' will get you places - it won't. It will, however, ends you up with poorly executed code due to lack of foresight and way behind deadlines due to constantly restarting as you begin to understand the scope.  Tldr: the point of mockups and planning ahead is so you don't have to constantly 'throw away' code. "
"Yes, it's quite believable that big companies are trying to use market saturation to get cheap labor. But it won't work.","Hardly a nutty-sounding theory. I think a lot of businesses have unrealistic expectations (or for big companies, wishes) that you can pick up quality programmers at the dollar store. When confronted with reality, such companies/hiring managers rebel against it. How dare we demand more than minimum wage?  A lot of shops rebel against reality by hiring code monkeys by the ton and using those people as the foundation of the empire. Which, of course, leads only to leaning-tower-of-Pisa structural design.  The big companies are smart/aware enough to attack the problem with economics. If there's enough competition, even the best of us will be cheap, right? But this actually falls into the same quantity-over-quality trap, just over a larger scale, because pretty much any means of ""flooding the market"" really boils down to ""flood the market with crap,"" which is then mostly eaten up by the ""hire a bunch of code monkeys"" shops. It would be generous to say that 1 in 10 Year of Code-ish programmers end up as competent software engineers, and even this is mostly eaten up by the rest of the expanding hiring market.  tl:dr; Yes, it's quite believable that big companies are trying to use market saturation to get cheap labor. But it won't work. "
"Sure, garbage collectors are a hard problem - but app server frameworks should work around that problem.  Apache's a good existance proof it's possible.","Makes me wish Rails and equivalents had similar to the Apache MaxConnectionsPerChild directive --- where after some number of requests the process kills itself and forks a new one.  It's not just Ruby's garbage collection -- but also zillions of extremely convenient but sometimes sloppy libraries that have slow leaks.  While, sure, it's ""better"" to fix all the bugs -- I wish the higher level application server parts would use forking to be resilliant against crappy libraries and crappy garbage collectors.   The Apache guys never said  ""not our problem - just send bug reports to the php garbage collector team"" .  They added  MaxConnectionsPerChild  time that computer scientists research better garbage collectors.  TL/DR:  Sure, garbage collectors are a hard problem - but app server frameworks should work around that problem.  Apache's a good existance proof it's possible. "
"If you are to be a professional programmer, you  must - and  must want  to document your code.","Please allow me to annihilate your argument, right here, right now:  > it's pretty straight-forward  No it is not. It might not need much code, but even the basic purpose is easily eluded. I have trouble understanding what it is exactly the output/purpose of your program is. It has been some time since I worked with Parser-generators.  By documenting code, you think about the elements involved, and you might've actually chosen to separate your UI from your logic.  Conversely, you will have done that less by not documenting your code.  By not documenting your code, you have ensured that the next time something is changed. Neither you, nor someone else, will remember what the original code did.  After 5 months, this code might as well have been written by someone else, when you yourself try to maintain it.  By not documenting anything; you do not know what the original scope, purpose or thinking behind this thing was.  By not adding XML-comment, when working with the .NET defacto IDE, Visual Studio, you will  not  benefit from code completion showing the purpose and use of each and every single element you use, such as the AddRule method etc.  You must always document your code, the human mind works with ever decreasing certainty; We ultimately cannot even trust our own minds to keep critical information for a system intact over anything but minutes.  When solving problems, uncertainty  can  exponentially  increase the difficulty of solving a problem. This becomes a huge problem for legacy code.  TL;DR: If you are to be a professional programmer, you  must - and  must want  to document your code. "
"estimate 3-5% increase per year, but if you play ""the game"" you can get more.","I think the job title ""data scientist"" hasn't been around long enough to make reliable predictions of career earnings.  ... unless you are a data scientist.  But anyway; as a lower bound, I'd assume something on par with the above starting salary projected with inflation adjusted raises over the course of a typical career (~40-50 years). Mean I'd guess you'd expect slightly more than that... if you perform well then I'd expect ~5% annual bonus / raise per year. Of course, this wouldn't be perfectly smooth, as any individual may have peaks and troughs based on performance of their particular company / industry.  Upper bound, I'd expect some percentage of data scientists to make their way into management / decision making roles, which would then be in another tier of compensation tied to company stock / IPO / profit sharing somehow; so any average is relatively meaningless.  This is often the case with many jobs (and it's more related to how HR does things than how that job functions). Most medium to large corporations (the kinds that would hire most of the PhD level data scientists) will have set incentive plans determining maximum expected bonuses / raises for ""top performers"" and minimum bonuses / raises for ""average performers. But this cycle is often broken by lateral moves or vertical moves within the company, as well as between companies.  For example; many software engineers get set annual raises; but some then choose to hop between companies (or start their own) as a way to break the ""fixed"" annual raise set by a given company's HR policy.  TL/DR: estimate 3-5% increase per year, but if you play ""the game"" you can get more. "
candidates with open source have an advantage because there is more time to technically review them outside of the interview and more examples to look at.,"I work at a large Silicon Valley household-name company and interview candidates regularly.  Candidates who have a lot of recent Open Source work have an advantage over those that don't because I can talk with them about their projects, which they're generally more at home with and excited about.  I also have hours or even days to review their code online, so I get a strong idea of where their skill set is.  Candidates without recent open source work I have to grill a bit more to get an idea of where they are technically. Even then, my time with them is limited to an hour or so. Where the other candidates has gotten hours of my time.  People with open source experience also look like more passionate contributors than their open-sourceless counterparts. Right or wrong, this is the impression that it gives.  TL;DR: candidates with open source have an advantage because there is more time to technically review them outside of the interview and more examples to look at. "
"there is no definition under which the statement was true, the intention was to be suggestive and a catchy beginning to an argument, not an argument itself.","I mean it in the sense that the statement is literally speaking false (or even nonsensical) but nevertheless a useful hook onto which you can hang a more careful argument.  Of course git is a system, but like most systems it is used as part of other systems. What the phrasing was meant to suggest was that although Git works with distributed repositories it does not solve many of the problems involved in coordinating distributed repositories or complicated topologies of connections between repositories.  This is not a criticism of Git, because that is not the problem it was created to solve. The documentation for Git makes it clear that distributed repositories can be coordinated in a variety of ways depending on the project needs, and Git facilitates that coordination but does not do it alone. This is almost certainly the right choice for what Git is. Software projects don't have that many contributors compared to many other networked systems, so some degree of manual oversight is feasible. Additionally, since the changes to the software need to be manually reviewed by project management anyway, the degree of social structuring to how Git is used adds relatively little burden.  TL;DR there is no definition under which the statement was true, the intention was to be suggestive and a catchy beginning to an argument, not an argument itself. "
so I know that its my form you're criticizing and not the content?,"OK, I generally try not to complain about downvotes. I understand that I might not provide a helpful comment sometimes, or that you might disagree with me. But I spent over half an hour writing a comment, in order to demonstrate why copyright has real value while being abused in its current form. It's as long as a decent college essay or a short magazine article, though I'd need to put more work referencing it and revising it to improve the accuracy of some of the historical details if I wanted to submit it anywhere. I think it spells out pretty well the analogy between intellectual and real property, and why even though they limit some fairly essential freedoms, we put up with them because they provide considerable benefits.  I know that typing takes effort, and clicking a down arrow is easy, but could you maybe try to write just one sentence of criticism, of an argument refuting my argument, or even just a ""tl;dr"" so I know that its my form you're criticizing and not the content? "
"version:  Two rules of parsing HTML with regex: 
 
 Don't do it. 
 (expert users only) Don't do it  yet","There is a vicious cycle of HTML processing. A novice programmer looks at the web and their tools and says, ""I'm sure this can be done with regex; after all, I've never encountered a problem that couldn't be solved with them.""  Some novices are eventually introduced to formal language theory and compiler construction, and discover Chomsky hierarchies and parser generators, and say ""well, HTML is context free, so you can't parse it with regex."" When confronted with a novice who insists otherwise, they declare ""for every regex you write I can offer valid HTML that breaks it"".  With more experience, some will discover that writing their own parser is crazy, and that we have tools designed for this purpose. SAX2, DOM, XSLT/XPath all serve them nicely.  Finally, a grizzled veteran practicioner comes along with a useful example: HTML written by a novice, and naturally, it's  invalid HTML .  What do they do? It's not valid HTML, but the browser renders it and there's data stored within it's pages. All of the XML parsers written for the purpose refuse it, as the XML spec demands invalid XML not be processed. The grizzled vet turns to the one tool left to them: Perl,  and a lot of regex .  TL;DR version:  Two rules of parsing HTML with regex:   Don't do it.  (expert users only) Don't do it  yet  "
Learning about compiler design also gave me the skills necessary to recognize and apply design patterns in  almost everything ...computer related and otherwise.,"That redundant statement I wrote is making me cringe because it's redundant. (sorry can't help myself!)  Anyway...This was 7 years ago. But, I remember the process of writing a simple compiler in that class over the semester, was like an awakening, as corny as that sounds. Kinda like I didn't appreciate or grasp the fundamental  abstract patterns  of what was going on when high-level code is transformed to machine instructions. Those new insights allowed me to draw parallels between software and seemingly unrelated areas of knowledge such as linguistics, philosophy and neuroscience. (Hard to explain what I mean succinctly...but you know what I'm talking about if you've been there.) It also serves as a good springboard into the AI fields. Overall it forces your mind to engage in the practice of extreme  abstraction  which I believe, is an important cognitive skill that enables one to apply domain-specific knowledge and skills to a much wider set of real-world problems.  Ironically, the compiler that we had to gradually code up over the course of the semester was written in Java! It targeted the MIPS architecture.  tl; dr:  Learning about compiler design also gave me the skills necessary to recognize and apply design patterns in  almost everything ...computer related and otherwise. "
"There are reasons to ride a fixed gear, single speed bike.  You may not care about them, but some do.  Don't hate what you don't understand.","I also just want to say that with the purpose of a bike there is more to it than just function.  The extras determine its value.  Like a road bike will be geared differently, be lighter, and have thinner tires with higher pressure.  A fixed gear is normally more solid due to a thicker heavier frame, and the weight is taken off through the lack of a rear cassette and derailers.   Since there is not gears, some people (like me, you may not care) find the ride to be smoother and more enjoyable.  Shifting gears is jarring, and requires thought.  Its hard to explain the differences in ride enjoyment without getting philosophical. Think of bikes like cars.  Some cars are impractical in the greater sense, but serve purpose to some.  tl;dr - There are reasons to ride a fixed gear, single speed bike.  You may not care about them, but some do.  Don't hate what you don't understand. "
"Launched Compilr.com 2 years ago on Reddit.  Now has 32,000 users.  If you have any open source projects, it would be awesome if you could upload them.","Hey Reddit! About 2 years ago,  Zenox , [launched Compilr.com on Reddit](   [Compilr]( is an online IDE and compiler for languages like Java, C#, and VB.  Today, we are both working hard on Compilr and are really happy about its recent growth.   In just the last year we have grown the traffic about 400%.  The userbase has grown from 3,000 at the beginning of 2010 to 32,000 users.  We expect our userbase will be well over 100,000 by the end of this year.  The IDE component of Compilr has come a long way since our original launch. You can see our IDE in action [here](  We plan on improving in 2011, with features like the ability to compile XNA applications and edit code with Intellisense. Other major planned features are version control and project management.  Two features that will enable users to code from their own environments should they feel our IDE is not adequate.   Ideally subversion will be our first version control component we add in, which would virtually make our system like GitHub without the Git (a paradox?), with a very nice IDE attached.  So anyways [check us out]( and let us know if you have any feedback.  If you are a hacker and happen to have any code sitting around on your hard drive, collecting dust, why not upload it as a public project on Compilr?  It may help fellow hackers!  If you want to try us out and don’t feel like creating an account use bugmenot as the username and bugmenot as the password.  tl;dr Launched Compilr.com 2 years ago on Reddit.  Now has 32,000 users.  If you have any open source projects, it would be awesome if you could upload them. "
After a week using it the biggest thing is it's faster. Like a fuck ton faster. It's also pretty.,"I've been using VS 2013 Ultimate Preview for about a week. Previously I've used 2008 Professional -> 10 Ultimate  -> 12 Ultimate, so I feel like I can safely comment what differences I've found between them.  All fancy features aside, 13 is just fast. Like really goddamn fast. The splash screen for 12 was quicker than 10 but 13 makes it look like a beat up old clunker. Similarly all internal screens, menus, operations internally are much faster too.  The new blue theme I'm sure will be nice for some and it's good they are adding options (being comfortable leads to better code) but the biggest improvement visually has to be outlining around control groups.  Before they just put controls on a matte surface together and used their location and labels to discern their meaning. It worked well but it led to situations where a button with no border and a label looked identical. This wasn't a debilitating problem but it's nice that they worked on it and found a solution.  The integration with TFS and actually TFS itself is a massive improvement. Social coding is becoming an important part of distributed coding environments or even as a way to mean your office doesn't have to have 5 meetings a day to stay on track with one another. It's not going to help everyone but it's good.  I personally quite like the visual call stack idea. Yes it's not perfect for actually debugging but have you ever tried to explain to a manager or non techy person how a problem up here is actually caused by a tiny tiny error all the way down that call stack? Something tells me this will not only save time and sanity but possibly the lives of managers who would otherwise have driven their engineers to murder.  Conclusion (and TL:DR)  After a week using it the biggest thing is it's faster. Like a fuck ton faster. It's also pretty. "
"changing languages is not a silver bullet, you still have to just be careful.","The plane analogy kind of breaks down a bit, because software development tends to be driven by a ww2 aviation paradigm. Get it out there, get it back in the sky.  Secondly, blaming C/C++ for having the ability to produce buggy code is like blaming the English Language for having combinations of words that produce hate speech. Any programming language that lets you do anything will by necessity give you interesting and creative ways to shoot yourself in the foot. A language that manages your memory for you solves one class of bugs, but then maybe down the track your airplane software running GCAwesomeLang runs into a memory bottleneck and the plane crashes as the runtime works on freeing space on a fragmented heap.  tl;dr changing languages is not a silver bullet, you still have to just be careful. "
I am the most incredible programmer since G_Morgan and loudZa,"Instead of writing to the assignment spec, I once coded something along the lines of The Matrix which enveloped my entire class. So I battled the AI including the auto-grader which was manifested as a digital Chuck Norris. I defeated him with the aid of on-the-fly injection of instructions to phase my fist past his impenetrable beard then re-materialize through his neck. This, of course, only stuns him so I manifested a bag of holding and staff of ruin. I placed the bag of holding at his/its rectum and forced its inversion w/ the staff of ruin. The combination of the inverted bag of holding and sodomy caused a profane singularity that eliminated the auto-grader. The auto-grader was defeated so I moved onto more mundane tasks such as slaying dragons, rescuing my classmates & orphaned pointers, and general walk-on-water type of stuff. Once I grew weary of this program I tore through the virtual fabric riding my gargantuan transforming robot tiger / tron-cycle through it -- all while making sure to write a dirty limerick about how awesome I am in the light-trails of the tron-cycle. Lastly, it should be known (and you'd know if you read the comments), that if you open an IO stream to a dot-matrix printer then write a reversal of the byte-code to the printer; consuming the printed sheets of paper will cure HIV. The caveat here is that if you print the byte-code in normal order instead of reversing the characters, you will actually progress the condition to AIDS.  TL;DR I am the most incredible programmer since G_Morgan and loudZa "
"Yeah, they're charging inflated prices on the bandwidth, but in the grand scheme of things, it's not exactly a huge drain.","I certainly wouldn't be surprised to learn that they use that fee to make up some of the low margins they get on the Kindle hardware. I find it fairly hard to care though. It's not like a text message rate or data rate on your cell phone where the costs add up to money you owe. It's just a small decrease in the per-sale revenue given to the author.  A typical kindle book appears to be around 500K. It's not clear to me if that charge is factored into all sales or only those sales that go over Whispernet, but presumably most do anyway, so we can assume the worst case and that it's all of them. Suppose you set the price to $15. That means you get 0.7*15-0.07=$10.43 per book sold, versus $10.50 if they gave away the bandwidth completely. That amounts to roughly a 0.6% increase in Amazon's fees (i.e., instead of 35%, you actually take about 34.4%). If you sell 100,000 copies of your book, you give up an extra $7000 or so in revenue on a total take of a bit over a million dollars.  If they're consistently selling your book at a fraction of the selling price, the relative costs to you go up of course, as the distribution costs are constant. But even if they're selling it at 80% off, you're still only leaving $7000 on the table on $200,000 in revenue.  (It's also worth mentioning that Amazon pays for the bandwidth for repeat downloads, downloads to other devices, etc., and the costs of that bandwidth aren't charged to the author or the user and have to be factored into the overhead of running the system. I'm not suggesting that makes up for all the difference in their per-megabyte pricing, but it does make any sort of analysis more complicated that it might first appear.)  TL;DR: Yeah, they're charging inflated prices on the bandwidth, but in the grand scheme of things, it's not exactly a huge drain. "
"writing software to manage these things is harder on windows and fewer people care, these two things feed into on each other.","Two reasons:  Firstly it's hard, relative to linux, to install  most  development environments that aren't windows centric in windows. Linux distros and Windows involve different design tradeoffs. This isn't specific to python. The guys who work on haskell at microsoft research all run linux and haskell is apparently a bitch to install in windows as well.  It's silly to compare a dev focused environment like most linux distros with windows in terms of how easy it is to do developer related things.  Also, it's just generally much much harder to install  anything  on windows than on a modern linux distro. Windows is decades behind Linux when it comes to packaging software (as is most everyone to be fair).  Secondly, most of the developers who make these tools are using linux or osx so they build things to work on the platforms they care about. There aren't many windows users doing this kind of work or it would be further along, although it is more work to support windows than a *nix or osx. Interestingly I bet OSX gets more effort put in than linux at this point, there are a lot of OSX using python library devs but also the linux support is very very easy and so it gets done quickly.  tldr; writing software to manage these things is harder on windows and fewer people care, these two things feed into on each other. "
"Delete this script and never speak of it again. 
 EDIT  Spelling and better word choice.","I see that you have two or three quite often used projects on GitHub so I'm going to treat you as an adult.  > here is mine:  There are so many issues on so many levels with this. First and foremost: you provide a script that needs a config file but you don't show how it's supposed to look like. And you don't even show how to use the script.  Some other examples of already mentioned issues:   Indention and code style in general is inconsistent. Lines 64-67 have 3 spaces instead of 4. The parts where you raise  OrganizerException : why does it look like that?   The functions  getFileList  and  getRules  are both basically one-liners although in  getFileList  you assign the result to a variable before returning it and in  getRules  you don't. In addition both are only called once. So why do they even exist then?   This one is really weird. You have a function  getExtension  (by the way, [Python has this built in]( The name suggests that it gets me the extension of a file and it does so ... more or less; it returns  False  if there is no extension. Why not empty string? But okay, that's not the weird part. This is the weird part: you call this function only once again ... to determine whether a file  has  an extension in  hasExtension . And that one you call .. again .. only once. That makes one of these function completely pointless. But that's not the end of it. You use the latter function to filter out files without extension ([L63]( although right in the next line you call a function that matches rules from the config file against every found file. So why do you check for the existence of an extension again? Files without extension will never match a rule from the config file except there is one exactly for that case.    There's more but I suggest ...  TL;DR  Delete this script and never speak of it again.  EDIT  Spelling and better word choice. "
"Use lots of classes when redesigning to minimize style breakage, but clean up the bloat once you're finished.","It really seems like the author is advocating binding css to the class attribute for maximum flexibility. The problem though is this leads to markup bloat. This is a good way to retain styles during development to lead to minimum breakage, but I think you really want to keep the class usage to a minimum once development has finalized.  You have 4 ways to bind CSS to HTML: Element, Position, Attribute, and Event. The cost of that flexibility for having your site flexible during changes is classitis, where a significant part of your markup is class declarations.  The author implies the statement that if you change the html with rigid css selectors, you are going to have to redo the css. This is pretty much unavoidable with complex html, but if you are adding new elements to your html during development, you are REDESIGNING your site and unfortunately that means redesigning your css.  If you're using a good IDE, the IDE should inform you when your changes are breaking things.  I feel, in my opinion, the best way to reduce fragility of your css is to declare an anchor point (via ID or another class) for the major sections of your html and then work your complex selectors. It doesn't sound like the author uses any kind of template, hence why he's advocating heavy class usage.  Then you only have to be careful of overqualification where you're using element#id or element.class for different functionality because that's getting disorganized.  This is only my opinion though. Everyone does everything differently.  I had a train of thought but my job has been interrupting me a lot and I think I've lost it.  tl;dr:  Use lots of classes when redesigning to minimize style breakage, but clean up the bloat once you're finished. "
"There is no graceful way to handle errors and notices, so convert them to exceptions and let your customer handlers do somethign about them if they go uncaught.","They become catchable, or if uncaught, they will go through the exception handlers you supply. From here, your custom handlers can include raw text logging, emailing, db logging, etc. Where I work, I slowly deployed this onto our production machines. On every exception now, we 1) write to a flat file 2) log the error to mongodb and 3) email the error to all of it  The bigger gain from this really isnt converting errors, its converting notices. Think about an undefined variable notice. The script will continue executing, potentially fine, or potentially could cause some serious issues, and you'd have no idea that it was happening (other than someone calling you to let you know something broke).  The goal of this is to make PHP close to as strict as most other languages, and force you to handle exceptions that are occuring in your code, and also give you a clena means to notify you that they are happening.  The question back to you is, what are you doing about notices and errors that you ""know"" aren't occuring sporadically.  EDIT: I forgot to include what this did for us. We are a large company, so as you can imagine, sporadic errors can go unnoticed and unreported. When this was deployed we discovered several scripts that had the leagcy mysql_query() class not being checked for a result, and would cause and undefined offset notice when it was used and the server was unreachable.  TL;DR There is no graceful way to handle errors and notices, so convert them to exceptions and let your customer handlers do somethign about them if they go uncaught. "
"Mozilla rightfully has a big, but not exclusive, say in the future of HTTP/2.","I did give a citation. I pointed out the part of OP's article that proved HTTP 2.0 supports port 80 unencrypted connections, and explained why it was proof.  Furthermore:   HTTP/2 takes a lot of lessons and ideas from SPDY, but it is not SPDY. Just because a decision was made in the latter, does not mean it will be duplicated in the former as the universal ""future of the web.""  Your first link is people considering options for requiring TLS, with at least one statement to the effect of ""we will also work out how admins who consciously choose to use unencrypted connections, can do so."" This has since been worked out, as per what I was pointing out in OP's article.  Mozilla's statement is interesting because Firefox does have significant marketshare. However, there is a defined way to do it, and other user agents will likely implement it (Mozilla tends to make controversial decisions that more pragmatic browsers don't). If it becomes commonplace enough, the Firefox devs may very well change their minds. tl;dr Mozilla rightfully has a big, but not exclusive, say in the future of HTTP/2.  "
Don't inherit for the solely for the sake of gaining functionality from a derived class. (Which is exactly what the article did),">I agree avoiding inheritance as much as possible will help solve this problem, but it also kills your code reuse.  How so? If I create a two seperate classes can I not use them both later? I think you are confusing code reuse with laziness. If you take class A and derive class B from it just so you can use its code you are being lazy. What happens if I change the base class functions that class B relies on?  Proper code reuse should rely on making a utility class one(say a rectangle) and using it in multiple areas. Not makiing a genric base class then deriving things from it just to gain its functionality. If you need general functions for say tokenizing strings in different ways you make a class called Tokenizer and give it some functions that can tokenize a string, or better yet skip the class step and put the damn thing into a namespace.  tl;dr Don't inherit for the solely for the sake of gaining functionality from a derived class. (Which is exactly what the article did) "
The snapshot model sucks. HTML has been effectively unversioned for years and changing the name just reflects the reality of the situation.,"The WHATWG spec maintains section specific status markers that can be used to identify whether a given feature is only a draft. in last call or widely implemented and stable.  This has been in effect for a couple of years now and has replaced the old model that simply stated the stability of the whole specification.  This is also why the WHATWG has effectively been working with an unversioned model, except in name only, for years without any problem, and why dropping the version from the name is not a bad thing.  I've been involved in specification development for a long time, including working on HTML, and I have witnessed first hand how the old, slow and overly bureaucratic W3C snapshot model is actively harmful to specification development.  The old model provides zero benefit to implementers who should follow the latest editor's draft; and it causes problems when some developers mistakenly work from a snapshot under the false assumption that it's more ""official"", and in doing so fail to adapt to more recent changes based on the continuous cycle of feedback.  TL;DR: The snapshot model sucks. HTML has been effectively unversioned for years and changing the name just reflects the reality of the situation. "
It's a pain in my lazy ass to check the source. Write better docstrings!,"It may surprise someone in Hong Kong that 8 AM is False, but I agree that one should check for  is None  in general (edit: for a default value) instead of relying on the boolean value. It would be a pain to have to check the source code for a package at every step to figure out when a class is False.  For example, the docstring here is insufficiently detailed (typical, IMO). Python 3.2.1:  &gt;&gt;&gt; ?datetime.time.__bool__x.__bool__() &lt;==&gt; x != 0  Obviously this comes from the fact that in Python 2 the check for a boolean value uses the  __nonzero__  method. Python 2.7.2:  &gt;&gt;&gt; ?datetime.time.__nonzero__x.__nonzero__() &lt;==&gt; x != 0  It's just not obvious, IMO, that the definition of time 0 should be globalized with respect to UTC. In the case of Python 2, you can't easily check the source since it's a built-in module. Instead you have to load up hg.python.org (unless you keep a local copy) and [check there](  static inttime_nonzero(PyDateTime_Time *self){    int offset;    int none;    if (TIME_GET_SECOND(self) || TIME_GET_MICROSECOND(self)) {        /* Since utcoffset is in whole minutes, nothing can         * alter the conclusion that this is nonzero.         */        return 1;    }    offset = 0;    if (HASTZINFO(self) &amp;&amp; self-&gt;tzinfo != Py_None) {        offset = call_utcoffset(self-&gt;tzinfo, Py_None, &amp;none);        if (offset == -1 &amp;&amp; PyErr_Occurred())            return -1;    }    return (TIME_GET_MINUTE(self) - offset + TIME_GET_HOUR(self)*60) != 0;}  It's pretty much the same as the Python code. It's just a tad more convenient in Python 3, since you can easily dump the source into the interpreter:  &gt;&gt;&gt; print(inspect.getsource(datetime.time))  TL;DR: It's a pain in my lazy ass to check the source. Write better docstrings! "
We were really in the business of employing people to do our previous jobs for less money.,"That's a good line, but not necessarily true.  One of my main tasks at my last job was to make our system configurable by non-programmers.  We had purchased a company who knew fuck all about web development, but was great at account management.  So we had 300 clients, each with their own directory, with their own PHP code and their own MySQL database.  It took 1 developer (me) full-time and another part-time just to maintain the system and fulfill client requests.  We put it all in a unified system with one codebase and one database, and made it so that the account managers could do most of the maintenance.  Then  we  could develop new features that the business wanted.  We ended up hiring more account people and more technical interns to do what the account guys still couldn't do as the business grew.  TL;DR: We were really in the business of employing people to do our previous jobs for less money. "
"Real life is gray; work is work, play at home.","Two things I've learned:   In college, you learn that there is a right answer and wrong answer. In real life, you learn that  nobody  knows the right answer and in many cases it's impossible to know. So you keep doing things in the mere hope that it will be right in the end.   You can try to base your happiness on your work, but it will probably be an exercise in frustration. To stay sane, I have succumbed to treating work as what I have to do to earn a living. I save the ""joy of engineering"" and experimenting, learning, and solving fun problems for personal projects (opensource or otherwise) at home. I play at home. I work at work. I studied computer science because I loved the topic, not because I felt it would get me a high paying job. Mixing pure CS passion/enthusiasm with the politics and bureaucracy of companies will just backfire.    TL:DR; Real life is gray; work is work, play at home. "
Any software developer who claims to have any insight into what the industry wants or needs is probably about almost everything.,"This tool seems like exactly the kind of guy who keeps good, capable people out of positions for which they are a good fit.  Anyone who is just starting out should probably ignore most of what this guy talks about.  I love my job and I can't see myself ever doing anything else.  But it's people like this who made it very hard to break into the field.  I despise those who think they have all of the answers about who is a good fit, what one should do to excel, or what makes someone a capable programmer in general.  Typically, their ideals match their own experiences, shockingly.  tl;dr Any software developer who claims to have any insight into what the industry wants or needs is probably about almost everything. "
"All aspects of your process are important to the final output, but it's not always easy to see that.","I'm not sure why this is getting downvoted.  He's making a couple good points:  1)  Software quality is an end-to-end thing.  You can't have a good interface and bad backend or your users will still be unhappy.  2)  If it's difficult to build your app, it will make your developers unhappy, and they are less likely to build it often.  That means less testing.  And one he missed:  3)  If you say it's 'good enough' about 1 aspect of the system, and don't let your programmers fix it, they're likely to let the rest of the system be at that level as well.  I worked at a company that had some code written to get them off the ground.  It was horrid, horrid code, and that is okay!  Launch code needs to get the business going.  But then you need to fix it.  They did eventually fix part of it, but another part was left to get worse and worse.  I'll even admit it:  I wrote the code that was rotting.  (And rotten.)  It badly needed a rewrite, and it was on the plans to eventually get done.  As a result, I was the only person who understood the code.  Others could make small changes, but large changes were such a nightmare that it always fell to me.  The code worked and was bug-free*, I was busy with other things, and it just never got fixed...  (* bug-free here means that no bugs had been reported in over a year.  It probably had some, but they had not been found.)  Eventually, they hired a new developer with an eye toward having him help fix what was wrong with our process.  Unexpected, I learned a TON from him about code cleanup and general sanity in programming.  Now that I understood the problems my code was causing, I desperately wanted to rewrite it...  But still the company wouldn't allow it, yet.  That new programmer eventually quit because the company wouldn't let him clean things up.  Just before I quit, they finally added a rewrite to the timeline.  I have no idea how that went, but without me, the only person who really understood it, they were basically tasked with writing it from scratch without detailed knowledge of how it worked.  I expect it was a bumpy road.  tl;dr - All aspects of your process are important to the final output, but it's not always easy to see that. "
"You agree with Dijkstra, you just don't know it, because language you learned bears only a superficial resemblance to the BASIC Dijkstra complained about.","All of that is cool, but it has nothing to do with Dijkstra's complaint. :)  This is what BASIC looked like in the dark ages:  Procedures and if/then statements were confined to single lines of text (which were numbered).  The only loop was the FOR loop. Pretty much everything else you did was via GOTO or GOSUB / RETURN.  The first successful high-level language (FORTRAN) was basically a set of macros for assembly language. The paradigm at the time was to program everything in assembly language. John Backus over at IBM had to fight tooth and nail just to get people to accept it.  Later, Backus lead the team to define ALGOL, which introduced ""begin..end"" notation and the idea of nested blocks laid out in the actual syntax. That was around 1958-1960.  Now, BASIC came along several years later, and completely ignored the nice structured elements that Algol brought to the table. It borrowed a few keywords, but mostly went the "" high level assembly language"" route.  So here's a language that pretty much starts you off with GOTO, and it's aimed at beginners (BASIC =  Beginners  All-Purpose Instruction Code).  So the original BASIC philosophy was to start people off by teaching them to do almost everything with GOTO.  Dijkstra, on the other hand, was a vocal advocate of structured programming. He was a professor (keep in mind that most computers back then belonged to corporations or universities), and his complaint was that students who had been exposed to unstructured programming first relied so heavily on GOTO that they had little interest in adopting the structured style.  After all, who needs a WHILE loop when you have IF and GOTO?  The structured programming idea caught on, but the Dijkstra of the 1960's probably wouldn't have been satisfied with the languages of today, even Java.  Dijkstra (and Niklaus Wirth) finally convinced the world about the harmfulness of GOTO, but he would have also argued for the removal of the the BREAK and CONTINUE statements from all loops, and restricting all procedures to a single RETURN statement.  Anyway, because of guys like Dijkstra, BASIC (and FORTRAN) got much much better, and that quote no longer makes much sense.  TLDR : You agree with Dijkstra, you just don't know it, because language you learned bears only a superficial resemblance to the BASIC Dijkstra complained about. "
"Not everything Apple produces is closed. And Apple is not dumb, what's already open is spectacularly difficult to close.","Steve is aware of market realities.  Consider the music and movie platforms on iTunes for example. Both started out as DRM-encumbered but only the music portion transitioned to an unencumbered format. The music genie was out of the box the day the first CD shipped, whereas movies at least had the pretense of DVD's DRM. There was no point in forcing a DRM model on media that was already freely available without DRM. At best, the iTunes music experience offered a solution that strived to be better than the alternatives i.e. buying a CD or copying MP3's online, perhaps withering them from market forces.  Therefore, I see the same scenario playing out with Mac OS X and iOS. Mac OS X inherently allows any sort of software to be installed from day one, but iOS only allows App Store apps to be installed from day one. The Mac App Store will compete against existing distribution channels but not replace them. At best, only withering them via market forces.  TL;DR: Not everything Apple produces is closed. And Apple is not dumb, what's already open is spectacularly difficult to close. "
"Don't worry about ""exploits"" requiring local access. It's not worth your energy.","> As of right now, you can access anyones pictures on their iPhone, as well as all their contacts. You can send emails, SMS, MMS, and place phone calls. Even if the device is locked.  That's if you have physical access to the device?  Here's how to get access to all that and  more  on  any smartphone with a hard drive :   Crack it open  Take the hard drive out  Use an appropriate adapter to connect it to a computer with enough free hard drive space  Use  dd .   Ways to prevent this:   Make sure every single app encrypts its data, or auto-encrypt the hard drive.   Fair question I don't know the answer to: does Android offer full-drive encryption? iOS? WP7?  tl;dr: Don't worry about ""exploits"" requiring local access. It's not worth your energy. "
these types of interview help but you still need to discuss the nuts and bolts computer science issues with the candidates.  This other link also makes a good point:,"I find this type of ""interview"" to be a good judge of how someone attacks a software task.  The problem i have with these types of interviews is that it doesn't test how well they understand some pretty key computer science principles.  ie, i find I still need to ask someone to analyze the Big O notation for a function.  This one metric is the only one I've found that correlates well to a candidate being successful.  Other problems that this types of interview doesn't do a good job of exposing are design skills.  Because the task is so small, by design, it's really hard to see how well a user can design systems.  I find that simple questions, like Steve Yegge's, design the object system for a car or parking garage work much better.  YMMV  TL/DR  - these types of interview help but you still need to discuss the nuts and bolts computer science issues with the candidates.  This other link also makes a good point: "
DVD queue management will only be possible through their website; instant watch catalog sucks compared to DVDs/Blurays. Netflix is lazy.,"This is really obnoxious.  As I posted on their blog, I love streaming and it's definitely the future, but right now their streaming catalog is ABYSMAL compared to the Bluray and DVD catalog.  The utter lack of pervasive subtitles is really annoying as well, considering that they already have all the data.  Most Anime dubs are just horrible.  Anyways I don't see why their website should be the only place to manage DVDs, especially considering that they keep demoting them relative to streaming.  Probably 75% of the movies in my queue were added from my phone;  I'll be talking with someone and they'll tell me I should watch so-and-so movie and instead of trying to remember it for the next time I'm sitting in from of my computer, I can just pull out my phone and add it right there.  Even if you are lucky enough to have an Android phone that is supported by the official Netflix app, it doesn't do DVD management, and their mobile website is pathetic.  TL; DRDVD queue management will only be possible through their website; instant watch catalog sucks compared to DVDs/Blurays. Netflix is lazy. "
"Yes, Python supports some basic functional programming, but it's nowhere near actual full-blown functional programming.","Sure, all that is kind of true. Still; python is not really good at functional programming - it has some basic features like first-class functions and lambdas (with severe limitations), and with a little twisting and hacking, you can even get something that resembles currying, partial application and higher-order functions. The language however lacks some other hallmark features of functional programming, most notably a concept of  purity . The language doesn't have any way of expressing this concept at all; the only thing that comes somewhat close is the idea of mutable vs. immutable types, but since Python is dynamically-typed, types bind to values, not variables, which basically removes any practical use of the concept other than performance considerations and some quirky non-intuitive behavior that occasionally bites inexperienced developers.  TL;DR: Yes, Python supports some basic functional programming, but it's nowhere near actual full-blown functional programming. "
"in modern computing, best story wins; being right is (unfortunately) optional.","I think evmar may be noting that your argument would have stood on its own merits even without the private email thread. Keeping it private might have been a nice courtesy.  Likewise, I tend to agree with your points and think you argued them well; however, I'm doubtful your post will win many people over (though I'd love to be wrong on this point!) for the simple reason that it doesn't present well.  Computer science has come a long way, but a surprisingly large amount of programming is still a matter of folk wisdom and home remedies. I don't see this changing greatly in the near future.  tl;dr: in modern computing, best story wins; being right is (unfortunately) optional. "
Either we really have a major misunderstanding or you are talking from an enormously theoretical POV.,"The same argument could be made for not having your own data center and a lot more. And the point of CDN is to make content available at edge locations globally - I don't think it's reasonable to expect that everyone has their websites running in 10s or 100s of locations on the globe. It's not only about bandwidth, it's about making first page loads faster (since asset requests are terrible) and possibly even be able to deliver certain pieces of content/pages without crossing oceans. I'm pretty sure big(ger) CDN providers all have pretty strict policies about not giving the data they collect to other parties.  Now, if you're talking about some fishy free or freemium CDN providers, then the same is true as for every other free or freemium offer: chances are it's not 100% secure and you're exposing your users to risks. But I fail to see how that's connected to CDNs in general. I saw servers almost die where I work after caching headers weren't sent correctly - and those were otherwise pretty beefy beasts. So I'm pretty sure CDNs are not just to ""save a bit of bandwidth"". Can you tell me any major/bigger website that doesn't use a (3rd party) CDN? Maybe Google has its own. But even Facebook is using Akamai.  TL;DR: Either we really have a major misunderstanding or you are talking from an enormously theoretical POV. "
"parent is great. think first, write code later, try new things. rinse and repeat.","in my opinion, this is the best comment in the thread. mod parent up.  for me, programming wasn't something i really felt like i ""got"" until it turned my brain inside-out about 10 times. learning to think of things as objects as opposed to just data or just code has turned out to be the most useful.  as far as learning this kind of stuff goes, the suggestion to read code is a great one. the suggestion to just doodle is great, too.  another approach that i have found effective is to force myself to view a problem in one particular way or another, especially when i already know a different approach that works. for example, if i were writing a sorting algorithm, i might ask myself ""if this were an object, what would it be?"" and just keep imagining things until what feels like a ""right"" answer becomes clear. a good test here would be to imagine how your sort would interact with other objects. in this case, how does it ""drive"" a sortable list, or how does a sortable list ""drive"" it? can your ""object"" sort be used in all the same places that a ""classically implemented"" sort can? when you feel like you've got it nailed down, write it and see if you got it right. expect it not to turn out. no worries! we all fail at everything until we don't. be  thrilled  if and when it does work. :) this is all about the learning, so the failures are almost more important anyway.  tl;dr: parent is great. think first, write code later, try new things. rinse and repeat. "
"Just because you are new and a student, does not mean you should allow yourself to get scammed.","The tool that codefocus provides is nifty and important.  It is important as it will give you a base of what you should be charging around. It is also important to help you when your run into the types of people that prey on students and the inexperienced with promises of future work(never happens), a link to your portfolio(not worth it) or they force you to a fixed price then give you a bunch of scope creep(not fun).  Don't get lured into the trap of those who are weasels. You can always put your questions/experience into real world examples.  e.g.If I am worked McDonalds would should I let someone order a combo meal, then upon completion of the decision and/or payment would I let someone add on chicken nuggets at no cost?  Tldr; Just because you are new and a student, does not mean you should allow yourself to get scammed. "
"The author is right for a lot of systems out there, but for some Mocking makes life way easier.","While I agree with the general premise - mocking is not a panacea for making testing easy - the point the author makes about system architecture is important. In a well-architected, high-cohesion, low-coupling system, mocking can be great for speeding up your tests and simplifying the test writing. In a terribly-architected, spaghetti code system, mocking is going to be a total pain. Of course, if you're having to change your interfaces a lot (or your interfaces are useless), the rewriting unit tests is going to be a pain whether or not you mocked anything.  I will say, the author having tried several mocking frameworks over a number of systems does lend credence to his argument. Perhaps the usefulness of mocking is limited to systems well-designed enough that the testing is more pro-forma than a requirement to have code that doesn't barf at the drop of a hat.  TL;DR - The author is right for a lot of systems out there, but for some Mocking makes life way easier. "
I think you underestimate how many crap programmers call themselves or are called (by their education  cough  or job) software engineers nowadays.,"I would think that engineers, % wise (good/total), were actually better. It doesn't take a lot to be called a software engineer these days. The word has been bastardized, so I do think OP has a point. Engineers, in the meaning of the word,  were  engineers, which is, on the total amount of people who call themselves engineers in software currently not true anymore generally.  Also, being able to write your software on paper, type it in and have it working the first (or among the first few times) time is a skill almost no-one has anymore. Which I would  call quite important still; as long as we don't have reactive or  live coding in practice, the codecompilerun is still a waste of time compared to reasoning about your code in your brain properly.  TL;DR I think you underestimate how many crap programmers call themselves or are called (by their education  cough  or job) software engineers nowadays. "
"Set Theory, while appropriate for many situations, is inappropriate for some problem spaces. Also, performance and ease of scalability is worth reducing features sometimes.","There are a few reasons:   RDBMS' are built around Set Theory (Well, technically  SQL  is built around Set Theory, which I'm sure you've noticed you can't work or think iteratively using SQL). This works great for  most  applications, hence their popularity. Some operations, however, cannot be expressed using set operations, which leads to:  NoSQL databases take many forms that are tailored to a specific problem set. For example,  neo4j  is a graph database that can store and query graphs far more efficiently than your typical RDBMS (case in point: dealing with arbitrary trees and hierarchies in SQL). Further examples include systems like Redis which is hyper-tailored and optimized for looking up data by simple keys (works great as a cache). Redis, being very tailored to it's problem space, is simpler to scale (it doesn't need to worry about much complexity) and faster  for it's very specific purposes .   tl;dr  Set Theory, while appropriate for many situations, is inappropriate for some problem spaces. Also, performance and ease of scalability is worth reducing features sometimes. "
"plumbers don't give correct estimations either and either way can only be compared to bug fixing, not new development","> That whole argument is like a plumber saying they can't give you an estimate to fix the water heater  Fixing a water heater is a bit more of a standard process, but even then there many problems with this statement.  First of all it's about fixing what it's most likely a known device and a typical type of defect, like a sealing being worn out. The estimate is based on that.  If you want to compare this with software, you have to compare it with fixing a bug, not with new development. You don't call your plumber to have him design and implement a new type of water heater that's 20% more efficient and works in a way that water heaters normally don't work (e.g. heating up very hot water even more).  But even with a software bug the chance of it not being a standard defect is much higher. Software does not wear out. Bugs in software are much more likely to be a unique kind of problem.  There is a small chance that the water heater has a non-standard defect. In that case the estimation of the plumber is also not correct. We had a heater installed a few years back and it had issues all the time (mostly it would simply stop working). It was a new heater of a well known brand, but plumber after could not fix the problem. It would work for a short while and then very soon after break down again.  It could also be that the heater is some ancient model of a weird and unknown brand, where parts are not readily available and the plumber has no idea what types of defects are typical for that particular model. In that case too the estimation of repairing it will be totally off. What happens a lot in these case; plumber says he comes to take a look at this will cost you 60 euros. He will then TRY to fix the issue, but if he can't he will tell you. Rather often this is EXCLUDING parts, so only after his investigation (which thus sets you back 60) once he knows which parts need to be replaced he will give you a more accurate estimation.  TL;DR plumbers don't give correct estimations either and either way can only be compared to bug fixing, not new development "
"Prestashop is popular but the code is horrible, Magento is complex and klunky but worth learning and Sylius is not ready for anything.","Honestly, all of them suck.  I have experience with Prestashop (1 year), Magento (6-8 months) and recently Sylius.  Prestashop  has a huge community who write modules. And all of them are utter, utter shit. Mind you, they do work, but they are written in a horrible, 2005era style of PHP where echoing HTML from the PHP class is OK.  The core of Prestashop is similar. They use Smarty for templates but they construct a lot of the markup in PHP with concatenations and whatnot. If you want to override a class you can, but you have to write in a similar fashion.  Magento  is also far from ideal. It's complex, heavy and over architected with a rather lack luster documentation. BUT: IMO it's still the best current solution. If you can understand OOP you can get up to speed with Magento in a month. What really helps is that the code is rather well written. It's also really flexible, at one point I completely overrode the way products are handled without touching any core files. It also has support for heavy caching which makes it usable on shared hosts. I used it only on shared hosts and never had a problem with speed.  Sylius  is far, far away from being ready. I tried to use some of their components separately (because in theory you can) and they wouldn't build. They talk about BDD constantly, then push up code that doesn't work. I tried fixing it at some point but decided to write a simple shopping cart for my Symfony app instead.  TL;DR : Prestashop is popular but the code is horrible, Magento is complex and klunky but worth learning and Sylius is not ready for anything. "
"Learn basic programming, core statistics, and data analysis techniques.  Learn the algorithm you need to do your job when you need it using the above skills.","I would like to throw an opposing course of action.  Learn core statistics.  If you dive into machine learning, support vectors, and blah blah too early you won't be able to properly route your data to the right analytic techniques.  Also learn sql and a scripting language really really well.  As far as methodology just google and read the wikipedia pages but approach them with an eye to understand the algorithm, rather than just implementation.  Agresti's book, an introduction to categorical data analysis, is exactly what you are looking for if you have the cash.  The older versions are just as good as the newer ones.  The big problem of diving right into a data science boot camp style training thing is that they teach you a little bit of everything when being able to understand a specific use case and route it to the correct algorithm is more important.  Data Science specific training can only make up for years of experience in every area by taking shortcuts to your core knowledge.  Its the statistical equivalent a script kiddie.  TL:DR Learn basic programming, core statistics, and data analysis techniques.  Learn the algorithm you need to do your job when you need it using the above skills. "
The bad guys are always out there.  They find you quicker than you'd think and they hit you hard.,"I set up an out-of-the-box ubuntu system a few weeks ago (using it as a nas to replace Windows Home Server, which was decent enough but I couldn't use it as a time machine for my apple computers).   I thought it would be nice to ssh in from work now and again so I opened the ssh port to the world in the router.   The next day I log in and am going through the logs to see what I need to tweak and fix and there are like 20k failed login attempts in auth.log.  All in the span of a few hours.  Fortunately there was only one login account (mine), root couldn't log in, and my password is VERY strong.   Still, when I got home that night I moved ssh to a different port (didn't make much difference) and set up public/private key logins only.   Still get a few attempts but nothing at all like the default setup.  TL;DR: The bad guys are always out there.  They find you quicker than you'd think and they hit you hard. "
"Nah, not really.  PE is more about mathy programming puzzles for programmers than a learning tool for beginners.  YMMV though, so ignore me, and try some out anyway.","I always loved project Euler, even though I've only done about 30 or so problems.  For me, I always thought of it as useful for people who want to learn some Math  via  programming-- but that's just the direction I'm approaching it from.  I love Math, even though I am pretty bad at it, and PE kind of makes it easy for me to develop some deeper understanding of Math concepts by viewing them in my natural habitat: deeply nestled within computer code.  To answer your question, it's pretty useful for learning the syntax and libraries offered by a  new  language, but probably not very good for your  first  language.  The reason is that many of the tools and techniques you'll need to make some of your programs actually finish in your lifetime (memoization, lazy evaluation) are not found in most beginner books and tutorials.  TL;DR: Nah, not really.  PE is more about mathy programming puzzles for programmers than a learning tool for beginners.  YMMV though, so ignore me, and try some out anyway. "
"25 / hour for web development, as a serious coder, would be severely selling yourself short.","Freelance or for a company? $25 is decent as an in-house developer, but releasing code publicly that you wrote during your 9-5 is a little more questionable legally. As a freelancer though?  $25 * 40 = $1k / week. You'd normally get about three weeks worth of vacation and sick days, so that's $49k / annually, then you subtract overhead, liability, lack of company health insurance and 401k and you're looking at a pretty low annual rate for all the extra responsibility you've taken on, not to mention that you have no job security as a freelancer, so you have to set aside some extra money for when work inevitably dries up sometimes.  tl;dr  $25 / hour for web development, as a serious coder, would be severely selling yourself short. "
"No, they actually found a way to reduce it to an even lower level than learning how Word, Excel and Photoshop work.","When I was in high school (2004-2008), there were two classes: Computer Math (really basic introduction to Java) and AP Computer Science (more of the same thing, but this time it counts for a college credit).  The year after I graduated, my school decided to replace AP Computer Science with a class called ""Dekstop Publishing"" which was advertised all over school with fliers featuring a photo of the teacher and the logo for PC Gamer Magazine (seriously, it wasn't even a fake magazine cover; it was just a photo of her and a logo).  And don't even get me started on that stupid typing elective they offered.  I never took it but I saw the textbook  couple of times and it was actually thicker than the Computer Science book.  tl,dr:  No, they actually found a way to reduce it to an even lower level than learning how Word, Excel and Photoshop work. "
"quantify and report.  If, after seeing the losses, management wants to continue ahead, you don't want to work there, as they are ONLY interested in short-term gains.","The answer is staring you in the face: the printed financials.  Never mind the ""hidden"" cost of outsourcing... there is plenty of evidence regarding the financial costs of outsourcing, only they are rarely quantified in a manner readily digestible by management.  By this, I mean:   if outsourced projects are indeed late, then put a number on this.  Make a projection of how much this extra time is costing, but put it in writing.   Look online for other evidence of similar budgetary crises related to outsourcing and failing to meet goals.    You have to be willing to speak in management-speak, which means projections and estimations: this is how all accounting is done.  Rarely do you speak of the money you actually have, but rather the  expected  value for a particular investment, and then use  that  value in calculations.  if indeed it is more costly to outsource, then this should show up in the figures, once the total cost of a project is taken into account.  By being disciplined about such accounting, the ""hidden"" costs can be quantified and shown to be excessive.  TL;DR : quantify and report.  If, after seeing the losses, management wants to continue ahead, you don't want to work there, as they are ONLY interested in short-term gains. "
"Just because it didn't happen, doesn't mean it shouldn't.","Yeah, it's kindof like how in the 1970s it was thought that strong AI was around the corner.  Disappointment with progress lead to the  AI winter .  Such research is much more glamorous now that there are billions to be made.  20 years is really a very short time, especially for something like a language.  Think about how long it takes to do a complicated product that you can sell - maybe 4, 5 years?  That means in the time since you were in university, people have only had a few iterations from which to learn such meta-level lessons as ""what language makes a project work out"".  With natural language, it takes an entire generation for a pidgin language to creolize into a full language.  Adopting an entirely new paradigm is like getting people to switch from Flemish to Chinese - the distance can be daunting.  This is the kind of fallacious thinking that surrounds PL - I've often heard ""If it was useful, then companies would use it"" - that is not sound reasoning, as companies make many imperfect decisions, and are highly influenced by zeitgeist.  tl;dr: Just because it didn't happen, doesn't mean it shouldn't. "
"No, it doesn't really matter except in a few obscure edge cases.","Yes and no. If you use  *.txt  without quotes, your shell will expand the argument into the equivalent full argument list (or at least that's what zsh does for me), whereas if you add the quotes, the argument gets passed directly to git without expansion, such that git expands the argument. Normally, it doesn't matter who expands the argument, but in a few cases, like if you are using  git rm  on a file that has already been deleted from the working directory, you do want git to expand the argument (as your shell can't properly expand it because it doesn't see those files).  TL;DR:  No, it doesn't really matter except in a few obscure edge cases. "
"new-workdir is nice, but don't let it get out of hand.  too many can be a bad thing.","while this is a VERY nice feature, i had the absolue, JOY /sarc, of working with my previous coworker.  its great, if you want to compare two branches or work on multiple branches simultaneously, making a similar change in multiple versions.  but my coworker decided they needed somewhere to the tune of 12-15 new-workdir.  one for EVERYBRANCH THEY WORKED ON.  i like to equate this to putting 12-15 desks in your cubicle.  at the end of the project when this person was released (contract work), i looked into the repo.  not a single commit, beyond my initial in all but 2 of them.  FFS.  TL;DR;  new-workdir is nice, but don't let it get out of hand.  too many can be a bad thing. "
"Ruby isn't dying, Rails gave it a jump start, it will grow on it's own going forward.","In my (probably biased) opinion, Ruby isn't dying. There was a ton of hype around Rails, and you now see the Rails influence in all modern frameworks. The language itself never really stood on it's own, and now I feel like it is finally separating from Rails. Yeah, it doesn't scale to Twitter levels, but what interpreted language does? It is great for programmer productivity, at a cost of performance. People should know this going in like they do with Python, etc.  Shoot, in terms of libraries, language design, and community, Ruby has it all. It's won't scale to 10 website levels. And that is ok for it's use. They usually have to make super specialized software anyways.  TL;DR: Ruby isn't dying, Rails gave it a jump start, it will grow on it's own going forward. "
Why does everyone want static typing? Except performance reasons.,"I don't really get why everyone wants (static) typing everywhere? You don't only see this in Python, also in other languages (e.g. Clojure). Maybe I missed something, but I fucking love dynamic typing (and duck typing, too).  So often I actually don't care what kind of collection I receive, whether I get a Int, BigInt, Float, or Double or whether it's this exact kind of object or the parent of it, as long as it supports my operations.And in those rare cases where I care, I either just write it into the docstring or do a simple type check, which is bad too, but not as bad as the alternative.  TL;DR: Why does everyone want static typing? Except performance reasons. "
You don't have to host your packages on packagist or a satis server.,"If accessing private repos is your concern, you can actually specify the private repo containing your package as a VCS. Composer will use that VCS to pull any packages that match from your require sections. You don't even need satis or anything like that.  {    ""repositories"": [        {            ""type"": ""vcs"",            ""url"": ""        }    ],    ""require"": {        ""monolog/monolog"": ""dev-bugfix""    }}  This is really useful for situations where you might want to fork a package and send a pull request. You can specify your vcs repo (fork) of the package in your composer.json, and without changing your require sections composer will pull the package from your repo rather than the original source. Then you just keep it that way until your pull request gets merged, remove the VCS definition in composer.json and voila.  EDIT: TL;DR - You don't have to host your packages on packagist or a satis server. "
"browser detection used to be taboo, nowadays, it's not that bad. Just make sure you're aware of and accept the downsides of using it.","Detecting browsers is useful if, like Google, you're only supporting recent versions of browsers. I no longer support IE9 or Safari 5 on my sites. I  could  use feature detection to warn my users, but considering that my sites are  guaranteed  to break in those browsers, why try to detect all 20 features when it's easier to just warn if their browser is old?  Usually, I get the response of ""what if they're using an obscure browser?"". In that case, browser detection doesn't work. But considering how few of my sites users will be using an obscure browser, and the chance of somebody using one already knowing the pitfalls of using it, I don't really care.  TL;DR: browser detection used to be taboo, nowadays, it's not that bad. Just make sure you're aware of and accept the downsides of using it. "
"It's a novel idea, but I don't believe there are sensors sensitive enough to do what he wants.","> While the dude that made the website and the company is probably old, it looks like he might be on to something.  The problem is that he want to use a single sensor and derive values for millions of regions (for each axis) from that.  If he wants the amazing resolution he is talking about, let's say he needs 256 levels (8 bits) for each pixel (though most CCD cameras today can provide 12 bits in RAW format).  A million regions amounts to 20 bits.  So he needs a sensor with an accurate resolution of 28 bits or more.  (I haven't thought about this very much, but it's possible that for a trillion pixels, the sensor may really need 48 bits of accurate resolution.)  Anyway, I don't know of any sensor that can accurately measure 28 bits of resolution in a thousandth of a second (actually he probably needs to measure that in 1 millionth of a thousandth of second -- i.e. a nanosecond).  I don't think he'll have very good luck with this.  tl;dr : It's a novel idea, but I don't believe there are sensors sensitive enough to do what he wants. "
"None of this maintenance is necessary.  Android provides you the  option  of tinkering and messing with settings, though.","> Apps to cleanup other apps  Wat?  > others to backup applications  Not a necessity.  Your contacts are backed up to your Google account, and most of your user data (photos, etc) are stored on your SD card.  To the best of my knowledge, this is far more than you get OOTB with iOS.  Then there's Titanium Backup if you're changing ROMs a lot, but if you don't like the maintenance, why are you customizing?  > some more to kill tasks (on my first phone, it got much better now), to fix battery discharge  These don't help, they make battery life worse for 99% of background tasks.  You have been taken in by FUD.  > That's not counting the crap you have to go through to keep a year-old phone up to date, thanks to the manufacturers' poor support  You don't  need  the new OS versions.  If you don't want the maintenance, stick with what the manufacturer releases.  tl;dr:  None of this maintenance is necessary.  Android provides you the  option  of tinkering and messing with settings, though. "
"basically I think celery has too many moving parts for the actual functionality I use 
 example of someone else avoiding celery for a map-reduce job","I use celery extensively in production but I still disagree. I think celery is way too complex, has too much of a dependency chain that breaks all the time on upgrades.  For all this pain you really don't get that much in my personal opinion, it can't effectively solve map-reduce type problems for many reasons not the least of which is data locality.  When something like  (which I haven't tried) comes along supporting the basic simple task functionality of celery (along with working well in single server setups), I can't see why anyone would stick with celery, its just too much of a pain to manage with too many options.  tldr: basically I think celery has too many moving parts for the actual functionality I use  example of someone else avoiding celery for a map-reduce job "
"Every time you modify any bit of mutable state, you need to consider  every  place that has access to that state to make sure you're not introducing a bug.","> How do they get out of control?  How do they get out of control?  If mutable state is shared, you can end up with ""spooky action at a distance"".  Shared mutable state is generally hard to reason about because of this, particularly in concurrent contexts.  For an example of ""spooky action at a distance"", consider a calendar with mutable dates in it.  Two consumers ask for January 1st.  One consumer modifies that date to be June 30th.  The other modifies it to be March 15th.  Now, the first consumer thinks he has June 30th but really has March 15th.  The Calendar thinks it has January 1st, but also really has March 15th.  A much better option here would have been immutable dates, i.e. dates where you can't modify which date they refer to (only change which date object you hold onto).  tldr:  Every time you modify any bit of mutable state, you need to consider  every  place that has access to that state to make sure you're not introducing a bug. "
models always abstract from reality. imo schemaless storage like json documents doesn't help with solving the real problem of finding the right abstraction level.,"Here's the problem I have with the article you linked and the original article: Nothing we ever model in a computer program reflects reality 100%. There are exceptions to every rule. The question we as software engineers have to answer is what part of reality matters for the goal we want to achieve and which parts don't. We then  abstract  from the chaotic reality and construct a model that fits our purpose and resembles reality in a ""close-enough"" fashion.If a key component of my application is name handling, then I will need to find a model that can deal with all the different rules and exceptions of names and I will need to find multiple different ways to store the different ways a name can be.  Using schemaless storage like JSON only hides the actual problem. I don't have to deal with the intricate details about storing all those names parts - sure - but I haven't solved any of the actual problems of dealing with the name parts in a business sense.If the names aren't that important, I can just as well chose a 'close-enough' storage method like 'title, firstname, middlename, lastname' or some such and be done with it. Real world exception to my abstracted model will have to be worked around somehow and my model will not reflect reality 100%, but I can live with that.  tl;dr: models always abstract from reality. imo schemaless storage like json documents doesn't help with solving the real problem of finding the right abstraction level. "
"too many companies rely on tech leads to protect employees from bad managers, instead of identifying and firing the bad managers.","Good tech leads insulate their developers from external pressure from  good management  when that pressure would have an adverse effect on the teams performance.  Bad management  on the other hand should be fired.  If you keep asking tech leads to insulate fellow developers from bad management then you'll just build resentment in your tech leads.  In terms of their interaction with management, I like to think of a tech lead as like a dam in a river.  The dam doesn't change the average flow rate in the river, it just insulates everyone downstream from the peaks and lulls in the flow.  If bad management is causing too much inflow then the water will gradually rise behind the dam and eventually it will break.  tl;dr; too many companies rely on tech leads to protect employees from bad managers, instead of identifying and firing the bad managers. "
"before you accuse someone of lying, try to understand what you're talking about first.","Do you still not believe me? I think you need to learn a bit about the  COM file format , a 16-bit computer with an 8-bit data bus.  First of all, there was no separate data segment. All code and data resided in the 64KB segment (actually, a bit less, but still 64,000  bytes  for the entire program). Where did I ""take the strings from""? I didn't have to take strings from anywhere -- I built them into the program in memory locations greater than where my own code resided.  > I suppose you hand-assembled it as well.  There was no hand-assembly needed. I was using the  debug . It also had the ability to save modified programs. If I recall, it also made it easy to add text into a memory location (i.e., you didn't have to do it byte-by-byte; rather, you could place a string all in one go).  tl;dr: before you accuse someone of lying, try to understand what you're talking about first. "
"Agile is a method from which Scrum is the process to implement it. The method is not flawed, the implementation is.","I am fairly new to this whole Agile methodology and in particular the Scrum process. For the past two months I have been attempting to read through pages and pages of ""opinions"" on how the Agile methodology should be used. Notice how I stated Agile = Methodology and Scrum = process. A lot of people think of the process rather than the methodology. The method itself is not flawed imho, the implementation of the method is what is flawed in most cases.  The Agile methodology is basically like the 10 commandments in Christianity; these are simply rules to live by. Scrum however is the process to apply this methodology and takes it own spin on the methodology and attempts to give you a base on how to execute it.  <--- shows how a spotify (1000+ organization) has many Srum teams where almost none or any are the same.  The main thing that needs to be understood from Scrum is that this process is only a base and that each of the teams/companies who utilize this process will apply it differently. The biggest part of Scrum that makes it agile (not Agile) is that during the retrospective the process the team is using can be adjusted to better fit the needs of the team. Therefor, no two teams should utilize the process in the same way.  Another part of Scrum that is the biggest failure point is that, as stated by others, the entire organization and outside world needs to buy in to your process. Even though the team could be implementing the process properly, the outside disturbances will throw off the process time and time again and it will be a constant battle.  TLDR; Agile is a method from which Scrum is the process to implement it. The method is not flawed, the implementation is. "
I suspect the author of this piece doesn't really know threads very well himself.,"That's because what he's describing is stupid.  First of all, bytecode means little relative to performance. As you pointed out JIT compilation and inlining can behave in surprising ways.  Second, the problem with synchronized methods is that they synchronize against ""this"", which means  all  synchronized methods on the class are serialized against each other. The real advantage to synchronized blocks is the flexibility you get, not the bytecode differences.  FWIW his description of ""volatile"" is seriously suspect as well.  Protip: Do not trust anyone who talks about the ""volatile"" keyword without using the words ""memory model"".  The real issue is that processors will re-order memory reads and writes for performance improvements and in multi-threaded code that can burn you.  So the JVM and CLR provide ""memory models"" that make some promises about when and how such re-ordering is permissible.  ""Volatile"" is a way to basically say ""Don't do that!"".  I am not very familiar with it in Java but in the CLR we typically don't want to use it because it enforces volatile semantics on  every  read and write which is usually not necessary.  TL-DR: I suspect the author of this piece doesn't really know threads very well himself. "
"Honing your ability of documentation to an unconcious state is a far more important variable in concentration and retention of ideas. 
 [Four stages of competence](","There's several problems with the approach people have given to this problem. We rate typing speed as slow, average, fast on a overall scale and place people in those categories. This doesn't work, as we're skipping the most important factor. It's typing ability that should be concerned, from which typing speed and other variables stem. Once you make your method of documentation second nature, be it pen on paper or typing on a keyboard, then there is no distraction from the act of documentating your ideas.  The way I see it is similar a  stack  onto the stack, evaluate them (think/refine the idea) and then unload them (write them down). Improving the method of documentation so it's more efficient and optimized simply lets you get through your ideas at a faster rate.  The big problem is that people see these gaps of time between the idea being recorded as skips in retention. If you could instantaneously record your ideas, you'd still need to mentally work on them to get them to the final product ready to be recorded. The work here is two-fold, not just thinking of and detailing the idea, but also thinking of how to put it into words and expressing it properly. If the act of documentation takes concentration away from your line of thought, then this will definitely conflict with your ability to define and refine your idea to the state you're aiming for.  tldr: Honing your ability of documentation to an unconcious state is a far more important variable in concentration and retention of ideas.  [Four stages of competence]( "
"foo()"", which seems to imply that everything beyond that is a waste of time/hot air.","Perhaps, but the generator would nevertheless need something to generate.  And this is not just dynamic dispatch this is also state/continuation passing which cannot be automatic because its not a generic problem... the second example shows writing three independent functions (one operating on apiOne, one on apiTwo and one on ""some combination of them"") to do this ordinarily one would have to nest callbacks (onSuccess...) and create highly coupled code.  The first example was to use quasi-continuations as a way of routing (the final example too) so that there can be ""stages"" to routing that all feed together.  Your ""point"" seemed to be that the title of the post was sufficient to understanding all of it and therefore ""tldr: $$foo()"", which seems to imply that everything beyond that is a waste of time/hot air. "
Print media naturally decays and shrinks; publicly exposed digital media naturally multiplies and is theoretically timeless.,"There's not a lot on the internet that isn't mirrored, rehosted, or archived somewhere.  It would be just as difficult to simultaneously find all of the copies of any media on the internet and then break through security and replace them with an edited version.  With print media, I'd argue it's actually easier to modify it since you could just start printing the new version and then print 10 or 100 times as many as the original unedited run.  Drowning out the original version with volume has been done, just look at the Bible for instance.  And simple losses over time of the original edition due to decay, misplacement, and destruction would add to the effect, as long as they keep printing the edited edition and blocking organized reprinting of the original edition it will be marginalized and may even eventually become extinct.  TL;DR: Print media naturally decays and shrinks; publicly exposed digital media naturally multiplies and is theoretically timeless. "
think through your problem before you write code. Ensure that you can see the end before you begin. Then don't be sad when your structure isn't perfect.,"It's a bit late, so pardon me if this makes no sense, but I'll try.  I've never been one much for paper and pen planning, if that's what you're asking. Obviously when working on a team planning your approach is crucial, but for personal projects I find the single most important (and often only) planning step is thinking through my project until can picture it entirely in head. I know how each part will work, I know how each concern will be separated.  At the end of the day, code is a bit like a fractal (if you imagine the deeper you into a stacktrace the farther you're zooming in on the fractal). You have objects, which have methods, which you call from other objects' methods, which you call from other objects' methods, and so on. Looking at any layer (i.e. any method), your code should be descriptive enough that you know what's going on in all layers below if not exactly  how  things are happening. For example, if I've already written and tested my reddit.getUserSubmissions method, then when I use that method, I don't particularly  care how it's implemented, I just know what it does.  Ideally your code ends up with both horizontal separation (I.e. your code to pull reddit posts is separate from your code to optimize your database) and vertical separation (your code to make HTTP requests is separate from your code to parse the returned JSON). If you imagine this as a grid you end up with beautiful, modular code which is perfectly organized.  This doesn't actually happen. It's just a dream, but this is the extent to which I try to think through my implementation before I start work. Once I think I have the perfect implementation planned then I start writing code and let reality blur some of the lines.  Tl;dr think through your problem before you write code. Ensure that you can see the end before you begin. Then don't be sad when your structure isn't perfect. "
dynamically generating keyframes gives some real power to mundane elements,"For the jQuery skeptics, I really want to make a non-jQuery version of a plugin I contribute to, but I just haven't got around to it yet. Here is the link:  Also check out the other plugins including support for [spritesheets]( and [more advanced paths]( like bezier curves.  On the jsfiddle, if you look inside the head element of the jsfiddle output frame, you will see an ungodly amount of style elements and css. The browser actually handles this like a boss, rendering many circles and rotations without a hitch.  Canvas is cool, but I love that I can do this without it.  TL;DR dynamically generating keyframes gives some real power to mundane elements "
"The phase distinction in strongly-typed languages is rigid, and this precludes some use cases benefitting from more gradual distinctions.","> As a simple example, accessing a record field (or method) in OCaml requires that the method name be specified explicitly - it can't be a function parameter. In python, a string can be used, so it can be  This is almost the canonical example: you simply cannot have  dynamic  type checking in OCaml, short of embedding an entire type-inferencing interpreter in your program (which is quite viable but not conducive to rapid prototyping). It's either static, or nonexistent ( e.g. , convert your record/object into a string-keyed (method) table). You'd say you're no better than in Python, and you'd be right.  tl;dr: The phase distinction in strongly-typed languages is rigid, and this precludes some use cases benefitting from more gradual distinctions. "
Think like a writer: bring clips of your best work to the interview.  Make sure they see them.,"You need to take control of the interview.  During my last job search I put together a folder with all the code I was proud of - overview, screenshots if applicable, key code excerpts, my role if it was a team, for Web apps that were still live, I included the links.  Then, after the psych questions and all the pre-programmed stuff, when they asked about code, I would open the folder, flip to a page, spin it around on the table and say I did this, and this this way, and for this one we had to do this because...  The interview would then become not just me telling them what I could do, but showing them how and when I had done it, what the challenges were and how I tackled them.  I didn´t have to remember anything - just point to it on the page and keep talking.  This was critical for me, because I´m older and I need to prove that 30 years of experience really does mean I´m worth more money than a new grad, and that I have kept up and still know the stuff the new kid knows.  tl;dr - Think like a writer: bring clips of your best work to the interview.  Make sure they see them. "
I never took a class on programming but I did read the entire manual.  It was worth it.,"I started at my company as an administrative assistant, aka, secretary.  I picked up phones and transferred calls, and occasionally made copies on the copy machine.  I had a lot of free time, so I learned basic automation scripts like AutoIT to make my job easier, then I downloaded a WAPP distribution and learned PHP from scratch, starting at ""Hello World"" all the way to installing custom PECL repositories, using memcached, querying databases, using SOAP and XML to get data from other places, etc.  I did all that without ever taking a class on PHP.  I just started at the beginning of the manual on php.net until I reached the end.  I then proceeded on to learning a lot C# the same way.  I've doubled my salary since then and now I do R&D for the company.  TLDR:  I never took a class on programming but I did read the entire manual.  It was worth it. "
The only asshole in the IT world that I can see from here is you.,"Here is a picture of me dealing with it:  this space intentionally left blank  You're assuming that OP made this hack because the program should have a future, or that the hack should be useful, or that the hack should generate revenue (money not necessarily involved). Those three assumptions don't need to actually apply here. Maybe OP wrote this for fun, but we'll never know, since OP deleted his post, because you're...  > ...not here to make anyone feel better. [You're] the voice that says what other people...  ...  who may not be assholes , are too polite to say.  TL;DR: The only asshole in the IT world that I can see from here is you. "
"When you put your mind on ""OO-ey mode"" and filter the inconsistencies, Java APIs start making sense.  shudder","> What happens if you put ENTRY_CREATE more than once in the varargs list?  Indeed, without further detail, a possible interpretation would be ""put N ENTRY_CREATE watches on this path"". I'm pretty sure they stick to the sane interpretation, but it may be confusing.  > Why is the ""WatchService"" a method of the standard ""FileSystems""?  Actually, when you read:  FileSystems.getDefault().newWatchService();  ""Create a watchservice for the default FileSystem"", it makes more sense, doesn't it?  > A semi-sane API would be this: (...)  Sort of, but it'd make more sense on a Python api. Java's standard library has this thing for trying to be OO-ey and somewhat foolprof (that's the reason for using Enums instead of a single ""flags"" parameter), but sometimes it contradicts itself, as in using varargs instead of an EnumSet for the flags.  tl;dr: When you put your mind on ""OO-ey mode"" and filter the inconsistencies, Java APIs start making sense.  shudder "
"It's irresponsible to give salary advice without knowing a lot more information than we presently have. 
 I agree completely.",">Let's not get carried away here.  I'll answer your questions/points. No worries.  >I've seen elsewhere in the thread you've 2 years post-graduation experience and are on sub-25 (perhaps even sub-20) GBP, which would lie somewhere between ""acceptable"" and ""mildly lowballing"" depending on 1) your ability, 2) your firm's size, 3) your firm's actual purpose (i.e. if you're at Accenture that's really bad, but if you're the IT dept of a non-IT firm who just needs to keep BAU ticking over, less bad).  1) I'm more than willing to accept it if someone said that my abilities where lacking. I don't think they are, but I try to be... for want of a better word 'humble' like that. I'm fully aware that that will make me sound quite pretentious.  2) Multi national. 5 offices in the UK, and several in other countries.  3) International regulation testing and compliance. I've simultaneously worked as a software engineer on BIG projects, and as a test engineer for several commercial products.  >It also depends where you are in the country. For most of the civilised areas of the country 25'd skew closer to ""acceptable"", for backwater villages in the middle of nowhere it'd even be ""pretty good""; for London, ""dire"".  In a relatively large city near Leeds and Sheffield.  >And, what language(s) you're working in. As you're a recent grad let's assume Java - Java devs have been churned out of universities here for over ten years. There's a lot of you about. That skews things south too.  Languages: embedded C, C++, C#, javascript and SQL.  Testing requires that I run Debian based distributions to get low level data for interpretation and logging.  >TL;DR It's irresponsible to give salary advice without knowing a lot more information than we presently have.  I agree completely. "
It's irresponsible to give salary advice without knowing a lot more information than we presently have.,"Let's not get carried away here.  I've seen elsewhere in the thread you've 2 years post-graduation experience and are on sub-25 (perhaps even sub-20) GBP, which would lie somewhere between ""acceptable"" and ""mildly lowballing"" depending on 1) your ability, 2) your firm's size, 3) your firm's actual purpose (i.e. if you're at Accenture that's really bad, but if you're the IT dept of a non-IT firm who just needs to keep BAU ticking over, less bad).  It also depends where you are in the country. For most of the civilised areas of the country 25'd skew closer to ""acceptable"", for backwater villages in the middle of nowhere it'd even be ""pretty good""; for London, ""dire"".  And, what language(s) you're working in. As you're a recent grad let's assume Java - Java devs have been churned out of universities here for over ten years. There's a lot of you about. That skews things south too.  TL;DR It's irresponsible to give salary advice without knowing a lot more information than we presently have. "
MDN is a great reference and could become a great instructional reference by moving their examples to the top of the page.,"Because W3Schools is much, much easier to use, especially for newbs.  I teach a couple ""this-is-the-web"" courses. For years I specifically forbade W3Schools, but the students would use it anyway.  Why? Take a look at the [w3s page about  &lt;ul&gt; ]( vs the [MDN page about  &lt;ul&gt; ]( This is how a newbie sees it:  The MDN page:   It starts off with a long text description. I just want to know how to make a list and I don't see any HTML there, so I'm not going to read that.   Next there's a table called ""Usage Context"". Some stuff in the table makes sense (permitted content), some of it we talked about but I'm not sure I understand very well (""Flow content""), and some of it is Greek (""Normative Document""?). I'm going to ignore it anyway, because there still is no HTML.  Now we get to the attributes. I know that attributes are important, but I'm more comfortable seeing them in an HTML example than arbitrarily listed out. Unfortunately, they are arbitrarily listed out here.  There are some words I'm not comfortable with yet (""Boolean"") and no examples of what the attributes do. To make matters worse, I read about the attribute  then  MDN tells me I shouldn't use it.   Next is ""DOMInterface"". I just want to make a Web page, so WTF is ""DOMInterface""?  I give up! Which is too bad, because I won't see the next part, which is...   ...some pretty good examples     W3Schools, on the other hand:   Here's some HTML with a  &lt;ul&gt;  in it. I'll copy that!  Who cares about the rest!   TL;DR: MDN is a great reference and could become a great instructional reference by moving their examples to the top of the page. "
"Right Way: 
 
 Idea (optional) 
 Plan 
 Capital 
 Business 
 
 Craigslist Way: 
 
 Idea 
 ?? 
 Profit!","Here's what people don't seem to get about entrepeneurship, and why business ideas are essentially worthless:  A good idea is not essential to a good business.  You can start a successful car wash, CPA office, web design company, or whatever without a whiz-bang new idea.  You  can't  have a successful business without a good business plan.  This is pretty much what SeanLazer is saying, in so many words.  A great idea might be nice, but without a business plan, it's a dead end, and investors know that.  If you  do  have a solid business plan, investors can see that, too, and you should be able to drum up some venture capital or take out a loan to make it happen.  Either way, jumping straight from idea to taking on employees is The Wrong Way To Do It.  tl;dr:Right Way:   Idea (optional)  Plan  Capital  Business   Craigslist Way:   Idea  ??  Profit!  "
"Great for tasks where you have no choice but to use a GUI, but for most it would be better with an API or text file.","First thought, it looks really good and is really cool. I like that finally someone has come up with a sane solution to handle GUI;s and macros.  But... I can't stop thinking about how slow it is and you are still forced to watch it go through all the steps since you would need to use the same window manager if you wanted to work while the script was running. Once again I find having the text file option for configurations to be superior.  This said, it will still be terrific for testing GUI;s and for automating programs that does not provide you with an API or text file to control their behaviour.  tl;dr  Great for tasks where you have no choice but to use a GUI, but for most it would be better with an API or text file. "
I want a REPL exactly because it is  very  useful for writing large applications.,"> I'm interested in writing applications, not one-liners  So am I.  But your rant was about not wanting a REPL as the first point of contact with a new language. I pointed out to you why a REPL is exactly the first point of contact that some people want in a new language.  Then I went on to explain how a REPL is something that some people want, not only as a first point of contact, but also while  developing  applications. I then hinted that even for the  deployment  for some applications REPLs are useful.  Many people who are interested in  developing  applications are very much interested in REPLs. If you only care about applications as things you launch, then you don't care about what language they are written in and therefore you don't care about REPLs. So it is the needs of a  developer  of applications that I am addressing in insisting that REPLs are exactly what is wanted throughout the development process in general, and when first evaluating a new language in particular.  While you claim that your needs arise from being a developer, to me they seem more in line with the needs of an end-user. A developer is interested in a smooth development process, among other considerations; and end-user doesn't give a hoot about development and only cares that deployment is painless: you seem to care lots about deployment but not so much about ease of development.  Maybe this is too subtle a point for you; maybe you are being deliberately obtuse for comic effect: I can't tell in this medium.  In any case, my original point stands: I consider your advice to language creators, ""Don't give me a REPL"", to be, no offense, bloody awful advice.  Put another way, no offense, I am interested in writing applications, not the hello-world that caused you so much distress, and experience tells me that in developing applications on a grander scale than hello-world, a REPL is going te be damn useful to me.  > And bash is a better REPL than the REPLs that come with most languages.  From this I am tempted to conclude that you haven't seen many decent REPLs, which would explain why you fail to comprehend their use as a key  development  tool. (Though really, we shouldn't be talking about REPLs but incremental interactive development tools in general.)  TLDR: I want a REPL exactly because it is  very  useful for writing large applications. "
"scripts automatically looking through PDFs to find certain words will miss a lot of things, like sometimes ligatures etc.",">the PDF will preserve your text  As someone who wrote PDF parsers for ebook readers, no. I wish it did.  Adobe introduced a ToUnicode table where you look up which glyph is which unicode character, so it theoretically could work just fine, yes...  Many documents still get it wrong for ligatures like ffi, ffl, so just looking at the decoded text of PDFs that actually say ""ffi"" each, you won't match ""ffi"" in  some  PDF documents, but match ""ffi"" just fine for others :-(  (Extra weird if there are multiple different tables saying contradictory things... )  In practise it only mostly works and that could cost you your interview.  Edit: TL;DR: scripts automatically looking through PDFs to find certain words will miss a lot of things, like sometimes ligatures etc. "
"There's entertainment and career value up the ass in NYC. Yes you have to endure the monthly rent fistings, but you don't have to live there forever.","Something that the article doesn't go into great detail on is the value of living in a certain city. I live in NYC, and yeah I get fisted every month on rent, but I also get to do some badass things. Being a young single guy, you get to meet tons of chicks, see tons of events, and just generally have a huge variety of things to do. I make enough money as a new dev to go out, save a little money, invest in stocks, and generally not fret over finances.  Alternatively, I could move to Minneapolis (where our other office is located), pay half rent to live in a bigger apartment and keep the same salary. But Minneapolis (or St. Louis I would bet) is fucking boring compared to NYC. Not only that, but my career development opportunities are WAY greater here than the Midwest (Google, LinkedIn, Facebook, etc. are all here or in San Francisco. Minnesota? Nope). Once I want to start a family and my priorities change, I'll likely move somewhere with a lower cost of living.  tl;dr  There's entertainment and career value up the ass in NYC. Yes you have to endure the monthly rent fistings, but you don't have to live there forever. "
"He seems like a really nice guy, just not very sociable. College probably wouldn't have been much different for him if he had waited.","I think for him it was best. I saw him in an origami documentary recently. He's not your average college student. No disrespect to him, but (at least from what I saw) he's a very stereotypical nerd. (From what I remember from the documentary) he spent all his time on the computer solving problems and programming. I don't think his life would have been much different/better if he had gone to college at 18. It probably would be the difference between spending all of college on the computer at 12 and spending all of college on the computer at 18.  If anyone actually knows the guy feel free to correct me, but that's what I gathered from the interviews he was in.  tl;dr  He seems like a really nice guy, just not very sociable. College probably wouldn't have been much different for him if he had waited. "
"not just hackers who re-flash their devices. 
 They are the only people that bought the N900 anyway.","I have an n900. I've had it since February as my primary phone.  I love it.  I  wouldn't  recommend it to any of my non-hardcore-geek friends.  It gets around many of these issues but also has many of its own.  Why should someone have to download and configure two applications on a modern flagship device to  MMS ? I did, and it worked (mostly). Even thinking of the N900, gigantosaurus makes some good points.  > Even if they make good hardware, they will never have a great ecosystem for 3rd party app developers this way  > They have to make sure that this openness actually reaches the end-users, not just hackers who re-flash their devices.  The N900 was not a device designed for the average Nokia (or even smartphone) user.  Sure, it's all open.  Sure, it can do wonderful things. Sure, it can be wonderful to develop for.  It still could not do what it should have out of the box.  I've been a Nokia fan for YEARS.  I've been through their dumbphones, Symbian devices, other tablets, and I'm finally rested with the N900. Something I have learned from this is that I can do amazing things with all of their products, but I shouldn't be shocked when I have to jump through hoops for it.  This turns away the mass appeal of the device.  Developers looking for a platform to distribute find a smaller audience.  Companies will not want to invest money in it. I was shocked that even Nokia was so far behind with its own Ovi suite.  Take this along with the fact that it was not sold subsidized by carriers (at least in the US, don't know about the rest) and you find a good reason Maemo didn't change the world of mobile phones.  Sure, Android has plenty of issues, but look at the amount of Nexus One users vs Droid/MyTouch 3G/Galaxy S users.  People don't mind the carrier crap if it means a $200+ price break. If the N900 had been subsidized, whichever of the Big Four adopted it would force Nokia to on-load a bunch of their crapware to reach a price both could sadly agree on.  The E71x was a great example.  Bought without a carrier lock, this phone was the best business class smartphone I had seen.  Sold through AT&T, I had upset customers.  I'm getting Way off track here. In closing:  tl;dr  > not just hackers who re-flash their devices.  They are the only people that bought the N900 anyway. "
having a preference for ruby style syntax really has nothing to do with making anyone a better javascript programmer.,"coffeescript does not help me write javascript.  Coffeescript is for people who prefer ruby/python syntax and who just never got the C style syntax of javascript. It has less to do with 'helping people' write javascript and more to do with making ruby programmers comfortable with their syntax preference. because it compiles to javascript does not in any way make it appropriate to post in r/javascript.  It is like saying if someone tacked-on a perl syntax to javascript and then started posting to r/javascript about how beautiful their 'made up language' is compared to ugly javascript. same thing with coffeescript, and it is quite annoying to people interested in reading r/javascript for javascript.  tl;dr: having a preference for ruby style syntax really has nothing to do with making anyone a better javascript programmer. "
identify problems so that the whole team can learn. Identifying the person just starts up the blame game and wastes everyone's time and energy.,"Writing good software is about continuous improvement.  Everyone  can improve, from the intern you hired yesterday to the guy with the gnarly beard that wrote embedded code for satellites.  The key to improvement is healthy and positive discussions about defects. If something went wrong, how do we fix it? How do we avoid making the same mistake in the past.  A bug report is the place to identify the symptoms of the problem. Although your investigation should attempt to isolate the root cause of the issue, it doesn't help anyone to lay the blame on a person. Identifying the faulty section of code, configuration values or database structure allows the team to make improvements. Explicitly identifying the person who made these decisions (which may have been perfectly valid under the circumstances and known use-cases at the time) only puts negative pressure on the person.  TL;DR  - identify problems so that the whole team can learn. Identifying the person just starts up the blame game and wastes everyone's time and energy. "
Big projects have more bureaucracy and bureaucracy at work kills the attempt at agile development.,"At work, there is 8 million lines of COM C++ and 8 million lines of VB6 code. In the last year or two, each team has been adopting ""scrum."" I'm using quotation marks since the team I am on has mostly embedded a lot of the waterfall process into scrum. Other teams in the company may be more agile than ours.  I think a big part of the issue is there is a fear of risk. We don't want to have wasted even a one month sprint so we want to still have stuff modeled up and documented heavily before we start. Partly this is enforced by schema freezes and code complete dates so there is a rush to get stuff done in time and it has to be right. Essentially, one or two people spend a sprint working up documentation on various approaches to the problem.  Currently, the team has not started several agile processes like writing unit tests before developing new features. The team is documenting stuff before coding (instead of after) so by the time the coding is done three sprints later, the development documentation has drifted out of date.  Call it scrum and agile if you will. It just sounds like I get two days of meetings every month and stand up meetings daily on top of the waterfall process.  TL;DR: Big projects have more bureaucracy and bureaucracy at work kills the attempt at agile development. "
burnout nearly gave me a breakdown. Put my mental health first and recommend it to everyone. Better now!,"Good suggestions!  As a freelancer who used to work from home I got caught in the burnout trap. I went days without leaving the house, without showering and on some occasions forgetting to eat.  My engineering/tech brain loved it. Until things started to get too hard. I pushed myself too far and nearly broke my head. I overloaded my brain so much that even normal conversations were hard, I couldn't code and function. Even heading out to the shops was hard. The advertisements, signage, etc on the way to the shops (and in the shops) all demanded my attention - shouting at me. My already overloaded brain couldn't cram anymore information in.  I felt very close to having a breakdown, something I would not want to wish on anyone.  Now, my life is different. I have down time and look after my mental health (it's my top priority now). If work is getting in the way of my mental well-being I take time off. Sometimes I allow myself to work half days, sleeping in a few extra hours. It works a treat and oddly enough when I get to work I'm hungry to keep going.  TL;DR burnout nearly gave me a breakdown. Put my mental health first and recommend it to everyone. Better now! "
Does everything TextMate does and more. It's easier to extend and navigate.,"SublimeText supports TextMate tmbundles and tmthemes out of the box. I was a devoted TextMate user for years and I completely converted to SublimeText after using it for a day.  I'd almost liken it to a TextMate-QuickSilver hybrid. Pressing ⌘⇪P brings up a nice auto-completing command search. Need a snippet and can't remember the shortcut? ⌘⇪P start typing ""snippet"" there you go, the snippets for your current language.  The development is much more active (there is already a build available supporting the new retina MacBook). It's cross-platform so the community building extensions is much larger. Tack on [Package Control]( and installing extensions is a breeze.  I've only scratched the surface but I highly recommend you check it out. The icon  is  pretty ugly, but I found [this nice replacement](  TL;DR Does everything TextMate does and more. It's easier to extend and navigate. "
_SESSION is best served as an immutable structure containing common initialization and identification data for the user.,"If action maps to some kind of URL or query string parameter, it can be considered a detail of the view, something apparent, open and manipulable by the user. If you allow the user to have control of something that is inserted into $_SESSION, which many developers tend to consider ""safe"" you might be opening up a possible security vulnerability for yourself or your successor(s).  In addition $_SESSION usually comes from the file system, or a database if you are using some custom wrappers which many do. I/O Is always expensive especially from the file system. Many (including myself) consider good design to keep your session structures immutable. This allows in load balanced architectures with many web servers to have session data cached via local memcache, APC or other caching systems; reducing the load on your session servers and cluster.  For example, imagine you have a Users table containing all the details of your sites users, which has a one to many relationship to Accounts; both of which you use every request for application logic. It would not be a bad design to have a session structure like:  $_SESSION = Array(    'user' =&gt; Array('id' =&gt; 750, 'name' =&gt; ...),    'accounts' =&gt; Array(        Array('id' =&gt; 1500, ...),        Array('id' =&gt; 1525, ...)        ...    ));  tldr;  $_SESSION is best served as an immutable structure containing common initialization and identification data for the user. "
"shoulda used a real parser, and used parse tree context data to know what to change and what to leave alone.","This seems like a compilation of strange decisions to minify/obfuscate javascript, and I'm not just saying that because it uses php.  The first problem is the parser. You can't just grep for identifiers, and using regexps to solve the problem just gives you two problems. You want a parser that knows about javascript syntax and grammar and produces a parse tree. There are a bunch of open source ones out there that are generally expected to be correct, and will provide a good foundation for transforming JS code in a non-destructive fashion. Unfortunately, there are also bad open source parsers out there, and the author happened to pick one of those, which is [thusly described]( by its maintainer:  > This project is unmaintained. I stopped using it years ago. You shouldn't use it. You shouldn't use any version of JSMin. There are much better tools available now.  The other issue mentioned is knowing which identifier to minify and which to leave alone. If he had a proper parser, he would know what scope an identifier is in, which would him allow to know which ones can be safely minified (those used only in local scopes), and which are trickier because they're attached to the global scope. Normal minifiers usually just quit there, but an opinionated obfuscator could catalog and recognize known global identifiers and obfuscate the rest.  tl;dr: shoulda used a real parser, and used parse tree context data to know what to change and what to leave alone. "
"I love Python, but sometimes I tear my hair out over issues that have nothing to do with the language, per se.","Heh.. this is for an introductory course in programming that is light on formal CS and heavy on developing problem solving skills and a appreciation for figuring shit out on your own. Kids from across the academic spectrum take it and enjoy it and many go on to study CS in college.  My comment was mostly inspired by A) the difficulty of setting up interesting development environments for Python and B) the easy ""showability"" of Javascript apps.  Setting up lab computers for teaching Python 3 with Pygame was a bit of a stunt this year, and I'm still not done with it, even though we're a week into the new year. But that's another story.. I am actually quite happy with Python. And Python 3 is a great improvement over 2, if I can get all the pieces in place (not being given admin rights to the lab computers is frustrating).  I think I was struck by the relative effortlessness of developing in Javascript (I had some students last year doing special projects with Javascript). You need an editor and a web browser. Kids can do it anywhere and immediately see applications in web development. With Python, on the other hand, the code they write is relatively brittle and dependent on the environment they have set up. It is fundamentally hard to write a program and e-mail it to someone outside of class (without using py2app or the like).  TL;DR: I love Python, but sometimes I tear my hair out over issues that have nothing to do with the language, per se. "
"don't worry about it, just go for it and see what happens.","I doubt you can prepare. You know what you know and that will fly or it will not.  You won't know until afterwards.  I've worked with recruiters exclusively since 2008 (and they've worked fantastic for me) and I can't tell you how many different recruiters/firms contact me.  I've  never  been asked by  them  (recruiters) for a technical screen - they're recruiters, not technical.  Most don't even know the difference between java and javascript . They get your resume and do the buzzword matching bullshit (sometimes very poorly).  Technical screens for me have always come via the company the recruiters are trying to place me at...Phone screens, then in-person, the typical bullshit that tells them little about me.  Also, keep in mind so much has changed in the JS/front-end ecosystem over just the past couple of years.  So even if you supposedly get a ""technical"" (former dev/engineer) recruiter ""evaluating your skill"", they may be asking you questions which aren't really relevant - at least for modern dev.  TL/DR, don't worry about it, just go for it and see what happens. "
just use the post's link! That is almost always going to be what you want!,"Cool!  You can also link in blobs like this:  /* blobtest.c */#include &lt;stdio.h&gt;extern const char _binary_blob_start[]; /* 'blob' comes from the filename */int main(void){    printf(""%s\n"", _binary_blob_start);    return 0;}/* end blobtest.c */$ echo -ne ""Hello world\0"" &gt; blob    # some test data$ objcopy -I binary -O elf64-x86-64 -B i386 --rename-section .data=.rodata blob blob.o # change this for your arch :S$ gcc -o blobtest blobtest.c blob.o$ ./blobtestHello world  which works well for bigger files, but it's a pain in the ass. Incbin should work pretty fast compared to the  xxd -i  approach though.  tl;dr just use the post's link! That is almost always going to be what you want! "
"do what you like with the core, but please start supporting us extension creators.","I wish there was at least a flag on wordpress.org where plugins could declare their minimum PHP version. If they want to keep core at 5.2 then that is one thing - not ideal, but liveable. However, put a PHP5.4 plugin on wordpress.org and call it ""My Plugin (PHP5.4 only)"" and put ""PHP5.4 ONLY"" in the comments, and FAQs and description, and changelog, and you  still  end up with a stream of PHP5.2 install tickets coming in. We need a way to start separating off the PHP5.3+ plugins and theme so they don't get mixed in with the older ones.  That  is one move that I believe will help move things forward, without stranding anyone. It will encourage people to upgrade too, as they will have lots of visibility of what is new and cool, but will not be able to install without upgrading PHP or their hosting first.  tl;dr: do what you like with the core, but please start supporting us extension creators. "
"Flask = Good for prototyping, go with it and if you start seeing issues evaluate if there may be a better fit.","I think for prototyping a basic game that can work with rest calls, Flask would be a fine choice. It's worth noting if you need real time updates, there is socket.io support for up to 0.9 (but the lib is beyond 1.0 and I've not seen updates on an eta for python servers - due to wsgi iirc).  In my experience I wrote the same site in both Node.js (using express) and Python (Flask) and the node version was much more performant, but my needs are fairly high I/O and real time data streaming (websockets) - so the use case is fairly different. That said, I tend to prefer python for most other tasks, however.  Tldr; Flask = Good for prototyping, go with it and if you start seeing issues evaluate if there may be a better fit. "
"It's a shitty fucking time to be in the industry, and it's a really shitty time to be trying to get in.","> Electronic Arts where you have the dual issues of reputation and crunch time that play against each other.  Crunch is still bad, but it's gotten better. Aside from that, EA tries very hard to take care of its developers. Benefits and work environment are excellent. Pay is OK, but game dev jobs typically pay a step less than other industries from what I've heard.  Because there's little education (at least historically) that could effectively prepare you for game development, the industry relies  very  heavily on experience to determine who to hire. This, along with the millions of other applicants, makes it notoriously hard to get in. Once you're in, it's much easier to stay in (again, from what I hear).  The real trouble you're having with the game industry right now is likely because the entire industry is contracting. Most big public game companies are way down over where they were a couple of years ago. (ERTS is $18 right now. Two years ago, it was $54. My stock is worth  half  of what it was when I got hired  eight years ago .)  When the industry contracts, the total number of jobs shrinks. EA is in the process of laying off 1,500 people, about 20% of its staff. Other game companies are doing similar. This means the bar to get in gets much higher: there are already plenty of unemployed experienced developers to choose from.  tl;dr: It's a shitty fucking time to be in the industry, and it's a really shitty time to be trying to get in. "
"A programmer is only as good as the tool's at his/her disposal. If you have to beat in a nail, dont throw the hammer away for a monkey wrench.","While that is entirely true I have seen programmers with 'years' of experience write a ton of code could be reimplemented with 1/10th of its original size (and a great deal more clarity) with the right API calls. As I said each of these has its place  Example #1: ""Webby"" application that might be expected to scale. have a potential huge input string, though in that case I dont know why the stringbuilder is not passed 'in' to the function.  Example #2: reads more like C code, Probbably 're-inventing the wheel' with the implementation, taking up a lot of screen real-estate, & having a increased complexity inside the function. . Though I do really like its use of ram, on a system where ram is precious then this would definitely be the way to go.  Example #3: Smallest Example here, Uses provided API, least points of failure, if library is well implemented then call to Array.reverse could be quite well be fobbed off to native code (see: java's System.arraycopy() );  Example #4: object creation on this one is hideous, would not like to see this code run a million times in some kind of large batch processing look, the GC would be going nuts (though I have to admit I am not a .NET programmer therefore I have no knowledge of the inner workings of its GC)... In addition large strings would kill it. Though it feels like it would be a good solution for a embedded system.  tl;dr : A programmer is only as good as the tool's at his/her disposal. If you have to beat in a nail, dont throw the hammer away for a monkey wrench. "
"stop being a ""Java Developer"" and become a "" Developer ""","Every time these things come out, the title gives me an instant feeling of doubt.  It's great that you're sharing your opinion. But, when you label yourself as a "" x-language  Developer"", I instantly lose confidence.  I am a developer. And, I also use Java (10+ years now, I like it). I also use other languages. But, Java is just a tool. I use it when appropriate. I'll use something else when that is the best tool for the job (ie... I install nails with a hammer, screws with a screw-driver and cut wood with a saw.)  Don't take offense but, if you are a "" Java Developer "" (one trick pony) instead of a  developer , you're really just a well paid code monkey.  Step away from being stuck to any particular language. Think about problems with an engineer's brain. Apply the best tool for the given problem.  Of course, when you add another language to your toolbox, learn it inside and out. Learn it's strengths and it's weaknesses. Is it better for a one man team or a big team? Is it better on servers, desktop clients, phones, or embedded systems. Every language is useful in at least one niche (it's why it was created after all).  TLDR; stop being a ""Java Developer"" and become a "" Developer "" "
we are currently understaffed and aren't doing too well in the revenue department,I get paid a salary based on a 40 hour work week and there is no overtime or anything of that sort. I telecommute when the office is closed on nights and weekends which makes it not quite as bad (at least I am not having to stay in a cubicle for all those hours).  It is my understanding that as soon as we can afford it they are going to hire me some help so that I can reclaim my nights and weekends. The owner has personally talked to me and has expressed his concern over me burning out.  TL;DR - we are currently understaffed and aren't doing too well in the revenue department 
"Got overwhelmed by my first project, too much to handle for a newcomer. Owned second project and proceeded to grow in the industry.","I felt it at my first job.I had no skills in the industry whatsoever, and started on a huge project running for quite some time. They tried to explain to me the parts I had to work with, but the code was so messed up that I couldn't do much progress. This along with my 0 experience, caused my anxiety to skyrocket, I started to feel I was not good enough for the job, even for the profession, because it seemed to me that everyone around was getting things done without problems. This went on and on until, I was assigned to a new project. Small project, very simple and requires the skills I already had, needless to say that I owned it.Later they told me that they got impressed, they didn't expect me to be so good due to my poor performance on the first project.  tl;dr.: Got overwhelmed by my first project, too much to handle for a newcomer. Owned second project and proceeded to grow in the industry. "
"If you want that URI, simply route it to a model that will map the request into a proper data structure.","That post is full of advice that does not scale.  The database should not map to end-points. Table naming conventions should follow relationships. Have fun working with a database that has 100 randomly named tables that are based on end-points.  > Saying a user has-many mailing lists doesn't really make sense. They don’t own the mailing list.  Right, but what does this have to do with architecture of things? What makes no sense is ruining the naming conventions of database for an URI that looks better.  As the complexity grows you will end up with stuff like:  mailing_list_user -> subscription (/subscription)mailing_list_group -> subscription_group (/subscription/group, does not look that pretty now)  From DBA end, that would not make any sense, who is subscribing to group? What group? Do we mean subscription is for something called group or a group id that represents many to one relationships within database?  Now we need to denormalize data for performance reasons, first it has to go into pending table and then we use workers to transfer it at their own pace.  We create subscription_group_pending and switch /subscription to insert data to that table.  Boom your /subscription end point that you designed for your precious subscription table no longer has a point; but you have a useless table named 'subscription' in your database.  TLDR: If you want that URI, simply route it to a model that will map the request into a proper data structure. "
It's C.  Munge the bignum via direct memory manipulation and win the game.,"Well, I mailed the author with a bit of a chuckle, but it bounced.  So I'll post this here.  ==The right way to solve your problem, of course, is to press you on  precisely  what you mean by, ""arbitrary size integral types"".  In C, typing is merely a compiler artifact; everything is a pointer to a buffer under the surface.  So the right thing to do, to get the largest value  IN C , is to determine the underlying memory representation of the ""number to be returned"", and set those bits to the maximum number that may be expressed by that encoding.  People have been saying ""merely printing 9's does not return a value"".  And this is true.  But force-setting bits, and then returning a pointer to the transmogrified bignum?  That totally works.  You may get around this exception by refining your contest to discuss a C-Like language where the underlying integer memory representation is required to remain opaque.  tl;dr:  It's C.  Munge the bignum via direct memory manipulation and win the game. "
"I generated about 30K lines of mind-numbing, soul crushing Java code with just less than 3K lines of Python. 
 And yes, good programmers  are  lazy.","This is certifiably un-cool because it could probably have been addressed more elegantly, but it got me out of an extremely stressful spot under a tight deadline.  We had to do a bunch of admin screens for essential tables with basic CRUD and validation on a Java Enterprise 5 (with JPA/TopLink Essentials and JSF) system, and I needed about 15 of those done in two days.  So, I picked one of them that had been done reasonably well by hand by one of our programmers and refactored that into a Cheetah template, then created some data with the screens' file names, titles, variable names, JPA class names, etc. in it, and then created a driver program that would generate all the pages in a pretty much workable state.  The rest was mere adjustments.  Of course, when I handed it off, I explained how to maintain the templates, run the Python driver, etc., but he opted to manually maintain the source for the individual screens instead, so now the driver and templates are essentially useless.  :(  tl;dr=I generated about 30K lines of mind-numbing, soul crushing Java code with just less than 3K lines of Python.  And yes, good programmers  are  lazy. "
We need to devise new techniques for software engineering that embrace chaotic requirements instead of trying to avoid it.,"Well....after 20 years as a programmer for people who need computers but know nothing about them, you end up developing a different kind of methodology for building your software.   You use techniques that allow you to....start work immediately with a fuzzy target and bring the end result into focus via iterations thru the users.  Admittedly I've been fortunate to work in a situation where this feedback loop is really tight and there isn't much between me doing and users using.     It does make the lack of clear requirements easier to deal with.  Personally, instead of having a IT department, I'd just take some clever creative programmers and drop them in as team members with the business units.    I'd save the ""IT dept"" for infrastructure development which will bubble out of the business units and since it's bubbling out via a programmer, things might go smoother.  tl;dr: We need to devise new techniques for software engineering that embrace chaotic requirements instead of trying to avoid it. "
"if you don't enjoy learning new things, and do it instinctively, this probably isn't the career for you.",">I don't have much experience with frameworks. What should I do?  Learn to use frameworks. I'd suggest CodeIgniter is the easiest one to learn the basic concepts. Laravel and Symfony are maybe more useful for production. You might want to familiarise yourself with composer and git and stuff too, if you're not already.  >I'm beginning to think to dropping my career altogether and looking for something else with longevity.  If you didn't think straight away that the answer is just to learn this stuff, then maybe this career isn't for you. I've been a developer much longer than you, and I spend about ~50% of my time learning new stuff.  I started with Pascal.. not much call for that nowadays..  tl:dr;  if you don't enjoy learning new things, and do it instinctively, this probably isn't the career for you. "
Keep your fucking system up to date or be willing to pay for people who know how to use it/are willing to learn,"Basically this. I'm only an intern but because I know bits of the language they're using alright(mysequel, which is what theyre using to transfer the data) and i know my way around excel pretty well, I work with the data team as a ""junior developer"", so I attend all of the meetings to report the problems that are occuring. In all of our meetings all I hear all goddamn day is why we don't have results or why doesn't said transfer program doesnt work. When it reality the data we we're given(which sales and business made) was shit. Literally half done. So in two weeks when this new program launches and people have quit because they've stopped maintaining the old system because the business decided it is ""no longer important"". Everything has basically gone to fuck and now while trying to find new hires, they are required to know legacy, which no one does at the price they are trying to pay.  Now we have to do a practice load by Friday that they assigned yesterday and I told them with only five people working on it, I wasn't sure if it would be ready. Then I got shit because they didn't read any of my emails explaining the problems with the data we recieved. Now we are seen as part of the ""main problem area"". To top it all off my boss left for corporate so I'm in charge of pricing decisions and I have no clue what to do half the time (literally bullshit till you understand type deal).  Sorry for the rant. Thought you guys would understand.  TLDR:Keep your fucking system up to date or be willing to pay for people who know how to use it/are willing to learn "
"I'm a new programmer on my team and can't wait to learn from veteran engineers, most of whom are in their 50s and masters at what they do.","Maaan same. I looked at the new codebase we had, beautifully written by a software engineer who happened to be in his 50s and was asked to extend its functionality. Looking over it I marveled how all the variables were self contained, outside of a Logger there were no global variables meaning no side effects which makes debugging a joy especially compared to some of the older legacy stuff (this is all Java btw). And even after scratching my head for a good hour there wasn't any way that was easily feasible of extending the functionality he wanted without utilizing another global variable. I'm hoping he tells me there's nothing wrong with the way I did it, but I'm also hoping he shows me an awesome new way to go about it.  tl;dr I'm a new programmer on my team and can't wait to learn from veteran engineers, most of whom are in their 50s and masters at what they do. "
"without knowing more, I don't think you can conclude that the private spaces are not very, very valuable just because people appear to prefer the open space.","When you say that they prefer working in the open room, do you mean that they  never  use their private spaces or that they use them more rarely than the open space? Because there's a huge difference between the two.  If most folks on the team are generally in the open space, but retreat to their private space when needed, it's hard to quantify the value of having the private space. It could easily be the case that even if they spend 25% of their time in the private space that time is some of the most critical and valuable.  Additionally, they might be using the private space to ""recharge"", which could have a cascading effect on increased productivity, even in the open space.  TL;DR: without knowing more, I don't think you can conclude that the private spaces are not very, very valuable just because people appear to prefer the open space. "
"They're both great, but there are niche issues with both that you'll want to watch out for and/or mitigate.","I was a big fan of Knockout when it first came out, and I'm also a big fan of Angular now. And as others have mentioned, Angular has a lot more than just templating databinding, which is all Knockout does, so it's not a fair comparison. That said, I'll focus entirely on the databinding aspect of Angular as compared to Knockout.  What you need to know first and foremost is that they're both great templating/databinding frameworks. They are both fast and they both do a great job of enabling you to segregate your user interface logic and coding from your user interface layout.  For the general case, they're both  very  performant, but because they take very different approaches to databinding, either framework could have serious performance consequences in certain niche situations.  The differences are thus:  Knockout uses an immediate-mode update model while Angular uses a dirty-checking update model. What that means is that when you make a change to your viewmodel in Knockout, Knockout immediately re-renders the template. When you make a change to your viewmodel in Angular, nothing happens until all of the code you were executing completes, at which point Angular checks for changes to your viewmodels and then updates any affected templates.  That means that Knockout can slow down significantly if you have a lot of things to update at once, like, say, updating every item in a repeated list (every row in a table, for example), because the template builds that list is rendered and re-rendered after every update. Angular, on the other hand, can handle all of those updates in a single execution pass (which is normally fast if you're not constantly rendering and re-rendering templates) and only update the affected templates once, but with the consequence that the more items Angular has to track for dirty-checking, the slower the overall application becomes.  Both frameworks have techniques available to mitigate these issues, but they're important to know about.  TL;DR:  They're both great, but there are niche issues with both that you'll want to watch out for and/or mitigate. "
using Count() when all you need is Any() is dumb but MS is smart enough to protect you from your dumbness sometimes.,"Semantically, it's  much  smarter to do  Any()  instead of  Count()  if all you need to know is if the collection is empty. It will work in constant time for any collection, even an infinite one.  However, it's worth noting that if you happen to call  Count()  on an object that  does  provide a  Count  property by implementing  ICollection , .NET is smarter enough to use that:  public static void Test(){    // make a big list    var list = new List&lt;int&gt;();    for (int i = 0; i &lt; 1000000; i++) list.Add(i);    bool dummy = false;    // time the different ways to see if it's empty    Time(() =&gt; dummy = list.Count &gt; 0, 1000000);    Time(() =&gt; dummy = list.Count() &gt; 0, 1000000);    Time(() =&gt; dummy = list.Any(), 1000000);}private static void Time(Action action, int count){    var process = Process.GetCurrentProcess();    var start = process.UserProcessorTime;    for (int i = 0; i &lt; count; i++)    {        action();    }    var elapsed = process.UserProcessorTime - start;    Console.WriteLine(elapsed);}  Running that, I get:  00:00:00.015625000:00:00.046875000:00:00.0625000  Which implies that the implementation of  .Count()  is returning in constant time. The implementation of it is likely:  public static int Count&lt;T&gt;(this IEnumerable&lt;T&gt; collection){    // try the fast way    ICollection&lt;T&gt; numbered = collection as ICollection&lt;T&gt;;    if (numbered != null) return numbered.Count;    // do the slow    int count = 0;    foreach (var dummy in collection) count++;    return count;}  tl;dr: using Count() when all you need is Any() is dumb but MS is smart enough to protect you from your dumbness sometimes. "
"I am tired of reading disgruntled programmers talk about being the only ones who matter in companies and the only ones who are under tight, impossible deadlines.","Every job has their concerns that other professions don't understand. Programmers aren't gods who have both programming knowledge and accounting, finance, and marketing knowledge. Sure, marketing people may not have the knowledge of the hoops you need to jump through to get something working. However, at the same time, programmers have no idea what the marketing team has to do to coordinate with their clients, what the finance team has to do to work the value of the project into the future earnings forecast, etc.  Note: I do a TON of programming, will likely get a career doing it, but still understand that programmers aren't the only people who matter in a company. Everyone is under deadlines and stress.  TL;DR: I am tired of reading disgruntled programmers talk about being the only ones who matter in companies and the only ones who are under tight, impossible deadlines. "
college doesn't teach you shit about programming. you teach yourself all of that. learn a language or 2 in your spare time. DO SOMETHING related to your trade.,"Ask yourself this: why did you take CS? I'd wager to guess that you have an interest in programming.  But what do you do in your spare time? Would you spend a boring Wednesday night playing video games or something, or contributing to an open-source project, or learning a programming language, or starting a cool side project? Where is the passion? Your ability to program must be worked at, in this industry, trends change so fast, you need to stay on top of things. Before anyone criticizes me for the wording, I'm not saying go and program every day, constantly. Go the fuck outside so you stay sane. But a few nights a week to do something that is presumably a hobby of yours shouldn't be a chore if you enjoy doing it!  Right now I develop in PHP at my job, but that doesn't mean all I do is write in PHP. You've gotta stay current. Programmers who or older than ~30 programming PHP today weren't programming in PHP when they were in college. Nobody graduates from college with an IT or a CS degree and knows  everything  about programming -- but the goal is to be develop a programming ""muscle"" that you work at, that enables you to learn other languages and to understand the engineering methods and general thought processes that programming requires.  So spend 2-3 nights a week actually DOING something related to your career. Learn Ruby on Rails, spend a few nights learning MVC frameworks, like CodeIgniter or CakePHP, hell, write a Windows program in .NET that does something useful, learn Objective-C and try to get an OS X or iPhone app of decent complexity to actual compile. If you find you don't ever want to learn anything related to your field or spend the time working at it, and I hate to say it, then you probably picked the wrong field.  I've been programming since I was 11-12 years old (VB6, woot!) and to this day, I have several side projects that I work on, most for the sole purpose of learning more things and increasing what I already know.  tldr; college doesn't teach you shit about programming. you teach yourself all of that. learn a language or 2 in your spare time. DO SOMETHING related to your trade. "
"custom, large enterprise apps = roll your own. large apps with lots of unknowns with no established market = framework.","Frameworks, for me, have 3 purposes:  1) Enforce a proper structure on the application  2) Take care of boilerplate code that is not fun or unique to write  3) Facilitate more rapid development  On a typical small project, I won't use a framework most of the time. Sometimes I might though, CakePHP is my goto for small projects because I can whip out things a lot faster.  When it comes to large projects though, I would advocate always starting with a framework. The exception would be, if it's a large project for a pre-existing market where the specs and scaling requirements are known, and you have plenty of time to implement it, I might advocate rolling your own.  For most large projects though, up front the specs are rarely known. If you look at things like facebook, twitter, reddit, etc one of the key things that they have in common is that they started from nothing with no idea what kind of user base they would have, what kind of features would be requested, what kind of scaling would be needed, etc. This is absolutely the kind of situation where a framework makes sense. When faced with a project like that the most important things you need to worry about are getting an initial product to market VERY quickly, and keeping it flexible enough to adapt and determine what is successful and what is not. While you can do this with any kind of app, a framework excels at this by providing a set of tools and logical structure that make it simpler to get up and running not have to worry about the common functionality of all web apps.  In the grand scheme of things, if you were to write twitter/facebook/reddit in rails or Kohana or something, it likely wouldn't last. As it grows and the need to scale comes into play, you're likely going to start to see massive benefits from moving to a more custom solution where you can control the entire request chain. Twitter is a perfect example of this, started out as a rails app and now has grown much beyond that. But, on the flip side, if twitter hadn't started out as a rails app and instead had spent multiple years in development with a custom solution, we might not have twitter around today.  TL:DR - custom, large enterprise apps = roll your own. large apps with lots of unknowns with no established market = framework. "
"use the right tool for the job, sometimes the right tool is the ones you know best.","The whole SQL vs No-SQL are the same type of arguments from MySQL vs PostgreSQL. There are always going to be pro's and cons's of each. The best thing for every project isn't saying I'm going to to always use one over the other, but instead use the best tool to solve the problem. Some are going to work faster with a schema defined in the database layer, some are going to prefer to have a schema defined in the software layer.  As for quick prototyping, the best tools are the ones you are most familiar with. If you are a JS guy, then use NodeJS don't try to learn C just to prototype. Same goes with your data store. If you know CouchDB better than MySQL use it.  TLDR; use the right tool for the job, sometimes the right tool is the ones you know best. "
"Internet browsing is not private. Surprise! 
 Selling your browsing information without your consent is wrong.","I never said they were allowed to use the information they collect to sell to other companies.   I only responded to this guy's comment on his/her surprise that his/her internet browsing sessions were never private.  Hell you even see this warning when you try to browse in private mode via FireFox/Chrome.  Seeing what you search, what pages you visit, when you visit them, how you visit them, where you visit them (yes even your IP is tracked) is not only data retrieved by the ISP but the pages you visit themselves (search [web analytics]( This information is freely available, that's all I was trying to say.  It's not a breach of privacy, it's more like misuse of your information and abusing your trust. It's like signing up for a newsletter that promises not to sell your email address and whatnot to someone else. But they will still have your information for themselves.  The number of downvotes I've gotten seem to come from people who just installed Windows 98.  Welcome to the interwebs.  tl;dr. Internet browsing is not private. Surprise!  Selling your browsing information without your consent is wrong. "
BSD- or MIT license is usually the way to go.,"I'm afraid that is actually bad advice:  Consider that not all languages have a license. Probably no  language  has one (what license does C++ have? Javascript? Assembler?), but just language implementations (e.g. it's possible that CPython, Jython and PyPy have entirely different licenses, although they are all valid python interpreters). So your statement is highly abiguous. At the same time, you'd have to specify the version of the implementation, so you'd end up with ""this is licensed the same way as CPython 2.6.3"".  But that's also not really practical: a piece of software can have more than one language. I think Mozilla (or was it Eclipse? Or OpenOffice?) was dual-licensed for a long, long, long time. In such a case, which one of the two licenses applies?  I think it pays off to invest a little time to look into the topic. The  MIT-License .  If you don't agree with commercial use, have a look at [Creative Commons](  Their licenses used to be usable for software, too. And they're pretty nice. Or you want to be a true open-source evengelist, look at the GPL. But if you're going into GPL-territory, things become a bit complicated. The GPLv2 was pretty easy to understand, but much, much lengthier than than the BSD- or MIT-License. The GPLv3 is way too long and contains too much lawyerspeak, IMO. And of course if you're developing something that can be incorporated into commercial work, but you want your library to stay Open Source, you also need to check out the LGPL.  Thus, if commercial use is of concern either check out CC, or do some research on your own.  TL;DR:  BSD- or MIT license is usually the way to go. "
"When there's a hammer on the shelf, don't build one yourself with a ruler duct taped to a rock. Use the hammer.","I don't really understand approaches like this. ""Instead of doing the right thing (using PBKDF2 or bcrypt or scrypt), I'm going to invent my own method that is a tangled web of hashing/encryption/etc. that gives me 'extra strength' because they won't figure out what I did.""  This is a very dangerous line of thinking in security. Real security comes from assuming that the attacker has perfect knowledge of the system, and that strength comes from well vetted algorithms and the fact that you have the keys.  What if the attacker breaks into the server and steals the code? He knows exactly how many times you hashed/encrypted.  Secondly, if he knows you're just feeding SHA1 hashes into the encryption algorithm, he won't even bother searching for passwords. He'll just start cranking through the keyspace until he finds some hashes that match and then reverse backwards from there using rainbow tables.  Rather than invent something that you  think  is more complicated or more computationally intensive,  just use the right thing . PBKDF2, bcrypt, and scrypt all have a tuneable work factor that lets you crank up the computational load to compute the hash. It's already exactly what you're looking for and it's well understood and well vetted.  TL;DR: When there's a hammer on the shelf, don't build one yourself with a ruler duct taped to a rock. Use the hammer. "
"Python is not a bad choice, but this article promotes it for the wrong reasons.","Hmm... someone's a bit biased there...  ""so developers are less likely to make syntax errors. This means fewer bugs and faster development."" -- syntax errors are easy to catch very early. If they are a problem at all, then something is horribly wrong with your process and/or your tools, and striving for programmers not making syntax errors is the wrong way to go at it.  ""It's also very fast."" -- Python is a dynamically-typed language with automatic garbage collection and powerful introspection. This makes it a comfortable and moldable language to work with, but these things do not come for free. If you do need/want them, which is quite common, python implements them as efficiently as one could expect from one of the world's most popular general-purpose programming languages, and unlike some alternatives, it does not sacrifice correctness to do so. It still has to implement them though, and if ultimate runtime performance is your priority, then python alone is not the way to go (but, mind you, a codebase that's 98% python and 2% hand-crafted C might very well be).  TL;DR: Python is not a bad choice, but this article promotes it for the wrong reasons. "
You have a  wildly  inaccurate view of what's going on.,"Hah, what? I know hundreds of mods, and I don't think a single one considers it as a ""power"" or a ""title"". The only people who think it's anything other than janitorial work and cleaning shit off the subreddits are the users, which means that being a mod is actually worse than being a janitor (at least janitors aren't shat on for being power-hungry).  The reason inactive people stay as mods is, literally, that it doesn't hurt anything. If someone can unspam one submission per month, wouldn't you rather they did it, rather than stepped down? Most mods aren't even that bothered about being mods, many people have stepped down from the subreddits I mod because other people basically said ""hey, we're trying to make the mod list shorter, do you mind stepping down if you aren't very active?"".  TL;DR: You have a  wildly  inaccurate view of what's going on. "
"Article's author has little idea what will ruin a career. Perhaps the article could have been titled ""things novice programmers do that slightly irritate more experienced programmers.""","Skipped 1 because it's not a mistake, it's a natural part of life.  2 is utter bollocks. Even in their made-up example the former is absolutely easier to read, metally parse and reason about. Ternary operators have their uses but suggesting that they be used in place of if/else blocks in the majority of cases, or that ""real programmers"" use them instead, is ridiculous. Not going to ruin your career.  3 I can see as a possibility. Unfortunately, the author confuses backups with VCS like git and subversion. The only way I could see this ruining a career would be if you built an entire enterprise application single-handedly and never backed it up, and accidentally deleted the only copy of the source code.  4, using whitespace inconsistently is not going to ruin your career.  5 can be just as bad followed as not. If you're iterating over an array, using i for the index variable in a for loop is perfectly fine. Having giantMethodNamesThatGoOnForPagesAndPagesForNoGoodReason() doesn't make programs readable, it makes them hard to maintain because it takes forever to type the names, it makes them hard to remember, and there's more chance for typos that cost time to correct. Also not going to ruin your career.  At least with 7 the covered all their bases. Adding a++;//adds 1 to a is not going to ruin your career.  8... I'm not sure I understand. You're saying it takes an expert to recognize the difference between Joomla and Drupal? If you're familiar with Joomla and you apply for a job doing Drupal, you may not get the job, but it's not going to ruin your career. Or maybe this refers to confusing PHP (language) with Cake (framework) or emacs (IDE)?  9, ""Use case consistently"" is in the top ten mistakes that can ruin your career? Seriously?  And 10 is just 01 flipped around backwards. Unfortunately I doubt that clever wordplay was intended.  tl;dr  Article's author has little idea what will ruin a career. Perhaps the article could have been titled ""things novice programmers do that slightly irritate more experienced programmers."" "
"It's already very possible, and it probably doesn't need to be easier.","IMO static initialization should be  discouraged . There are many Java-isms I'd like to see in PHP, but this isn't one of them. If you find yourself wanting a dedicated language-feature for it, that should be a warning that you may be over/mis-using it.  For the rare times you truly need it, there are already perfectly-good ways to do it, such as:  namespace Example;/* * A static initializer is truly just procedural code sneaking back into a  * class definition. If you have a good reason for it, then you shouldn't * need to lie to yourself about what's going on. */Widget::init();class Widget {    private static $initted = false;                   private static $hostData;    public static function init(){        if(self::$initted){            return;        }        self::$initted = true;             self::$hostData = trim(`uname -a`);                 }    public static function getHostData(){        // Look ma, no ""If-initted-already"" logic.        return self::$hostData;    }}      The only real win I could see from a dedicated mechanism would be some sort of static-analysis to limit people to ""safe"" activities, but that's often difficult/impossible in PHP anyway. Meanwhile, stuff like the above example has the benefits of:   It'll run before the class can be used  It will  probably  run lazily, when the class is auto-loaded  It won't run twice  It does not execute in global scope  It has  private  access to the class  Exceptions and debugging work in a way people should already understand  No need for a new PHP version   TLDR:  It's already very possible, and it probably doesn't need to be easier. "
"dev would break the build every day and go home, was let go, and now works at NASA.","We had a developer at my company who had been there about 10 years.  I honestly don't know how bad the code he wrote was (I wasn't inspecting it), but it was usually bad enough to break the build.  As a build guy, this is a pet peeve.  ""Compile your shit before you submit"", that's my motto.  As our company was switching from waterfall to agile, there was a lot of training that went out to developers.  However, it seems that the only pertinent piece of this training that he picked up was that you should check-in every day.  So, without fail, he would take a fairly massive code dump on our tree at around 5pm, right before he would head out for his 1 hour commute.  Inevitably, it would break the build, and leave the rest of the team scrambling to either fix the build, or revert his changes, or some combination thereof.  When it was something quite obvious like a missing semi colon, or obviously mismatched quotes, I would occasionally check the fix in, sometimes leading to another break further in the code.  It was probably a bad idea and only served to reinforce his belief that he was somehow OK.  We did have a graphic which was meant to give a sort of social stigma for breaking the build, however, he seemed to be somewhat oblivious to it.  The numbers were something like Him = 50%, other 30 devs = 50%.  Among the team, his name became a synonym for breaking the build. If his name was Frank, we'd say, ""You just Franked the build"".  He did thank me several times, and even offered to take me to lunch one time, for fixing his breaks.  However, he was let go before that ever took place.  Now he works at NASA.  tl;dr dev would break the build every day and go home, was let go, and now works at NASA. "
Finish an application from start to finish. You learn important different things at each step.,"The best way to learn is to finish a program and ship it.  To use a chess or Starcraft analogy, you need to practice your opening game, your midgame, and your endgame.  You need to learn how to define the problem and come up with a system that effectively deals with the issues and realizes the solution. Most people can get this far.  Then you need to be able to deal with the discovery that your understanding of problem is incomplete or completely wrong. Most people (who aren't doing this project for a job) will scrap their project at this point. But you need to persevere here because this is where much of the learning happens.  Finally, the last part is polishing your application. This is where an Ok application becomes a great application. You'll need to observe people using your program.  The first time you put your app in front of a user, you will notice that they're using it in ways you hadn't anticipated! Resist the urge to give them directions. Observe them and fix your app so that it's more usable.  tl;dr;  Finish an application from start to finish. You learn important different things at each step. "
I hate strong password enforcement. Who is with me?,">If a tech-savvy person wants a weak password then I say let them have it.  I wish I could remember a site that took this attitude. I am convinced that strong password enforcement is less safe, as it makes the person entirely more likely to reuse passwords.  Before the onslaught of the 'strong password' bullshit, I had almost 20 different passwords using a combination of EITHER lowercase or uppercase letters and numbers. The reason for this is I can put the caps lock on and still type my password with no difficulties.  God forbid the place pulls some shit like ""can not be one of the last 12 passwords"" while needing an uppercase, lowercase, number, symbol, and be beyond a specific length.  TL;DR: I hate strong password enforcement. Who is with me? "
"switch  definitely has valid use cases, so don't ignore it just because it  might  be slower.","switch  is great if you have a variable, a set of possible values, and you want to perform different actions for each value.  It makes the code more readable and easier to maintain.  switch  is slightly slower, but not by much.  I found a [benchmark comparing the time it takes to run 100,000 iterations of  if - else  and 100,000 of  switch ](  >total if / else time: 0.66318321228027  >total switch time: 0.68480849266052  >if / else is quicker by 0.021625280380249  Let me put it this way:  if a  switch  is appropriate for what you're trying to do, and you do a messy  if-else  instead, you've increased performance a mere 0.02 seconds, but when it comes time to update the code later, it will take MUCH longer than 0.02 seconds to refactor/update/whatever.  If the performance truly concerns you, do it both ways and benchmark it.  TL;DR:   switch  definitely has valid use cases, so don't ignore it just because it  might  be slower. "
Optimizer is to fix the DFA -> Regex conversion.,"The way RegexEngine works is by using a DFA as an intermediate form for the Regex. Since they both represent the same space of languages there will always exist a way of transforming the Regex into a DFA. Initially I didn't anticipate that a optimizer would be necessary at all for this interface, but quickly found out that while you can always transform a DFA -> Regex it's almost always larger and more ugly than the initial Regex. So my current solution is to implement a number of [""rules""]( that are applied to the regex after the DFA -> Regex transform. I've found that even these aren't enough for complex Regex and I will need to implement a genetic solution finder to have a shot at finding the optimal solution every time.  I hope with this framework to take it a step farther and allow people to evolve generic Regex based off a positive match set. For example provided with: """", ""11"", ""1111"", ""111111"", ""11111111"". I should be able to solve for  ^(11)*$  as the minimal, generic solution. I believe this will let this work an order of magnitude more useful as it would let people create Regex based off strings they want to match.  Also the reason I default to using the  ^/$  bounds on every regex entered is really to get around some limitations of the graphing strategies in place. Even something like  ^1  is ungraphable because it has to many edges to show (I'm planning on fixing this). If you want to get around this you can enter the regex like so:  ^[^]*abc[^]*$ .  TLDR; Optimizer is to fix the DFA -> Regex conversion. "
"The code you are proud of today, will shame you tomorrow.","One I was proud of, although now I shake my head in shame of ""what the hell...."", was an approval grid / contact search control in JavaScript. The list of required approval roles would change depending on a bunch of inputs/rules on the page.  Now, looking at the code - and how horribly coupled the rules are to the grid, and the grid is to the contact search dialog. It was so tightly coupled that when my work decided 'we want to use this search dialog on other screens' - it was a total nightmare to get it working.  That said, the end result was still pretty good - even if the actual implementation is total shit. Especially considering that at that time, my work had a 'don't use JavasSript' mindset. It really sort of sparked my interest in doing client-side programming 'the right way', learning from my mistakes and setting out to build our own in-house JavaScript framework for building apps/widgets, it's main thing was pub/sub communication between widgets, common elements/functions/utilities/etc.  The framework I built as a result of that, has been used in 3 major projects since then + countless small ones. It's getting scrapped/re-written soon - my overall understanding of programming in general has improved a great deal since then, and it's time to get a fresh start for my next big project.  tl;dr -- The code you are proud of today, will shame you tomorrow. "
Compilers are supposed to fail to compile when your code is syntactically incorrect.,"> JavaScript is much more permissive than, say, C++, and this was nothing but an good thing for many  There is a difference between permissive and not helpful at all. Mismatched parenthesis are always wrong and anything branding itself as a ""compiler"" should throw an error.  Small example:  Say I wrote this: (def a (sum 1 2) (sum a 4)  Instead of: (def a (sum 1 2) )  (sum a 4)  Instead of throwing an error your compiler outputs (a=(1+2)=(a+4)). When I eval this I get:  Uncaught ReferenceError: Invalid left-hand side in assignment   How do I debug this? I have to look at the lisp, the javascript, have some knowledge about how things are converted, and know that some missing syntax is inferred  all for a forgotten parenthesis .  TL;DR Compilers are supposed to fail to compile when your code is syntactically incorrect. "
use existing framework/CMS that's well documented and has active community if you want to save time.,"You keep mentioning CakePHP and how you like to work with frameworks. I guess you know the answer, just need a nudge.  I used to ""roll my own"" frameworks and stuff for various projects because it gave me control over everything. And it also took too much time to reinvent the wheel. In the end I realized it's faster to just use as much already made things and modify them to do my bidding.  I chose WP because I really had no intention of making themes from scratch and I really suck on the (theme) design part. Everything else just fell in place after that: client needs some specific behaviour? Quick google search later and I have a plugin that can be customized further without too much hassle. If the plugin does not exist I just roll my own. Specific thing here is that I do lots of small sites (6 pages at most), without too much of complex things but the clients do have the strangest of requests.  After a while I ended up working with a designer and we manage to crunch up one site every two or three weeks on average, from start to the point where client is happy with the end result and goes live. Client making up his own mind about what he actually wants is taking up the most of that time, as usual. I should probably mention that this is not my full-time job, but can be considered as a long running side jig.  Whatever you choose make sure it's actively developed, has an active community and is well documented.  It also eases up maintenance if person who ""inherits"" the site doesn't have to spend days wading trough custom frameworks and can instead just check couple of files to see what's going on.  TL;DR : use existing framework/CMS that's well documented and has active community if you want to save time. "
"Just because you can write an incorrect solution to a problem in any language, doesn't mean it isn't worth trying to make languages more safe.","> What's stopping me, the worst developer in the world, from using stateful data in FP languages?  Because people tend to follow the path of least resistance, and it's a lot harder to do it that way.  Also, if you stick everything in the IO monad, the other developers you work with won't be able to invoke any of your code from their non-IO code.  There are ways to bend the rules, like using the ST monad, but I'd consider that an intermediate/advanced topic that has a steep enough learning curve that it's a lot less work just to try to solve the problem as a pure functions if you aren't very good at Haskell and/or don't know what you're doing.  I have wondered what using Haskell would be like if Haskell was mainstream and was used by a lot of mediocre programmers, but I think it would hold up pretty well.  The type system doesn't eliminate the possibility of bugs, but it is a very powerful nonsense filter.  It's just as easy in Haskell to implement an incorrect algorithm, but it's a lot harder to accidentally ask the computer to do something that just doesn't make any sense at all.  I would much rather work with bad programmers in Haskell than any other language I know.  Tl;dr:  Just because you can write an incorrect solution to a problem in any language, doesn't mean it isn't worth trying to make languages more safe. "
"The graph appearance is bad, because it can make people confused about what kind of graph it is, especially if they just look at the first one.","The  real  problem is  the visual style the OP chose .  The rest of the post makes it's clear he  intended  a [radar chart]( but its visual-style doesn't match the usual convention for that chart-type... Basically a miscommunication in the visual-language executives should be pretty fluent in.  So anybody who just glances at the first picture may assume a data-sample is a  point  (rather than a volume) which implies that different motivations  cancel out . (Not what the author is trying to say.)  It's like... suppose an executive coming up to you, telling you that the code needs ""a facade"", and only much later do you realize he's actually describing a factory-pattern. Their idea isn't necessarily  wrong , but they're still mis-communicating.  TLDR:  The graph appearance is bad, because it can make people confused about what kind of graph it is, especially if they just look at the first one. "
In my experience stress is more about the person than the job.,"I worked in the medical field and I was stressed out all the time. So I thought to myself ""let's switch to something not so stressful"". I chose programming and now do web development for a radio station. The strange thing is, I am still stressed out even though the stakes are lower, the hours are less, and the money is better. Maybe you are like me, stress just seems to be a part of me in the work place that I can't shake.  On the other hand I think it has made me a better employee, I take things seriously, big or small and ""get'er done"".  tl;drIn my experience stress is more about the person than the job. "
"Make a website, put it on GitHub, don't be afraid of criticism.","I think you need to create a website for it, and answer the questions /u/PilotPirx asked on its front page.  For example, [rust-lang.org]( contains:   a tag line containing the three most important features of the language (""a safe, concurrent, practical language"");  a list of features (What sets it apart from other languages? Why would I want to use this?);  a code sample to show what it looks like.   ruby-lang.org  and a code snippet.  golang.org , and a short blurb (containing the words open source, easy, simple, reliable, efficient).  I think those are some nice websites for programming languages. ( python.org  Basically, this is  not  trying to start a flame war. There is a specific niche you're focusing on for your language, and there are certain design decisions you made as a result. Your programming language will be either statically or dynamically typed (or are types optional?), it has garbage collection or not, it is interpreted or compiled, etc. In all cases both are valid choices, and both will be useful for some types of applications.  But you need to make these choices clear to the people interested in your language. That way, when I look at your website, I can mentally put your it in a box, and decide ""this is useful for the sort of application I'm building because it seems high level and easy to use"" or ""this is useful for the sort of application I'm building because it seems low level and I need close control over the hardware"".  (You seem to say ""these are details that don't matter"", but they  do  matter! No language is perfect for everything, we need to know ""Is this the right tool for the specific job I have in mind?"")  Also, keep in mind that people will always criticize what you made, some criticism will be stupid (""I [don't] like types""), some will be constructive. You want that constructive feedback!  tl;dr: Make a website, put it on GitHub, don't be afraid of criticism. "
"phpbrew offers some additional features (downloading/installing/configuring) while not requiring knowledge of TCL, compilation options, and system-specific details (like search paths and library locations).","There's some overlap in features/solutions provided, but phpbrew and environment-modules seem to provide different sets of solutions.  environment-modules does also provide the ability to switch between different versions, but it doesn't provide seem to provide too many other features (like downloading, installing, and configuring the modules). Its TCL-based [modulefiles]( provide its configuration, but may be a bit unfriendly to most PHP developers.  On the other hand, phpbrew probably doesn't have the same level of work to make sure some things work in multiple shells: manual pages, library paths, conflicting commands, etc. Are those things important for a PHP developer? Are they important for Ruby, Node, Python, or Java developers?  It may not be too complicated to manually download, configure, and compile a specific version of PHP.  Perhaps it wouldn't be too bad to write and install the TCL modulefile that will modify the appropriate environment variables.  Perhaps, for some, there's room for a tool that downloads, compiles, and configures a php installation with one command.  tl;dr  phpbrew offers some additional features (downloading/installing/configuring) while not requiring knowledge of TCL, compilation options, and system-specific details (like search paths and library locations). "
"I'd probably listen to your news segments, but I doubt most people would.","I disagree - the point of the story is that this video was so popular that it surpassed the maximum number of views youtube expected and required updates. That's all your average viewer is going to walk away with. Sure you could go into an explanation of binary, spin off on a tangent about default word sizes and the prevalence of 32bit systems, and then wrap back around and tie together the significance of 2,147,483,647 but I'm pretty sure the entire point of the segment is just to provide a couple minutes of filler that they really don't care about.  tl;dr - I'd probably listen to your news segments, but I doubt most people would. "
"While I understand it's important to point this out to the web developers, why do I owe you anything as another user?","Again , why should I base  my  evaluation of everything on  your  resolution?  Do you realize how ridiculous that sounds?  Should I dislike the site because it's not able to run on IE6?  Nope, I guess I'm a browser snob.  Dislike a game that needs DX10?  Nope, I guess I'm a GPU snob.  Should I dislike an app that doesn't have a version buildable on Solaris or OSX or something?  Nope, I guess I'm an OS snob.  Each of those things is a choice made by the developer regarding the lengths to which they will go to support every user's needs.  Pointing out such a shortcoming is fine, although it's not always helpful if they're aware.  Calling them/me ""snobs"" is just rude.  It's also quite arrogant that you think your problem supersedes all other concerns or opinions.  I think it's more helpful for me to provide feedback on its actual functionality.  Anyway, I will assume that you know how to zoom out and are just complaining for complaining's sake at this point.  Easy to curse the darkness after all.  edit:  tl;dr:  While I understand it's important to point this out to the web developers, why do I owe you anything as another user? "
"Lazy programmers repeat their first year of programming every year, never learning or improving.","Except, you learn to use emacs to edit MANY languages. It's an incredibly useful tool, and learning to use it is an investment, like learning to use a shell properly, touch typing, or understanding the basics of language design to more easily learn or better evaluate different programming languages.  One of the points made in the article, if you actually bothered to read it, rather than jumping in here and spouting nonsense, was that the author didn't want to have to learn a new editor or IDE for every language, preferring proficiency in a more versatile tool.  If this really is your attitude, then you're just lazy, and if you ever need to use more than one language then your laziness is actually creating more work for you since you'll need to learn a new editor or IDE for every language you need to use in the future.  TL;DR Lazy programmers repeat their first year of programming every year, never learning or improving. "
Sublime is slightly faster for really large files of repetitive GUI code.,"Because Sublime Text 2 has the minimap, multiple cursors, graphical limits on find/replace all (which uses multiple cursors) and it's faster to hit cmd-b or ctrl-b than to type ""wq;!py"" or click on the file. Sublime text is also extremely consistant between windows/mac/linux, which is really important because I code on all three simultaneously because I need perfect cross-platform support. (I guess the only reason that vim isn't that consistent is because I tend to use it in terminal only)  All of the above only really matter when I'm writing 2000+ line files of super repetitive GUI code, so I  do  use vim for everything else; if I'm going to type  self.fooSizer.Add(foo, 1, wx.ALL | wx.EXPAND | wx.GROW, 10)  on 15 lines, with only ""foo"" changing, it's slightly faster to do that using 15 cursors than using ""yypppppppppppppp""  TL;DR: Sublime is slightly faster for really large files of repetitive GUI code. "
Stop comparing Numjitsu with jQuery. Numjitsu’s modification of the Number prototype is explicitly stated in the README.,"I feel like some of you are comparing my library with jQuery which I feel is unfair. The main reason is the objective of the libraries aren’t even remotely close. jQuery’s objective is DOM manipulation and Ajax, Numjitsu’s is to give numbers significant units. Comparing jQuery with Zepto.js makes sense since they both have the same objective. If jQuery started to mess with the Number and String prototypes then that would be unexpected behavior for jQuery because they are outside the scope of the objective. Numjitsu on the other hand is very focused only on the Number object and the README’s usage section says “Numjitsu works by extending the prototype of the Number object built into JavaScript.” Extending the Number’s prototype is very expected behavior for this library. I had planned on doing some more as stated in the expirmental sugar section, but after thinking about the scope of my project I have decided not to since it would be unexpected for my library to do that and would better fit libraries dedicated to dates and conversions.  TL;DR Stop comparing Numjitsu with jQuery. Numjitsu’s modification of the Number prototype is explicitly stated in the README. "
"With Xen you (typically) get experience very close to what you have on real hardware, with OpenVZ you don't.","Umm, that's [OpenVZ]( not [Xen]( I had a bad experience with OpenVZ VPS so I don't trust it anymore.  You see, with Xen you run your own OS kernel and Xen merely provides it with virtualized hardware. With OpenVZ, all VMs share same kernel, but it is a special kernel which can ""pretend"" that each container is a separate Linux instance.  Xen in typical configuration allocates a part of physical RAM for each VM, and each VM can do memory management itself, it can have its own swap etc.  With OpenVZ, memory management is implemented in host kernel, and it is free to swap anything out whenever it feels so. Also, it controls all aspects of your VM.  E.g. that host I had bad experience with had a limit on amount VIRTUAL memory -- that is, on the number of pages which are merely reserved but not physically allocated. So I had problems with running my software which was reserving considerable amounts of memory...  Maybe other hosting companies have sane policies on memory, but I'm just not comfortable with the idea that my VM can be intrusively controlled by the host.  tl;dr: With Xen you (typically) get experience very close to what you have on real hardware, with OpenVZ you don't. "
Flexibility and context-awareness trumps religious observance of 1980's Usenet rules.,"Right. That's the stereotypical, and usually only, response one gets when questioning the sacred holy order of bottom-posters. But sometimes it's  appropriate  to ""reverse the normal order of conversation"". If I've kept up with the conversation, and I just want the answer, it's easier to read if it's at the top. The conversation history can be in the rest of the post/email/whatever, in reverse chronological order, so I can go back in time if needed, but I don't need to read the entire conversation before I get to the answer.  It seems to me that different ways of posting are appropriate in different contexts. On Reddit, it's rare to quote the entire post you're replying to; what we do here tends to be inline-posting, not top-posting or bottom-posting. On the other hand, sometimes, such as now, one doesn't include  any  quotes from the prior post. In a context like email or Usenet, if one were to write something of a relatively standalone nature that nevertheless happened to be a reply to something else, it seems that including the context would be useful, but that putting it at the top of the post would be counterproductive. Not all posts are of this nature, but some are.  TL;DR: Flexibility and context-awareness trumps religious observance of 1980's Usenet rules. "
sometimes engineers aren't giving you the runaround even when you think they are.,"I recall another cpu bug a number of years ago, I think it was also AMD but it might have been Cyrix that was reported when compiling hte linux kernel.  The processor company looked into it, and confirmed it was indeed a bug handling self-modifying code, but didn't give much more info.  There was a lot of outcry about how they were being given the run-around, because the code wasn't self-modifying and the bug was still there when all possibilities of self-modifying code was removed and everything.  And when the full story finally came out, it was indeed a bug in the handling of self-modifying code, but it was triggered because the test case accessed memory in such a way as to look like self-modifying code, something like writes to close addresses, but the 'close' test masked off the high 12 bits of the address, so memory writes halfway across the 4g address space accidentally looked like they were writing to the stuff coming next in line in the instruction cache.  Which all would have been fine, if thee code for actually handling self-modifying code worked right, but it didn't.  tl;dr: sometimes engineers aren't giving you the runaround even when you think they are. "
"how do I list a table in order of an activity's sub-activities, and sorry about the wall of text.","Hey guys, fairly new programmer, just started a new account pycrete (yup, I know it's spelt pykrete) for just my project interests. I was wondering if anyone could help me with this - It's a pretty big ask since this is asking for a full solution not just a little fix, so sorry for just rudely requesting a complete function and lots of gratitude to anyone that is able to help!  I have a table 'activities' with activityid, activityname, parentid etc an activity can have a parent, and the chain can be endless theoretically (no parent-child loops though! I will figure out how to prevent those at a later date).  Anyway, I need to list these activities by heirarchy (i.e. top level parents echoed first then the tree of activities listed in a way that indents each child slightly. I will then implement a secondry order that puts sub activities in order of priority or date due). For now though, all I want is for a function to:   set $parentid = 0  for each row in activities table, if row['parent'] = $parentid then echo row  $parentid = row['parent']  Goto (sorry couldn't resist) Step 2  Somehow set $parentid back to previous if there are no more rows.   keep going through the table until all have been presented to the user in proper heirarchy     Here is the code I have:  require_once ""../model/databaseconnection.php"";  $query = ""SELECT * FROM activities"";  $result = mysql_query($query) or die(mysql_error());  while($row = mysql_fetch_array($result)){    echo $row['activityid']. "" - "". $row['activityname'];    echo ""<br />"";}  and then I draw a blank. I have loads of examples of failed branches of this to actually put into a proper heirarchy, some involving variable variables, one using push_array, but I am getting nowhere - they just turn into messy piles of code that do nothing whatsoever. There seem to be useful websites but I think the problem is me actually deciphering them and adapting them to my project.  tl;dr how do I list a table in order of an activity's sub-activities, and sorry about the wall of text. "
"Need a job? Call J.G. Wentworth. It's your money. Get it when you need it. 
 ::::::CONTACT::::::: 
 
 Email - Contact@imagelol.com  
 Skype - imagelol","We are looking for a php coder for our website. The website is an image hosting website written primarily in php. I'd like to offer earning shares and free advertisement if you're able to host our website. All of the hosts we've been on go down all the time regardless if we're using a VPS / Dedicated / Shared (horrible).  You don't have to upvote this but if you do you can help someone with a job. As for traffic, with this new domain we've been pulling close to 50k unique consistently depending on how hot our images our but we can not service this many people with the time being due to our hosts.  TLDR: Need a job? Call J.G. Wentworth. It's your money. Get it when you need it.  ::::::CONTACT:::::::   Email - Contact@imagelol.com   Skype - imagelol  "
"Looking for a programmer for a LAMP + AJAX website. Charity website. We're a nonprofit startup, remuneration is negotiable.","I am looking for someone to work on two projects I'm developing, both are non-profit related. The first one is almost finished. If you are good with PHP drop me a line, I am hiring part-time. I am a student and have saved some money to work on this. Pay might not be the highest but I commit to a fair internship-like remuneration, let's talk. Also, both projects are aimed at helping NGOs. Thanks!(If this is not the best place for this post could someone please point me to the right direction?)  EDIT : Some scope:  The first project is called TopThatDonation.com, you can see a preview here:  Design sucks, it was just a quick scratch created to give programmers an idea of how it should work.Companies or websites donate and they are ranked according to how much they donated. Companies can upload their logos or just write a text which links back to their website. Every month we will donate the total amount to a different NGO. The idea is to give companies batches they can use on their websites.  The site will run on LAMP and use some AJAX.  TL;DR: Looking for a programmer for a LAMP + AJAX website. Charity website. We're a nonprofit startup, remuneration is negotiable. "
Where can I find info about Python/Django specifically relating to making a game with a static HTML front-end?,"I would say that I am a novice programmer(including python) that simply likes to fiddle with programming as a creative outlet.  I'm an IT professional, but never was schooled in much programming.  My experience with Python is limited and does not include web apps. I have zero experience with Django.  I have experience with PHP in writing webpages that interface with a SQL database, and do some simple mathematical calculations and display to an HTML page.  For learning purposes, I want to develop a web-based game (a Risk clone) with Python, and I hear that Django is a good framework for web-based serverside work.  This game would have a static HTML front-end based on an SVG ""map"", and would need to be able to work with a SQL database.  Does /r/python have any specific resources that might include examples of what I am looking to do (server-side python game with HTML front-end, interacting with SQL)? My google-fu is not good this morning, or is it that I need more coffee?  TL;DR Where can I find info about Python/Django specifically relating to making a game with a static HTML front-end? "
"Want to make a scalable site, want some information/tips. [Editted: Formatting]","I have been interested in programming since I was 11 (Now I'm 19) and I have got quite a lot of experience with PHP and done a few freelance jobs. But I have never really attempted a project of my own because I felt like there was always  better  versions of my idea out there. (+ It would be a great hobby whilst starting University in 2-3 weeks)  However I think I have found a niche, and feel quite passionate about it and wish to develop this idea PROPERLY. When freelancing the jobs I've done haven't required a lot of planning or extensive research etc they're usually black and white.  What I want to know is:   How do I keep things scalable?   How long is  long  when it comes to page loading?   What are the best practices for large scale projects?   How do I check for vulnerabilities, as I expect to be handling user information. SSL is obviously a must, but what code security can I implement to make sure the worst doesn't happen?   I want to do this properly, so I want to mock up a project scope. What is the best way to plan a large project to keep things organised?    Sorry for the wall of text!tl;dr - Want to make a scalable site, want some information/tips. [Editted: Formatting] "
"I want to find Douglas Crockford's [JavaScript: The Good Parts]( but for Php. 
 Edit: I accidentally a word","I have been programming for several years now, Php being one of the first languages when I started to be interested in coding. I have some deeper knowledge of the underlying concepts in Java and Python and am currently starting to understand the core of JavaScript. Php however has always been a language where I went for quick & dirty solutions and copy/pasted my scripts together. I would like to change that.  A book I am reading at the moment is Douglas Crockford's brilliant [JavaScript: The Good Parts]( that focusses on how to write good JavaScript and how to avoid badly designed features of the language. I am searching for a book like that, just for PHP. There is ""Php: The Good Parts"" but it hasn't received good reviews.  I want to learn and understand everything that was introduced after Php5+, want to learn best practices on how to structure larger Php projects. I don't want to learn MySQL for the billionth time, I don't need to learn about every single php library out there. Is there a book like this?  TL;DR I want to find Douglas Crockford's [JavaScript: The Good Parts]( but for Php.  Edit: I accidentally a word "
"Any ideas for refreshing PHP knowledge? 
 Edit: Thanks for all of the advice!  Does anyone know if there is a sandbox like jsfiddle for PHP?","Hey, I used to do some PHP stuff for fun and I also did some PHP in college.  I have never done any professional work and one of the guys has requested that I do some light PHP work for them (our current environment is ASP).  I asked if it was super advanced work because I wouldn't want to let them down and he said no it wasn't.  He knows that I enjoy development work but that I haven't really worked with PHP in a little while.  My question is if there are any suggestions to refresh my knowledge of PHP.  I have an account on net tuts premium and I've been going through that but it seemed pretty basic.  TL:DR  Any ideas for refreshing PHP knowledge?  Edit: Thanks for all of the advice!  Does anyone know if there is a sandbox like jsfiddle for PHP? "
"How make this: 
 &lt;option title=""Title!!"" value=""title""&gt;Title!!&lt;/option&gt;
=
echo
    &lt;h2&gt;Title!!&lt;/h2&gt;
 
 ?","So basically, my table is filled with about 150 records with specific tags attached to them (some only have one tag, others have up to 7). My table has the tags listed in simple form so that I can run a query of SELECT * WHERE tag LIKE '%$tag%'. Since I only have a total of 12 tags, I manually input my tags into a dropdown menu, displaying a varchar long version of the category.  So, here's what I want to do. Since I'm writing this to be 508 compliant, I have title tags in my dropdown options. What I would like to do is to echo that title in an H2 when that option is selected. I could easily write 12 'if else' statements for each tag, but I'd rather just let the PHP do it for me. Ideas?  tl;dr:  How make this:  &lt;option title=""Title!!"" value=""title""&gt;Title!!&lt;/option&gt;=echo    &lt;h2&gt;Title!!&lt;/h2&gt;  ? "
Drunk midgets fight robot bears to the death over who can scrum the hardest,"I am a programmer who has just started getting into web development. Most of the stuff I write is physics libraries or other low level code, so I already know how to program, I am just trying to pickup a new language.  Here is my question. As an exercise in teaching myself PHP I want to create a simple website that will assist me in searching Craigslist. I want this website to be able to accept different search parameters and then go to CL, search its listings and return back a custom formatted result. I need to know what general aspects of PHP I should be researching to get there. I can already build a basic page in PHP but I am looking to do more. So, where in the PHP manual should I start?  Thanks in advance to all the PHP masters who might help me.  tl;dr Drunk midgets fight robot bears to the death over who can scrum the hardest "
need php file manager with user access control that runs in ie6.  upload progress a big plus.  Do you know of one?,"Please,  I have a client with multiple international offices, each behind separate firewalls.  They're too paranoid for dropbox.  Their solution is to build a file server that can be accessed from the web.  Rather than teach them how to use FileZilla, I'll take their money and say thank you.  RBAC + CRUD for directories and folders isn't too exciting.  I'm sure it's been done before.  Do you know of a library out there that already does this?  Something like  Relay [ but less hideous and more user access control.  I don't need a windows-style directory tree.  Javascript gives drap & drop and/or upload progress (files might be 100mb) would be nice.  I'm betting some of their networks are still running IE6 so I'm not counting on it.  My biggest concern is uploading large files, then providing feedback.  If all else fails I can write it myself.  I'm confident you've seen (or have!) the code to do what I need already.  Hints, anyone?  Thank you!  TL;DR: need php file manager with user access control that runs in ie6.  upload progress a big plus.  Do you know of one? "
"a script that grabs an image that matches an item, downloads onto server and saves source, is it possible?","Hello /r/php  I'm in the stages of evolving my code and creating a site. I have several functions I want to create, and be as effective as possible for the end user.  I'm wondering whether it's possible to have a script that auto adds an image to a name/item. Eg. user types in Lego, and a script will run and find an image (like take the Wiki image, or first image in a Google search) that matches the item. It should download the picture onto it's own server, and save the source.  Is this possible, and is there any scripts out there that does this or something similar? - So I can read/get ideas of their code.  I want this so a user doesn't have to find an image herself, though optimally there should probably be an option to mark it as ""inappropriate"" if the ""autoImage"" doesn't match.  tldr; a script that grabs an image that matches an item, downloads onto server and saves source, is it possible? "
"Flex SDK 4.1 compiler is slower because it goes over every file in your source path, and not just relevant source files like it did in 3.5.","We recently moved our project from Flex SDK 3.5 to 4.1 at my workplace, and to our dismay the compile time quadrupled, even when the changes are tiny. When sword-fighting while waiting for the compiler to finish got boring, we checked what exactly the compiler's doing, and discovered that every time we compile, the Flash builder  goes over every directory and file  under the include path and checks for changes, which unfortunately includes lots and lots of SVN files. 3.5 did the same, but ignored the SVN files. We couldn't find any configuration parameter to revert this dumb behavior, and there's no telling if and when Adobe will fix this.Be warned.  tl;dr: Flex SDK 4.1 compiler is slower because it goes over every file in your source path, and not just relevant source files like it did in 3.5. "
"Need a recommendation for a security book with in-depth information about buffer overflow attacks, exploits, and prevention techniques.","Proggit, I am an avid reader of reddit (mainly /r/programming) and could really use some expert help.  I am in a Master's Program for Security and as an independent study for this coming semester I decided to learn about buffer overflow attacks, exploits, and prevention techniques, but my professor said that most of the learning would be 'on my own time.'  Because of this I really would like to maximize this time and was looking for a good book to work with to help me tackle this topic.  I found [Buffer Overflow Attacks: Detect, Exploit, Prevent]( after some searching and that seems to be the only book I can find that is completely dedicated to this specific type of information.  The only issue is that nearly all of the reviews for the book say that there are multiple errors with the information.  My biggest issue is that I would like a book with some large culmination of data about only buffer overflow techniques and methods.  I really don't want a typical security book that goes over buffer overflow attacks in roughly 1-3 pages without any in-depth information about it.  Proggit, I'm asking for your expertise to help with any book recommendations that I can look into to help me learn about this topic.  Also, wasn't sure if I should have posted this in /r/netsec so if that's where this should have gone, my apologies.  TL;DR:  Need a recommendation for a security book with in-depth information about buffer overflow attacks, exploits, and prevention techniques. "
"I want to make games, but get hung up in every project I start. I think I'm wasting my time.","I've been a hobbyist programmer since I was 16. I've always enjoyed programming. I've read countless books and hacked out tons of code for fun and occasionally for work. I know four programming languages well enough to dive right in and use them, and have programmed for two operating systems. I have a pretty solid understanding of basic to intermediate algorithms and data structures (lots of reading).  I've always loved video games (who doesn't!). Most of the ""hard"" work I've done in learning programming has been to make games. Most of the time it's in searching for a different angle of inspiration for me to get my butt in gear, but I've had the tools I needed to make games for years.  The problem is that I just haven't ever finished anything. I've got dozens of projects in various states of completion, and they all have various problems that give me hangups about them. I'm slightly OCD, and if it's not right, it's not right.  One of my most common hangups is art. I'm terrible at artwork, and I have trouble fitting the sparse free artwork together into a game. I've tried looking for artists, but the ones who have made art for games before really aren't interested in working with me. I'm not picky about art. I can make 8-bit rainbow pony adventures for all I care. It just has to fit the game somehow. If it doesn't fit, it agitates me.  I'm 27 now, and I'm working and married. I don't have children (yet), but even now I just don't have time to do anything. The task of making a game myself just seems so daunting now. I don't have the hours and hours of time to throw at fun programming that I used to. I have friend obligations and a wife that starts to think I'm mad at her when I don't spend a couple of hours with her every day.  I'm starting to feel like I'll never get there. I look at the stuff lots of indy game developers have made and I just wish I could do that too. I know how they do it, I just don't know how I could do it. I feel like I'm wasting my time.  I don't even have a question... I just don't know where else to say it.  TLDR: I want to make games, but get hung up in every project I start. I think I'm wasting my time. "
I want to build a fairly in-depth website and am willing to learn how. What should I start with and where?,"As the title suggests, I'm hoping to build a website. I have some programming experience, but there are so many different aspects of building a website that I don't know where to start. I understand that it's my no means an easy thing to do and that people make a lot of money doing it. I'm hoping to build a fairly in-depth website, not just a blog sort of thing, and I realize that it will takes tons of work before I can truly even get started. I'm looking for pointers on what to learn first, and where I can learn it.  tl;dr: I want to build a fairly in-depth website and am willing to learn how. What should I start with and where? "
"I need a better book to learn Python than Lie Hetland's ""Beginning Python.""  Any suggestions?","I'm 27, was unemployed and had lots of time on my hands so I decided I should be productive.  Decided I'd try to learn to program.  After lots of research I decided Python was the language I should go with.  I did lots more research and found a bunch of books on Amazon with equally good reviews.  I ended up getting ""Beginning Python"" by Magnus Lie Hetland.  It says it's for beginners in programming, but it very quickly got over my head. Beside this, the examples and practice activities seemed pointless -- busywork that didn't lead to any type of product.  I wanted a book that would let me see how combining each concept with previous concepts could build something useful. The book might be good for some, but it didn't suit my learning style.  Hetland showed me all the parts of the car but didn't really show how they are put together or what kind of cars I could make.  I've seen another book intended for children that seems somehow attractive to me.  Then I thought I'd ask /r/Python.  TLDR: I need a better book to learn Python than Lie Hetland's ""Beginning Python.""  Any suggestions? "
What skills should we teach at technical school so students can actually be hired out-of-the-box?,"I am a college teacher and I am currently formatting a Technical School course, and would like your feedback on what topics should be covered to provide a practical education that doesn't suck.  Some background: I work at a Brazilian NGO that provides courses to poor teenage kids in order to give them professional skills in software development. The students are sponsored by local software companies that hire them after a 5 month course. The current course today comprises five 90-hour modules:   Introduction to programming (usually with python)  A programming language (we already used Java, C++, VB.NET, C#, Pascal, it really depends on what the sponsors are using. We even used COBOL once)  Databases and SQL  Information Systems programming (they learn how to create a business app using the same language)  A project (they have to write a larger app in groups of 3 or 4, each group writes a module of the same app)   Now we're thinking of taking things to a new level, and extending the duration since when they finish the course they still need some months of in-house training. We want to run another six-month course that simulates a software company, developing software for our sponsors (the goal is learning as closely as possible to the real thing)  I see that the universities (at least here in Brazil) largely ignores issues as simple as concurrency control, source control, issue management, time tracking and other odds and ends in software development.  TL;DR: What skills should we teach at technical school so students can actually be hired out-of-the-box? "
"article tag = web page - ads = $0 for ads; therefore, make article tag = article + ads. 
 Your thoughts?","The primary benefit to a user viewing an HTML5 document with article, aside, section, header and other semantic tags is that their browser can intelligently handle this information to make reading easier for the user.  As a programmer, first thing I'd do with this is write a plugin which extracts the article and formats it nicely, ad-block on crack.  As a programmer in the employ of an ad company, the first thing I'd do is try and stop this.  The only real option would be obfuscating the ad content in such a way as to seriously reduce the meaning/usefulness of the semantic tags in extracting only the articles, asides, headers, etc with out also extracting whatever else it is that one wants to force the user to see.  tl;dr  article tag = web page - ads = $0 for ads; therefore, make article tag = article + ads.  Your thoughts? "
Can anyone explain [advogato trust metrics]( with code examples? I would also appreciate links to learning how to implement semi/full autonomous moderation systems that scale.,"I'm trying to create a content submission system, mostly text based in the form of instructions to building small things. Example:  (I'm hoping not to require as much man power like that!). Like that, it's different to reddit in that it's not supposed to fluctuate like news every minute.  My main concern is attacks and ongoing attacks. So after some searching, I came across this:  and  ([Google Video]( This seems great in theory! But I can't find any implementation guides or lessons.  The areas of website to be moderated would be:   accounts: Profile details, I could just make it empty like Reddit   comments: Can allow author/category-moderator to moderate but I'm concerned with Youtube-style horror when it scales up  articles: Most important. It may have pictures but even that has potential for abuse. So initially, it would be mostly text-based. But how to take hands-off moderation like Wikipedia?   tl;dr: Can anyone explain [advogato trust metrics]( with code examples? I would also appreciate links to learning how to implement semi/full autonomous moderation systems that scale. "
Can anyone explain [advogato trust metrics]( with code examples? I would also appreciate links to learning how to implement semi/full autonomous moderation systems that scale.,"I'm trying to create a content submission system, mostly text based in the form of instructions to building small things. Example:  (I'm hoping not to require as much man power like that!). Like that, it's different to reddit in that it's not supposed to fluctuate like news every minute.  My main concern is attacks and ongoing attacks. So after some searching, I came across this:  and  ([Google Video]( This seems great in theory! But I can't find any implementation guides or lessons.  The areas of website to be moderated would be:   accounts: Profile details, I could just make it empty like Reddit   comments: Can allow author/category-moderator to moderate but I'm concerned with Youtube-style horror when it scales up  articles: Most important. It may have pictures but even that has potential for abuse. So initially, it would be mostly text-based. But how to take hands-off moderation like Wikipedia?   tl;dr: Can anyone explain [advogato trust metrics]( with code examples? I would also appreciate links to learning how to implement semi/full autonomous moderation systems that scale. "
"It's late, and MacArthur may have made the best quote to describe Iterative deepening depth-first search.  /r/programming may or may not find it interesting.","As a student in COSC, I can only imagine how tiresome it can be to grade student's code.  So I like to add comments to try and cheer everyone up.  While doing a project in which the exercise was in developing a [Iterative deepening depth-first search]( in which you traverse a tree using depth first search  until you've gone down a max number of levels, if the goal still hasn't been found the search is repeated, but this time the search is allowed to go down one more level.  During my search, once the deepest node is reached, I wanted to leave a comment saying something about the search is leaving, but may return.  Which immediately made me think of Douglas MacArthur's quote  > ""I said, to the people of the Philippines whence I came, I shall return. Tonight, I repeat those words: I shall return!""  And I thought it was clever in that the search is not guaranteed to come back, much like MacArthur never returned (well not right away).  tl;dr It's late, and MacArthur may have made the best quote to describe Iterative deepening depth-first search.  /r/programming may or may not find it interesting. "
"Are there any well-run portals/lists/agencies that specialize in short-term fixed employments, anywhere in the world?","I’m a rather seasoned (umami, mostly) developer/architect/copywriting/marketing guy, entered IT about 15yrs ago, with a Masters degree. I’ve been running my own one-man agency for a couple of years, but business has been slow in recent months while expenses have risen. (Living at bare minimum, but with wife & kid, there are things that  need  to be bought.)  So, some debt has accumulated. Not much, about €3000, but that’s enough to make me want to find a short-term  gasp  job, limited to about three months, nearly anywhere in the world, with which I can rake in enough money to work in a structured environment, earn living expenses and cleaning up debt, so that I can re-enter my own business with an upright spine again.  So … I checked Monster etc., found 99% fixed contracts, which is not what I need. Then I checked elance and found that 99% of providers charge less per week than what we would need per day.  TL;DR:  Are there any well-run portals/lists/agencies that specialize in short-term fixed employments, anywhere in the world? "
"I want to work on an independent project all summer, how can I best prepare?","I am graduating next year, so this summer and next are really the only time I'll have 3 months with nothing to do.  Instead of a regular internship, I'd like to work on an idea for a project management web app I've had lately.  I plan on using a desk at my dads office and putting 40 hrs/week in to make it like a real job. Ideally, I would finish the summer with a 1.0 version of a product.  My question to r/progrramming is, how can I best prepare for the summer so that I actually am productive and don't just waste my time?  tl;dr I want to work on an independent project all summer, how can I best prepare? "
I’m freaking out about testing our companies web interfaces (SOAP and REST) and looking for advice.,"I work for a small start up dealing with web services. I started working for them fresh out of school, CS major, as a black-box tester. At that point there was little of a QA department. Since then we have started to develop a gray-box test department consisting of...me. The company had an existing SOAP interface which everything ran on top of. To keep up with the times they have started rewriting the core code base, everything from database to web services, and are  adding a REST interface. Here is where I come in.  I have been tasked with testing the new ‘core’, mainly through the SOAP and REST interfaces. There are a lot of unit tests written by development so I’m mainly focusing on functional tests from the perspective of the API consumer. The trouble is both interfaces are quite extensive. We started out using SOAPSonar to implement our test cases but I’m not impressed with the way it is turning out. Honestly, the tool is giving me some doubts as I use it more. It’s sometimes unreliable and modifying it/extending it seems like a lot of maintenance work. All of the projects are stored in a special project file and are a pain to combine. I'm just not satisfied with what's being produced and what I'll leave behind. Overall I’m looking for another option.  Some more background on me. My overall goal is to test these new interfaces with a lot of coverage and do my job as QA engineer. At the same time, any more code writing I can do along the way to show case that would be helpful. I would like to eventually work on the web service or apps team while leaving behind a nice, well implemented, and semi easy to maintain testing suite.  The two options which I’m considering are reimplementing my test cases into soapUI. The tool seems much more mature than SOAPSonar, and extensible. Or I am looking at implementing the test cases in vs2010 and using something like NUnit. I like the second option because it seems easier to extend and modify. Also, I feel like it would have a cleaner look to it and be easier for what I’m trying to do.  If anyone has done some related work, or has advice in general, please let me know.  TL;DRI’m freaking out about testing our companies web interfaces (SOAP and REST) and looking for advice. "
"For the keys of an assoc. array, numbers with a leading zero were interpreted as strings, but numbers without a leading zero were interpreted as integers. Why?","I was working on some code for a courthouse website that would take the filenames of court document images, determine the book # and page # from the filenames, allow the user to select the book and page to view, and then retrieve the file based on the user's selections.  The part of the code that read the book numbers into an array looked something like this:  preg_match('/^DEED-BOOK-(.*)\.TIF/', $filename, $match);$book[""{$match[1]}""] = ...  The problem was that some of the book numbers in the filename had leading zeros, and some did not. When the variable was expanded for the assoc. array's key, the numbers with leading zeros were interpreted as strings, but the numbers without leading zeros were interpreted as integers. Needless to say, this screwed my sorting all up.  I tried forcing the key to be a string:  $book['' . (string)$match[1] . ''] = ...  but it still was stored in the array as an integer. I tried forcing quotes around the variable:  $book['""' . $match[1] . '""'] = ...  which did make it a string, but the string had an extra pair of qoutes:  $book[""""123""""]  I finally had to add the letter x to the beginning of the number, then ltrim it off before trying to match the number to the file name. This works, but is hardly elegant.  What gives?  TL;DR For the keys of an assoc. array, numbers with a leading zero were interpreted as strings, but numbers without a leading zero were interpreted as integers. Why? "
Thinking of changing to Data Science from Software Engineer. Will my career opportunities be just as great over the next 30 years with Data Science as with Software Engineering?,"I've been working as a software engineer since graduating from college with a BS in CS for 5 years now. After being in the industry for this long I've been really trying to identify my career goals. While I love working in this industry, I've noticed a pattern emerging in the projects which have lead to big wins in my career. All of these involve data analysis and not core large software development. I've been learning more about Data Science and the more I learn of the field the more I think it's the right field for me. I love data, looking for answers in data, and creating visual ways of conveying the results to diverse audiences. I still love programming and use it to write tools which help with data collection and analysis, but something just hasn't clicked. My husband is concerned that if I leave software engineering I will be limiting my career opportunities over the lifetime of my career. He's also concerned that I currently have a wonderful manager and I should stay with her as long as I can. My managers have changed about every 6 months during my career, so I don't feel confident in choosing to stay just for a few individuals.  TL;DR; Thinking of changing to Data Science from Software Engineer. Will my career opportunities be just as great over the next 30 years with Data Science as with Software Engineering? "
Where should I tell my users to put the library file to interface with my SaaS on their server?,"I have a PHP library that interfaces with  my SaaS  will extend Zend_Captcha_Base. Having not really worked an extensive amount with Zend I have a noob-ish question ... where should I tell my users to upload the library files to?  I have read the Zend documentation and a tutorial and it seems as though I can tell them to put it in the Zend Captcha directory, a sibling directory to the Zend directory in their application's library directory, or just somewhere else on their server that they can reference it. The thing about the first option is that whenever they update the their version of Zend they could easily end up losing the library file and messing things up. I'm leaning towards the second option but just want to make sure I am doing things the correct Zend way.  TL;DR Where should I tell my users to put the library file to interface with my SaaS on their server? "
"Wordpress is cool and I love it, but I think I may have accidentally 
 Edit: Wasn't sure if to post in here or in /r/Wordpress","I've been using WordPress for years now, first started when it was in version 1.5  I'm  so  comfortable with the platform, find hacking around in themes & working with bits of custom PHP generally pretty easy to deal with. I'm very much front-end when it comes to web development, but I've picked up bits of PHP here and there.  I've never really felt I could feel the the walls  surrounding the WP platform, but on this latest project I'm working on I'm really starting to get frustrated with some of the limitations. For example   Lack of native support for filtering by categories & tags.  I'm using TDO tag fixes for getting around this issue for now, but it's not perfect, and it's pretty tricky to get everything working the way I want.   Lack of options for (decent) email communications  I'm still struggling with this. The email platforms for Wordpress leave a lot to be desired, so it looks like we're getting something custom-built that will either integrate with an existing email supplier, or send from our servers directly.   Overall speed performance of the platform  Now this one isn't completely down to Wordpress itself, as this latest site is pretty complicated & I've still got a bit of work to do tidying up the front-end code, but because of all the plugins & hacks we've had to make to date, the site is loading  real  slow and I'm sure it's having an effect on conversions & search positions.    I'm looking into CDN's to try and get this sped up a little more, but I'm struggling to get my head around AWS still & it's taking a while for me to sort it all out.  I've been looking into frameworks, seen Zend mentioned a couple of times in /r/php - is this the way I should be going?  I guess I'm a little intimidated at having to learn a whole new platform when all I've really been using for years now is WordPress - I've played with other platforms, sure, but WP is where I feel at home.  How easy is it learning Zend to the point of being able to work with it and launch bespoke platforms? How far removed from the Wordpress  ethos  is it?  I'd appreciate any help really on this as I feel rather like a fish out of water.  tl;dr Wordpress is cool and I love it, but I think I may have accidentally  Edit: Wasn't sure if to post in here or in /r/Wordpress "
I want to compute a more accurate estimate of π using Python 3 as a fun project.,"Like the title said, I want to compute π to as many digits as possible using Python 3. I'm a 'beginner level' programmer who just started learning python this past summer and the only other programming I've ever done was on my Texas Instrument TI-86 calculator (my first piece of code was a dice rolling function for D&D =D ) and some design with html/css/php. I also have about the same level of experience with TSQL. I really enjoyed how easily Python was to pick up, and it's just plain fun to work with.  I just learned about python's built-in gc (garbage collector) module which is a memory management solution. It made me start thinking how I could utilize this or something like this in some code that would otherwise be unapproachable due to it simply eating all of your memory. Pi popped in me head, t's infinitely long, so I could try forever and never find the last digit. How long can I go with some nice code that is able to recycle memory and return the solution to an output file in continuous increments? Hopefully, in doing this, it would be able to continue calculating an even more accurate estimate of pi.  How do you think I can accomplish this without requiring the use of a supercomputer? I hope I'm not sounding naive when I post this. I realize the superiority of supercomputers and I'm not attempting to break any records by any means, I just thought this would be a fun little project that might hone my skills in Python.  tl;dr I want to compute a more accurate estimate of π using Python 3 as a fun project. "
can't find a good solution to create a user input generated PDF from a pre-built template - with a low learning curve,"I have a template (in html) of what I'd like my PDF file to look like.  I want to auto-fill the PDF with an input from a form and allow the user to download it (It's a Bill of Lading, for what it's worth).  I've tried these options:  FPDF - too cumbersome, we are budgeted for 12 hours or so of work, and working with X,Y coords is time consuming.  dompdf - Very little docs, but got it working anyway, but doesn't format my CSS, weird table breaks, etc.  tcpdf - seemed the most comprehensive of all of what I found, but converting to HTMl didn't work.  Also, I recreated part of it in tables, and it would just not print parts of it for some reason.  Couldn't get the CSS parsing to work.  ppdf - couldn't get it working on the local machine and when I finally got it up on a dev server it too didn't parse my xhtml at all.  I'm willing to spend a couple hundred bucks on a good solution, as long as it saves me some time, I just don't know of any, free or otherwise.  Any suggestions?  I'm not sure how well Adobe works with PHP, can I do merge forms or something?   Any suggestions are greatly appreciated!  TLDR : can't find a good solution to create a user input generated PDF from a pre-built template - with a low learning curve "
The new features in PHP 5.3.x are awesome... some of us are stuck at 5.2.x for a number of reasons.,"I have been a PHP developer for more than a decade. 95% of that time has been spent as a freelancer because I earn more money running my own business than I would as an employee. Most of that money is earned in the form of ongoing support for custom applications and hosting. Some of my clients have hundreds of domains with a multitude of business models. Some of them were clients that needed someone to install a third party script that needed to be integrated into their site or their site integrated with a third party script... sometimes they needed multiple scripts integrated. Some of these third party script are encoded with an older version of Zend Guard and the developer is AWOL... so there is no way to obtain a version encoded for the newest version of Zend Optimizer. If I update PHP to 5.3.x, Zend Optimizer throws an error that the script encoding is outdated.  Some of you posting here love to make snide remarks about using namespaces or other cool new features in 5.3.x without realizing that there are a great number of us stuck in similar situations. Yep... it's way cool to use these new features... but it isn't cooler than the 5 figures a month that I earn by supporting these clients... and I don't lose a single hair off my ass because of it.  Thanks for reading...  TL;DR; The new features in PHP 5.3.x are awesome... some of us are stuck at 5.2.x for a number of reasons. "
"Cannot run python-nest on MacOS because of SSL, Xcode, and general Mac nonsense, can virtualenv be updated independently on a system to deploy on other MacOS machines?","Please, please, please can someone point me in the right direction on my problem.  So I am building a set wrappers for API's for a Home Automaton/Control system which is Mac based.  So far all is good, but today I started to build a Nest thermostat wrapper and started to use python-nest installed with pip. What happens is that I got an SSL error from requests, and down the rabbit hole I went ...  I managed to find out that in Python <2.7.9 SSL in requests have a vulnerability and throws an error, to fix it was to upgrade Python (not possible) or install requests to the latest version. When attempting that Mac throws an error when using PIP or easy_install with clang cannot install, which lead to installing Xcode. Installed the version relevant to my MacOS version and had no effect.  Now I don't know how to do this, but can I install a newer version of Python (2.7.9) without the SSL security flaw just into my virtualenv project that I am working in? So I can deploy my virtualenv onto target machines and run isolated from the MacOS Python?  TL;DR: Cannot run python-nest on MacOS because of SSL, Xcode, and general Mac nonsense, can virtualenv be updated independently on a system to deploy on other MacOS machines? "
I want to go to a top data science PhD progam and am looking for tips to be competitive.,"Hi everyone,  I'm a junior computer science major at the University of Oklahoma looking for PhD programs to apply to next year, and I've got some questions about what would make me the best possible candidate.  I'm really passionate about data science and want to go to a top program (I've had my eye on Stanford, Caltech, and Berkeley) extremely badly. I've got a pretty reasonable GPA, will graduate either magna cum laude or cum laude (not sure on the specifics) and, like I said, I'm really passionate about this field.  My central question is this: when applying to PhD programs, one tip that I always seem to find is that I should have research experience. I'm currently working on some pretty basic projects ([see my recent post]( and am looking for advice. Should I continue working on this stuff? Or should I just try to find a professor to work with? Is it important to be published? What other things could I be doing to make myself as competitive as possible for these programs?  I've also heard that my letters of recommendation will be important; I've got one from the dean of OU's honors college already, but where else should I look to get them? Would it be appropriate to ask the professor I'm TA'ing for one?  Quick edit that I forgot to ask: what's the typical route to a PhD program? Do you go there straight out of undergrad or do you pursue a masters first and then apply to PhD programs?  Thanks for taking the time to read this.  tldr: I want to go to a top data science PhD progam and am looking for tips to be competitive. "
Which commenting system would you recommend for Django with Python3 that has an  effective  Spam blocker?,"Hi everyone. I was wondering if anyone had any experience/recommendation for commenting systems for use in 2015 with Django.  There's a comparison chart [here]( for Django based solutions, and I guess then there are the three big Javascript based ones, namely Disqus, LiveFyre and IntenseDebate. I'd like to use Python 3 so that would narrow it down to something like   django-threadedcomments, django-disqus, djsango-fluent-comments, django-comments-xtd (Django)  Disqus, LifeFyre, IntenseDebate (JS)   My main concern is with spam, I don't want to be sifting through and moderating comments all the time, so it would be great if you could post your stories about the effectiveness of the system you're using.  My first choice originally was Disqus, but I noticed it would hog up loads of resources when opening multiple tabs of sites running it, causing unresponsive script popups and general slowdown. So I'm having a good look at the alternatives. Any similar experiences?  TL;DR : Which commenting system would you recommend for Django with Python3 that has an  effective  Spam blocker? "
Is it beneficial to group everything in one self-executing anonymous function and how would I go about doing this?,"I want to avoid having external scripts interfere with our jQuery implementations and also improve load times and not have so many JS files.  I've been digging through Stack Overflow and reviewing some source code for popular projects (jQuery, Bootstrap, etc) and I've been told that  the best way to 'uglify' multiple js files into one  is to...   use a build system like Grunt / Gulp + Concat  have the first file contain the opening of a self-executing anonymous function with your dependencies (in my case it's jQuery) like so:  (function ($) {  concat the other files with their various js components  and finally include the closing statement of the self-executing anonymous function:  })(jQuery)   I'm doing this all to improve my site's load time by loading one javascript file.  tl;dr  Is it beneficial to group everything in one self-executing anonymous function and how would I go about doing this? "
Can I make UrlFetchApp fetch data from a url using | as delimiters.,"I try to fetch data to a googledoc spreedsheet using the command UrlFetchApp.fetch .  The problem is that it is giving me argument error messages for building an API adress that could look like this:  I the sites apireturn expands using  | as delimiters. It appears that UrlFetchApp doesn't like that type of url.  So my question is: Is there any way to circumvent it and still obtaining the data.  My script looks like this:  function GetID(names){  var name = new Array();  var dirtynames = new Array();  var cleannames = new Array();  var url=""  var url2=""&amp;format=xml"";names.forEach (function (row) {        row.forEach ( function (cell) {        });      });var o,j,temparray,chunk = 100;        for (o=0,j=names.length; o &lt; j; o+=chunk) {            temparray = names.slice(o,o+chunk);            var xmlFeed = UrlFetchApp.fetch(url+temparray.join(""|"")+url2).getContentText();            var xml = XmlService.parse(xmlFeed);            if(xml) {                for(var i = 0; i &lt; rows.length; i++) {                var nameid=[                           parseFloat(rows[i].getChild(""typeName"").getValue())                           ];                name.push(nameid);              }}}        return name;        }  So basically what I want it to do is to get a name from a cell, add it to the url, get all the data then return the name.  But yea, can't get pass the UrlFetchApp thingy since I am not really a javascript guy, I just pick up bits and piece from others.  If anyone got time to help me it would be greatly appreciated.  Have a nice evening and thanks for taking your time.  Tldr: Can I make UrlFetchApp fetch data from a url using | as delimiters. "
How would you implement an SPA where you have no access to the HTML of the page and data is loaded from remote? jQuery or MVC framework?,"I haven't had much luck on  StackOverflow  Consider this scenario: I have to implement a rather simple list view (read-only) including some filtering/searching mechanisms. The twist: I have no direct access to the site it will be running on.  The site it should run on is managed by a CMS and run by a different company. My company's job will be to provide said list view and an administration interface (on our own servers, where we have full and complete access), and to include it somehow into the CMS site. Their suggestion was iframe, I'm thinking Javascript app.  I have worked with jQuery and Ajax before, so it would be easy to generate the necessary HTML on our server and fetch it with jQuery. All that would be needed to include in the CMS is a script tag and maybe an empty div.  However, thinking about the filtering/searching, and making this accessible via fragment URLs, I've been thinking if an MVC framework coupled with a JSON API on our side would be a better fit (and a faster experience?). I have no experience with JS MVC frameworks, but watching a simple AngularJS tutorial, I'm not so sure it can be pulled off without access to the containing DOM. Ideally, I'd like them to only include a script tag, and have everything else under our control. As far as I understand, most MVC frameworks store their templates in the HTML body, don't they?  What's your opinion? Is there an MVC framework that would fit the description? Or would you go with jQuery + Remote HTML + history.js? Why?  In case it matters, this thing needs to run on mobile, too.  tl;dr: How would you implement an SPA where you have no access to the HTML of the page and data is loaded from remote? jQuery or MVC framework? "
"Need a method in python to convert text to html with different colors , tables etc.","Hello All,  This is my first post. I am working on a python project and have encountered a problem :  1] The project goes thru a list of log files and analyzes them and creates a ""analysis.txt"" file.  2] The ""analysis.txt"" file has the following items :  a] summary of the logs.  b] issues that might be found while analysing the logs.  c] warnings that might found in the logs.  3] Now the thing is this analysis.txt file is big and has many sections. So the user has to scroll all the way to get to important sections and in doing so might miss the important errors and warnings.  4] What i want to do is create a html report from this analysis.txt that has  a] table of contents on the top of the file so that the user can go to a particular section.  b] The html file highlights the errors by painting them in red.  c] Html file highlights the warnings by painting them in yellow.  d] if there is information that can be grouped together then it puts all the information in a table.  I want to write generic code like""text.toWarning()""""text.toError()"" etc.  that can then be extended by future users.  Is there any library in python that can help me get going , any suggestions highly welcomed.  tl;dr : Need a method in python to convert text to html with different colors , tables etc. "
boss wants to keep using legacy code instead of start over. It's making me tear my hair out. what to do?,"I work for a small (6 person) online business. I am the only development savvy person here, so I do basically everything. I inherited a ten-year old account/order/invoice system when I started working here a year ago.  Lately my boss wants me to implement things like discounts or coupons. Unfortunately, the old system is.. well, it's old. The code is awful - everything is manually implemented page by page. There are only classes for things I have updated for security purposes (i.e. CC processing.) There is no MVC architecture. There is no documentation.  Rewriting the entire thing (or implementing someone else's system) seems like the logical course of action. However, my boss just wants to keep using it the way it is. To me, this is just writing more bad code to patch up old bad code..  I'm going crazy. If you've ever dealt with this situation, what did you wind up doing?  Oh, one more thing. I live in a small town where there isn't a huge job market for IT, and I get paid decently well.. it'd be pretty tough to quit.. but I could if I had to.  tldr: boss wants to keep using legacy code instead of start over. It's making me tear my hair out. what to do? "
"I know the login password, but not the WPA key. How can I get to a login prompt if they have an ip filter?","Details: Its a Linksys WRT54GS. What I have learned is that the password is the default 'admin'. They have a list of ip address that can get access to the network (i would have to add mine to this i think) or i might not need to if i can input an ip range, i know how to do this stuff once i get access.  They have a WPA which is just a string of letters and numbers that i dont know...i would need to steal this once i log in. What im worried about is if they have an ip filter, how do i get it to talk to me so i can log into it. like i said the password is 'admin' so i just need to get to a login prompt and im good. the only way i know how to do this is by the standard ip address of 192.168.1.1 (which is the one they use).  Does anybody have any advice? I need to win this bet, because I know his network is vulnerable.  TL:DR: I know the login password, but not the WPA key. How can I get to a login prompt if they have an ip filter? "
Are programming language structures or formats subject to licensing terms?,"I'm trying to settle an argument about copyright, regarding programming languages.  It's well-known that to use someone else's software, you must follow their license. If I use PHP's interpreter, I must use it in the way in which their license specifies. Likewise for .NET and such.  However, if I'm using a  language , such as PHP, VB, C# or even HTML or CSS, am I subject to any licensing or copyright terms? This is irrespective of the compiler or interpreter terms. The question is specifically regarding the language structures and format.  This question is also in regards to what hoops an entity has to jump through in order to be able to compile or interpret a language developed by another entity. If I write my own C compiler, or PHP interpreter, am I subject to any terms, assuming I do not borrow any code or objects from the original compiler or interpreter?  tl;dr: Are programming language structures or formats subject to licensing terms? "
"For some reason people don't like me as a programmer, should I learn another language or create a project?","Hi reddit,I'm going to be a senior CS student next year and I had been looking for a summer internship all year-- I probably had about 5 interviews. At every interview I felt good, my resume had a 3.71 with a 3.88 major GPA and I'm a pretty sociable guy.  Many times I would get ""you were our second choice for our candidate, we'll let you know if something happens"" or ""We really liked you, but the person we chose said he/she could start earlier"".It was really frustrating because I know if I am just given the chance I can show how valuable I am.  I narrowed it down to two possibilities...I don't know too many useful languages(only java,C/C++,MIPS,MPI,CILK++) and I don't have any experience programming stuff outside of school work.  I was wondering, if you were me, what should I do over summer, take some time to learn a new language(perhaps a scripting language, a database language, or some web-based languages since those seem to be all the rage) or should I sit down, think of a little project, and code it in a language that I already know?  TL;DR: For some reason people don't like me as a programmer, should I learn another language or create a project? "
Building a CMS in Rapidweaver using Google Docs - looking for commenting and order tracking services or scripts .,"I'm rebuilding my website in RapidWeaver and decided that I wanted an easier way to manage the content. Current CMS systems require MySQL and are difficult to customize for someone who doesn't have a lot of experience in PHP.  Looking at Google Docs I found I was able to publish spreadsheets as a CSV. That gave me the idea that I could use PHP to parse the CSV and make the content dynamic. This is working beautifully. All I do is update the Google spreadsheet, and my site updates with the new data.  My ultimate goal in this is to create a downloadable RapidWeaver eCommerce CMS Document where the user can change a couple of variables and upload. Because of this I'd like to avoid using MySQL. Which brings me to my questions.   What easy/customizable/free/embeddable comment and product review system/script do you recommend?    Is there an order tracking service/script that can be embedded in a site that you can recommend?   tldr; Building a CMS in Rapidweaver using Google Docs - looking for commenting and order tracking services or scripts . "
Need an easy to read book about Object Oriented Programming.,"I've actually been doing object oriented programming a lot in University, but the one class where we were supposed to formally learn it was run by a complete fool, and I didn't actually learn a thing from him. Everything I know about OOP is that which I have read on the internet and just sort of figured out through best practice.  With my graduation looming I didn't want to get out of school and not have a formal understanding of it. So anyhow, I'm wondering if any of you have read a great OO book. I'm looking for something that basically just goes over the different types of classes (singletons, etc), and maybe some common practices. Doesn't need to be language specific or anything. Any suggestions?  TL;DR: Need an easy to read book about Object Oriented Programming. "
"Script above works, it could work better. Please let me know how I can improve.","I've been wanting to learn Python for years now; but, I was never able to commit to sitting down with the code. After AKH threw a bunch of false positives on my compiles I decided that I would no longer just ""want"" to learn it. I would.  I've never been one for books, I'm a throw me in sink or swim kind of guy. Recently (at work) we switched from a DNS based webfilter to a proxy based. On all of the machines (static IP) in the building we had the primary dns set to our web filter and the secondary to one of our domain controllers (there was no consistency). Also recently we noticed that some of the computers were pulling a system time with a good 10-15 minutes difference from real time. Since all of the computers have the wrong primary DNS and the secondary DNS is unknown changing the computer times was a pain.  I know if we had DCHP (dear baby jesus of IT please let this happen with our next server/switch refresh) changing the DNS would be no issue, but with everything static and about 200 computers there had to be a better way. I was told that changing DNS remotely is pretty difficult if not impossible. With a little Python Batch and PsExec I was able to throw together this script which allows me to launch a batch script through PsExec on the remote machine with admin privileges that was able to make changes to the DNS settings.  Since this is my first python script I am 100% sure that while functional the code behind it is probably trash. Python gurus of reddit, please take a moment to review my first script and viciously rip it apart. Let me know where I went wrong and how I could improve upon it.  TL;DR  Script above works, it could work better. Please let me know how I can improve. "
"Can I display an interactive window in Excel. 
 Thank you!","Hey guys, im currently working with excel during an internship. To make life easier I want to work to build a small add-in to display Information in the worksheet and run simple test.  I found the office.js Framework from Microsoft to work in Office-Documents but haven't found a solution to display Information regarding the file (e.g. if cell A1 ==""Factory A"") then the add-in should display a window showing Text (Information about the Factory for example).  Another question is that the add-in should open a form and save the input for later usage.  Is it possible to do that things?  TL;DR: Can I display an interactive window in Excel.  Thank you! "
retarded CS major doesn't know how to use GUIs. Needs to learn since everyone else doesn't know how to use CLIs.,"So I have a python project that I'm working on, essentially an IRC bot.  I've gone through many iterations of this bot and I figure I should finally add a GUI to it.  This is where I've run into a bit of trouble.  For my GUI I've chosen to use wxPython.  I don't know if this was a good choice or not. For some background, I'm a computer science major, so programming isn't a huge daunting task. I just always seem to struggle with GUI programming. (Why add a GUI when a CLI will work just fine?)  Is wxPython a good choice? Is there a better framework out there? Something with lots of documentation?  When using wxPython, my problem is: I don't know how to tell my wx.TextCtrl to grab the string data from my IRC class.  In my CLI, I just have a while(1) loop that prints out the data to the console.  I don't know how to stick this ""update the TextCtrl with the new data"" into the app.MainLoop().  I understand GUIs are supposed to be event driven, so do have an event sitting on a timer every .5 seconds to grab the newest string?  Can I make a new event that detects when new data has come into the stream?  Thanks for all the help though!  tl;dr - retarded CS major doesn't know how to use GUIs. Needs to learn since everyone else doesn't know how to use CLIs. "
Tips for learning Python for someone with previous experience with R?,"So on this subreddit, there's a common theme of knowing R and Python. I'm fairly new to the data science world, but it seems that my question might relate to many others. I'd say I'm intermediate-advanced in my capabilities with R and it seems that the next logical step would be to pick up Python. However, doing beginner Python tutorials are generally repetitive to what I already know from R (for loops, conditionals, etc.), and jumping right into reading Python scripts is too confusing. Do you guys have any tips on trying to pick up Python (and maybe DS-style modules like SciKit-Learn, specifically) for someone with experience in R?  TL;DR - Tips for learning Python for someone with previous experience with R? "
please give a brief description of libraries when posting links to their page,"This subreddit is great for posting when cool/useful libraries have been updated, but a lot of them I havent heard of or looked into before.  Being on a shotty connection on mobile, its not always easy to visit the link and see what the library really does or what it is good for, expecially on gethub repositories or confusing sites.  I would like to suggest posters put a short info blurb in the post so people can keep current with the scene without having to dig through another site.  Example:  title : pyFoo 1.0 Released! description: pyFoo is a library for creating example descriptions for others.  Not sure if this would be helpful to anyone else, so take it or leave it.  But its possible that others browse on mobile and have difficulty opening more than the comments too.  THANKS!  TL;DR : please give a brief description of libraries when posting links to their page "
I need someone to teach me the basics of JavaScript,"Hey,I need your help dudes.  I'd love to learn JS,but I don't know anyone who can teach me how to use it decently. The online tutorial thingys are kinda ""okay"",but it would be really nice if someone could answer all of my questions and clearly explain everything. (I mean the basics.)  So..Yeah,that's why I'm here. I'm searching for someone to teach me the basics of JavaScript,and to give me some beginner's tips. I do have previous experience with C++,so that helps out a bit. (the fact that I'm not a complete programming virgin)  I'll accept everyone's help as long as someone really tries to help me,and not just ""yea,I'll help you. noob. you suck."".  Thanks in advance.  TL;DR I need someone to teach me the basics of JavaScript "
"It's obfuscated, and I suck at Javascript. Is there any way I can de-obfuscate this manually without going through each array item and replacing in the code by hand?","A client of mine came across some strange files on their webserver and sent them over to me. The server seems to have been compromised, and the 'hacker' has uploaded some scripts which seem to be account checkers for mainstream services.  The meat of the scripts is in PHP, but I found this Javascript file which is obfuscated and seems to handle the processing between files (information is passed to another PHP script from the main one using Ajax).  I've ran it through JSBeautifier so it's easier on the eyes and uploaded it to Pastebin if anyone can give me any suggestions.  Note: The array of encrypted strings is just the hex of the letters. I didn't include the plaintext version, I've uploaded it completely as-is.  Tl;dr It's obfuscated, and I suck at Javascript. Is there any way I can de-obfuscate this manually without going through each array item and replacing in the code by hand? "
"I created a small and super simple PHP framework with my skills and knowledge acquired in three months 
 You can see the public repo on [Github](","Hey there!  Three months ago I started learning PHP out of curiosity, beginning with the basics, I spent a week having fun with it and exploring all the functions available, after that I felt the need to create something more concrete and functional, so I started learning OOP and then created a small CMS for my personal website.  I took a brief break from PHP and when I decided to return to it, I discovered Laravel and started fooling around with it. At first I was in love with its capabilities and solidness, but when I tried to create a personal blog I felt it was a lot more complicated working with Laravel for small projects.  So I started looking through Laravel's source code and some other small frameworks to study how they where built and all that stuff, after making tons of mockups of my framework I decided it was time to build it from the ground up with all my skills and knowledge acquired so far... and then Luna was born.  The prupose of this post is to obtain feedback from other more experienced PHP users and see how far this project can go.  TL;DR: I created a small and super simple PHP framework with my skills and knowledge acquired in three months  You can see the public repo on [Github]( "
How can I make this code alternate the forwarding between the two addresses? Or generate a more equal distribution between the addresses?,"I am working on a project in which I will have to publish a questionnaire on a website and have people fill it out. I have two versions of the questionnaire and they have to be randomly assigned to the people taking the questionnaire, and ideally I would end up with the same number of participants on each (the total number of participants will most likely be between 100 and 300 people). I could do as my peers have been doing for the past few months and manually change the link on the website every day, but I thought I could find a way to automate the process. I do not have much experience with js or with programming for that matter (other than playing games like TIS-100), but I am somewhat tech savvy and can usually find my way around things. What I have done so far is setting up a personal page (or whatever it is called) on github and used the following code, which I found online and changed a few things:  &lt;!DOCTYPE html PUBLIC ""-//W3C//DTD HTML 4.01 Transitional//EN""""&lt;html&gt;&lt;head&gt;&lt;title&gt;&lt;/title&gt;&lt;meta  content=""text/html; charset=iso-8859-1""&gt;&lt;script&gt;var randomlinks=new Array()randomlinks[0]=""randomlinks[1]=""function randomlink(){ window.location=randomlinks[Math.floor(Math.random()*randomlinks.length)] }&lt;/script&gt;&lt;/head&gt;&lt;body onload=""randomlink()""&gt;&lt;/body&gt;&lt;/html&gt;    The code works, and it seems to be sort of random, however I am not very happy with the distribution, as when opening 30 pages I have had distributions as skewed as 20-10. Is there any way to make this more evenly distributed? Or a way to make it so that every even click will lead to one site and every odd to the other? Or do you think this is just an issue of sampling and over the 100~300 clicks it should normalize? Or is there another simpler way to do this, using ifttt or something similar that I am not aware of? Sorry for the wordy post.  tl;dr  How can I make this code alternate the forwarding between the two addresses? Or generate a more equal distribution between the addresses? "
"Devs with consulting experience, how did you get started?","After reading the recent jobs posts and the rebuttal, it's made me re-evaluate my own job situation.  I have a BA in business, 12 years of web development experience and 9 of which were mostly PHP based.  Past 4 years, I've worked almost entirely as the Senior (only) developer on sites/web apps for a very large company (""enterprise"" experience).  I do everything from the frontend javascript to the backend coding.  I feel like I'm at a point where it's time to either start using the business degree to move up the food chain as an analyst or something, or be paid more consulting.  I think consulting sounds like something that could be fun, but I have no idea how you start down that path.  tldr; Devs with consulting experience, how did you get started? "
"Can't find a way to uninstall Python 2.5, can't find any sign of it on Windows machine even though it seems to be running.","First of all I do not usually use Python so I am probably going to sound ignorant to most of the people in this sub.....I was going to run a windows .exe that requires python 2.7 and wx.  All this refers to a Windows Vista Ultimate OS.  I installed 2.7 and wx (using windows installers provided). If I go to a command prompt and type python I get a python prompt but running version 2.5 (that I had installed some time ago).  I went to uninstall and start over but: I found my 2.7 installation and removed that with the uninstaller. I cannot for the life of me find where 2.5 is???? There is no folder for it. There is no entry in the add/remove programs dialogue, if I search the computer for files containing 'python' in the file name I get all the 2.7 stuff I installed but nothing that would indicate a 2.5 installation.  Yet when I go to the command prompt it is still 2.5. This is even after uninstalling/reinstalling 2.7. The only way to start 2.7 is to change to the directory it is in. However the win .exe I wanted to run that requires 2.7 with wx of course gives an error when I try to run it that wx is not installed.  So where is Python 2.5 and how do I remove it?  The only thing I can think of is that it might be pulling a version number from a registry value and that 2.7 is running but it is reporting the wrong version? But that is of course a wild ass guess......  tl;dr: Can't find a way to uninstall Python 2.5, can't find any sign of it on Windows machine even though it seems to be running. "
"experienced dev looking to stream video from IP to server to browser, how do I handle the server part?","Hey /r/php hopefully someone here can help get me pointed in the right direction.  I am working on this project at works that needs to be able to display an h.264 stream from an IP video encoder (a little device turns your normal security camera into an IP camera).  I can handle the display of the stream once I get it, but for the purpose of this project I need to use a PHP script for a proxy instead of just pointing directly to the IP of the encoder.  Can anyone point me in the right direction to be able to grab the stream with the server (probably via cURL) and pass it on to the browser so that I can use  as the url for the video player in the browser?  Like I said, I do not need someone to hand me a finished script (although that would be nice) but point me in the right direction of where I should be looking. I am an experienced PHP developer but I have never worked with streams before and I am a little clueless as to where to start.  TL;DR experienced dev looking to stream video from IP to server to browser, how do I handle the server part? "
can I use threading and bypass the GIL if my threads will only be used to spawn processes?,"I'm tossing around the idea of using threading vs. multiprocessing.  I have some code that will be concurrently spawning processes via subprocess.  I want the spawned processes to be truly concurrent (I'm using multi-cores/processors) and my understanding is that if I use Python threads, once a thread has spawned an external process, the GIL is no longer a factor because I'm executing code outside of the interpreter.  Also, I'm planning to share very little if any data between the threads.  Also, I'm open to any ideas/suggestions about this.  I'm aware that subprocess offers synchronous and asynchronous options.  I'm also using RPyc (which is amazing, not sure why it's not more popular actually) to run code on remote machines.  I also realize that I could just use RPyc for my concurrency needs but my code needs to be cross-platform and the only option I see there is with RPyc's threaded server. . .  tldr; can I use threading and bypass the GIL if my threads will only be used to spawn processes? "
"I'm basically trying to get Variables from one file to another, without opening and writing a file, memcache or sessions for module portability and page load efficiency.","Hey everyone,  I'm attempting to write a new module system, but keep running into old ways of thinking.  I'm going to do my best to present my X and Y.  I want to write self contained modules that perform all their own tasks when called.  These modules will be called from a master class which calls a viewer template that requests the modules.  Now the CSS /JS is obtained from the module needed and  assigned to a var, then returned to the viewer template.  Where it's compiled by the master class which assembles the header and calls a CSS/JS file (...src='file.js'...)which, is able to be compiled as php (and calls the modules css/js var) can then render the modules CSS/JS needs.  I find this allows me to create on demand CSS/JS files with minimalist CSS/JS size for faster sites in theory.  My problem is I have no way out of my limited thinking to pass the module needed CSS and JS from the module to the compiled without assigning the CSS/JS var from the module to a session or memcache. I hope I made this clear and would appreciate any help.  I have also used an the idea of building a query string that can be attached to the src='file.js?querystring=XXX' and have the modules build the string and the CSS or JS segments of the modules are in the CSS/JS files called instead, but I lose my portability I'm looking for in the modules.  EDIT:  For simplicity of the example, I have my index file and I have a CSS stored in $css and JS stored in $js.  Is there a better way to get those variables to the files in the index header <script src='file.js'></script> <link rel=""stylesheet"" type=""text/css"" href=""file.css""> both of which are actually compiled by php, without sessions or memcache?  I'm currently using a SESSION for testing, but I'm concerned.  And it wouldn't be to hard to write a dynamic master key for memcache depending on what JS/CSS modules would need, but I'm not always guaranteed memcache is available for my projects.  I know I can write my custom CSS and JS into the index file just fine, but I'm trying to pass these variables to external files for on demand fast page loading and to reduce the size of the page.  TLDR: I'm basically trying to get Variables from one file to another, without opening and writing a file, memcache or sessions for module portability and page load efficiency. "
Which is better for a review site. Custom PHP or Wordpress?,"Hello PHP Redditors!  I am looking for some advice.  I am thinking of starting a review site. (I know i know, its been done many of times - but mine will be special ;p )  The site will be a place where people can review things and rate them on a scale of 1-5. Then an average review would be calculated so on and so forth.  I have been speaking to a few PHP developers. Some say custom PHP would be better, others say Wordpress is the way to go. I need to know the man im going to hire will have my benefit at heart (as well as his wallet's)  Me being in sales and business management, I am lost. Luckily I have Reddit.  I am hoping someone can solve my dilemma, which would be better for me in terms of updating in the future, adding functionality etc. Wordpress or Custom PHP?  If you require more info, please ask or feel free to throw me a PM.  Thanks in advance for your help.  TLDR - Which is better for a review site. Custom PHP or Wordpress? "
please point me in the right direction for getting started in writing web apps that will run off my home computer.  Thanks.,"I'm interested in writing my own web apps, e.g. a personal scheduler or a file browser, than will run off my home computer, allowing me to access it anywhere from the web.  These apps would just be for my personal use.  I'm a fairly experienced python programmer and would prefer to do most or all of this in python if possible, but I've never done any web programming and am not familiar with approaches or issues that go along with web programming.  So I'm wondering how to get started with a project like this? What is a good basic software setup for this type of thing?  While I'm happy to dabble with any tools you might recommend and learn as I go, I'm concerned about security and want to make sure I'm not exposing my system while I experiment with this project.  TLDR: please point me in the right direction for getting started in writing web apps that will run off my home computer.  Thanks. "
new text editor written mostly in Python and with some C - heavily extensible using Python - not released yet but will be soon - screencasts [here](,"I am writing a new text editor. Its main purpose is to be easily extensible using Python. Other goals include being very minimal (zero defaults, although it comes with emacs-like or vim-like keybindings you can  import ) and to have a powerful API. Writing text editing routines should be easy. Writing extensions should be easy.  The editor has a small core written in C (~1.6k loc) which handles the text structure and all of the low level UI. The editor logic is all written in Python. This includes keybindings, window management, window splitting, prompts, scrolling, cursor control, and more.  I have a few screen casts that chronicle the development rather nicely. They are available on asciinema [here]( [This]( is the most recent one, although it is already out of date with development.  Development is moving forward at a decent pace. I plan to release it on GitHub within a month. I will most likely release it into the public domain.  tl;dr  - new text editor written mostly in Python and with some C - heavily extensible using Python - not released yet but will be soon - screencasts [here]( "
"A wild script appears after another dev works on a project I never had anything to with, am asked to check it out. Can anyone help?","Image of the script [here](  Basically my boss asked me to check out a report from the client that their site is throwing errors to some users, that it's serving a script that matches a virus pattern.  This is on a site that was built before I got here, and I'm not a JS programmer so I don't really know where to start with this one. I know enough that if I remove it the site is going to work without any problem because the rest is static HTML/CSS and a couple of forms that are powered by PHP scripts that I located as well.  It's not in a CMS and it's not tied to a DB in any way.  TL;DR  A wild script appears after another dev works on a project I never had anything to with, am asked to check it out. Can anyone help? "
"USA senior PHP dev says Sitepoint Market advertising for projects is now fruitless. Seeks other affordable, productive project advertising venues. Not interested in recruiters. Desires freelance work only.","I'm a senior PHP Web Dev in the USA. My other expertise is Kohana, WordPress (custom themes and plugins), PSD->XHTML/CSS conversion, various offer marketing APIs, Amazon and eBay APIs, jQuery/AJAX work, and Android mobile development (using PhoneGap).  I used to use SitePoint Market to catch gigs. I simply paid $20, posted an ad, and within minutes I would have a stream of emails come in from potential clients. This lasted me for 5 years. However, in the last 1.5 years I didn't have to use it because I had two clients come back to me and ask for retainer work until their budgets ran out.  But now I'm in a bit of a funk. My latest effort on SitePoint Market has fizzled. I don't know where else to go where I can afford to place ads and find my type of clients, which tended to be web designers, startups, and internet marketers.  Now my cashflow is starting to dry up and I don't have a whole lot of dollars to gamble on advertising. I have perhaps about $200 USD to play with, and I don't want to put it all on one option. I looked at Freelance Switch, but man that's some expensive advertising.  I really need the cash in order to make the mortgage payment in 45 days. That's about as far out as my savings will take me.  Where do you suggest I invest now for advertising since no one seems to be going to SitePoint Market anymore?  TL;DR: USA senior PHP dev says Sitepoint Market advertising for projects is now fruitless. Seeks other affordable, productive project advertising venues. Not interested in recruiters. Desires freelance work only. "
Need help comparing horizontal slices of an image for a repeat.,"Hi all,  I'm currently working on a project where I get an design image.  This image will start to repeat itself somewhere in the bottom 20% (or there abouts).  I'm currently trying the Imagick class/library (Module: 2.1.1-rc1  | Version: ImageMagick 6.3.7 11/17/10 Q16).  My general plan is to take the first line of the image as my base of comparison, then looping from the bottom upwards, copmaring to see if there is a close match.  If there is compare a ~10px horizontal slice to see if it is indeed a good match, or just a coincidence.  While I believe this approach will work, I'm aware that it is probably a terrible, resource heavy method.  My problem is that the images aren't brilliant quality, that it's very slow, and that I've had no discernable results from the  Imagick::compareImages() ]( functions.  I've had the numerical results from the above two functions put into arrays, and I can't seem to make sense out of the numbers, or see a spike/drop where I estimate the pattern to start repeating.  I've had a look around, and seen a couple of libraries, such as [OpenCV]( but haven't yet delved in to them, I was wondering if anyone here had any experience they could share, or let me know if I've missed something awesome and powerful for just this.  TL:DR:  Need help comparing horizontal slices of an image for a repeat. "
Are there any IDEs which allow you to edit your code on windows and run it remotely on debian via an ssh connection? (not using X-windows),"I have a project which is run on a remote headless PC/104 device running Debian. Currently I edit my code with notepad++ on my PC, use the NppFTP plugin to upload to the device (over a high latency wireless connection) and then use a SSH terminal to run the code. Between the time to upload an edited file and the time for the program to start up, I'm doing a lot of waiting. Every syntax error costs me at least a minute.  The project can't really be run locally because it involves a lot of very specific IO hardware (motor controller, sensors, instrumentation, etc.)  Now that my code is over 8k lines, I'm beginning to think I need some sort of IDE so I can more easily debug my program without all the waiting. It doesn't help that I've been teaching myself OOP along the way so the code is a little messy.  I saw an article on using [VIM as an IDE]( but my vi skills are very rusty and the learning curve on this looks daunting.  tldr:  Are there any IDEs which allow you to edit your code on windows and run it remotely on debian via an ssh connection? (not using X-windows) "
"Intermediate .Net programmer looking to take my skills to the next level, but using Python instead. Need learning resources.","I have no formal training, but plenty of on the job experience, primarily in the .Net world (C# and to a much lesser extent, VB). My introduction to programming was actually VB6 in 1999, which included all of the bad habits that the language in known for (lots of code in event handlers, etc). Thankfully, I only used that for a couple of years before moving to VB.Net, and C# not long after.  My biggest issue I think is that I have been working at small companies for most of my career, and as a consequence have learned and worked in a bubble where I was the only programmer (or sysadmin with some programming knowledge) and had no one else to learn from. Over the years I have made a sincere effort to look at other code, tutorials, books, etc to hone my skills, to the point where I believe I am a pretty solid intermediate programmer... but I feel like I am stuck there.  The vast majority of my work has been building and maintaining websites and web applications. Other projects include things like screen-scrapers, building product feeds for Google Product Search & Bing Shopping, some task automation, plugging into PayPal and USPS/FedEx APIs, etc.  When looking for a framework for building a personal project, I ran through tutorials for the big 3 open source web languages and their popular frameworks (PHP/Zend&Yii, Ruby/Rails, Python/Django). I immediately fell in love with Python (Ruby/Rails is great as well, but it just didn't speak to me the way Python did). The language was easy enough to pick up, and the Django framework is about as simple as they get, but I quickly felt like I was falling into some of the some old habits (functions with lots of code, not enough reuse or inheritance, and projects that seem to become unwieldy as they grow).  I don't plan to abandon .Net entirely, but for the time being would like to focus on advancing my skills as a programmer using Python. Most of the resources I have been able to find through Google and by browsing some old /r/python threads seems to focus on new programmers. I'm already comfortable with the core OO concepts, so I'm looking for something to help me take my skills to that next level.  Any suggestions? Books, online training, existing projects, workshops. I'm open to just about anything.  tl;dr; Intermediate .Net programmer looking to take my skills to the next level, but using Python instead. Need learning resources. "
Want to learn PHP but starting to hear that PHP is terrible. Why do people say PHP is terrible?,"So I've been on this 5 month self learning course where I've been swimming through HTML, XHTML, HTML5, CSS, CSS3, jQuery, Javascript, and now I've finally arrived and server side scripting. I was really considering PHP but I've recently been hearing bad things about PHP. People saying that it's terrible and that everyone should stop using. Could anyone here elaborate on why people think that PHP is terrible? I mean, I've searched it up on google but I've only heard things from people who have heard things from people who have heard things form developers. So I wanted a first hand opinion from any PHP programmers themselves? Thanks!  tl;dr: Want to learn PHP but starting to hear that PHP is terrible. Why do people say PHP is terrible? "
"help with installing NetworkX to the appropriate version of Python, and not to the framework that uses 2.7.","I attempted to download and install NetworkX( as a useful data visualization tool earlier today with my Macbook Pro. After suffering through the process of how to install it, it appeared as if I had installed it into the framework (changed directory in Terminal to the appropriate folder, entered ""python install networkx""). This shouldn't have been a problem, except that I need to use it with 3.2.3, and the python that comes with the computer is 2.7, so it was attempting to run the program through the wrong version. Now that I've uninstalled (after searching through my hidden files for ones marked networkx...), how can I install again to align the program with the directory for 3.2.3?  TL;DR help with installing NetworkX to the appropriate version of Python, and not to the framework that uses 2.7. "
I am trying to understand how to use the commands listed at  . The 'manual' isn't easy enough. How do you go about learning this stuff?,"Simple HTML DOM is basically a php you add to your pages which lets you have simple web scraping. It's good for the most part but I can't figure out the [manual]( as I'm not much of a coder. Are there any sites/guides out there that have any easier help for this? I asked on /r/phphelp but no answer. Is there a better place to ask this kind of question?  The site for it is at:  I can scrape stuff that has specific classes like  &lt;tr class=""group""&gt; , but not for stuff that's in between. For example.. This is what I currently use...  $url = '$html = file_get_html($url);foreach($html-&gt;find('tr[class=group]') as $result)  {    $first = $result-&gt;find('td[class=category1]',0);    $second = $result-&gt;find('td[class=category2]',0);    echo $first.$second;  }}  But here is the kind of code I'm trying to scrape.  &lt;table&gt;  &lt;tr class=""Group""&gt;    &lt;td&gt;      &lt;dl class=""Summary""&gt;        &lt;dt&gt;Heading 1&lt;/dt&gt;          &lt;dd&gt;&lt;a href=""#123"" class=""ViewProfile""&gt;Cat&lt;/a&gt;&lt;/dd&gt;          &lt;dd&gt;&lt;a href=""#032"" class=""ViewProfile""&gt;Bacon&lt;/a&gt;&lt;/dd&gt;        &lt;dt&gt;Heading 2&lt;/dt&gt;          &lt;dd&gt;&lt;a href=""#143"" class=""ViewProfile""&gt;Narwhal&lt;/a&gt;&lt;/dd&gt;          &lt;dd&gt;&lt;a href=""#642"" class=""ViewProfile""&gt;Ice Soap&lt;/a&gt;&lt;/dd&gt;      &lt;/dl&gt;    &lt;/td&gt;  &lt;/tr&gt;&lt;/table&gt;  I'm trying to extract the content of each  &lt;dt&gt;  and put it to a variable. Then I'm trying to extract the conten of each  &lt;dd&gt;  and put it to a variable, but nothing I tried works. Here's the best I could find, but it gives me back only the first heading repeatedly rather than going to the second.  foreach($html-&gt;find('tr[class=Summary]') as $result2)  {    echo $result2-&gt;find('dt',0)-&gt;innertext;  }  Thanks to anyone who can help. Sorry if this is not clear or that it's so long. Ideally I'd like to be able to understand these DOM commands more as I'd like to figure this out myself rather than someone here just do it (but I'd appreciate either).  TL;DR:  I am trying to understand how to use the commands listed at  . The 'manual' isn't easy enough. How do you go about learning this stuff? "
"I am barely slightly more than a beginner in PyOpenGL, but cannot find any helpful resources online about Picking. Can reddit help?","I am working a hobby 3D application that utilizes PyOpenGL. I have been interested in taking my simple application to the next level by implementing picking, which will pave the road for many new exciting features, but have so far failed to implement it. I am no PyOpenGL guru (not even a python or opengl guru either, my project is a learning experience), and tutorials online proved to be just barely too difficult for me to easily implement. The several attempts I made to plug some (possible) picking code into my existing code has essentially failed (hopefully not due to my existing code).  Google gave me a reasonable amount of results, but nothing that provided a clear example of what the PyOpenGL picking code should be. For example, I found some questions asked about the same subject ( but the answers were confusing to somebody of my skill level. As well as finding code that, based on what the forum poster said, doesn't yet work ( and completely lacks an answer.  I also examined the NeHe tutorial ( about picking in OpenGL, but the C++/python language barrier proved too difficult to be of much help with any of the online material I found via Google, and that tutorial doesn't have a port to python.  I also searched this subreddit, as well as Reddit as a whole. So /r/python, asking you directly is my last hope! Are there any snippets of example code online I could use? Or do any of you know the code that would solve my problems and let me move on with my project? I don't want you to think that you are doing all the work for me, so I would be happy with links to proper tutorials that I have missed to learn it all myself as well.  And I should say that I am not currently willing to switch to Pyglet or something along those lines. I want to keep the project in PyOpenGL for now as a learning experience. Also, once my project is more complete, I will gladly share it with the python/reddit communities!  EDIT 0:  Is there a way to determine which point on a simple flat quad (for example) is being clicked? Perhaps much like an RTS game would do it.  tl;dr I am barely slightly more than a beginner in PyOpenGL, but cannot find any helpful resources online about Picking. Can reddit help? "
i have some php framework that needs documentation and i dont know what i'm doing.,"I know, I know i'm about as backwards as you can get.. However when it comes to documentation me = epic suck. Anyways I'd like to make this code open source, but there is no real documentation on it.. The basis around it is that it uses php to generate javascript variables of static xhtml code (layouts, link bars, etc). That generates a .js file the client downloads. So the client now has almost all the html code for the website loaded into cache. it uses jquery to make ajax calls to a controller that returns raw data. jquery loads js variables up of raw xhtml code and inserts the raw data to be displayed. everything is pretty much api based to a controller thru a php proxy that detects stuff.  Anyways as you can see i don't write documentation very well. I am willing to explain things to someone and let them go wild. Everything is in my head and explaining it doesn't come out thru my lips or fingers very well..  I'm sure something like this already exists.. I haven't looked.. however it's really cool and I believe someone may just benefit from it.  tl;dr i have some php framework that needs documentation and i dont know what i'm doing. "
"My head's hurting. Please help me get started with OOP and best practice php. 
 p.s. Sorry for the life story.","I'm sure I'm not alone in the way I picked up web design - thrown in the deep end as a ""web editor"" during the dot com boom (1999), ostensibly to write content and ended up designing, building and developing websites.  Around 2004 I realised everything I knew about HTML was wrong and I went back to basics in order to separate style from content and - while painful - using CSS and XHTML was a clear step forwards.  Now I'm approaching a similar crisis regarding php. I'm still writing procedural code, copying and pasting functions I write from website to website. Recently I got a a scare when a site I worked on was easily compromised and the database was wide open. Looks like I finally need to switch to OOP. Not only that, but I realised I know nothing about MySQL security either.  But unlike the tables > css conversion of the front end, this is making my head spin. It's like learning a whole new language. Basically, I need help in getting started and also in choosing a route to take. I make fairly simple database-driven websites - usually nothing more complex than user accounts, events systems etc. I'm not into making fancy web applications. So do I continue using procedural code, but just tighten up my security, do I switch to OOP and use a framework? OOP and write my own classes? A mixture of both? Use a CMS driven framework like Expression Engine?  TL;DR:  My head's hurting. Please help me get started with OOP and best practice php.  p.s. Sorry for the life story. "
Is there any way to get only pages with pictures from the random button on wikispecies?  Doing a research project with 2nd graders.,"So I'm doing a 7-week after-school program with some 2nd graders.  They're really interested in animals, so I'm going to do a book publishing project where they work together to create a factbook about interesting animals.  I figure they each get to pick (at least) one interesting animal, ask some questions about it (Where does it live?  Why are its ears so big? Why do the wings look like eyes?) and then do some research to answer those questions.  Book gets published all official-like, the students have something to be proud of, and LEARNING!  Since I know these students are already interested in animals, I want them to research an animal they don't already know very much about.  One of the ideas I had was to use the ""random page"" button on the wikispecies page.  Unfortunately, 90% of my clicks end up having no image, which is not very interesting.  Is there any sort of magic programming/internet trick that will allow me to hit the random button and come up with an article with a picture every time?  I know these students would not be the only ones interested in a random animal generator!  Someone from /r/answers sent me in your direction, /r/python.  I really appreciate it.  Reddit's been a big help in my teaching career.  One of my fondest memories is the questions from 6th graders thread in askscience which led to an awesome scientist skyping into our classroom.  So, thanks again!  tl;dr: Is there any way to get only pages with pictures from the random button on wikispecies?  Doing a research project with 2nd graders. "
Making shopping Cart package. Need suggestions or support if anyone is interested in what I have in mind.,"I am designing a shopping cart package for the Laravel framework but most of my questions/requests for advice are not directly related to Laravel.  I want it to have the following features:   Guests can browse, and add stuff to their cart   The contents persist (in DB via a FK that is stored in a cookie)   If a guest logs in or registers, their cart will ""transfer"" from a guest cart to a user cart.   user carts are persisted through the DB (FK is user_id)   if a user has cart contents, but they added stuff to a guest cart, when they log in, the user cart contents will become a 'wishlist' but the user will have the option to add them to their new cart (the guest cart that became the user cart)    My setup so far: I had made 85% of this functionality already (except the wish list and stuff) but I wasn't happy with it...  It's kind of sloppy. I had 2 classes to handle the anonCart (guest) and userCart which extended off of CartDb, handling the DB model.  I know this isn't the best way to handle this situation... I'm looking for suggestions. If someone has encountered code which accomplishes what I'm looking for, I'd love to take a look at the source code.  If anyone is interested in developing one with me from scratch, that'd be a fun project too.  tl:dr - Making shopping Cart package. Need suggestions or support if anyone is interested in what I have in mind. "
Play my [remake game]( and let me know what you think.,"I recreated the  original Helicopter Game  in standards-compliant HTML5 (XHTML syntax) with inline SVG.  The original game's creator (David McCandless) was happy to give me the approval to remake it. My goal was to have as much parity with the original game while being standards-compliant and inter-operable with the latest browsers (Chrome 5, Firefox 3.6, Opera 10.6, Safari 5, and the IE9 third Platform Preview).  The game takes advantage of some new ECMAScript 5 functions,CSS3-Fonts, LocalStorage and DOM3 Events. Also, all of the graphics are vector-based (no images). I'll be making a blog post soon that describes some of the technical difficulties I encountered with various browser bugs and technology limitations.  tl;dr  Play my [remake game]( and let me know what you think. "
Anyone have an idea for a programming project that they feel like they can't start on their own? I'd like to get involved with you.,"Let me begin by saying that I have read the FAQ about /r/programming self posts and I believe that this falls a little bit outside of any of the areas covered there. If I'm mistaken, please just let me know and I will be more than happy to promptly delete this submission.  Anyway,I'm a current CS student at Boston College (entering junior year). In my free time I like to be more or less constantly busy on a programming project of some sort to keep my skills/knowledge growing at all times. Recently, however, I've been forced to abandon my latest project because my partners (there were three of them) decided to bow out. Normally I would simply continue on my own or start some other project, but continuing alone simply isn't an option for my previous project and I have hit an unfortunate snag - for the first time I don't know what to work on.  So, basically, this post is a request. I'm looking for someone with an idea for some kind of programming project who either hasn't started it yet, or is at the very beginning, or has expanded to a point where you just can't handle it on your own any more. Someone who feels like all they really need is an extra push or a partner to work with to get rolling on some new endeavor or to move to the next level. I'm not asking to be pointed towards an OSS project or anything like that; I'd like to get involved with a single person or perhaps a small team - somewhere I can make a noticeable impact and be involved in large scale design/implementation.  I'll be able to continue work on this project far beyond the end of the summer, but for this month 80% of my time/energy will go into whatever I decide to work on, which is why I chose the title.  As far as languages/knowledge etc.. I can both write and understand Java, C, ASM, Python, PHP, Javascript, and Objective-C. If you want to work with another language, I can get comfortable with a new syntax but there will be the obvious learning curve involved there.  Tl;dr - Anyone have an idea for a programming project that they feel like they can't start on their own? I'd like to get involved with you. "
I'm starting a web startup and I want you to join me (equal shares). Read the text for more info. I know you want to.,"First of all, I'm in Switzerland. I'm a 20 year old guy, working as web developer, marketer and designer. I know what I'm doing, but I also know what I can't do. And I know I can't do this all by myself. Basically I'm inviting everyone who wants to join to send me a PM - I'd give everyone an equal amount of shares (not a company yet, but we'll work on getting funding)  The website is taking form and I'm sure I can finish it all until August 6, but I need people to help with administration, marketing and app development (I'm on Windows and never ever did an iPhone / Android App). I'm looking for maybe two more developers, one app developer and two or three marketers / admins.  The whoel thing is built on LAMP, that is mostly PHP / MySQL, and is SIMILAR to Yelp, but that's only for lack of a better example. I obviously don't want to shout the idea out to the world, but lets just say there is no such thing around here (Switzerland / Germany / Austria) and I see a big potential.  As I said, everyone would get an equal amount of the site / company. Also  this is not meant to be a full-time job . I have a regular job and in September I'll go to uni again, so this would be more of a 20-30% job  without payment . hat's right, because I don't have a lot of money, but again - equal shares for everyone.  Feel free to ask anything in the comments or drop me a PM along with your qualifications (or link to your website) and we'll work from there.  Oh, something more - I want this to start in a few cities. You don't need to be from Switzerland or even Europe, but that wouldn't be bad either.  Also, the site is currently german only, but we'd make an english version before launch.  TL;DR:  I'm starting a web startup and I want you to join me (equal shares). Read the text for more info. I know you want to. "
I want to be more than a code monkey and I want to design quality software to run on tiny 8-bit embedded micro-controllers.  What should I read?,"I'm a self taught software developer and I've accomplished some very cool and useful work in my professional career.  However, I've always worked as the sole developer and have always coded in a way that reminds me of many of the new programming buzzwords I run across.  ie, I've always been able to start small with useful results, and build up the systems one step at a time.  Each time something was accomplished, it seemed to open doors for more possibilities which would greatly benefit my employer.  However, now I'm faced with my most challenging project yet, and I feel my usual development style is really not ideal for it.  What are some things I should be learning in regards to real software design as opposed to simple software programming?  I'm working on describing how the system works using flow charts, but I'm floundering as I try to find the best ways to do so.  How should I be properly designing complex software to run on a µC which has to deal with precise timing, interrupts, power-save modes.  A system that needs to handle transmitting, receiving and relaying information via a SPI controlled radio?  tl;dr I want to be more than a code monkey and I want to design quality software to run on tiny 8-bit embedded micro-controllers.  What should I read? "
"Write code like PHP, but using C/C++ and compile it natively.","Just a little idea I got after playing with PHP earlier today: ""Can I write a site a-la PHP where you can mix HTML and PHP in the same file, but with the C/C++ language?""  My idea: mix HTML and C/C++ together seamlessly in the same file, just like PHP does. Solution: super easy! Write a script that parses files before passing it to the compiler. Every time I see the starting and ending tags of a script (i.e.: ""<?"" and ""?>"") I just replace it as a string literal within a special printf(...) function that prints it out to a client.  Clearly this will need a decent framework to do the ""other"" things (request parsing, correct URL handling, parallelism, actual server software, cookies, etc..) but why hasn't anyone else done this? Or maybe there is some out there?  TL;DR:  Write code like PHP, but using C/C++ and compile it natively. "
"can anyone recommend a usable, practical tool for web testing that I can use from a Mac OS X platform?","I have a largish website I maintain.  It's basically a ""brochure"" site, not a web application; there are a couple forms the user can fill out to get more information, but the site is basically static, from the user's point of view.  HTML is generated dynamically via a CMS.  I'd like to automate some basic regression tests against the site in at least one major browser (I'm on a Mac, so Firefox, Chrome, or Safari are the best alternatives).  I've seen [Sahi]( and [Fake]( and have also read [this screed against UI testing]( but want to know if anyone out there has practical recommendations for basic automated regression testing of web sites.  Tl;dr : can anyone recommend a usable, practical tool for web testing that I can use from a Mac OS X platform? "
How to sequence a set of floating point calculations to minimize numerical errors?,"[Binet's formula]( can be used to find the nth fibonacci number in constant time.  However, it involves performing the following floating point operations which make it inaccurate for high N, which is exactly where it would be useful:   Computing the square root of 5 (to obtain the golden ratio phi)   1 - phi (phi is a floating point number)   both phi ^ n and (1 - phi) ^ n   dividing by square root of phi.    So my question is, is there a way to do these operations in a smart way so that the errors could be minimized for high n?  tldr: How to sequence a set of floating point calculations to minimize numerical errors? "
Why can't Android make some kind of bundle to just fucking install a .exe file and not have to manually configure crap a newbie wouldn't even care about?,"I'm a complete newbie to mobile development. This weekend I decided to dive into both Windows Phone 7 and Android.  For WP7 I just had to download a 70mb .exe file and run it. Then within Visual Studio I just created a new Project of the Silverlight for Windows Phone 7-type and after a few seconds I was greeted with a screen showing the phone layout and that's it.  I could start working on my application!  Now let's look at the Android side of developing. I Google 'android development' (remember I'm a complete newbie to this) and I find my way to the download page.  After a couple of minutes reading something about a plugin for Eclipse and having to manually select an emulator version, I just download the .exe for my system.  So I download and install both Eclipse and Android.  Now what?  I open Eclipse and I can't even find the fucking Android project type.  WTF!  I've been reading some forums and I still can't get this piece of shit to run.  tl;dr: Why can't Android make some kind of bundle to just fucking install a .exe file and not have to manually configure crap a newbie wouldn't even care about? "
I want to become a better programmer. All books seem irrelevant for me. Help?,"There are lots of texts on how to plan software projects (with user stories, etc), but they usually assume you have a large budget, liberal timeframes and/or a real dev team available. While they sound fantastic, they never seem to account for solo devs working on a short deadline.  There is also a lot of talk about test-based methodologies where you write a test case for every method before you implement it, but I feel that these are difficult to impossible to apply if your software is GUI-focused (e.g. (server-side) web programming or Flash/ActionScript).  Although I try to make heavy use of refactoring to improve my code whenever I have finished a section of it, last minute hacks and additions tend to make this incredibly frustrating and I often feel that there should be a way I can utilise at least some of the planning theory that's apparently meant to help large dev teams and developers of software libraries first and foremost.  What is The Right Way to go about writing small-ish applications as a solo dev and how do you prevent last minute changes from making your code worse?  tl;dr : I want to become a better programmer. All books seem irrelevant for me. Help? "
"Building a project with some real time functionality, do I use PHP/Racthet, PHP/Node, or just Node","Ive read other posts on /r/php regarding this subject, but nothing seems to give me the peace of mind that I want/need.  I have spend the last two days deciding on what is the best way to approach a new project that I am working on.  The project is a appointment booking system with a real time component.  Users can book meetings ""in-person"" with each other, however, they can also choose to ""meet online"" which will include video, chat, editing/sharing PDFs and images. This needs all be done in the browser.  I am primarily a PHP developer but have played with Node.js (created a chat application).  The project as a whole will be pretty large and needs to be scalable.  I feel my choices are: Should I stick with PHP and use WebSockets with Ratchet, OR should I use PHP for most of the CRUD functionality and use Node.js for the real time functionality OR should I better my skills with Node.js and develop the web app with front-end/back-end javascript.  I know this may not be completely clear, but comment away and I will try and clarify!  TL;DR: Building a project with some real time functionality, do I use PHP/Racthet, PHP/Node, or just Node "
"Thinking about learning PHP, give me examples of breaking changes (for the better).","Fitsly: I am primarily an ASP.NET developer, but I do a lot of node.js applications on my spare time.  I have been thinking about PHP for a while in order to match my resume to local demand. Im my years as a developer I have heard a lot of shit about PHP. So I am what you might call a skeptic.  The thing that holds me from investing in PHP is a fear that backwards compatibility is central enough that old mistakes are never repaired.  I am familiar enough with consistent development stacks, that I don't want to regress to guess work when it comes to APIs and language features.  My main concerns are not with frameworks and other easily interchangeable parts but in the language and standard libarary.  Does any old timer have any examples (with release notes if possible) when old blatantly wrong things have been phased out?  TL; DR  Thinking about learning PHP, give me examples of breaking changes (for the better). "
What is the best way to distribute a standalone python program EXE for windows (trying to keep it as SMALL as possible)?,"I'm currently using pyinstaller to package my scripts for distribution to employee machines amongst the company.  A basic script that contains <100 lines comes out to 2.5-3mb after making it into a standalone exe using pyinstaller (yes I am compressing with UPX as well, doesn't make much difference).  I have tried py2exe as well (ends up around the same size).  I understand that the interpreter is being bundled every time, which probably amounts to most of the size.  Is there any way to shrink these down further?  I would like to see them shrink down to under 1mb if possible.  I send out small little support tools quite often that are usually batch files, vbs scripts, ahk scripts, etc.......but I'd like to use Python if possible (I enjoy coding in  Python, I'm trying to learn as much as possible, and Python is pretty awesome).  I'd rather not go the route of installing a full blown Python install on over 5000 machines if I don't have to.  Is there a way to distribute just the interpreter to each machine, and then just bundle the source (and required libraries) and send those out instead?  TLDR:  What is the best way to distribute a standalone python program EXE for windows (trying to keep it as SMALL as possible)? "
"I need reliable encryption training, what do you suggest ?","Even though I am interested in information theory and read up on it once in a while, I am still far from clearly understanding how it all actually works out and the functionnal why of bad practices being bad.  There was a post yesterday that dealt with url-pretty encryption which was responded with a blog post by someone here basically destroying bad noob practices. What that reply made me realize is that I am much more of a noob at encryption than I had previously thought.  A big problem I see with php encryption articles & tutorials is that they all contradict each other. One says that's how it should be done, the next says it's extremely unsecure but offers no alternative, another gives an alternative, but gets discredited somewhere else for not being scrutinized enough.  So I guess the goal of this post is: Encryption is an important part of an application and should be implemented correctly. Could some encryption-savvy people post their most trusted methods for encryption,  maybe even have them in the side bar, as I believe I'm not the only developer in this situation.  TLDR:  I need reliable encryption training, what do you suggest ? "
"I got it working, but through a workaround, and wonder why I couldn't get it working the intuitive way.","I got it working, but only by treating the code like a bookmarklet.  Story  - There is an object, and I need to access its properties, but the name of the object is random.  Refresh the page, and it's different.  After finding a reliable method to get the name of that object, I store the name in a variable foo.  Now, to get this variable to act like the object, instead of just holding the string name of the object, I'd use eval(foo).propertyBar.  That works.  I'm able to get whatever propertyBar's value is.  But that only works if I do it from the command line, or if I wrap the code in a bookmarklet, like location.href = javascript:...;  Without that wrapper, eval(foo) doesn't represent the Object as expected.  Test  - Go to a  Vimeo.com video , and paste this code:    var elem = document.getElementsByClassName('f player')[0].getAttribute(""id""); var elem = window[elem.replace(""player_"",""clip"")]; alert(elem.config.request.signature);  It will work.  It'll return a value.  But put that same code in a blah.user.js, and install it on Chrome or Firefox+Greasemonkey, and it won't work.  It's as though it's not finding that object.  tl;dr  I got it working, but through a workaround, and wonder why I couldn't get it working the intuitive way. "
I need a button that will increase the size of a div by an amount each time the user clicks.,"I am fairly new to javascript so please be nice. :)  I have a page with a div element that side scrolls independently from the rest of the page. I have never been able to use height: auto or 100% on this div for some reason. This has never been a problem until I am now told I must add buttons on the page to let the user make the text larger or smaller. The problem I am having is I cannot figure out how to make the button both increase the text size  and  increase the div height as well. Here is the code I am using to make the buttons along with my non-working elementary way of getting the div to resize:  $(document).ready(function(){        $('input').click(function(){            var ourText = $('#test1 p, #test1 h2, #test1 h3');            var currFontSize = ourText.css('fontSize');            var finalNum = parseFloat(currFontSize, 10);            var stringEnding = currFontSize.slice(-2);            document.getElementById('#test1');            var d= $('#test1').height();            if(this.id == 'large') {                finalNum *= 1.2;                d += 125;            }            else if (this.id == 'small'){                finalNum /=1.2;            }            ourText.css('fontSize', finalNum + stringEnding);        });    });  I am not interested in getting into a discussion as to why making buttons to increase/decrease the text may or may not be a good practice as I do not have that decision, I just have to implement it.  tldr:  I need a button that will increase the size of a div by an amount each time the user clicks. "
I guess this post has little content.  Maybe just visit my project and comment on my bad coding practices?,"So, I've been fooling with Python for about a year.  By fooling, I mean I've played with samples, watched some Google stuff, toyed with Pygame and recently started working with Django.  I coded daily for about 10 years through the nineties and just walked away in 2001.  Last year, I started to venture back and this year, I actually have projects I want to complete.  My code sucks.  I'm self taught, always have been.  Django is confusing the heck out of me and I decided it was time to share.  I put a project up on GitHub that I wrote last year and would one day like to finish, but it works currently so I haven't touched it.  I want to share, honestly because I want help.  The problem is, my projects are all going to be little CLI scripts and micro-database stuff with very light web interfaces.  It makes me feel silly that I've put up near-useless code on a website flooded with great developers in hopes that someone will come along and offer some assistance or even develop some code with me... but it's there.  So, I guess my question is, sharing code.  How?  I am a fickle person and if I don't get passionate about something I find it very difficult to stay focused for any length of time.  I read the suggestions about working on open-source projects, but I feel I have little to nothing to offer and any solution I may begin working on will likely be completed by someone else much faster and much more efficiently.  tl;dr I guess this post has little content.  Maybe just visit my project and comment on my bad coding practices? "
"help with multiple applications setup in CI or other framework - how would you structure yours? 
 thanks","Basically, tomorrow I'll start building a new bespoke ""platform"" for a client. They prefer to use CodeIgniter (but could be persuaded to use another framework). It should use a single CI installation; there will be any number of ""stand-alone"" applications which need to use common libraries (which I'll need to write) to receive and send data over http to 3rd party REST services. Could someone point out a good (directory) setup in CI (or some other framework such as zend, symfony etc). Is CI good for this particular situation, i.e. will it be able to handle this setup/requirements well? I have no problem using the above mentioned frameworks for a single app product but I want to make sure I start in the right direction for such a multi-app setup. It will need to be able to handle new application and new 3rd party data/service providers in the future.  tl;dr; help with multiple applications setup in CI or other framework - how would you structure yours?  thanks "
"Have 3 books downloaded to teach myself Python programming/coding, but want to know if there are tips or tricks to learning it.","I don't know any programming languages, basically (a few lines of java and a tiny bit of C++), and decided on starting with Python.  I downlaoded/have  This book ( A Quick, Painless Tutorial on the Python Language  by Mustapha Elmekki) ]( , and [This one ( Invent your own computer games with Python  by Al Sweigart)](  Are there any other pdfs or free/cheap books I should get?  Also, two more (fairly minor) questions (and one has a subquestion):  1) Is there anything that no book (or at least those books) won't tell me that one should know about learning programming in Python?  2) What are the main differences between Python 2.7 and 3.2? (and the subquestion)  2a) Is there a list of the available packets/paths to import (like the code "" import math "" allows you to do more complex mathematical expressions) and the list of coding words/shortcuts they carry (like after you "" import math "" you can do "" math.sqrt( (a number) ) "" which figures out the squareroot for you)?  .  Sorry if these questions seem dumb, or stupid, etc. I am just really interested in learning programming and coding, but don't know where to start or figure everything out.  End-note: As always, Links are much appreciated.  Tl;Dr: Have 3 books downloaded to teach myself Python programming/coding, but want to know if there are tips or tricks to learning it. "
Has anyone here found a way to use a debugger to step into eval() statements? Help!,"I'm on a project (PHP 5.3.x) writing a plugin that's loaded into the main code base by ""injecting"" it within an eval() statement. Due to the complexity of the parent code base, it's not possible to use any other type of alternative method to load and run the plugin.  Through my research Xdebug and PHPED, do not support stepping into eval() statements. I heard PHPDBG does, but that's only available for PHP >= 5.4.x-- also can anyone confirm this claim is indeed true?  For the moment, I'm using FirePHP for my debugger. But, I really miss the ability to step through my code-- it makes the feedback loop so much better and speeds development time.  tl;dr: Has anyone here found a way to use a debugger to step into eval() statements? Help! "
looking for something fast and light to pull data and make nice graphs and charts,"I'm not new to the data science and analytics space.  I'm also very well versed in Linux and server administration, so please don't hold back.  I've been looking for something easy and lightweight that can spit out quality graphs, charts and tables from an RDBMS and (or) NoSQL (key value) store.  My company is a Splunk partner and I've been pumping data to a demo Splunk instance and I couldn't be happier with its capabilities.  It's lightning fast and creates great visualizations.  The catch is that Splunk charges by amount of data indexed and while it handles JSON like a champ, aggregating and then pushing the data to Splunk proves to be limiting in that my license is limited to 500MB of indexed data per day.  So I ask.  Does anyone know of a lightweight but powerful engine that can query and generate visualizations (and do aggregation/stats) on JDBC/JSON/NoSQL/RDBMS sources?  I don't want to go all out with something like Pentaho's BI suite, looking for something a little less cumbersome to deploy.  TL;DR: looking for something fast and light to pull data and make nice graphs and charts "
"Use [Symfony]( if you don't know about architecture, yet. Use [Silex]( otherwise. Read the documentation. It helps becoming a better developer.","So this will be a rant post as I am right now looking into Laravel and I'm coming from the Symfony/Silex world. I see many things that I would not want to have in any of my projects. Please correct me if I'm wrong.  Here is what I found and think about it:   I can install it with  download . Where is the composer.json with the dependencies? We have composer ( why  not  use it? The problem? What if I'm deep in project-development and laravel updates a security leak ... do I then copy my application in the next downloaded  version? Without composer, how do I get all the new cool libraries with already developed nice functionality in my app. Do I download them also? What if there is a new version ... get my point?  I don't like tabs. I prefer spaces. That's my own problem, I know.  Heavy use of statics and constants. Problem here: [hard to test, hard to extend]( What if part of my code needs to be setup differently. At runtime maybe ...?  SOLID, not so much .  No guideline to write good architecture . Just a ""put your code here and all will be fine"" mind setting I get. There is this bundles folder but the documentation does not state you should use them as first citizens to keep it clean. The same thing I criticize about Zend, there is nothing about how to use it the right way. I saw many projects of new developers, starting a fresh project with a framework and it was pain to read that code, but if they stayed on the framework-documentation they would just not know it better.   One good thing: Laravel uses Symfony/HttpFoundation. It wraps PHPs superglobals. Read the article of igorw about the [Value of HttpFoundation](  To state my point why I would from here on always choose a framework that uses best practices of our time with composer, components and vendors, and a guideline to get the fresh developers a good start over every hype:  The first page of Symfonys ""The Book"" about [Symfony2 and HTTP Fundamentals]( A introduction about what we are really doing here in PHP.  tl;dr: Use [Symfony]( if you don't know about architecture, yet. Use [Silex]( otherwise. Read the documentation. It helps becoming a better developer. "
"Which GUI Library/Builder/Path to create clean, functional, and/or cross-platform GUI-based applications? 
 Thank you very much for your help in advance, 
 InfiniteCuriousity","Hey guys, I hope this doesn't come off as rudimentary or a repeat of something else, if so I apologize in advance:  I'm trying to write an application that's GUI-based that has the ability to look clean, be intuitive to work with, display contents of files within the GUI and speak to the filesystem.  I have gotten most of the way through Zed Shaw's ""Learn Python the Hard Way"" but GUI's weren't really mentioned in the book. After doing some light reading I heard a lot of people recommend Tkinter / PyQt and a couple more.  I'll first be writing everything for use on a Mac, but I'd like it to be able to operate well on other OS's such as Windows if it didn't add a whole lot of complexity. However, since I'm relatively new at programming if it were easier to write it on Mac to begin with and then add Windows functionality later, I'd defer to your guys'/gals' expertise.  TL;DR : Which GUI Library/Builder/Path to create clean, functional, and/or cross-platform GUI-based applications?  Thank you very much for your help in advance,  InfiniteCuriousity "
Advice for a bad programmer to become a better programmer.,"Just graduated with a bachelors degree in information technology, majoring in software engineering. Now I know this is not going to sound very clever or anything but I never got good at programming (LOL FAIL software engineer), I focussed more on the high end design, project management and planning aspects of my degree, hoping I would get a job based more on my interpersonal skills rather than my technical ability (which would be pretty small anyway from degree based work (all really retarded examples(yes for nesting))).  I have incredibly poor programming abilities in almost every language I ever picked up (was always the best in class from 14-19), though I don't think I am a bad learner, I just never practised or experimented (due to a whole range of issues, one of the major contributors being poor teaching standards).  So what am I asking reddit to help me with? Give me some advice and learning areas so that I can improve myself. I don't want any handouts or anything just point me in the direction with some advice and maybe some links (also I have a few online resources, I just find they are poorly structured for me, and incredibly dull).  Finally, I am looking to do something in the Java/C++ vein, any advice here would be welcome, even if its not what I'm suggesting.  tl;dr Advice for a bad programmer to become a better programmer. "
Give a complete newcomer to the field resources/advice on programming.,"Little backstory first. I am in my second year of college at a local community college(Didn't know what I wanted to do with my life, what i wanted to major in, etc etc, Community College is significantly cheaper if I ended up failing miserably at classes), Switched my majors at the end of last semester, Liberal Arts to CS. I was a LA major mainly because I had no idea what I wanted to do.  So anyways I'm currently taking a Intro to C++ course and a few other required CS courses. I'm finding actually coding what I have to is insanely easy, ad it is fast approaching the end of the semester. I was wondering if you guys here in r/programming might be able to give me some good resources for learning more about programming, If i should attempt to learn more languages now, and any projects/things I could attempt in the month long break between semesters in order to challenge myself.  tl;dr - Give a complete newcomer to the field resources/advice on programming. "
"we need a new undergrad programmer, we want the best to come to us without interviewing them all. Any challenge or test you could suggest? 
 Thank you.","Hello proggit. I am myself a CS undergrad but I landed a programing job last year and I like it a lot. We are currently 3 programmer in the development section of the company and we have to work with pretty much anything were asked to do, so we deal with many different languages and learn them as needed for some quick jobs etc etc. We want to hire a 4th programmer and I'm asked suggest some in my class (a year younger, since I failed a class..) I don't really know any of these guys except my teammates which I wouldn't suggest. we don't really want to interview them all so I thought we could make a little challenge to help us choose who to interview.  We're in need of someone who understand the business eventhough they're new to it, and likes to learn new stuff and code.  Any idea on a programming challenge or a kind of letter saying why we should take them?  TL;DR : we need a new undergrad programmer, we want the best to come to us without interviewing them all. Any challenge or test you could suggest?  Thank you. "
CompSci student who likes coding but chose to work in QA still wants to code but can't self-motivate or self-set work particularly well.,"Idea  >An experienced coder (or coders) set weekly tasks designed to get less experiences coders to learn new things.  I think this would be much more interesting for fairly basic level coders like me who can only dedicate spare personal time to the pursuit of coding but also have trouble self-motivating or setting myself tasks.  It could easily just be a subreddit with assignment threads, etc.  Note  I know there's a programming challenges subreddit and the University of Reddit, I suppose I'm suggesting a website or subreddit that combined the two and was actually active ;)  Background  I'm a CompSci Graduate who works as QA for a software house. I did have dreams of becoming a coder and throughout University I proved that it was certainly something I could go on to do as a career. I even really enjoyed programming in my spare time (though not often) and coding for University tasks.  About two years ago I made a decision.  Rather than going in to work every day and programming and, by the virtue of the fact it would then be recognised as  work , begin to hate it I would diverge my career path from my immediate interests.  Before anyone suggests this was a mistake, I'm happy as QA so a career switch is not on the cards.  My problem is that when I go home I struggle to focus on self-assigned tasks since I spend most of my day following rigid process it seems laborious to go home and do the same. So what I really need is someone who is an experienced coder to almost tutor me in the form of assignments so that I can continue to work towards intelligent goals rather than the ones I assign myself and inevitably bail on.  tl;dr   CompSci student who likes coding but chose to work in QA still wants to code but can't self-motivate or self-set work particularly well. "
"We're a newly-founded, newly-funded Python shop, and we welcome collaboration, job apps, and PyCon sprint help. Contact us. 
 edit: paren","I am one of the co-founders of the COS ( As I said in the title, we're a newly-founded, newly-funded   non-profit with the goal of decreasing the gap between scientific values and scientific practices . We're in the process of hiring developers, but we'll be a  predominately Python, exclusively open source  shop.  The center has three main activities:   create an infrastructure for facilitating open science ( the Open Science Framework; OSF  foster the open source/open science community (coordinate events, give out grants, etc.)  conduct meta-science (science of science) research.   You can find out more about us at our [website]( or through the [press-release]( announcing the starter grant we received (also some press coverage: [science]( [nat. geo.](  Why am I here? We've already benefited a lot from reddit: I've asked reddit questions, we discovered a great design/UI consulting group that we are eager to work with, and I've met a few student programmers who have taken on projects with us.  Now, I'm looking to see if others are interested in getting involved.  We're working on a variety of packages that we'll use for the OSF, but want to be highly modular to maximize use for others in the community. Our stack currently consists of Flask, Mako, pymongo, beaker, git, jquery, ember.js, d3.js, mongodb, nginx, uwsgi, celery, and rabbitmq. We also want to support  API-style collaborations with projects that will facilitate openness (github, for instance; lots of API development needed) and have a redditor-led project creating a citation parser and input API to create a freely accessible dataset of the academic citation network (parser, database, visualization help/advice needed).  We'll be hosting a sprint this year at PyCon, and welcome any help/advice we can get. Even if you aren't going to PyCon, I'd like to make your acquaintance (maybe we can do a group Google Hangout) to talk about common interests. You'll also be seeing ads for jobs pretty soon, so turn off ad-block. ;)  So, if you're interested, leave a comment, send me a message, or contact us through the COS site. If you have any questions, let me know.  tl;dr: We're a newly-founded, newly-funded Python shop, and we welcome collaboration, job apps, and PyCon sprint help. Contact us.  edit: paren "
How can I get the coordinates/vectors that map out the surface of an object?,"Hi everyone.  I'm fairly new to vpython, but have been working with python for a while.  I'm trying to write a program to drive a 8x8x8 led cube. What I would like to do is be able to model anything in Vpython, say for example, a sphere moving across the screen, and have that get translated into the 8x8x8 pixel grid of my led cube.  My thought process behind how I would do this is take every point (to some degree of accuracy, but still be very quick) and find the closest led to this point, and turn it on. This way, anything that was modeled using vpython, could be displayed on this 3-d low res monitor of leds. Right now, the leds are being modeled and a cube of spheres that change colors when on.  TLDR; How can I get the coordinates/vectors that map out the surface of an object? "
My prompt box appears onload when it should just appear onclickl,"I'm having trouble with some javascript I'm working on. I made a button with HTML/CSS and I want to be able to set the background color by clicking it and entering a color into a prompt box. Here is my javascript:  function changeClr(){    var getClr = prompt(""choose a color"");    var btn_clr = document.getElementsByClassName(""btn"");  for(var i = 0; i &lt; btn_clr.length; i++){btn_clr[i].style.backgroundColor = getClr;}  };  changeClr();  The problem I'm having is that when the page loads the prompt box pops up and it doesn't matter if you enter anything, click ok or cancel, the page still loads fine. After that when you click the button the prompt come up, takes a color and sets the button background like it should. How can I get the prompt to stop appearing onload?  TL;DR My prompt box appears onload when it should just appear onclickl "
Python2->3 transition is confusing the hell out of lots of ppl... need some guidance :),"Hey guys,  I've just started programming for real.Before, I was just scripting in bash, since I am a sysadmin.  So I've decided I would learn python...I've been doing some stuff with GTK and I thought about developing some configuration checking for all my nagios3 server in python3.Then I've searched the web and found paramiko, but I could not find a python3 port that works for me.  This kind of thing keeps happening to me and it makes me feel like I am wasting my time banging my head on the wall...  Since there are a lot of people lost on this transition from Python 2 to Python 3, answer me this:  What should we know?  I've seen plenty of talks telling people to just do python3, to forget about v2.7.x and don't look back.But is it even possible?If so, how should one go about finding what library to use outside of the stlib world?  Thank you!  TL;DR - Python2->3 transition is confusing the hell out of lots of ppl... need some guidance :) "
Is it worth pursuing a way to better match API calls to users than API keys when it comes to JavaScript?,"I've found one of the great things about web pages is I can inspect them and find out how other websites have written their HTML, JavaScript, and CSS. This becomes a problem, though, when I want to use services with API keys: I can't use API keys inside JavaScript without them being exposed to the client.  I originally ran into the problem when creating my  Circles , so I looked into API services to send email through JavaScript. I didn't find anything, so I started thinking about the problem a little more and fully recognized the API key conundrum. I've been making a lot of GitHub hosted pages and figured I should pursue API key-less services for JavaScript. Has anyone else found a need for services where calls are from JavaScript only or is there really no application where there isn't a server available? I've got a couple ideas about how to get around the API key problem, but figured I should see if it really is a problem before going further.  tl;dr Is it worth pursuing a way to better match API calls to users than API keys when it comes to JavaScript? "
"Making flash cards. If this ALL of the current Operator types in PHP. There are 11 listed here. [PHP Operator Types]( 
 Thanks!!","Greetings!  I am rather new to PHP and I am learning it from scratch.  I am making some flash cards for all of the operators 'types' in php. (and then the operators themselves). IE Arithmetic Operators, Assignment Operators, Bitwise Operators, etc,. Every site that I visit has a different listing of operators that they say is ""complete"". Yet each site has no consistency. Each site has a different list of operator types.  I finally found this list here on the PHP website. --> [List Of PHP Operator Types]( There are 11 operator types on this list. This is the most extensive list I have found. Is this all of the operator types in PHP? I know a lot of people will tell me not to focus on this too much but I am pretty OCD and when it comes to learning something new I really like to learn it all and in order.  TL:DR Making flash cards. If this ALL of the current Operator types in PHP. There are 11 listed here. [PHP Operator Types](  Thanks!! "
"Is it really going to ask all those questions your IDE autocompletes? 
 edit: ok, certificationy is symfony2, not 3.. Silly. Still wondering though.","Doing the test over at  (and not even surprised at all since this it always was this way in IT certification land) and most questions are those like parameter names, function name trickery, config semantics, swapping parameter positions, you get the drift.  Basically everything your IDE autocompletes OR available via your best pal Google to search in Symfony's reference to get going in 5-10 seconds..  Is it really like that again? I thought i read something about them actually testing knowledge required to make solid apps. I know certificationy is user contributed but i just want to know what im preparing for, knowing symfony inside out, or knowing the order of all the parameters and the possibility of them swapping around camel cased syllables and bullshit like that.  tl;dr:Is it really going to ask all those questions your IDE autocompletes?  edit: ok, certificationy is symfony2, not 3.. Silly. Still wondering though. "
"Dynamics AX is terrible spaghetti code in a terrible development environment, but I'm stuck with it.","Does anyone else have to do development in this environment?  I'm really frustrated with it right now.  First, I had an issue with a button on a form not opening the correct form.  I finally found out that you have to create a new menu item to open the correct form, then drag it to the design of the calling form, and it will automatically create a button.  You can't just edit the properties of an existing button to point to the new menu item (well, you can, but it still calls the old form).  Took me like two weeks to figure that one out.  Next, I got an error popping up that other people have fixed by recompiling the class and form it referred to (a random system class -- SysSetupFormRun -- and the form I created).  The actual error is ""Error executing code: FormRun (data source) has no valid runable code in method 'init'.""  That didn't work, so my company said to go ahead and put a support call in to Microsoft.  I did this morning, and haven't heard back yet.  Then, a guy on my team, who is working in AX as well, told me he saw errors in the form I created, because there were red squiggly lines under one of the method names.  So, I tried opening the form again.  Same error as above.  Then, I closed the application, reopened it, and checked out the form again.  Now, that error does not show up, but I do get errors about fields not being declared.  At this point, I just don't know what to do.  My company bought this, I need to get changes to it done some how.  They are (luckily) relatively patient with me and other people here going slow because we're learning.  I just don't know what to do when I can't trust the application development environment.  I never know if something will work or not after closing and opening it.  I never know if an error will pop up or not with something that simple.  How do I trust it to work correctly and give me the correct feedback?  I've worked with poor coding in the past, but never on poor coding and an untrustworthy development environment.  TL;DR: Dynamics AX is terrible spaghetti code in a terrible development environment, but I'm stuck with it. "
Highschool senior wants to start doing small freelance jobs and is looking for advice on how to begin.,"I know no one likes these kind of posts so I'll try and keep this short. (I've been putting it off for a while but the payment for my webhost is coming up and I don't have the money)  Essentially, I'm a highschool senior who has been programming for a while and I consider myself competent, surely capable of creating most basic applications. (Going to New Mexico Tech next year if it matters)  I am really at trying my hand at some freelance web programming (and possibly desktop apps) but am not sure exactly how to start.  I have been considering this for a long time and have seen a lot of advice, but I""m not sure how much would apply to a kid with no work experience but no living costs.  From what I can tell my options are:   Local jobs to build up a portfolio ( how would you go about finding companies for this?)   volunteer work or for very cheap ( but I've heard a lot against this)   build up some experience by contributing to open source projects (How much value would this have in a portfolio)   Pet projects for a portfolio (had trouble coming up with ideas until recently... got a little bit going now)   Build contacts in the industry (No Idea where to start with this)    Given that I really don't know anyone who would know anything about this, I have turned to you. Anyone have any advice to give an aspiring freelancer?  tl;dr Highschool senior wants to start doing small freelance jobs and is looking for advice on how to begin. "
"looking for a virus that only destroys and replicates, for non-nefarious purposes.  Do not want the actual code","Not to use.  Not the code.  Just to know it exists.  I don't want anything to actually do with it.  Just its name, and possibly a place where I can verify your information.  That's all.  Here's the virus I'm looking for:  I'm looking for a virus that only does two things.  It should replicate, and destroy.  Nothing else.  Why am I looking for this, you might ask?  A friend and I are having a debate about the future.  I suggested that when nanite tech becomes ubiquitous, someone, somewhere will develop a virus-style nanite that will destroy us all.  He said that there were 2 reasons this would never happen. 1) No one has created a virus just to destroy, that all viruses (other than organic) have been designed for a purpose (attacking a particular network, damaging a particular individual and series of networks) not just for the sake of glorious destruction.  My rebuttal was that human are much better at creating destruction that undoing it, and that people are inherently irrational.  No one needs a reason to create it, just that it is a possibility.  Please help.  tl;dr looking for a virus that only destroys and replicates, for non-nefarious purposes.  Do not want the actual code "
"Does anyone know how to get ctags (version 5.8, if it matters) to pick up variables defined within a namespace as extern?","I'm working on writing a basic StarCraft AI script using BWAPI and BWSAL (both are google code projects that expose Broodwar's API for controlling units). Since I don't like Visual Studio all that much, I managed to set up a development environment in Vim instead.  Now, here's the problem. In order to get code completion working in Vim, I need to use exuberant ctags to create the tag file for all of my headers (the API are written in C++). And for the most part, it works great. But there is one particular case where it doesn't:  namespace BWAPI {    namespace UnitTypes {        extern const UnitType Terran_Marine;        extern const UnitType Terran_Ghost;        ...    }}  Any variables marked as ""extern"" in the BWAPI::UnitTypes namespace (and frankly, any other namespace) are simply not picked up by ctags. If I delete all of the extern's and re-create the tags, they're all there, but leaving it like that causes the AI script to crash, so I have to delete the extern's, create the tags, put them back, and  then  compile the script.  TL;DR: Does anyone know how to get ctags (version 5.8, if it matters) to pick up variables defined within a namespace as extern? "
"online C++ test tomorrow, any tips for someone who knows C.","Hey /r/programming, I have a job interview coming up, and as part of it I have to take an online C++ test tomorrow evening (although I haven't been emailed the details yet, which is worrying).  I am fairly experienced in C, but don't know a lot about C++. The test has a time limit for each question and from what I can gather is multiple choice with the choices being pretty close.  I am reading this now, [C++ tutorial for C users]( and it seems there are a few different C++ tests online, [such as this one]( which I am going to do.  I have also downloaded the [Thinking in C++]( book which is supposed to be aimed at C programmers but I might not have time to go through it.  Any extra tips you can offer are appreciated, and sorry if this has been posted before or if I chose the wrong reddit.  Thanks.  tl;dr  online C++ test tomorrow, any tips for someone who knows C. "
what python distribution do you use for science stuffs.,"Greetings, sports fans. Any thoughts on dealing with Python / NumPy / SciPy / OpenGL on the Mac? It seems that there are a few options when it comes to 10.6 / Snow leopard-  1) I could stick with the distro shipped with the OS. It's 2.6.1, 64-bit, has a numpy included. I've added ScyPy and OpenGL, but then there's the whole problem of keeping the interpreter itself up to date. 2.6.5 exists, and now I'm livin' in the past baby. (I'll admit that I'm not 100% aware of the packaging / self-updating tools out there)  2) I've use Enthought  which is aimed at science folk like myself. It'd likely be the best, since they package a bunch of science related packages and have a nice autoupdate service. But, they want $199 for this. The free academic version is 32-bit (we do big-butted calculations on machines with big butts) and doesn't include the update server. I'm certainly not against spending the $. It looks particularly promising for those of you looking to migrate away from the much-loved-and-universally-celebratd Matlab :)  3) I've recently stumbled upon ActiveState's distribution,  the free  'community version' of which is 64 bit, doesn't include Sci/Numpy or OpenGL, but I'm sure its easy enough to integrate. I'm not sure how well it plays with the OS in general because I haven't played extensively yet.  4) I could obviously do macports/fink, but I think I'd rather stick with the Apple distro.  5) The ScyPy superpack (originally at  was interesting to use- basically just a nice shell script that linked to a bunch of hand-tweaked ports of the various tools above. It's handled by just one guy, and he's moving around so I'm not sure if it's the best idea to hitch ones wagon to that yet.  So- what I'm wondering is- what do those of you who need lab-grade Python do on the Mac? Especially when you want to keep a few machines up to date, make cross platform stuff practical, etc?  I don't necessarily need much of the 'value added' stuff- IDEs, etc, since I just use Emacs or BBEdit, but if someone came up with a nice interface for the debugger I'd probably be interested somewhat. I really just would like something easy to maintain.  Thoughts? Opinions? Bueller?  tl;dr- what python distribution do you use for science stuffs. "
What skills does a programmer need to know nowadays and what broad classifications of programming jobs exist? Also: What is a waste of time learning?,"I've heard the terms downwind/upwind in terms of describing programming jobs before, and basically what is meant by ""upwind"" is a job that keeps all of your skills sharpened, you're using up-to-date technology, you're being regularly challenged by the work, and you're skill set is constantly expanding.  In contrast to that hypothetical ""upwind"" job, I've been working a ""downwind"" job. I work with VB6, find the work mostly boring and tedious, and worst of all I feel like my skill set has been stagnating (which does not bode well for my future job prospects, should this one fall through). To some degree I feel like the programming world is passing me by.  There have been a lot of advances that I catch wind of but never have the time to try out, never knowing which ones to pick over others as I have limited time. I'm an applications programmer by trade, would it make sense to learn web application stuff? But more than just that, I want to know what basic skill set a programmer is expected to understand in this day and age in order to be considered competent. I don't want to waste time learning X, Y, Z technologies if I will never use them on the job, or if they will not help me find a job.  TL;DR: What skills does a programmer need to know nowadays and what broad classifications of programming jobs exist? Also: What is a waste of time learning? "
"I can automate my job, should I? If not, I need you to tell me how to occupy my time!","I find myself automating myself more and more at my daily job, to the point where I have hours of free time a day (be aware I count browsing reddit as non-free time since I NEED to do so).  Should management be to blame for not seeing this opportunities to streamline my job load? If I automate myself out of a position I'm sure I would have opportunity working on other projects, but wheres the incentive?  Anyone else experience such things? My real problem is how do you feel the idle time? I try to 'space out', but it's hard for hours at a time. My back is to the hallway, so I can't really play videogames or this thread wouldn't exist :P  Tips?  TL;DR I can automate my job, should I? If not, I need you to tell me how to occupy my time! "
"How do I 'click' a javascript button from python? 
 EDIT :  Thanks for the help!","Hi.  I'm trying to make a program that will regularly pull a page from a local http server, check it for new information, then do the equivalent of pressing a few of the buttons on the page.  Problem is, most of the buttons on the page call javascripts, and I don't know how to simulate the 'push the button' behavior through python.  I've tried manipulating the url I pass to urllib, but if that's the answer I'm still not doing it right.For clarification, the 'server' in this case is a copy machine on the LAN, with a webpage front end that stores our faxes.  I'm trying to get those faxes.  The buttons make calls structured in a few different ways.  The page redirects were simple enough to work around, but I'm lost on things like this:    javascript:ChengeShow(3,document.DocboxListForm)Any hints? edit:  Forgot to mention, I'm using Beautiful Soup to parse the html after pulling it in with urllib.  TL;DR  How do I 'click' a javascript button from python?  EDIT :  Thanks for the help! "
Is it a shitty thing to not want to give my program code to other departments in my company because they want to modify it?,"I work for a fairly large company and created a program for my departments use. Well it has gotten around the company somehow, and everyone wants it. Thats great and all, but they all want specific things changed, and others even want to be able to modify it themselves to their own benefit.  Am I just being selfish / greedy?   I don't want to keep making changes to this thing for others, yet I also don't want to give away my work that I slaved over for a while so they can modify it and claim it as their own.  I created it to help my department's functions and I don't really feel I am obligated to give it to everyone else.  tl:dr ; Is it a shitty thing to not want to give my program code to other departments in my company because they want to modify it? "
is Drupal as much of a chore to work with as WordPress? Would you take a job that focussed solely around building a custom application in Drupal?,"I currently work as a web developer for a digital agency. Sometimes we will build custom web applications using Laravel. Most of the time however, we end up heavily modifying and extending WordPress.  I've been working with WordPress for many years, am very confident building solutions with it, but just feel that there's more interesting work as a web developer. As such, I've started looking around for new opportunities.  One opportunity that a recruiter brought to my attention was working with Drupal. This is one CMS that I've never worked with before. Can anyone provide any insight as to whether it's a decent piece of software to work with and extend? Or is it another WordPress and is a chore or bore to work with?  Thanks in advance.  TL;DR is Drupal as much of a chore to work with as WordPress? Would you take a job that focussed solely around building a custom application in Drupal? "
contact me if you are interested in a project that involves the title.,"I am about to work on a WordPress project that requires a visual representation of donations received. There will be multiple projects seeking donations on this site simultaneously and each requires its own progress bar.  Example: Project 1 has a goal of $5,000, project 2 a goal of $20,000, and project 3 a goal of $50,000. If project 1 has raised $2,500, the progress bar needs to show 50% of the goal has been achieved, project 2 $15,000 of $20,000 progress bar needs to show 75%, and so on. Essentially, I need what Kickstarter displays for all the various projects receiving donations.  As stated previously, it needs to integrate with WordPress. It also needs to integrate with PayPal.  Is this best accomplished in JavaScript given the parameters outlined above? If not JavaScript, other suggestions? How long would something like this take for someone familiar/skilled in the programming language? What would something like this cost?  tl;dr  contact me if you are interested in a project that involves the title. "
"Maybe I'm overthinking this here, but I'm not sure if I should write forgiving code or strict code. Thoughts?","When I'm working on large projects, I find myself writing fairly forgiving code. That is, my functions will accept a large range of arguments, and then attempt to sort out what the arguments should be themselves. Here's an example of what I mean:  var things = {    foo: {a: 1, b: 2},    bar: {a: 3, b: 4}};function logThisThing(thing) {    // if argument is a string, go and get the proper thing from the things object    if (typeof thing === ""string"") {        thing = things[thing];    }    console.log(thing.a + "": "" + thing.b);}logThisThing(""foo"");logThisThing(things.bar);  This style of code has two results: first, my functions are going to wind up longer, because I'm spending extra lines ensuring that I have the argument I expect. Second, I don't have to remember exactly what to pass into a specific function: I just know that I've ""planned for everything"" and chuck in whatever I'm working with at the moment.  In the example above, if I happen to have a property of  things  as a string, I know that  logThisThing  can handle that--otherwise, if I have an actual property of the  things  object, I know that the function can handle that as well. If I can't remember what to pass in, this makes it a bit simpler for me, because I won't have to go look up that function to see what I should be passing in. However, I can also see this leading to me getting lazy, and passing in stuff that I've not managed to plan for, which could lead to obscure and confusing bugs in some cases.  How forgiving is the code you write? Are you intentionally extremely strict, only accepting arguments in one format, or do you provide a little more leeway / error correction? What do you see as the pros and cons of each method?  tl;dr : Maybe I'm overthinking this here, but I'm not sure if I should write forgiving code or strict code. Thoughts? "
How does the web application development process work from coding to real usage?,"Hi, as stated in the title, I'm interested in learning how does my application get from coding to the user. Being self-taught, I miss a lot of 'filler' knowledge from the uni that would allow me to see the bigger picture. Until now, I've been focusing on pure programming (mostly front-end but I played a little around simple APIs and know how to use REST) so I'm able to create my desired application but I'm quite confused about what to do with it, once I build it for production.  Let's take an example situation with a question - How do I put my app online when I have the app and a virtual linux server (e.g. google compute engine)  I've written a simple blog (simple user login, displaying/editing/creating posts and commenting on them) using React, webpack (with webpack_dev_server) and node. I have finalised the app, ending with index.html and bundle.js. This is where I start getting confused.  Do I need to set up the hosting server for the app to run? How do I achieve situation described in example situation? (working blog accessible from the internet)If I had an app with backend as well, do I need to separate it somehow?  Any enlightenment will be appreciated. Not looking for a detailed walkthrough, I'm much more interested in understanding how does the ecosystem work as a whole so what I'm looking for is a list of topics that I can use for broadening my web development knowledge and that would answer my questions.  tl;dr How does the web application development process work from coding to real usage? "
I have a system that has strict validation rules and new requirements have me unraveling all that to import incomplete customer historical data. How should I proceed?,"I'm working on a system that manages financial transactions and payment method details for organizations. The system was designed requiring certain data when creating objects (think payment type, account types, expiration date, and more of that sort of thing). Everything works great on that end. Until...  New requirements emerged that have us building import functionality via CSV. The data we've been given seems sort of incomplete, but it's all we'll be able to get. We've gotten a long way in translating that data into the correct objects and satisfying some of the validation requirements, but the data we're working with is missing key things needed in the UI and data model (like card types, last four, bank account type, etc). Basically we're missing some key things to create our polymorphic objects.  When creating transactions and payment methods, my service classes take the form data and translate it into the correct handler objects (BankAccountCreator, CreditCardCreator, etc) and each of these get validated according to their type (BankAccountValidator, CreditCardValidator, etc). Now that the new requirements have us basically ignoring our original validation requirements, then I'm unsure of how I should tackle such a problem.  I can write another implementation that ignores the data we don't have. We can handle the cases in the UI where the data is ambiguous and use, for instance, a generic card icon or 'N/A' in spots where we don't have what we need. But without the needed data, we're essentially throwing out any data integrity we have and the system gets a whole lot more hard to reason about. We'll only have more of these issues as we onboard new customers with their diverse data too, so has anyone ran into this situation? What did you do?  TL;DR: I have a system that has strict validation rules and new requirements have me unraveling all that to import incomplete customer historical data. How should I proceed? "
"My predicted probabilities never dip below the proportion of states in the train set, so the model always predicts ""No Churn"". The threshold needs to adjusted somehow.","Hey guys,  I'm interning at a software company and I've been working on a project to generate risk scores of our client churn (turnover). I'm using a 2-state Markov Chain (No Churn, Churning). Essentially the workflow is this...  1. Gather relevant data on clients (SaaS fee, usage, # of users...) on a monthly levelfor runs = 1000{    2. Split data in test/train sets.        - Stratified sampling, try to keep the proportion of good/bad accounts as in the original data        - Trim the last three months of data from test set, save elsewhere to compare for                   accuracy at the end    for n_cluster = 1:9{        3. Unsupervisedly cluster the clients into # of n_clusters            - This way we get groupings of similar accounts        4. Create a transition matrix per cluster        5. Forecast three months of state probabilities for the test accounts            - The forecasted probabilities are compared to the training set cluster's               proportions            - If {P(Not Churn) &gt;= Proportion of Not Churn --&gt; predict No Churn that month}               - Else{predict Churn}        6. Compare the forecasted states to true states; get accuracy    }}7. Select n_cluster with the highest accuracy to forecast for all current accounts, real forecast for upcoming months.  Steps 2 through 6 are repeated ~1000 times to bootstrap the accuracy. We have few accounts to work with (430), so the accuracy per # of clusters can vary greatly depending on the accounts that were put into the test set.  Problem:  In Step 5, the model is overly optimistic... No predicted probabilities fall below the clusters' proportion and the model predicts ""No Churn"" 99.9% of the time.  So the prediction threshold needs to be adjusted somehow, but I don't know enough about Markov Chains yet (2nd year stats undergrad) to know how to edit the threshold. Maybe compare the predicted probabilities to the proportions in the entire train set?  tl;dr  My predicted probabilities never dip below the proportion of states in the train set, so the model always predicts ""No Churn"". The threshold needs to adjusted somehow. "
I did help to understand how to parse data from a a specific web application service,"Hello /r/javascript. I am working on a project to collect, store, and display power outages for the entire USA. In most casses I am using a power companies API/ open services / or scrapping their web page to pull the outage information.  However In this project I have run across a program that many small power companies use called MapWiseWeb, I have figured out how to pull the data from their service but I can not figure out how to parse it. This is normally done a Javascript script but I know only the basics of javascript and it is way above my head.  I am hoping someone can help me out and take a look at this to figure out how the front end javascript is parsing the data.  An example website using this application:  The code to get the data using powershell:  $headers = @{}$headers[""coop.nisc.outagewebmap.configName""] = ""Outage Web Map""$headers[""Content-Type""] = ""text/x-gwt-rpc; charset=UTF-8""$headers[""X-GWT-Permutation""] = ""DD6ED31F104A049B10E5801C2C1142F5""$data = Invoke-WebRequest "" -Method POST -Headers $headers -Body ""7|0|4|$data  The javascript that calls this is also loaded from the service and is callled/created by the mapwiseweb.js script on that webpage.  tl dr: I did help to understand how to parse data from a a specific web application service "
"Really inexperienced, want to get started at literally any point in the data analysis. Understand self-study is crucial but I'm looking for a starting point career/job-wise.","Little background on myself,majored in economics B.A and through my second year of college decided to do a joint major in Mathematics which involved some abstract math classes as well as qualitative math classes, like statistics and data analysis class.I've graduated with a bit of experience with R, did a kaggle competition as well but other than that that's pretty much it. Quite frankly I would also say that my grounding in statistics would probably needs some working on.But all of this can be done in my own time and with some self study.  My issue is that I'm looking for a job and I don't seem to qualify for even entry-level jobs, or rather I don't think i'd even make the cut. Jobs i'm looking at are data-analysts/junior data analysts.I would like some advice on what possible job scopes/titles I should be looking out for that would help me progress as time goes by in the industry, or perhaps I need even more self-practice and projects before I start applying.I'm just concerned because while I understand I'm really green to the field, I'd like to know where I can get started career-wise if i'm looking to break into the industry.  TL;DR: Really inexperienced, want to get started at literally any point in the data analysis. Understand self-study is crucial but I'm looking for a starting point career/job-wise. "
"I've built a little tool and I need it critiqued for educational purposes. 
 Here's the [Repo](
Thanks.","Hello Reddit,  This is my first post here and I'm looking for your opinions, I've built a few web scrapers for Freelance projects using plain cURL and XPath and that would get dirty when the scraper has to grab a load of data from a page so I thought maybe I could organize it a little bit by putting it into classes and organize some of the code mess.This is also an educational project for me and for now it has got me to know of topics like Dependency injection, Autoloading, PSR standards and Unit testing, A fellow redditor has also helped by pointing me to some topics and did some code changes that helped me understand.So I'm putting the link to the Github repo here hoping it can help someone or get me some useful points, I know it doesn't have much functionality but I'm working on it as much as I can, It would be great if you like the project to contribute too.  tl;dr: I've built a little tool and I need it critiqued for educational purposes.  Here's the [Repo](Thanks. "
I'm a noob regarding dates can someone explain the correct way to handle them?,"So what's the good way to handle dates?  I'm producing a new future Date() in my client side (user can only pick time and day but can't set GMT) and then sending it to my node server which will store it on the database to later compare it with the current Date() (also not specifying GMT).  I have the feeling this is working in localhost because my server magically knows I'm in GMT -3 and my client also magically knows the same. So the comparation works.  Will this also work when I upload it to heroku? The server and the users may have different GMT, but since the date stored in the database seems to contain this information one part of me thinks this could still work.  tl;dr: I'm a noob regarding dates can someone explain the correct way to handle them? "
what is the best way to replace many searches with html in a very large string?,"I'm writing a forum where I want the post content to be parsed clientside.  I have the regular stuff like replacing [url=google.com] with a clickable link etc, but I also need to replace keywords like ""GOLD"" with an image (<div> with a sprite as a background-image and bg-position) for formatting posts. A snippet below.  // Help linkhtml = html.replace(/\[help[:|=](.+?)\](.+?)\[\/help\]/gi, ""&lt;a help='$1' href='/help/#/$1/'&gt;$2&lt;/a&gt;"");html = html.replace(/(\[help\](.+?)\[\/help\])/gi, ""&lt;a help='$2' href='/help/#/$2/'&gt;$2&lt;/a&gt;"");// Urlhtml = html.replace(/\[url[:|=](.+?)\](.+?)\[\/url\]/gi, ""&lt;a href='$1'&gt;$2&lt;/a&gt;"");html = html.replace(/(\[url\](.+?)\[\/url\])/gi, ""&lt;a href='$2'&gt;$2&lt;/a&gt;"");// Formathtml = html.replace(/GOLD\+/g, template_icon(""gold"").string());html = html.replace(/CHEST\+/g, template_icon(""chest"").string());html = html.replace(/MONSTER/g, template_monster().string());  I feel this is very ineffecient, considering that I have a hundred lines more of this.  It doesn't run slow yet, but the templates gets run regardless if the search is found, which is a concern.  I need to use the templates because they're used on the rest of the site too - the templates return a jquery html object a la $(""<div class='gold'></div>"") - but sometimes more complex.  Should I go with this? Or is there a better way?  tl;dr  what is the best way to replace many searches with html in a very large string? "
"How do I export this live Java chat data to a .txt, .xlsx, etc.?","I am wanting to collect and store data from a website's chatbox (using the now-unsupported [nChat]( to be analzyed later, potentially with [Leximancer](  Here is what the container and examples of the information I want to grab looks like (anything in all caps is a placeholder):  Code:  <div id='nChatCon'>  <div id='nChat'>  <p id='MESSAGE NUMBER IN SEQUENTIAL ORDER (e.g. 'msg7886804')' class='chatmessage'>  <a title='17.11.13 01:03:40' style='color: #8da0b9; font-weight: bold;' href='javascript:void(0)' onclick='nChatMenu(88077, ""USERNAME"", 7886804, event)'>USERNAME</a>: <span>USER INPUT</span></p>  Basically, I'm just trying to figure out what I need to do to is grab all instances of the 'chatmessage' class and export them along with the data, time, username, and most important the input of the user. So I guess the nChatMenu information isn't really useful, but I included it just in case.  TL;DR How do I export this live Java chat data to a .txt, .xlsx, etc.? "
"how to store credentials for re-use? DB, Sessins, some other methods?","hey r/php,  I have been thinking on this issue for a while and doing some research but I wanted the community's opinion before i begin implementing anything. also I am by no means a security expert so hopefully someone out there has some experiences with a similar scenario.  The problem to solve:  In my application(in house webapp at my company) i have a need  to reuse user credentials/store user entered credentials for the purpose of calling other application's APIs. let me walk through a scenario to make it more clear.  user logs in using LDAP so credentials are not stored on db my application allows the user to call application B's API  the problem here is I am not sure the best way to store the user's password. so that I can use it when calling other APIs.  Ideas I have had:   when the user logs in store the credentials in DB (encrypted) then decrypt when needed.(is this secure? what type of encryption to use?). this would also work nicely if the other app has different credentials, I could allow the user to store them for re-use.   when the user logs in store the credentials in the session and pull them out when needed. (encrypted?)    do either of these sound like good methods of doing this? I am leaning towards the DB option. are there any better methods for this?  TLDR:how to store credentials for re-use? DB, Sessins, some other methods? "
What the best way of storing multidimensional time series data in python for plotting with matplotlib ?,"OK...python newbie here...and this is doing my head in.So I'm collecting CSV data from 5 sensors (but I'd like to write code so it doesn't really matter if its 50, 500, 5e6).Every sensor sends light, temp, humidity, pressure, CO2 data, timestamp and a station id every minute. The data is a mix of floats, ints and a string for station id.I want to store 5000 of these sensor readings (about 3 days worth) and plot them using matplotlib (i've got it working with plotly but it has limitations for stream data).  My question is...can anyone offer some guidance about the best data structure for storing this kind of information.I started off with a dictionary for a single observation of temp, humid etcFor the historical version I've added these dicts to a list. The problem now is separating all of the temp readings from humidity from light etc. etc.I seem to be ending up with really complex data structures which are getting increasingly complicated to iterate through and given pythons elegance there must be an easier way.Should this be a job for numpy or pandas ? TL;DR What the best way of storing multidimensional time series data in python for plotting with matplotlib ? "
I'm pretty pathetic in the sysadmin department and need a new workflow/environment set up that is quick and easy to test changes before pushing live. Please help.,"Tried /r/webdev, didn't get a response there, I know it's not really specific to php but this is my favourite dev subreddit so hoping you guys may be able to help me out..  I'm looking for a new set up, at the moment I have a computer in one room with a virtual machine running a very similar set up to my production server. I use samba to make the www directory visible on the network, then I modify the files on the host OS or on another laptop in the network since I prefer to develop with Windows rather than Linux which is what my virtual machine/production server are running.  This way I can test my changes instantly in the browser and the files are usually good to go for a direct upload without having to do any staging since I've never had any config differences in the dev ""server"". I usually just rsync and it just works.  It's started to slow down now though, if I run any heavy server scripts like an MVC framework then that can result in memory errors. Netbeans is pretty much refusing to open projects from the network drive that I set up with samba and when it does open there can be an input delay.  This is definitely becoming too much for my host machine to handle so I'm wondering what is the optimal way to progress here.I really want to avoid a workflow which involves more than saving a file to test my changes, I don't want to be committing or rsyncing or scping anything in the development process. I would also prefer not to install any server software on the machine I develop from since I want to keep it running as snappy as possible and I don't really know anything about servers on Windows.  Would it be possible to get a remote server, like a clone of my live server and access the files through a samba share? Would this even be wise? Would it be painfully slow?  tl;dr  I'm pretty pathetic in the sysadmin department and need a new workflow/environment set up that is quick and easy to test changes before pushing live. Please help. "
"As a JavaScript grows, do you need classical inheritance, and does it improve the app?","I recently helped out as a tutor for a class in JavaScript called JavaScript OOP. While I'm familiar with OOP as well as JavaScript, for me that's not something I usually put together.  But by the main teacher on this course I was told that when building large apps with js you can use AngularJS and similar for smaller apps, but as soon as your app grows you will always need  classical inheritance  large JavaScript apps.  The library they were using on the course was [this, called classier]( In my mind that seems like forcing a language into something that it's not, but I might be wrong.  TL;DR As a JavaScript grows, do you need classical inheritance, and does it improve the app? "
Accounting graduate who just moved to data analyst role seeking for advice to further studies in data science/machine learning in the UK.,"Hi reddit, I am 25 this year, graduated with a commerce degree in accounting and commercial law, been in the workforce for 28 months now, spent my first 26 months doing accounts receivable, data entry, reconciliation etc, just moved to a data analyst role for the last 2 months and I found it's way more interesting.  In the meantime, I have done the course 20461C: Querying Microsoft SQL Server 2012-2014. I also have hands on experience with Microsoft SQL, Microsoft Reporting Services, Visual Studio etc. Basically I'm doing data extract and spending most time learning the basics as well. currently im working on a project in reporting service creating a new specific dashboard for my company.  I don't have much experience yet but I am thinking to do more studies, possible in the UK or US. I was thinking about MBA or Master in Science and technology etc. If I want to move further into data science or machine learning in the future, which path should I be taking? Is MBA really that important for future managerial role like analytical manager? What sort of computer science studies should I take if I want to move forward into machine learning?  TL;DR - Accounting graduate who just moved to data analyst role seeking for advice to further studies in data science/machine learning in the UK. "
"How does Javascript interact with HTML and web browsers, and where/how can I simply write and execute Javascript?","I feel like this should be the most obvious thing, and I'm sure it's due to personal ignorance about some foundational concept, but I can't seem to find a place or way that I can write and run Javascript.  From what I can tell, browsers don't interact with Javascript in the same way they do with HTML, in that you can't just write a JS program in Sublime Text and run it by opening it in a web browser.  I guess what I'm looking for is something like the consoles that are provided by Codeacademy or the interactive curriculum for Eloquent Javascript.  I thought jsfiddle.net was something like that, but taking code from either of those sites and plugging it into the JS area of a fiddle produces nothing in the ""Results"" area.  A consistent piece of advice that I've seen in any thread about learning to code is that while going through an example based curriculum will get you started, the real key is messing around with the language on your own and coming up with original programs.  TL;DR - How does Javascript interact with HTML and web browsers, and where/how can I simply write and execute Javascript? "
"looking for arguments pro/cons listed frameworks. I'd prefer Aurelia or Flux over Angular, but the main concern is stability for prod, deploy estimated in Q4 '15.","I noticed all the fuss about the ""new great framework"", [Aurelia]( The whole idea and all the promising about fixing stuff that needs to be fixed in Angular seems amazing to me. There is also Facebook's [Flux]( which I haven't researched a lot, but from the success of React, I expect this to be just as great as React is. And of course there is [Angular]( which we all know already.  The application I'll be working on will include agency offer presentation view, which is not a requirement, but I figured it would be nice to have for better UX. The better part of the application is going to be admin panel, where we want to build a SPA, but I'm still missing a lot of details at this time.  Just for reference, I'm not too familiar with angular or any similar frameworks, so far I just did some really small projects, nothing requiring a lot more knowledge than what you learn in the demo projects. This means I'll have to learn a lot either way. To be honest, my first choice would be Aurelia, followed by Flux, but I'm  not  going to risk stability for the sake of the new sexy. What are your expectations for stability of the two in Q4 '15 or in early '16?  I'm curious about using any of the frameworks in production, what are their limits and pitfalls. There are a lot of post, rants and praises of angular, but I haven't noticed a lot of arguments for the other two, probably because they're relatively new. At this points I'm looking for any blogs, links, comments to develop a constructive argument of choosing the framework, so I'd love to learn more.  If this is important: I'm planning on working with Typescript. Also support for IE9+ would be nice, although it's not a complete must for now, we expect most of our audience to have most recent browsers or use browsers other than IE.   tl;dr : looking for arguments pro/cons listed frameworks. I'd prefer Aurelia or Flux over Angular, but the main concern is stability for prod, deploy estimated in Q4 '15. "
Need Affiliation Network with timestamps on each edge for thesis,"Hey folks, I already asked this in the r/sna subreddit but there isn't much going on, please tell me if there is a better subreddit to ask if this one is not right.I'm an undergrad student and I'm about to write my thesis, but I need help finding data that fits.The idea is to observe the evolution of communities in Affiliation Networks. To observe the evolution I need a network with timestamps on the edges. I googled a lot and couldn't really find an affiliation network with timestamps such as here:  or  Does anybody here know other sources that might be worth looking at? I found several bipartite graphs with timestamps, such as for when a user listens to a song from a musician. My idea was, if can't find anything, to create out of this data an affiliation network of a user that likes at time t=0 the band XY and observe how/if this changes until t=1. The assumption is basically that a user isn't into a band anymore if they didn't listen to it in the next interval. In general this model is far from reality I guess, but it's a start.I could group the data in the musician model by genre and investigate whether there is a pattern to if/how the users change there music preferences. Such that: If a new musician/band appears, does the user still listen to songs they were before, to the exclusion of the new music, or do they keep track with what is popular/new at the moment?TL;DR Need Affiliation Network with timestamps on each edge for thesis "
how do I properly assemble kmers to form a string using a De Brujin graph?,"Hello /r/python,  I've already posted a similar thread to /r/learnpython and they have been helpful in implementing a work-around naive algorithm to perform string assembly, however, I'd like to try to accomplish the same using a De Brujin graph.  I've posted below then intended pipeline:  reads:  ['GTCTCGATAA', 'CTCGATAA']all kmers and counts: [('GTC', 1), ('ATA', 2), ('CGA', 2), ('TCT', 1), ('GAT', 2), ('TAA', 2), ('CTC', 2), ('TCG', 2)]filtered kmers:  {'ATA': 2, 'CGA': 2, 'GAT': 2, 'TAA': 2, 'CTC': 2, 'TCG': 2}Intended output: CTC -&gt; TCG -&gt; CGA -&gt; GAT -&gt; ATA -&gt; TAA ---&gt;&gt; CTCGATAA   I'm honestly not sure if filtering the kmer frequencies is necessary in this particular example but my 'real' dataset contains 1000's of reads of length 50 which I'd like to split up into kmers of length 10 (so the real dataset is significantly larger).  What I'm having issues with is actually implementing a De Brujin graph with my filtered kmer dictionary. Any help would be greatly appreciated.  TLDR: how do I properly assemble kmers to form a string using a De Brujin graph? "
"Can you run a web scrape on a website using a repetitive loop using a search by zip codes, and would anyone be willing to help me","Hello,  I've never used Python, and don't have much knowledge about how it works. However my friend suggested I might be able to use it to collect data for a research paper.  There is this non profit organization (PM me for details) that has a database of locations that users can search to find the nearest site, and filter by desired attributes. The webpage uses google maps to feature show the locations, but only displays 10 results per page. In total there are 18,600 sites in the U.S. that I am interested in. You can filter by attributes of the site, such as join date or program type, or by geographic filters of address, zip, state, etc. Zip code seems to be the smallest unit of observation that is searchable.   I don't think there are many cases where a zip code will have more than 10 sites, but in very dense places like NYC, a few zips have more.   Would it be possible to have a code do repetitive loop entry of the zip codes and scrap the search results for each zip code in the U.S.?   At the minimum, I would want names and addresses. The other site attributes would be nice, but auxiliary and unnecessary.    TL,DR: Can you run a web scrape on a website using a repetitive loop using a search by zip codes, and would anyone be willing to help me "
"Having trouble syncing data between sockets, not sure if solution is feasible. 
 Thanks in advance for your response.","I'm writing an application which has to handle many connections with phones on a local network at once. For now i have trouble synchronizing just one stream. To solve the problem (temporarily) i've built in sending 'virtual' flags like ACK, SYN and FIN so that the methods sync each other somewhat.   with  chunk  i mean a piece of compressed  and encoded plaintext that may be larger than the buffer  with  packet  i mean that the chunk has been broken down into packets that fit within the buffer  with  number of packets  i mean the amount of packets that the chunk has been broken up in to fit within the buffer   The  send  method works like this:   Send the number of packets to be received  Wait for a virtual ACK  If the virtual ACK comes in continue to 4 else return false  Sequentially send the packets, after each packet wait for a virtual SYN  After all packets have been sent, send a FIN  Wait for the last FIN   The  receive  method works like this:   Wait for the number of packets to be received  Send a virtual ACK  Wait for the packets to come in, send a SYN after each packet  Wait for a FIN  Send a FIN   I haven't built in retry, windowing, encryption or verification by checksum yet. Because i'm not sure about my approach and i don't want to waste my time.  Now, my questions are: Is my approach right, can i use this when i'm handling many connections at once, and will i be able to distinguish data from different devices easily?  TL;DR: Having trouble syncing data between sockets, not sure if solution is feasible.  Thanks in advance for your response. "
"version: When making a shelve, open it with 'protocol=2'.  Also, namedtuples remain awesome. 
 Edited: to tweak spelling","I love namedtuples.  I use them all over the place.  My canonical example usage is ""I have a bunch of network switches.  Each has a username, password, and IP address.  They don't change (often) and if they do, I'll build a new namedtuple for them""[1].  So, I was informed recently about the shelve module.  Sweet! A better tool than a pickle!  Unfortunately, after I got to coding, shelve + named tuple blew up on me, repeatedly, and I couldn't figure out why.  I was under a time-crunch gun, so I went back to using pickles with a sad face.  Tonight I'm reading up, and I see that shelves use pickle format version 0.  Some further reading, and I notice that namedtuples are effectively a class-generator... and they generate new style classes.  A bit further reading and I find that pickle version 2 is required for new style classes.  Then I get sad, and start searching for PEPs to update the shelve module to use pickle v2 instead... and I find that you have to specify ""protocol=2"" when you open a shelve file.  [1] Other useful examples: a) in AWS firewalls have a name and an ID# that's globaly unique.  If you work in multiple regions, keep the names in sync, but in some contexts you need the ID, but others you need the name.  Named tuple to the rescue.  fw.id vs fw.name.  b) minding the running state of AWS instances.  Read previous version from pickle (or shelve) hold new version in dictionary, both keyed off instance id.  Running state in my case was (name_tag, running_state, region).  TL;DR version: When making a shelve, open it with 'protocol=2'.  Also, namedtuples remain awesome.  Edited: to tweak spelling "
Recovered photos. All jumbled. Script that identifies duplicates and takes file that has the lesser number (file000987.jpg rather than file000988.jpg),"Hi guys, I recently updated my laptop to OSX Yosemite and unfortunately lost over 3 years worth of photos. I was gutted at first but I quickly calmed myself and looked at my options. After no hope of finding my '.photolibrary' file I looked to data recovery software. I ran DiskDrill PRO and recovered all my files, however they're all jumbled.  This is where I need you guys. It would take me countless hours to manually go through the jpg files and pick out which ones relate to my iPhoto. If somebody could help me in writing this script then it would be greatly appreciated as there are a lot of precious photos in there.  Script Requirements;   Take the highest resolution of each image as there are lower resolution duplicates (It's always the file with the lower number that has the higher resolution e.g file000978 > file000979)  Only get images that fit within the confines of an iPod 4/iPhone 5s picture or screenshot (certain height:width ratio)   If somebody can help me with a script with just the first requirement then that would be amazing as I could get by without the second and do that myself. Thanks for reading.  TL:DR; Recovered photos. All jumbled. Script that identifies duplicates and takes file that has the lesser number (file000987.jpg rather than file000988.jpg) "
"つ ◕_◕ ༽つ giff code blocks 
 EDIT: [More examples of  codeblocks  module usage](
EDIT2:  parallel_for concurrent_for","Hi,  python lacks (EDIT: multiline) anonymous functions which may pose a problem for concurrent (callback-based) programming. While stuff like  gevent  alleviates the problem, asyncio (which is a part of stdlib now) still needs a named function for every single thing you want to do asynchronously.  with  statement would seem to be perfect for this task. Imagine:  with concurrent_for(workers=4) as i:    print(compute(i ** 2))  Unfortunatelly, it's not a trivial task. Context managers cannot access the enclosed code block and the code block doesn't even seem to be a separate context of it's own (see  There is some effort to hack code block support which follows this style: [description of the hacks required]( [Python2/3 module](  Is this something you would like to have in the language? Should python provide a better code block access inside of context managers (maybe a PEP is in order)? Do you have ideas for a different approach (maybe a  macropy ?  TL;DR ༼ つ ◕_◕ ༽つ giff code blocks  EDIT: [More examples of  codeblocks  module usage](EDIT2:  parallel_for concurrent_for "
"Those popups seem awesome and I kinda want to use typescript now, but there is absolutely no reason not to use this pattern.","In a few advice threads I've seen some assertions that the following pattern is ""bad"":  function ajax (options) {    var method = options.method,        url = options.url,        data = options.data,        // etc}  The argument is that people who use typescript don't get nice autocompletion popups/they can't be statically analyzed.  I'd just like to [point out]( the following:   I like options objects, they are extremely useful  I'm pretty sure this is exactly what interfaces and [typescript definitions]( are for  Advising people to use a particular pattern for the benefit of a different(ish) language seems pretty selfish    To anyone who saw that advice and thought it was worth following, this is a (simple) counterargument:  function doSomething (foo, callback) {    // whatever}  Let's say I need to pass an addition argument to this function.  If I add a parameter, I will also have to update  every place it is used .  If I had written it with an options object, I don't.   TL;DR  Those popups seem awesome and I kinda want to use typescript now, but there is absolutely no reason not to use this pattern. "
got offered a job as a data mining analyst for a marketing department. Should I give up current job as statistical analyst in medical device industry for new job?,"Hello,  I've recently been offered a job as a data mining analyst for a large supply company. I would be working with the marketing department to develop more effective marketing campaigns based on consumer data.  Currently, I work as a statistical analyst for medical device company. I program in SAS every day and I love programming. I also really enjoyed medical device industry, however I feel like this new job might offer me a better opportunity in the future. Right now I basically just know how to program and don't really understand all of the math behind what I write. The data mining job might offer me a chance to learn about different analysis techniques.  My question to you guys is what do you think I should do? Should I stay at my current position or take the new job? I'm nervous that I will forget how to use SAS if I don't use it anymore, but I feel like the new job offers room for more creativity. Any advice is greatly appreciated thank you!  TL;DR: got offered a job as a data mining analyst for a marketing department. Should I give up current job as statistical analyst in medical device industry for new job? "
"I wrote two packages in ES6, please review my code and tell me if I did anything wrong.","I have written two packages with ES6. One being [Uranus]( & the other being [Cressida]( Both are my first attempt to write anything with ES6 as I wrote them side by side.  [Uranus]( is a wrapper over [Validator.js]( Originally I wrote it for a project I'm currently working on, for api validation. The guy before me was validating every single input provided by the user like this:  var validator = require('validator');if (!validator.isNull('foo@bar.com') &amp;&amp; validator.isEmail('foo@bar.com')) {    // process email} else {    // throw validation exception.}  Imagine the APIs with more than 10 fields. Anyway, with uranus I did something like this initially:  var value = 'foo@bar.com';var rules = {   isEmail: true,   notNull: true};Uranus.validateOne(value, rules);  Uranus do offers some more functionalities, as described [here](  [Cressida]( is a very tiny utility that generates validation error messages. The sole purpose is to have consistent messages throughout the application. We have a very small team of 6 developers. And everyone of them has unique sense of telling users what is wrong about their input. Only for invalid email address, we have five different types of messages such as:   provided email is incorrect.  you provided invalid email.  email is wrong.  please provide a valid email.  wrong email.   I tried to set a few rules about error messages, but no gain. Since our client is very non-technical and our employer is getting money anyway, so nobody really cares about such issues. That's why I wrote it. Since we successfully integrated Uranus package, so I thought if I could generate verbose error messages while validating in that way I could kill two birds with one stone (I'm still working on it).  Since I was working off the clock, so I tried to get something for myself out of all the work I was doing and gave ES6 a shot. It would be awesome if you guys would spare some time to look at their source code and give some feedback/suggestion. I tried to use as much ES6 as I could with Babel as transpiler (too many as-es).  tl;dr: I wrote two packages in ES6, please review my code and tell me if I did anything wrong. "
What's the career path in this industry? I can't continue working on my skills and portfolio for free but I just don't see a way in.,"Long story short, after graduating with a M.Sc. in mathematics from a good university and spending half a year learning data science, I'm starting to question the viability of this path due to a perceived lack of open doors. Those with connections at industry meetups don't seem very interested in someone with my background, and of the extremely few seemingly promising postings I've applied to, I have not gotten so much as a response.  More than a few people have pointed out that the shortage is not at the entry level, but rather those with extensive experience. Browsing job listings on LinkedIn and my University's job board certainly makes this apparent. LinkedIn's weekly job recommendations consist mostly of positions with senior or architect in the name, and those that don't still ask for a minimum of 4 years of experience.  There's a major career fair with a lot of big names coming up in a week, and I'm starting to wonder if I should cut my losses and refocus my efforts in learning everything I can about C# and .NET development to snag a programmer position instead. Seems programmers are always in demand, and companies are actually willing to train them up.  TLDR: What's the career path in this industry? I can't continue working on my skills and portfolio for free but I just don't see a way in. "
"Regex Tuesday Challenges might be coming back, anyone interested?","Hey!  I used to run a website called [Regex Tuesday Challenges]( which was pretty popular in this subreddit and still gets a fairly large amount of visitors. It kind of died for two reasons: the first was that I ran out of challenges, and the second was that GitHub pages moved from github.com to github.io, meaning all my social links suddenly reset to zero.  I recently launched another new regex site called  Try Regex  with a mixture of completely new challenges and challenges from the old website. As I would be hosting it myself, I would be able to offer email subscription, which was requested quite a bit of the old website.  tl;dr: Regex Tuesday Challenges might be coming back, anyone interested? "
Whats the point in using Laravel for my web applications?,"Hey all!  Just looking for some input here.  I just got a job as the web developer for a small ISP.  The programmer that is leaving used Laravel for pretty much everything he made there.  I have spend the last couple days scouring the docs and youtube videos to learn the framework.  After getting a decent base for it, I have one question:  Whats the point?  In particular, I don't understand the purpose of Blade Templates and the general syntax.  It takes just as much code to write a simple input, AND I have to learn a whole new syntax to do so.  I can include this, and yield that, or I could do the same with a simple echo. It seems counter productive.  TL;DR Whats the point in using Laravel for my web applications? "
What would you advise me to use to build a RESTful php backend?,"Hey fellow redditors,  I have come to you for some advice on what framework to choose to build a restful backend. A little background on me, I work full-time as a Java developer and take on php projects in my spare time. For a new project I am going to build an AngularJS front-end which I want to completely seperate from my back-end. However, I am having some trouble choosing on how to build my backend and this is why I come to you guys. For the pas three years I have mostly developed my applications using CakePHP, but for all future projects I am looking to change this.  I have already researched several (micro) frameworks but I am having trouble making a decission. Things I have considered:  First option, create my own stack which would include:   Restler/Tonic/Slim as REST framework  Doctrine as ORM  Other libraries I will have to research   However since I am not a PHP guru, I will probably have trouble integrating the frameworks I would like to use.  Second option, use a full-stack framework:   Symfony 2: I have experimented  with symfony quite a bit and eventhough this is as close to mimicking a java stack as it gets it feels too heavy. There is so much configuration and gotcha's which I will have to pick up on that I dont think I will go for this.   Laravel: I dont know anything about it, so it will take me quite a while to figure out. However from what I found on Laravel and restful controllers it looks fairly straight forward and powerful.    A few things I would like to have:   Easy to pick up  Prefer to use Doctrine (I am used to JPA and Hibernate)  Annotations, man I love annotations. Restler and Tonic look like good options.  Prefer to limit the amount of dependencies pulled in which won't be used.  Well documented, supported and big community  Ability to seperate resources into multiple different classes. This project is going to be big so I don't want to force everything into a single class/bootstrap/gateway.   TL;DR What would you advise me to use to build a RESTful php backend? "
"What's the best way of handling lots of data on my small computer? 
 If I move to AWS is there a good walkthrough I could follow?","I recently pulled in a big chunk of the [public use micro data from census]( You can get a lot of data from there and now I have about twice as much data as I have ram. I'm struggling with how best to deal with the limitations of my hardware. I've been using workarounds like streaming data and doing a running analysis, but it is slow going.  I was considering a couple of options. Setting up some sort of local SQL repository, or porting everything to AWS. I was going to migrate all my research to AWS as a skill building exercise eventually, so maybe now might be the time. If it is, I will need a very detailed step by step walkthrough. I'm technical enough to follow directions, but serious trouble shooting is way beyond my abilities.  I've been doing some googling and there is a lot of information out there, so I was hoping the hive mind could point me in the direction of some vetted resources.  tl;dr:  What's the best way of handling lots of data on my small computer?  If I move to AWS is there a good walkthrough I could follow? "
"Any idea why 'looper = multiprocessing.Process(target=listen(s))' executes before I call looper.start()? 
 Your help is greatly appreciated.","I'm having an issue using the multiprocessing module in Python 2.7, and I've scoured the internet to no avail.I'm trying to create a simple client/server messaging system using python, and to do that I have to be able to print everything coming into a socket while simultaneously being able to send on that socket.Perhaps my design is flawed, but right now I'm struggling to even get multiprocessing working correctly.Code snippet is here, just the relevant bits with the debug statements left inside:  import socketimport multiprocessings = socket.socket(socket.AF_INET, socket.SOCK_STREAM)def listen():    print 'listening...'    while 1:        i = s.recv(1024)        while i &gt; 0:            print i            i = 0def loop(handle):    print 'looping...'    while 1:        msg = com(handle)        trans(msg, handle)def com(handle):    print 'com function...'    msg = raw_input('{0}&gt; '.format(handle))    return msgdef trans(msg, handle):    print 'trans function...'    s.send('{0}&gt; {1}'.format(handle, msg))    returndef connection():    server = raw_input('Server Address: ')    port = raw_input('Port?: ')    port = int(port)    print 'Initiating connection to server...'    s.connect((server, port))    handle = raw_input('Handle: ')""""""Here is where the problem exists, the next statement executes before I call    .start()""""""    looper = multiprocessing.Process(target=listen(s))  """"""No matter what I try, I can't even get it to make it this far...   After reading through the documentation and multiple tutorials, it seems   that this isn't a common problem""""""    communicate = multiprocessing.Process(target=loop(handle))    looper.start()    communicate.start()connection()  I can't figure out what I'm doing wrong. Also, I'm sorry if the code snippet is too long, or if it doesn't run like this. I tried to put together only the relevant bits without posting all of the source here. If you need more, let me know and I'll post the complete source to pastebin.  TL;DR: Any idea why 'looper = multiprocessing.Process(target=listen(s))' executes before I call looper.start()?  Your help is greatly appreciated. "
"I wanna write realtime strategy/tactical game( 2D, maybe 3D). Pygame hot or not?.
Alternative libraries are welcome as well.","Hello!  I'm a hobbiest programmer, from India. I learnt python( by myself) abount an year and a half ago. I've been writing small tools/softwares every now and then, though I haven't published any of them yet.  Recently I developed a liking to pygame after seeing a video of pseudo-paint program written using pygame, running on a raspberry pi. But after researching around a bit , I couldn't find any cool games written in pygame. So heres my question: is pygame worth the effort ? Or should I look into another python gamedev/multimedia lib? For the sake of reference, I'd like to write real-time strategy/tactical games, something simple to learn and fun to play. I haven't decided on 2D or 3D graphics yet, since I'm not sure if I can put the amount of work to make 3D graphics work, 2D seems fine(for now). TL;DR: I wanna write realtime strategy/tactical game( 2D, maybe 3D). Pygame hot or not?.Alternative libraries are welcome as well. "
What parts of jQuery or AngularJS should I learn about to make a file browser and uploader?,I'm trying to learn JS and the best way for me is to pic something I want and try to build it. What I want is the equivalent of a browser based version of Windows file explorer. Browse directories on the right with drag and drop uploading on the left. That's all. No coping files or directory or any other interaction. Just go to the directory and drop files in it.  I've spent a couple days with jQuery but I've gotten frustrated with it. I just started looking into AngularJS and really like the way you write code. I'm new to all this so I spend most of my time reading docs and looking over example code. At this point I could use some help.  What are some areas of jQuery and AngularJS should I be looking over? Any other suggestions or help?  TLDR: What parts of jQuery or AngularJS should I learn about to make a file browser and uploader? 
"Writing a book about Python. We are *NIX using devs. Should we talk about the windows solutions to isolation/etc? 
 Thanks!","We (my colleagues and I) are preparing to write a new book. It is aimed to be a guide for somebody who knows the Python language (not covering any language fundamentals) but wants to become more ""professional"" in regards to developing, packaging, and deploying real-world python applications and tools.  We will be covering the topics of python environments and environment isolation and this is where my question ""do we support windows"" comes into play. We (and many of the people we've met at meetups) develop in UNIX or UNIX-like environments. We use tools like [pyenv]( to create isolated python environments and other tools that are not supported on windows but are VERY common in the python development world.  How much demand is there to specifically target windows users in a book like this?  TL:DR Writing a book about Python. We are *NIX using devs. Should we talk about the windows solutions to isolation/etc?  Thanks! "
"How do you go about structuring your code in to JS files, and how do you decide what is part of model, view, or control?","I am a beginner/intermediate developer and have been working on a pet project for my company. I am using it as a learning experience since I am new to JavaScript, and a way to explore OOP while using JS. I have decided to utilize the MVC concept, but I am finding it difficult to decide what goes where, and how much to chop my JS into separate files.  Right now I have model.js, view.js, and controller.js but they are getting bloated and I don't really think that's the right way to do things anyway. I have read a bunch of websites, and have found a few good ideas, but I am hoping someone here has some suggestions for how to approach this.  For a tiny bit of background this is a visualization app that ingests XML generated by my company's product. I convert the XML to UI elements, and layer on a few features to make a useful research and analysis tool for our team members.  tl;dr How do you go about structuring your code in to JS files, and how do you decide what is part of model, view, or control? "
"I have a model, how do I make it a web app?","Looking for advice to a big struggle I have:I can mode x,y,z data in iPython. Awesome! All of a sudden I have this model. Great!...what do I do with it? How does one transition from an iPython notebook model of data to something that's usable as a web based tool?  Example: I modeled the win probability of an NFL based on a team's given situation*. How could I have a web-scraper that pull data in near-time, run the model on that data and post an image to a hosted site? How could I have my buddy go to my website and put in a given situation via forms & text fields to find out what that situation's win probability is?  TL:DR - I have a model, how do I make it a web app? "
"Create a  virtualenv 
 Run  source ./env/bin/activate 
 Run  scp -r env user@remoteserver:~/  and enjoy eternal recursion.","Just thought I would share a weird python  moment I had.  I was moving a virtual python environment up to my web server to deploy a django project.  Even though I could use  pip freeze  and a requirements.txt file, I knew it was a pretty small directory so I just decided to  scp  it up.  The command ran for a  really  long time, and I was starting to see a lot filenames repeating themselves.  Also running  du -h env  told me that the directory had long surpassed the original in size.  I realized before I sent the environment, I had run source env/bin/activate  and I had neglected to return to the local shell before copying the directory up.  Although I don't completely understand why that stopped the directory from finishing the copy, I think it might have something to do with the Python virtual machine and/or recursion.  Can anyone offer an explanation for what happened?  TL;DR   Create a  virtualenv  Run  source ./env/bin/activate  Run  scp -r env user@remoteserver:~/  and enjoy eternal recursion.  "
"I feel like this: 
 Me: Javascript, print a, b, c. 
 Javascript: b, c, a. U mad bro?","I'm in the beginning of a JS class. We started with Ruby, just switched to JS. Ruby is synchronous, JS is not. This is where many of my troubles are coming from.  Working on a simple example. Give it a keyword-- it hits the Instagram API, returns results of images matching that keyword.  I'm attempting to delay the API requests, as hitting them too quickly causes errors.  Where I'm confused is this:  setTimeout should solve this problem, but I can't seem to figure out the way JS handles this.  If i stick 5 things in the queue with a 2 second delay, when it finally runs them... they all run at once?  not one after the other?  How do you know how many things can run at once?  How do you know WHEN the queue goes off?  TL;DRI feel like this:  Me: Javascript, print a, b, c.  Javascript: b, c, a. U mad bro? "
How do I get a value from every objects certain property in an array of objects.,"I am getting an array of objects (representing a steam trade offer, doesnt matter in what I want to do though). Every object represents an item, and the items all have the same property. I am trying to get the property value 'market_hash_name' for every single object, and log it.I did this to get it from only the first object:  console.log(offer.itemsToGive[0].market_hash_name);  This worked . I have tried this to get it for every object, if there are not 999 objects it will return undefined which is fine with me  console.log(offer.itemsToGive[0-999].market_hash_name);  This did not work...  So does anybody have a solution on how to do this?  TL;DR:  How do I get a value from every objects certain property in an array of objects. "
looking for suggestions and ideas of what social media APIs can be used for and some good practical things I can build.,"I'm a designer and front end developer. I use JQuery and some Javascript but it frustrates me when I don't know why some code I am using works the way it does. In the last few months I have gone back to the beginning and tried to learn Javascript properly through code academy and some other online resources. I'm still at a beginner / intermediate level but my understanding has grown a lot.  Anyway, I want to start applying some of the concepts I have learned, particularly around JSON and social media APIs, but I don't really have any ideas for projects. I don't want to get into really intense application development, more looking for practical ideas and suggestions for what JSON and social media APIs can be used to do. Any suggestions are welcome. Thanks!  tl;dr; looking for suggestions and ideas of what social media APIs can be used for and some good practical things I can build. "
"Need help with project, went through two courses, didn't learn how to make project, asking Reddit for help.","For my Computer Science class, I've decided to make a project I was planning on making anyway. I went through the JavaScript and AngularJS course on Codecademy, and I need the project to access an API, (these two are the ones I need to access: [ONE]( [TWO]( take in the value from the first API, which is in cents, then take the value of the item from the second API, which is in an in-game currency, convert it to the price of cents, then divide one by the other to get the value of how much bigger one is than the other, and store it as a percentage. After it's done all of that, it has to sort them by the biggest percentage value for the difference, and finally output them to a text file which will be downloaded to the user's computer. My problem is that I have no idea how to start coding this, or what the code would look like. The Codecademy courses didn't really teach me how to code my own program, so if anyone can give me either some tips to start, a course that would help me, or just some assistance through messages here on Reddit, I'd be very appreciative.  TL;DR Need help with project, went through two courses, didn't learn how to make project, asking Reddit for help. "
Set of classes to build Regex strings using English language terms and fluent setters,"So as someone with 3 years industry experience doing PHP webdev, who's now doing application development in C# and .NET, I want to keep my PHP skills relatively sharp. So I've started working on some small PHP libraries in my free time, and figured I might as well throw it out there and let others critique it too.  This is the first time I've ever ""released"" anything to the community at large (and I have very little experience with Github) so please, if you have 2 minutes, take a look and give me some feedback! I'm sure there's a huge amount I've not done/done wrong/etc.  TL;DR  Set of classes to build Regex strings using English language terms and fluent setters "
"Looking to make my own self-contained, stand-alone wiki server; what security and ""framework"" things should I make sure I cover?","Hi /r/php ,  I've been interested in writing my own tiny wiki engine in PHP for a while, but I'm concerned with two things-- security, and making sure I cover edge cases properly.  So things like sending passwords as POST data instead of in GET requests I already (think) I know, and I know how to generate authentication tokens, but there's things like sanitizing URL parameters and dealing with intentionally bad input that I'm unfamiliar with how to handle.  Additionally, I'm curious of what common ""framework"" things I might not have thought of-- proper status codes for errors, an perhaps other valid browser requests I should expect to handle, other ""newbie"" traps I might not know about?  That said, I'm not a complete newbie with PHP, but I'm not sure where to start when thinking about this stuff-- Is there a good starter guide for this kind of thing?  tl;dr : Looking to make my own self-contained, stand-alone wiki server; what security and ""framework"" things should I make sure I cover? "
Are there any decent PHPMyAdmin replacements out there right now other than Adminer?,"Right I've tried sticking it out as long as possible but I cant take it anymore. Having used PHPMyAdmin for over 10 years I'm looking for a replacement.  I've hated it for a while now. Ever since they took the 'AJAX...AJAX everywhere' approach, but it's so damn clunky and buggy that it's actually being a massive hinderance now.  I've looked into Adminer as a replacement and frankly found it to be way too minimalist.  I'd ideally like a web-based one, however a Mac client would also be fine. I know people recommend HeidiSQL but it's not a native mac app. If I cant find anything else it'll probably be HeidiSQL that I go for, but ideally I'd like either web-based or a native mac app.  TL:DR Are there any decent PHPMyAdmin replacements out there right now other than Adminer? "
"I'm stuck. Have a working database using localhost, don't have access to third parties database, need a middle ground workaround. 
 Thanks guys.","Hey /r/PHP, quick question.  I have a small site at work I had to make, including a login for customers. I was hired as a web developer/designer because the third party they outsourced to originally wasn't meeting expectations. Well, I figured out how to create a local database/table using phpmyadmin, mySQL, and localhost.  The problem I have now is I don't know how to get it to work on the web side of things. I have access to the file server via FTP, but I doubt I'll get access to their MySQL databases. And I know from the last time I asked, storing the logins in a separate file on the web server is a terrible idea.  tl;dr I'm stuck. Have a working database using localhost, don't have access to third parties database, need a middle ground workaround.  Thanks guys. "
try adding function type annotations in a python3 project in pycharm 4 - you get IDE type-checking support!,"If you annotate a function e.g.  def foo(x):  with python 3 function annotations using the (mypy style) type convention e.g.  def foo(x: InClass) -&gt; pandas.DataFrame  PyCharm will figure out completions based on the argument types within the function, and the return type where the function is used. So for instance it will suggest completions for member functions/fields of x when editing the function, and when you assign a variable to the function result ( y = foo(x) ) it will suggest the appropriate completions for y.  I tried this out of optimistic curiosity, and it actually works! I don't remember seeing this feature much advertised on the pycharm site but it's great that it does!  tl;dr: try adding function type annotations in a python3 project in pycharm 4 - you get IDE type-checking support! "
"Been studying machine learning for a month, never done a single algorithm in practice. Wondering for the ""best"" or easiest way for a novice to implement a working solution.","I manage an eCommerce platform that hosts a variety of different products. For simplicity's sake, in this example I will say I only sell 3 different items: A,B, and C  These items can be added to a shopping cart and purchased in any combination.  Ideally , certain combinations should be more likely and I have been tasked with providing some exploratory metrics describing the relationships between related products.    Order/Cart ID  Sequence  Item      1  1  A    1  2  B    1  3  C    2  1  A    2  2  C    3  1  C     Assuming this sample data set, I'd like to create something that calculates the relationship for a given product. If I started with  C , I'd either like to compute that 66% of the time it is paired with A, or calculate average distance from other variables.  I've been trying to learn theory on this in the past month, so I suspect that there are a lot of ways to do this, and this is either a nearest neighbors problem, or perhaps even simple regression.  TL;DR - Been studying machine learning for a month, never done a single algorithm in practice. Wondering for the ""best"" or easiest way for a novice to implement a working solution. "
"Why can't I assign a prototype method of Math, and what are prototypes really used for?","I get that you can extend objects with new methods by doing something like:  Object.prototype.method = function(){};    You can even extend core objects like Array with code like this:  Array.prototype.average=function(){  var length=this.length,  sum = this.reduce(function(prev,current){     return prev + parseFloat(current);  },0);  return length=== 0 ? 0 : sum/this.length;}console.log([5,7,10].average());  But why can I not extend the Math core object?  A piece of code like this will not work, saying ""Cannot set property 'dice' of undefined(…)"":  Math.prototype.dice=function(sides,rolls){    for (i=0;i&lt;rolls;i++){        var res = res || [];            res[i] = Math.floor(Math.random() * (sides - 1)) + 1;    }     return(res);}console.log(Math.dice(6,1));  However, a piece of code like this, which is set using an ""own property"" of Math will work just fine:  Math.dice=function(sides,rolls){    for (i=0;i&lt;rolls;i++){        var res = res || [];            res[i] = Math.round(Math.random() * (sides - 1)) + 1;    }    return(res);}console.log(Math.dice(6,1));  So whats up? I think that the Math core object does not have a constructor and that is why I'm having issues.  But I really don't understand what is going on here logically, and I want to.  Do I just totally not get what prototypes are used for? Why would Math not have a constructor? Am I just very, very confused about this whole thing?  Thanks guys and sorry for the long post.  Tl;Dr:  Why can't I assign a prototype method of Math, and what are prototypes really used for? "
"Does anyone know of a book that teaches Python through real-world programs. Then breaks down the code line by line? 
 Thanks Guys!","Hello everyone,  So the title was a bit misleading. I didn't finish an entire book on Python. Rather, I finished a book called Raspberry Pi Cookbook, which has has 3 sections on Python: Python Basics, Python Lists and Dictionaries, and Advanced Python. It then jumps into much more complicated projects using the raspberry pi and python to control electronics like motors and whatnot. I chose this book because the author also wrote a great book for starters on Programming with Arduino in which he goes through a program and has comments on each line explaining what it means and what it does. I was able to understand a lot of the concepts and even come up with simple programs on my own.  This book, however, does not do that so I feel like I'm not learning anything. Rather, just mindlessly copying code without knowing what it's doing. I had the ebook opened up right next to idle, and I finished all the little tutorials. At the end I just didn't know how I would apply using lists, dictionaries, and all that other stuff in a practical way. VERY similarly to how a kid in high school math might say, ""Yeah that's great. But why do I need to know this and how is it gonna help me in the future?""  Does anyone have a better book to learn Python with? Something (like described above) that has code being implemented in simple programs that actually do something and then explanations of the code broken down line by line? I find this is much more fun and motivating.  Please take a look at this online tutorial by Simon Monk. I have linked you to the page where after finishing the tutorial, Simon Breaks down each section of code. THIS is the method of teaching that I've found the most fun and motivating.  TL;DR: Does anyone know of a book that teaches Python through real-world programs. Then breaks down the code line by line?  Thanks Guys! "
I want to add images to a map w/out having to type an address or lat long.,"In looking for the best solution to allow users to upload an image and tag the location of it on a map I came across this list of js map libraries and plugins (linked below). There's over 60 despite what the title says. While a very through resource it's a bit overwhelming.  While I'm hoping most users will upload images with gps data, I know many wont and I want to make it as easy as possible for them to add their images to the map. I feel being able to click on the location and have a pin generate there or dragging a pin to the location is the easiest way for a user to accomplish that.  Based on that would anyone with experience in the matter be able to point me in the right direction. I've spent half a day rummaging through lots of documentation and don't feel like I'm any closer to getting the documentation that would accomplish that.  tl;dr I want to add images to a map w/out having to type an address or lat long. "
"looking for a project oriented at fast, vectorized calculations on geometry objects to contribute to. Any suggestions? Is my concept too general?","Over the years, I have found myself solving variations on the same subset of problems, namely geometry intersections.  Every time, I've come up with a solution specific to the context of the problem.  Now at it again, I wonder if it's time I find an existing project to help turn into a general solution.  Currently, I am working on 2D raytracing on a weighted b-splines for optics design.  In the past, I worked on a 3D optical ray tracer utilizing cython.  It used enthough traits and was organized around the GUI it provided.  This project is now abandoned.  I also played with making a physics based game, utilizing pymunk, the python bindings to chipmunk.  This is the correct solution in this context, and I wouldn't try to replace this. But it does fit the general theme.  I also have used Geodjango's gdal library for doing analysis of GIS shape files.  Functions are fast, but not vectorized.  Each time, I have written or imported a solution for:  *Managing and applying functions across sets of geometry objects.  *Geometry intersections.  *spatial hashes   computing normals at intersections.   I know python isn't the best language to approach these problems with. But since I use python all the time, and come across these problems fairly often, and want to become a Numpy ninja anyways, I'd like to help make a fast general purpose solution.  So, if I or someone decides to implement ray tracing on a bivariate b-spline surface, the results will be available to others working on optics design, POV raytracing, E and M simulations, mechanics simulations, and maybe other fields.  Or if some wizard writes the fastest damn spatial hashing function in numpy, we all benefit.  Related projects already existing: python-raytrace (first project I worked on), pyopt-tools (duplicated much of python-raytrace), pyrr (functions are not vectorized.  I have contacted them), GDAL, lib2d.  Tl;dr:  looking for a project oriented at fast, vectorized calculations on geometry objects to contribute to. Any suggestions? Is my concept too general? "
"Looking for a secure solution for receive a request from an external server, then load up a customized application for the user in an iframe on the external server.","A little background:  I have an application (Javascript) that another company wants to use on their website.  I want to maintain the application on my servers, so I am not willing to just hand over my code.  I was planning on having them do a standard iframe on their site that points to the application.  Problem:  They want to individualize the program based on the users: change some of the text and options.  They also want to pass me the user identifier from their system, and then send the results back to their system when the user is finished with the application.  I also thought about doing a single Javascript file that they could just include and pass some options.  The script would then load up the dependencies, the program, and just do JSONP callbacks to my server to process the data before I send that to the user.  With an iframe, you can pass URL parameters, but it is not secure, and the user could change the information.  Plus, with the changing of text and what not, the URL would be pushing 2000 characters (it's a lot of text)  My second thought was to set up a server to server SOAP request, which they would send me the options they want set and the identifier.  Then I would take those options, and then return a unique identifier/iframe id which they would use to load up the iframe.  This may be a bit of overkill for a request and a response though, so maybe I could implement this with just a POST request?  Anyways, I am lost on this and would love to hear some other ideas anyone may have.  TLDR; Looking for a secure solution for receive a request from an external server, then load up a customized application for the user in an iframe on the external server. "
I am tired of my current field of study. I want to learn a computer language for career purposes. My background is basic C and HTML/PHP (both intermediate/beginner level).,"Hi Reddit!I am an Industrial Engineering major at Cal Poly, and one thing I still regret right now is studying Engineering in the first place. My initial intention was to study Economics, but due to pressure from family, I have studied otherwise.However, during the past 4 years in college, I have studied C programming and have indulged myself with HTML/PHP. For both cases, I am an beginner/intermediate level coder. My interest in programming grew a lot after hanging out with my friends and seen how it is easy to transform some ideas into reality by coding.I have read through the FAQ, but it was not thorough enough to provide me a good answer.Therefore, given my background in C and HTML/PHP, what language would you recommend me to learn for the current market?  TL;DR: I am tired of my current field of study. I want to learn a computer language for career purposes. My background is basic C and HTML/PHP (both intermediate/beginner level). "
"I want to help to develop to Firefox, is that possible being a C++ novice?","I'm a master student Computer Science and I want to get my hands dirty with OSS. I like to hack in python, java and php. But I also experiment with other languages as erlang, ruby, etc.  I've actually tried many times but everytime I get slightly comfortable with the codebase my motivation drops and I quit. What I hear a lot is 'scratch your itch', but that does not really seem to work for me. I've got a lot of little tools/scripts that do stuff but all the handy programs already exist.  I'm using Ubuntu and it does not help that I always want the latest and greatest and that I'm not loyal to any program. Firefox and gnome being the only exeption I can think of.  I think helping to develop Firefox would be super awesome, but it's written in C++ and I heard and think the language takes lots and lots of time. Would it be worth it to learn C++ to contribute to large OSS projects or is that definitely a motivation killer?  TLDR: I want to help to develop to Firefox, is that possible being a C++ novice? "
"fake jobs posted in /r/php, people selling resumes to technical recruiters.","There have been a number of 'job postings' to /r/php over the last few weeks.  A few of them seemed somewhat interesting... telecommute, contract work, solid frameworks, php... So I sent my resume in.  I didn't get hired, nothing unexpected... about 90% of the contracts  I apply for say no.  But heres the thing... I tag all of the resumes I send out.  This way, if people contact me in the future I know exactly where/how they found me.  For the last week I have been getting about 2-3 phone calls a day from technical recruiters.  Offering me jobs in PHP, Python, Ruby, and Java (Ugh, I don't know Java).  All of these phone calls came from a resume I submitted to an /r/php thread.  If you are looking for work, this could actually be a blessing.  However, I find this annoying since they are offering me jobs that I am no way interested in, but because this is my profession I cannot tell them to fuck off.  Instead I have to chat with them for 5-10 minutes explaining that I would love some telecommute contract work (thats what I do!), but I am not interested in a junior PHP developer position where I have to drive 45 minutes to work.  I just thought I would pass this information along.  Like I said, for those looking for work this could actually come in handy...  However, I do not want people to abuse /r/php for resume selling.  tl;dr: fake jobs posted in /r/php, people selling resumes to technical recruiters. "
"I made a GUI interface using visual basic, see if I can track a package.","All it does is look at the UPS tracking page every 5 minutes.  If there's an update, it sends an email containing the tracking table.  For this test, I'm going to have it send emails to me, but I'll gladly share the program with you if you'd like to run it on your computer and send yourself email updates as well.  [Here's a screenshot of the program with some of the code visible in the background.](  If you send me a tracking number to use for testing, the only way in which your privacy is compromised is that I will know the destination city.  Your name and any other identifying information is not bound to the tracking number.  TL:DR I made a GUI interface using visual basic, see if I can track a package. "
"programming can be annoying, and isn't as interesting as a story (but man writing this felt cathartic) 
 So, tell your stories;  what's the most annoying bug you've ever encountered?","I just finished up a programming project for one of my classes, and ran into a particularly obscure bug. I'm just starting to use Java (experienced with all flavors of C, AS3, etc.), so that may have contributed to my confusion. The program was spec'd to be a basic Client/Server for uploading files, and the username was passed to the client via command line. The client then sent that data as a null terminated string to the server. The server then creates a directory named by that username and an appended a timestamp. My problem was that, no matter what, the created folder would always just be named ""username,"" without the timestamp. After hours of debugging to no avail, I finally discovered the problem: I was reading in the username one char at a time into a big char[] (spec doesn't say how large the username could be), stopping when it read in a '\0' char, and then creating a string from that char[]. Unfortunately, the string constructor apparently sizes the string to the size of the array, no matter how large, and no matter how empty it is. Because I allocated plenty of extra space in the char[], when I appended other strings to it (eg the timestamp), it was apparently too long to write as a filename... despite the fact that when I printed the concatenated strings to System.out, it displayed fine. What a fucking headache!  tldr: programming can be annoying, and isn't as interesting as a story (but man writing this felt cathartic)  So, tell your stories;  what's the most annoying bug you've ever encountered? "
Want to learn how to code so I can get mo bitches.,"I think this might be too broad of a question, and I dunno even if I'm on the correct subreddit, but here goes. I am basically looking for guidance on where to start in terms of coding or learning how to code. I've read some Wikipedia pages on programming languages, and it boggles my mind how many different ones there. From BASIC to C++ to Python, I have no idea where to start. I sat through a basic html class a few years ago, but I don't think I mentally engaged with the class, so learned basically nothing other than open and close tags.  What do I want to use it for? Well I sorta want to build up a skills repertoire and always thought knowing some programming language looks very good on a the CV. Aside from that, I am in my final year of university working on phylogenetics of plants and one of the aspects of my dissertation is pulling vast amounts of data from PubMed, formatting it in a certain way, and then inserting it into a program that you use for DNA alignment. I had a lot of free time over the summer, so I kinda did this all by hand, but now I realise most of what I did could've been accomplished a lot quicker if I wrote a script for it. Also, the program that I feed this data into (Mesquite) has a scripting backend, which apparently opens you up to a whole new world, a world with hash tags, curly brackets and crazy indents. I would like to navigate this with relative ease, and I think knowing a bit about scripting would help me!  tl dr:  Want to learn how to code so I can get mo bitches. "
I've learned the basics of programming and I'm wondering how to specifically move on and learn more. Halp.,"I took an AP class my senior year of highscool (for perspective, I'm a freshman in college at the moment) that was based around Java. Yes, I know. Java. Har har. Well I really liked it and I realized I missed the satisfying feeling of successfully making a program work so I got into programming as a side thing I want to learn. I read all of CarlH's learn to programming and got perspective on C and pointers and such. I've done other tutorials and I've become somewhat familiar with C++. I've gotten passed all the basics of programming and now I'm wondering where I go to learn more. All the tutorials I look up teach you the basics I learned from CarlH's lessons, my AP class, and other such things.  So programmers .... how do I further my learning now? I've been told to look at source code, but where do I start? I've been told to get books, but they all seem to be too advanced or a ""Programming for Dummies"" which I'd be wasting money on because I know most the basics. I've been told to do online tutorials but I've exhausted the ones I've found and don't know where to look for more challenging ones. So anyone want to provide some specifics? :)  By the way I've decided to focus on C++, but I'm open to other languages.  TL;DR: I've learned the basics of programming and I'm wondering how to specifically move on and learn more. Halp. "
What's a guy w/out a programming degree need to show to prove he has enough coding skills?,"I'm not a programmer by trade, but I'm in a tech related field. I've had one or two programming classes back in the day, and I do regularly do quasi-programming with audio programs like Csound and MAX/MSP which are basically object oriented scripting languages that aren't nearly as formal as C.  I've been teaching myself bits and pieces of things here and there, and I'm working on improving my chops in C and objective C through texts made available at my job.  I see postings for things that would be much closer to a 'dream job' now and again, but I'm always worried that I don't have a serious enough programming base to apply, or to be seriously considered.  My question is, since I don't have a degree in programming, what would I need to do or have in a portfolio or resume to convince a company that I have programming chops?  TLDR  What's a guy w/out a programming degree need to show to prove he has enough coding skills? "
Need programmer to help with ad campaign. Who needs work???,"So basically here is our predicament.  We are a very small start-up advertisement production company. i.e. a bunch of recently graduated film kids. We are getting our first big shot to create a ""viral"" video for a relatively large restaurant chain. We want to create a campaign similiar to these:  [tipp](  [subservient chicken](  We however, are not programmers and are not sure exactly how/what we need to make the player on these websites work. Are these types of players just flash based??? How many hours of work?? slash people do you see working on these???  If someone steps up and can give us a good price estimate, of what it takes to get it done, and can show us that they can do it, and wants to take the reins of the programming side of a project like this. Then we are more than willing to hire you. As of now our budget is in limbo, because we need to get the final okay from the restaurant. But it should be pretty substantial.  THANKS!!!!!  P.S. sorry if this is the wrong area to post in, just figure some people in here would be capable, and could use the work.  TL;DR Need programmer to help with ad campaign. Who needs work??? "
Document management is blackmailing us to stay with them by not providing us a tool to make OUR data readable by anything but their software.,"My Office has been using a document management company for almost 10 years now. The Solution is not really ideal for us, and we are getting ready to leave. They are a small company who keeps the files encrypted with their own proprietary format (random numbered tiff files that are encrypted with AES-256 and organized with a database with a web front-end). This worries me because, a company like this could fold taking all our data with them, leaving us with nothing but encrypted files that we cannot access or migrate.  The problem that we've run across is when we requested a copy of our files we got 8 DVDs with random numbered Tiff files that cannot be opened by software other than the software they included with the DVDs. Am i in the wrong to request them to provide me with a tool to mass decrypt the files?  They first claimed they could not send us the files decrypted or provide us with such a tool because it violates HIPAA (they work with a lot of doctors offices) However we are not in the medical field, nor regulated by HIPAA laws.  The software they provide only allows us to view the files once we've selected it and entered our password.  Proggit, do you have any advice?  TLDR - Document management is blackmailing us to stay with them by not providing us a tool to make OUR data readable by anything but their software. "
Might turn down job coz I don't like being snooped on.,"I'm seriously considering turning down a fairly good job because of the background checks they want to do. By default the background check can be done (including an investigative consumer report!) at anytime during my employment. Oh, and they also do a drug test before I start and random drug tests during employment. This is for a programming job for a company that sells stuff online. I have nothing to hide and don't do drugs but I'm not a very trusting person and find this sort of intrusiveness into my life unacceptable. Anyone else feel this way? Is this sort of thing pretty common? Am I being a dumb ass?  tl;dr Might turn down job coz I don't like being snooped on. "
"I'm in over my head with a couple of projects, can't pay my bills and can't find another job.  I'm fucked.","I have a nice little office in southern California and as I can't complete my latest two projects I'll probably be fired from them at the end of the month.  Throwaway account here, and don't really expect any responses; I'm just a long time lurker and not a very good web developer.  Been a freelance HTML/CSS/PHP guy for a long time, can do html/css no sweat (because it is easy a fuckin chimp can do it) but php is a different story.  I know all you competent coders will laugh but I'm lazy and never spent any real time studying.  Just enough to knock something out for a client, (""hey, i saw this neat thing on this one site, can you do that for my site?""  ""sure"").  But of course, software is not that easy and nobody I socialize with has the first clue what I'm talking about.  Multi-dimensional array, what?  And no programmer will help me w/out a deposit up front or a 20 page tech spec.   Oh, but you're saying, there are so many helpful websites!  StackOverFlow, Reddit, or just even Google it!  Yeah, that is just a huge clusterfuck all unto itself...  because the minute I nail one thing down another crops up.  Fucking whack-a-mole...  Oops, that tutorial you just read describing exactly what you wanted to do doesn't just happen to work and you have no fucking clue what ""Division by zero in public_html/dev/includes/common.inc(1695) : eval()'d code on line 11."" actually means!   Wheeee!  tl;dr -  I'm in over my head with a couple of projects, can't pay my bills and can't find another job.  I'm fucked. "
I have a project requiring creating custom software/webpage and I don't know who to turn to.,"I'm a professional who works with a great deal of paperwork for a company that still does things the old-fashioned way: they literally sit down with clients and fill out said paperwork by hand. We're talking thousands and thousands of form with repeat information.  I have plans of opening up my own consulting firm in this industry. Basically, I want to create a website that clients can access and fill out their information themselves, which is saved to a secured database. The website itself would need to react to choices made by the client and provide appropriate future choices (e.g., client indicates they are in a certain income bracket, which triggers different choices). Upon submitting the information, an appropriate pdf is populated and saved to their online ""file."" I need Outlook Calendar to be automatically updated upon submission (e.g., ""File Paperwork for Client X"" scheduled for 2 weeks out). I would also like to be able to access this information via a Droid phone.  Heavy apologies for the rambling. I hope some of this made sense. Basically I would like to create a virtual firm that does not require a brick-and-mortar establishment or other employees, thereby eliminating overhead.  The only programming experience I've had was with Visual Basic, well over a decade ago. Clearly I'm not able to do this. I would like guidance on where and who I should go to for creating this monstrosity. Which technologies should I ask for? What should my budget be? How can I adequately convey what kind of product Any advice you can provide would be greatly appreciated.  tl;dr: I have a project requiring creating custom software/webpage and I don't know who to turn to. "
"I am young programmer that needs some money. 
 ﻿","I am a 12-year-old guy in rural Virginia. I frequently check Reddit and other tech sites. I have been a web developer since close to seven years of age. I know object-oriented PHP, and, despite many suggestions, am more comfortable with it than say... Python. I know how awesome and powerful it is, but what about support? Yes, I  do  know about the immense documentation, but how many tutorials are there on it? Not many. PHP is currently the most widely-used scripting language on the planet.  But on to my problem. Because I am 12, I don't have a résumé, PayPal, or a real job. I am ""webmaster"" of a website where all i do is update the content on a horrible CMS for a local American Legion post. I could really use some money, and I can't find a way to bring it up to my parents. Can you awesome guys here help me?  tl;dr:  I am young programmer that needs some money.  ﻿ "
Make the browser let my js code run until completion?,"So I thought I'd be creative and write some time-complex algorithms in javascript, but the browser interrupts and refuses to let it finish.  Chrome will first ask you to Wait or Kill the process, and if you select Wait, it still gives you the ""Aw, Snap"" page after a few more seconds...  And IE8 constantly pops up with a prompt every few seconds asking if you want to continue it.  I've done some google searches and looked at the options and I'm stumped.  Anyone know what to do?  I would be surprised this is the first time this situation has come up.. longest run time I get is around ~30 seconds in chrome.  tldr: Make the browser let my js code run until completion? "
I suck at programming and like hacking up CD drives.,"I haven't done much programming in ages.  Did quite a bit of Visual Basic in high school and some Java, but most of that is gone.  Anywho, what I need to do is a UserName/Password setup that opens and closes a CD drive, logs the time the drive is opened and closed, and then self e-mails the log file daily.  I can create a basic login and timestamp setup, but what would be best for auto-generating an e-mail?  How would that process work?  Also, is there a way to call and log the closing of the CD drive?  Additionally, I would like it to run full-time, full-screen, with a special password to exit the program.  Backstory:  People at work don't like using the paper log book to get a key.  Logging use of this key is very important.  So I am modifying an old computer and CD drive.  CD drive holds the key, they type in username and get key.  They put key back, close drive.  I get log book without beating people to write it down.  Tl;Dr: I suck at programming and like hacking up CD drives. "
"have a website template, what do I put on it?","Hey, this is my first reddit!  Anyways, I need content ideas for my website. I have a basic template set up, and an intro page to link to other sites.  I also put a fake ad on the intro page (all of these pages are just mockups for now, I don't want to buy hosting if I don't have a full site)  Anyways, I need a topic, or an idea to put on the website (I know the template I got has some hacking crap on it, whatever)  People have given me ideas like a warez site, tech news, and some other add-on services like image hosting or file hosting (not the main part of the site, just an extra service).  tl;dr: have a website template, what do I put on it? "
"Recommend some ""project-based"" programming books that someone would go through for fun.","It was my friend's birthday on Sunday and I never got him anything :( I have a few ideas but I need input from the more knowledgeable people at proggit.  My friend just started a Bachelor in Computer Science last year at a local university without prior programming knowledge. While they teach things just fine, they fail to make programming interesting, or maybe he is not finding it interesting, I don't really know.  I am looking for 2 things:  1/ A good ""hobby projects"" book or anything similar, something not too advanced since he is new(b). I'll look into books of any language but C++ is what he is focusing on in school at the moment.  and  2/ I remember reading about how most universities give students some basic hardware and get students to make a simple OS for it. A book along those lines would be awesome as well. 2/ is basically 1/ narrowed-down a little I guess.  tl;dr: Recommend some ""project-based"" programming books that someone would go through for fun. "
How to move knowing from Java to learning J2EE,"Hi RedditI am a core java programmer. However most of the requirements that I currently see online need J2EE programming knowledge which is a soup of abbreviations and frameworks.  >Struts Framework >Spring Framework >XML >JAXB >Transaction & Security mgmt >Core Java >Servlets >JSP >JDBC >Web Service  CXF >ANT >Tomcat / Apache >JQuery >Network and Socket Programming >Java Beans (EJB) >JMS >JBoss server config >JAAS >SOAP >Velocity templates >CFX framework >Connection Pooling >JNDI, Java Mail  Where do I get started. How do I move from core Java to knowing these technologies. Any pointers that can provide me in the right direction ?????   tl'dr: How to move knowing from Java to learning J2EE "
"How does a programmer begin web design? Any good books out there? 
 EDIT: Cross-post to stackoverflow.","I am an experienced programmer, and I have a few little ideas that I think would work really well as PHP based web applications. I have no problem with learning PHP, mySQL, etc, but I do have a problem with the design of a webpage in itself.  I am used to interface design ala Interface Builder and Swing, where there are some clearly defined classes with clearly defined behaviors etc. To me, web design is the wild west where I have to write my entire user interface, complete with little effects and stuff, on my own.  I'm not afraid of this by any means, I just need some advice on where to start. I've like to learn some proper HTML for starters, since everything I know how to do is static and ugly, and I'd like to learn Javascript to be able to make my pages more elegant as time goes by.  tl;dr : How does a programmer begin web design? Any good books out there?  EDIT: Cross-post to stackoverflow. "
Graphic designer 5 years out of school (and out of practice) looking for a program to build better websites,"Quick backstory:   I graduated in 2005 with a bachelor's in art (graphic design) and used to crank out sweet pages all the time using Notepad/HTML and some CSS/JS.  Of course this was all years ago when I had lots of time and class projects to dick around with.  Fast forward a few years and wow, things have changed quite a bit:  new languages, Web 2.+ (3.+?), etc. and I'm left with my proverbial coding dick flapping in the wind.  A lot of websites these days look really great and are incredibly simple, but I don't know where to start.  I've got a portfolio page that I built using Flash and HTML (written in notepad) but I want to make it look a lot nicer.  Any advice on a program to help me build a better site?  TL;DR  Graphic designer 5 years out of school (and out of practice) looking for a program to build better websites "
I want to make my computer sense small amounts of voltage for a brief time created by an external sensor and react each time it detects it.,"I am a first year engineering student and have to make a project for my matlab course (also a project I've been trying to make for a year or so). I am very new to programming and wanted to know how I can convert USB input into a matlab program (doesn't have to be USB).Here's literally what I want to do:I want to essentially recreate a drum module by hooking up piezoelectric transducers to my computer. Piezoelectric transducers are small pieces of piezoelectric material (material that creates a voltage when stressed/bend). So I want to send a VERY small amount of voltage into my computer, it then senses when a threshhold is met and plays a prerecorded sound, which is the entire purpose of a drum module. TL:DR I want to make my computer sense small amounts of voltage for a brief time created by an external sensor and react each time it detects it. "
I want to a framework to test all possible combinations of features by specifying how each feature works on its own/in combination with a few others.,"Hi Reddit,  Unless I'm doing it wrong, test driven development just isn't scalable. I start with one feature that can do x, y or z and add a test for each case. Then I add another feature that can do a,b or c. Now I need to test 9 cases: ax, ay, az, bx, etc. I add another feature which has another 3 ways to use it and I have 27 cases. This gets unmanageable quickly.  Now I'm a clever bugger and make sure that I write my features 'orthogonally' - the behaviour of one doesn't depend on the others, at least not to the external interface. But I'm also thorough and don't want to just test each feature in isolation.  So my question is are there any test frameworks out there that support this sort of model? I want to be able to say 'this is how feature 1 works in 3 cases, this is how feature 2 works in 3 cases and this is how feature 3 works in 3 cases' and the the framework will go away and test all possible combinations. In real life it will be more complicated and I will need to be able to specify how certain features should work together, etc. but I hope you get the idea.  I am coming at this from my frustration from using RSpec (Ruby), so please tell me a) why I'm being stupid and can already do this with RSpecb) why I'm being stupid by designing my tests poorlyc) where I can get a framework that will let me do thisord) nothing yet exists, but it's a great idea.  tl;dr:  I want to a framework to test all possible combinations of features by specifying how each feature works on its own/in combination with a few others. "
"need to trigger 12v to one of two spots, and also take some small electrical input","Our fuel terminal doesn't work anymore, my boss came in to kick around some ideas with me about solving the problem and we talked about throwing together a replacement terminal.  It's a low traffic airport fuel pump pretty much, so it wouldn't need much.  I don't need any specific answers, just something to get started as I have no experience with the hardware part of this.  Pretty much I'd just need to put out 12v to trigger the pump, and something to count the 'ticks' (I believe it's an electrical signal) signifying 1/10th of a gallon.  I would need to trigger one of two pumps.  I assume I would be using the serial port, but obviously that doesn't put out that kind of voltage itself, but am unsure of what the intermediary device would be.  tl;dr need to trigger 12v to one of two spots, and also take some small electrical input "
"Good programming is not midway between duck tape programming and architecture astronomy, but a completely different dimension which means the ability to quickly write small, well-designed programs.","There's been a lot of buzz around this today, but everybody is taking it as a given that there is a spectrum, that one side has fast/sloppy code, the other side has over-architected unbelievably slow code, and that Nirvana consists of finding the middle path.  Stop it.  Simple code and highly architected code are not at opposite ends of the same spectrum. Truly good programs are small AND simple AND architected in such a way that makes them easy to modify and extend.  It's true that there are people who hack together code with ugly dependencies, repetition, and hardcoded nastiness that make it a nightmare to modify. These are bad developers, and some develop quickly and some develop slowly. Fast ones do often endear themselves to managers, but it doesn't mean they're good (although they are better than slow ones who do the same thing)  There are also people who go bonkers over design patterns and architectures without stopping to think about it, resulting in bloated applications with thousands and thousands of classes like AbstractConcreteFactoryDelegateManagerImpl. These are also always bad developers, though you don't find too many fast ones.  Good programmers are not midway between these extremes. Good programmers don't write moderatly quickly, they don't write moderatly convoluted code with a moderate degree of architecture. Good programmers write small, elegant, perfectly designed programs quickly. They don't compromise: They are  orthagonal  to these two sterotypes, although like the others, some good developers work quickly and some work slowly. The quick ones are the true coding superstars.  Good architecture simply means that a program can be extended in expected and unexpected ways without extensive redesign. Architecture astronomy is just as bad for this as overcoupled spaghetti.  Similarly, code speed and ship-it-ness come from conciseness and simplicity  and  good architectural practices (such as reuse). I've seen spaghetti code just as bloated, heavy and slow to develop as overarchitected code.  tl;dr -  Good programming is not midway between duck tape programming and architecture astronomy, but a completely different dimension which means the ability to quickly write small, well-designed programs. "
"Which language is better for game dev: Python, Haskell, Objective-C or other?","I've been wanting to learn game development for a while. I'm a CS graduate and have been developing various apps for a while now. I don't really have a grasp of the fundamentals of game development, or of graphics libraries such as OpenGL.  I was wondering what a good language for learning game dev would be. So far I'm considering Python, Haskell and Objective-C (in that order).  I'd go for Haskell as I think it would force me to design the game in a better way than if it were imperative (probably lots of global variables, huge classes and such). However it doesn't seem like game dev in Haskell is easy, and there are almost no resources, in comparison to C++. Although it does seem that in the future, with the amount of cores increasing, functional languages will be more popular in the industry.  Python is a nice compromise between various programming paradigms, and it generally increases productivity. Prototyping would probably be a lot quicker than in Objective-C. I'm not too worried about performance (as if anyone would play the games), but I'm a bit wary of the GIL and generally multithreading.  I'm really only considering Objective-C because it's the language I'm the most comfortable with, it allows me to use C code directly, and C++ tutorials would probably be easier to ""translate"" to ObjC.  I'm not really considering C++ because I simply don't like it.  tl;dr Which language is better for game dev: Python, Haskell, Objective-C or other? "
"BIHs, y/n?  If y, help!  If n, what instead?","BIHs sounded like the perfect solution - Fast raycasting, works well with dynamic objects, and don't have any weak points...  However in practice I'm having trouble with them.  A bit of background - this is for a physics engine, but its primary use is going to be special effects (such as lots of particles, etc.) so fast raycasting is the highest concern, and regular-old physics is necessary but secondary since it won't be used all that much.  At the start I used a simple Octtree while getting the rest of the project up and running, but I wanted to do better with something like a K-D tree but neither of those things were suited for dynamic objects due to long creation times so I searched elsewhere.  BIHs I've had nothing but problems with.  The creation time is fairly quick, but after that the raycasting is fairly slow and I don't see how it's supposed to handle dynamic objects at all.  It might be due to misunderstanding the fundamental nature of the hierarchy due to the lack of info out there on the web though, since my results don't seem in step with the rest of the world.  tl;dr - BIHs, y/n?  If y, help!  If n, what instead? "
Kind of a useless feature in Chrome but I was surprised it worked in it and not Firefox.,"I was on Facebook (yeah, I know..) and I was playing a flash game on there. I was using Firefox at the time and this is what I was wondering. I was playing the flash game, and I wanted it in a a separate window, not in a tab. So I tried to drag the tab out of Firefox so it would create a new window. It did create the new window, but it refreshed the page, hence losing/restarting the flash application. If that doesn't make sense, I was playing the game, had it all loaded etc, and when I dragged the tab out of Firefox, it refreshed the window and the flash game so I had to start over again.  So that got me thinking, maybe it might work in Chrome, without the page refreshing. I then open up Chrome, log into Facebook, open the flash game, and I drag the tab out of Chrome so it would have its own window. And what do you know, it worked! It didn't refresh the page and the flash game was as if it was still in that tab.I think Firefox should implement this feature, but I think it only worked on Chrome because it has that whole separate process for each tab thing going.  tl;dr:  Kind of a useless feature in Chrome but I was surprised it worked in it and not Firefox. "
"2 majors, one is (probability/statistics/optimization/time series/finance), the other is Computer Science. I like both, but I like computer science a lot more. Which should I choose?","So, here is the situation. I am entering my junior year at Princeton University and currently majoring in Operations Research and Financial Engineering (ORFE) in the engineering program (BSE). ORFE mainly specializes in probability, statistics, optimization, time series, and some economics/financial investing. The other major I may want to switch into is Computer Science (COS), which I would take in the BSE program. As of right now, I am equally on schedule as far as classes go with ORFE and COS.  I have always liked playing around with computers, and since taking some COS classes, I really enjoy it. I enjoy some of the probability and financial investing of ORFE, but some of the other classes I find not interesting at all. Overall, I enjoy COS > ORFE.  As of right now, I planned on majoring in ORFE and getting a COS certificate (3 intro courses/2 departmental). My alternate choice could be to major in COS and take some of the ORFE classes like (Financial Investments). Both majors have heavy requirements from within each department so I would not be able to take more than a small number of COS classes being an ORFE major, and vice versa.  Any advice? Would ORFE with COS certificate be valued higher than COS with some ORFE classes by employers? Should I go with what I like the most?  tl;dr:  2 majors, one is (probability/statistics/optimization/time series/finance), the other is Computer Science. I like both, but I like computer science a lot more. Which should I choose? "
"I have two job opportunities, one is a larger organization that pays more, one is a small company but will be a cut in pay, what should I do?","I graduated from my local technical college with a two year diploma in Computer Engineering Technology. Upon graduation I took up a position with the federal government largely because it was the highest paying of the four positions I was offered. During my time here I have found that the working conditions are nearly suffocating because of the sheer amount of bureaucracy and red tape that you need to go to in order to get anything done, as well as I have found the the programmers that I work with are downright hostile to the idea of  trying to modernize anything despite the fact that they are currently considering their retirement packages. The above factors have led me to start looking for a new job and I have recently had two opportunities present themselves.  The first opportunity is with a local fairly large University. It is a job as a web application developer and is working with newer technologies and doing some consulting and analyst work which would be a nice break from riding the desk. This job also comes with a raise in pay.  The second job is working for a smaller tech company that provides a online tv publishing framework. This company employs 30 people and has been running for four years (I take this to mean that they are at least some what stable) and has a bit of a reputation for hiring younger hot shot programmers(take that as you will, as either a good or a bad reputation). In talking with this company it has become clear however that I would have to take a cut in pay of anywhere from 10% to 20%.  So what do you guys think, maybe give me some perspectives from people who have already tried these options.  tl;dr I have two job opportunities, one is a larger organization that pays more, one is a small company but will be a cut in pay, what should I do? "
Is there an easier way to find/hire web developers then putting up posters / asking friends?!,"Hey /r/programming,  I'm having a ridiculously hard time finding a web programmer to work a (well) paid position this summer.  Just this summer, no further requirements...  I'm writing this post as a last resort because I have plastered my school's CS department with posters and emailed every friend I know to no avail.  I officially have two weeks to find a great web developer for the summer or I lose $20,000+.  I'm a graphic designer in college who had an idea for a fairly simple website.  Recently, that idea made it to the finals of a $200,000 business plan competition and, more importantly, got accepted into a startup incubator for this coming summer (where we get around $8,000 a founder + office space + mentorship).  There was one contingency though... We have to find a 'technical co-founder' or we're out.  I've done everything I know how to, short of listing on online programming job boards that cost hundreds.  Does anyone have any suggestions on ways to reach college programmers?  tl;dr:  Is there an easier way to find/hire web developers then putting up posters / asking friends?! "
"How do I get the text from the ""logfile.txt"" to show up in the file I created ""test4.txt""?","The other day (monday) I started playing around with VB.  I have never done any programming/scripting in my life, but a friend has been showing me a couple of things.  He's has showed me how to .openastextstream, .readlines, .writelines.  He is extremely busy right now and I don't won't to bother him, because he has been so nice to show me some of this stuff.  So I thought I would ask reddit...  How do I grab text form one file and have it write the data to another file that I just created?  so far I have create a .txt file  const forreading = 1const forreading = 2  set myfilesystem = wscript.createobject(""scripting.filesystemobject"")  set myfiletxt = myfilesystem.createtextfile(""c:\vbtest\test4.txt"", true)  sPath = myfilesystem.getabsolutepathname(""c:\vbtest\test4.txt"")  sfilename = myfilesystem.getfilename(spath)  Then I get the files I want to extract from  set mygetfile = myfilesystem.getfile(""c:\vbtest\logfile.txt"")  set myopenfile = mygetfile.????? <this is where I normally streamtext from, but what goes here?????  tldr - How do I get the text from the ""logfile.txt"" to show up in the file I created ""test4.txt""? "
"Preselect my country automatically in those ""Choose your location"" drop downs","Examples:  Ah, so I want to go to the [insert example website], and I am met with the wonderful ""Choose your location!"" landing page. Fantastic. Just what I wanted to see. Fuck you NVIDIA for being one of the worst examples with your obnoxious Flash animation I must wait for.  The problem is that the drop down is not preselected with my location based on IP address geolocation to get the best guess for what country I am from. Obviously IP address geolocators will not be entirely accurate which is why I propose that these landing pages simply preselect your country rather than choosing for you and loading the next page automatically. This would save an enormous amount of time (when you aggregate every visitor to every website that uses these), and I see no reason to not preselect the users location, especially considering I've really only seen these on large, international corporate websites, so surely the funding is there for the extra few databases/servers to do this (/run-on sentence).  tl;dr: Preselect my country automatically in those ""Choose your location"" drop downs "
What does proggit recommend as a course of study for an aspiring young pre-CS student?,"My 14-year-old cousin came over to my place, woke me up, and asked me (a career software developer) ""What should I learn now to become a computer scientist?"" I didn't have a really good answer for him. I told him to pay attention to algebra, take a class in symbolic logic if he could find one, and take any programming classes he could find. I started learning to program in college (not counting little games in BASIC and hello-world level stuff in C growing up). I'm sure there's plenty of good teach-yourself-to-program books aimed at people his age, but I don't know what they are.  TL;DR: What does proggit recommend as a course of study for an aspiring young pre-CS student? "
My tranny grandmother is trying to pretend that her new baby is mine. There's no tellin who else that ho's been with...,"So i was fortunate enough to break into programming late in life. About 2 years ago, I made up my mind and just started doing it. I am now about 9 months into my first real development job.  I've worked hard to get here, I moved my family a thousand miles to take an internship for very little money, and it has seemed to pay off so far.  The thing is, now I am worried I cannot go back to what i was doing before, I simply can't. I love this far too much. The problem however, is that the company I've been working with is a small startup.  We use python/django/twisted/MySql. I've learned a lot, and have some very sharp developers showing me the ways. These guys are obsessive about maintainability, code reviews, and general cleanliness.  I'd like to think that I am now  in  but even though the talent is top notch (as far as I can tell) The survival of my company is not guaranteed.  Lets assume for arguments sake that I know django inside and out, python pretty well, and i feel like i can do  pretty much do anything in javascript.. If my company shuts the doors tomorrow...am I screwed?  Will i be ok on these not so popular technologies? I will do whatever it takes, code is code. I will learn php if i have to (not my first choice)I will never be satisfied with what I know, but do you think I am employable? If not...then what should i go after? ASP? .NET? C#? PHP?  I have been working with C, C++ in my spare time, but I don't think that that is any good to an employer (Sure you can hack on our low level code! If you can do project euler you can do anything!)  I don't think i could search all over the country and be willing to move anywhere again. The only reason i am where i am is because i was so hungry and was willing to do anything.  I appreciate any input you guys have. If you read this far, then thank you for your time.  TLDR: My tranny grandmother is trying to pretend that her new baby is mine. There's no tellin who else that ho's been with... "
I wanted to continue learning Java after a high school Comp Sci course but have no idea where to look.,"Hey Proggit,  I'm a senior in high school, and last semester I took a computer science class. I was very excited even before it start as I had always wanted to learn programming. The language we learned this year was Java, as opposed to the seniors last year who learned C (or C++? It was a derivative of C), which made me more excited because I have an android phone and I was looking forward to apps.  Fast forward to my question, the class (no pun intended) left a lot to be desired. Don't get me wrong, we covered a lot and the teacher was great, I just wish I could have gone farther. We ran out of time and didn't even get to finish our final project, which was a minesweeper clone.  Recently, I've been attempting to complete my minesweeper but I have no idea how to even continue. I'm definitely going for a Comp Sci degree in college, but I don't want to wait that long. So in conclusion, if someone (or many of you) can point me to a website which is geared toward teaching people with my level of programming experience, that would be fantastic.  My inefficient, uncompleted code for reference:  I almost feel embarrassed posting it..  tl;dr, I wanted to continue learning Java after a high school Comp Sci course but have no idea where to look. "
have an idea for Gmaps mashup but little coding experience. Should I pay someone or do it myself?,"Hey guys,  Go a question for you.  I've got an idea for a google maps mashup. Basically it's getting calendar events to show up on a google map.  I know there are pre-built aps out there, but they don't have all the features I want. I also found a site that has a similar look/feel to what I want but isn’t calendar based. It’s based on someone’s tutorial, so I don’t think there is too much of a copying issue there.  I’ve been trying to re-engineer their set up, but with not much success. I have some (very) basic understanding of web coding, so the learning curve is pretty steep. I’d like to know how do this kind of thing, but it would probably be more of a hobby rather than a career.  The question is, should I keep hacking at it until it kind of works, or just pay someone to write the site. I’m not really sure if there is money to be made from it, but I guess it is possible. I’ve got a couple quotes from eLance and it’s coming in about $700.  I’d like to say that I was the one who built it, and be able to do this kind of stuff, but I know it will likely be buggy and possibly broken.  tl;dr – have an idea for Gmaps mashup but little coding experience. Should I pay someone or do it myself? "
"I am after something like: ""amazon.com burns 1337 Gfucks/s while google.com burns 19345 Gfucks/s"". Gfucks/s taking all applicable computing (not network) values into account.","I am writing articles on lots of internet services like digg, reddit, bing and local ones for an on/offline encyclopedia.  One thing I'd like to include in my articles is the amount of computing resources that these services employ. Is there a standardized way of measuring this?  I of course know to ask for simple values like bandwidth and storage size, but what about more esoteric values like disk I/O, processing cycles etc?  I might be hoping for too much here, but is there a standardized value taking into account the numbers of servers and each servers capacity and load? Ending with a nice comparable number?  Haha, this question seems strange. Feel free to downboatbash me if I am way off.  TL;DR I am after something like: ""amazon.com burns 1337 Gfucks/s while google.com burns 19345 Gfucks/s"". Gfucks/s taking all applicable computing (not network) values into account.  "
"I know some C, C++, and assembly. I want to get into firmware programming. What are the best resources you know of?","I'm currently interning at an engineering company and they are desperately low on firmware programmers. Right now I'm working on some kinda boring tasks that don't really need to be done. Firmware is something I've been wanting to try out for awhile now. I have experience with C and C++ and a little bit of experience with assembly but have not yet  had an opportunity to take a course about firmware at the unviersity I attend. Are there are sites, articles, books, etc that you guys highly recommend for a beginner? I'd like to get the basics down before I talk to them about giving me shot at a small aspect of the project.  tl;dr - I know some C, C++, and assembly. I want to get into firmware programming. What are the best resources you know of? "
"Intern wants to weasel way out of spreadsheet work using computer magic, looking for your help.","I just started out as an intern at a small business that I was very excited to be at until yesterday when I was assigned the very mind numbing task of going through the company's records (stored as PDFs) and making sure that a rather large excel sheet summarizing them has all of the information correct. What I do is open files, stare at them until my eyes hurt, and then confirm that the excel row corresponding to the document has all the correct information (resulting in more eye hurt). I'm wondering if anyone knows of a program that will do this for me so I can move on to the more fulfilling less blinding work the office has to offer. Also acceptable are fun ways to commit time theft.  tl;dr: Intern wants to weasel way out of spreadsheet work using computer magic, looking for your help. "
Where can I find some advice on how to get open source projects to compile in a Visual Studio 2008 environment?,"I'm a new owner of a shiny new A.S. in Computer Science. While trying to get myself back into the job market, I'm hoping to get some real world experience with the languages I just spent a year and a half learning. These would be C++, Java, and VB.net.  My goal is to find an open source project for each language that I can build and compile on my own machine, and see if I can manage to make any useful contribution to the code. I downloaded the source for one project I was interested in, loaded it into an empty C++ project, and VS pretty much blew up when I tried to build the project. Where can I find some advice on how to at least get existing code to compile in a VS 2008 environment?  TLDR: Where can I find some advice on how to get open source projects to compile in a Visual Studio 2008 environment? "
"crappy expensive software for my industry, best way for non-programmer to make better web app","The industry I work in only has one real CRM software package. However, it's buggy, expensive, needlessly complex, requires some advanced hardware/networking, lacks features and is prohibitively expensive. It basically has a lot of room for improvement and is marketed to an industry that simply doesn't have the means or knowhow to maintain such a system. Other generic CRM programs often are confusing and/or lack some important features our industry need.  There's a real opportunity for a competing CRM package specifically designed for this industry. An online web app would address a lot of the problem encountered in this system, reduce client costs and eliminate their hardware/IT expenses almost entirely.  I've worked in my industry for approximately 6 years now and know how to vastly improve the software. However, being that I'm not a programmer I wanted to seek out advice on the best ways to take my idea and make it a reality. What is the best practices for planning, attracting investors, talent and finally developing.  tldr: crappy expensive software for my industry, best way for non-programmer to make better web app "
"In Python why should I not use instance(f, (int,float,long,complex)) to check if something is a number? What should I use? 
 Thanks.","Minor introduction, long ago when I was younger I dabbled in PHP and wrote some basic web apps (flatfile news publisher, guestbook, etc..). I recently picked up [ThinkPython]( and started reading through to get back to my roots.  So anyway, I wrote the functions to draw the box in exercise 3.5. I didn't have an internet connection, just the pdf and python manual, to move on with the next chapter and use Swampy. In this downtime decided to re-write the box with some loops. (making it overall cleaner, imo). I got it down to the function accepting a number to draw the box dynamically to however size I wanted the box to be.  Problem now is I was thinking of how I would verify that the object is actually a number. Now my old knowledge of PHP kicked in and I tried  is_numeric() to no avail. I started searching the manual for options and only really came up with isinstance(f, int) (or is instance(f, (int,float,long,complex))).  This fixed it. Once I got home and got on the internet I started reading more and people aren't ever recommending this as a solution to verify if something is a number. Why is this? All the items I see create a custom function to compare it and see if it is a string. Some even use type(f) == type(1) to check if it's a number.. Which is the best solution? Why should or shouldn't I use the other?  TLDR:  In Python why should I not use instance(f, (int,float,long,complex)) to check if something is a number? What should I use?  Thanks. "
"I'm thinking ""let's do this first project without classes and maybe add them later, or in the next project"".","I'm working with some folks who are beginner programmers, and I was thinking about how to explain object oriented programming to them.  We're going to start working on a group project and I was thinking that it might be an interesting idea to do it (the first one) without using classes at all.  Of course, we'd use standard lib classes and classes from any third party modules we use, but we could probably get pretty far without creating any of our own.  I had this thought and I wondered how ridiculous it sounds. . . so tell me, is this completely ridiculous?  The first project will probably be somewhere on the order of 1000 LOC.  I do want to delve into the world of designing and using classes but I also want to reinforce that Python is multi-paradigm.  tl;dr I'm thinking ""let's do this first project without classes and maybe add them later, or in the next project"". "
"my local Python env is a spaghetti mess and I want to clean it, replace with vagrant / virtualbox isolated instances.","I'm currently the data science team at a startup, but we plan for growth soon. Prior to this job, I did freelance data science, mainly Python based.  Over time my environment has grown to be an ugly mix of global packages that I'd like to strip out and start fresh. I'm thinking of skipping the entire virtualenv experience, and instead using vagrant / virtualbox to handle individual projects. (I also still have some freelance work as well).  Hardware-wise, I'm running an MBP with 16GB RAM, quad-core i7 locally, and I regularly rely on EC2 instances to scale out development. Hence the appeal of vagrant over virtualbox for deploys.  Anyone go through this before? If so, any gotchas before I spend a weekend potentially wrecking my environment? Would love to know what to watch out for so I can make this process as straightforward as possible.  At the end I'd like to have: only the OS X Python locally, the MacPorts Python and all packages reliant on it removed; MacPorts gone. All replaced by vagrant / virtualbox setup that allows my work to all be self-contained, and easily deployable to any EC2 instances I have.  TL;DR: my local Python env is a spaghetti mess and I want to clean it, replace with vagrant / virtualbox isolated instances. "
PR girl wants to transition into data science as a career. Am I completely in over my head?,"Hello /r/datascience!  As a 24 year-old media/PR professional who has worked with various web, email marketing, and social media analytics tools before (albeit at a very rudimentary level), I'm fascinated by the way that data increasingly influences the content that people want to see. As a lapsed journalist, it's also clear that data visualization is starting to play a bigger role in how people consume news and learn about current events.  However, in my experience, the media and PR professions (or at least the places that I've worked) are woefully behind in terms of harnessing data provided by social media and web companies, and the pay/advancement opportunities in this field are quite limited for me. Data visualization is also becoming a hot commodity in the media world, but there are few people who can combine traditional journalism skills with data viz chops to tell a compelling story (especially for the money that most media companies are willing to pay their employees.)  I'm increasingly fascinated by the career opportunities and academic/research questions posed by data scientists and think the field is extremely interesting. It's become clear to me that data science has applications that go far beyond just social media or online content. It's also clear that I have zero knowledge of what is required to break into the field.  I'm thinking of taking Coursera's Data Science MOOC to sharpen my data science/analytics skills, but I want to go a bit deeper. Is it possible to make a transition into data science as a profession, and if so, what do I need to do in order to make that happen? My math skills are average, at best, but I'm willing to do the work to beef up my skills.  After completing a MOOC, what else would I need to do in order to deepen my knowledge? For example, I don't think an employer with data science internship opportunities would take me seriously unless I was already enrolled in a data science-related master's program or had data-related work experience under my belt.  I'm fairly young, so I have the flexibility to do what it takes to transition into this field.  tl;dr  PR girl wants to transition into data science as a career. Am I completely in over my head? "
I need help translating the Japanese menus on this .Net/C++ application!,"I've recently acquired a Blackmagic Intensity Pro HD capture card to record some gaming on my Xbox 360 & PS3 and it just so happens that there is a great, lightweight program that is much better than the one that came with the card designed for doing exactly what I am doing. The only problem is that it is in Japanese, I've gotten a ways using  Resource Hacker but I don't speak Japanese and some of the items cannot be changed in Resource Hacker, specifically the submenu that changes while your recording. If reddit could point me in the direction of a tutorial or give me a hand with the translation that would be great! I've put a link to the page for the program below.  TL;DR  I need help translating the Japanese menus on this .Net/C++ application! "
"I know the syntax and algorithms, but don't know how do build a larger project that wouldn't make a proper programmer vomit.","I have a CS degree, but was never actually taught how to properly design a program.  I store my data in my view, which I then put in a global variable.  I'm not a bad person; I know this is wrong, but I don't know where to learn good software design principles before I go and subject the open source community to my bad habits.  Would some kind soul help me out with some direction/advice to bring my programming skills up to snuff? (C/C++, python, perl)  Is there a book that every good programmer has read?Would you share how you learned to design software correctly?Any wikipedia pages I should read?  tl:dr: I know the syntax and algorithms, but don't know how do build a larger project that wouldn't make a proper programmer vomit. "
I am a test/automation engineer and I would really like to move into development. Any tips?,"I was a test engineer for about a year before I went back to school for a Master's degree. I got the degree partly to explore if I wanted to work in academia and partly because I thought it'd help me land a good development job (in systems). However, when I graduated I got interview calls mainly for test/QA positions probably because of past experience. So, here I am back in a non-development position which involves testing,automation etc. Has anyone successfully transitioned to being a developer from such positions? Any tips on how I should go about reaching my goal, considering I have at least a couple of years in my current position? Its not like I hate testing, I like development better. And I'm afraid I'm being pigeonholed into testing by adding more and more testing experience on my resume.  tl;dr : I am a test/automation engineer and I would really like to move into development. Any tips? "
forced to major in economics. Will specialize in econometrics and economic statistics. Can I still wield the powers of data science?,"Okay so I go to UT but couldn't get into their math program. Instead I am doing economics and I plan to focus HEAVILY on econometrics/economics statistics. I will also be taking as many statistics and math courses as I possibly can (this could be difficult since I won't have access to upper level courses). I have already learned a good bit of Python and Java on my own and feel confident that I can pick up R also. My question is, if I focus on the econometrics part of economics, and land a research opportunity that can show I have a solid background as a programmer/statistician(also maybe an internship), is getting into data science do-able, or will I be at too much of a disadvantage compared to math and statistics majors?  TL;DR- forced to major in economics. Will specialize in econometrics and economic statistics. Can I still wield the powers of data science? "
"let a = 5; 
 now doesn't work as expected when typed into the console","So this friend of mine was given an assignment in class to get a three digit number from user, let's say he entered 123, and then return ""1+2+3=6"".  He said the teacher told him to use a while loop, and he didn't understand why, because his approach was of course naive, only working for 3 digit numbers.  I thought it was a cool assignment so I opened up chrome console and wrote the following code:  let foo = (x) =&gt; {let a = x.toString().split("""").reduce((a, b) =&gt; a +""+""+ b); console.log(a + ""="" + eval(a))}  You could then call it like so:  foo(321)  which would yield:  3+2+1=6  A day passed by and I wanted to show the code to a friend of mine. So I started typing the code, but it didn't seem to work. Nothing did actually, so I tried things like:  let a = 5;  a  Which gave me a reference error saying 'a' is not defined  I was starting to think that I am crazy, I couldn't help myself but wonder why this code doesn't work now and did before. Did chrome change something recently?I remember reading a day ago on this subreddit about a question, why did  {} + {}  result in  ""[object Object][object Object]""  instead of  NaN  like before.  Someone explained, that this was because chrome now wraps everything you type into the console into a pair of parentheses. And then it stroke me that this is exactly what essentially broke the  let  (and const) keywords for some use cases.  TL;DR;  let a = 5;  now doesn't work as expected when typed into the console "
"I have a MustacheJS template, I want to fill-in the template and inject the HTML at compile-time rather than at run-time.","For my portfolio site I have a list of projects which I want to show on it. The problem is that the code is highly repetitive, thus I used MustacheJS templating to avoid code repetition and dynamically fill in the template at run-time.  But I don't want to user to waste time downloading MustacheJS and have the browser fill in the template at run-time. I want to be able to have Gulp run MustacheJS, generate the HTML, and inject it into the page. This way, all the work is done at compile-time and the browser doesn't need to load anything extra.  I am currently in the process of moving away from Gulp and switching to Webpack, I am also open to switching to a different logic-less templating engine, but would prefer to stick with MustacheJS for now.  TLDR: I have a MustacheJS template, I want to fill-in the template and inject the HTML at compile-time rather than at run-time. "
How do you manage change tracking in your flat file DBs.,"Hey friends. Not really sure where to even ask this one. I build a flat file database of assets which is basically just a directory structure that contains json files. Whenever an asset is changes a new copy of the json file is generated so the result is every asset has a folder full of json files that represent changes to the asset.  All this is working beautifully but I'm trying to find a efficient way to display the changes in a historic fashion. (IE: Today Asset 1's value for A changes from X to Y, Asset 7's value for C changes from M to N, etc) without having to read every single file in the structure. I'm okay with calculating changes as they happen and storing them in a central location or even SQL DB but I just wanted to see if anyone had done something similar and what you went with.  TL;DR : How do you manage change tracking in your flat file DBs. "
"Django or Flask, for ecommerce service, with native mobile apps and push notifications ?","Hi guys, we're starting a new project, currently only developer.  I want some kind of ecommerce store where users can buy some products with standard shopping cart and payment service; which sounds typically Django. But also we're going to develop a native mobile app on Android and iOS, with push notifications also. The backend will likely also communicate with a python OpenCV module to do some extensive image processing.  I read that Flask is better as an  API and a backend for a mobile apps. While Django is much better for typical ecommerce. I neither used both, I've been using python for scientific packages mostly. I tried a sample Flask project, and I tend to like it, but I'm afraid it will take lots of time to have an e-commerce cycle developed on Flask than on Django.  TL;DR; Django or Flask, for ecommerce service, with native mobile apps and push notifications ? "
What can someone who is just starting M.S. in engineering do to improve the odds of landing a data science job.,"Hello everyone. So as the title says I am just beginning an M.S. program in biomedical engineering. I got my undergraduate degree in biology and worked in a bio informatics lab. During my masters I will be working in a computational Neuroscience lab.  I realize choosing biology was a mistake in undergrad and am trying to correct it now.  Hoping to get any advice on things someone like me can start doing to improve odds of landing a data science job after graduating with the M.S.  Any courses or topics are particularly useful? (beyond obvious things like python, R programming). Also any certifications people could go through to look better for an application?  TL;DR- What can someone who is just starting M.S. in engineering do to improve the odds of landing a data science job. "
I made a program to automate SSH scripts and would like any input.,"Hello /r/python. I am a bit nervous to publish this program because this is the first time I ever published any of my coding. Usually I make small scripts and stick to that.  I am looking for any input. I am sure there are things I didn't think of while developing this program therefore I am making it open source. I hope /r/python is the right place to go to do post about getting feedback for the program. I also wrote a post for /r/learnpython because I am sure the code can be tweaked a bit.  I wrote this program because I have a networking background and wanted to kindle my love for programming. I always lurk around /r/python and figured I would give it a shot.  My program is compiled on my website for easy install www.netscriptassist.com  It is also on github:  Description of the program:  I made a program called NetScript Assist. It basically takes lines of commands and allows you to save them as a script. With this script, you then can send it to 10 devices. (It connects to them using SSH only for now.)  When the script is done running, you can review your logs of what the console looked like when the script ran.  This program only works on Windows right now but there isn't that much code that needs to be tweaked for it to run on all OS from what I can tell.  tl;dr  I made a program to automate SSH scripts and would like any input. "
Python is considered slow. Why do some high performance web applications use Python backends to serve pages?,"Python is often called quick to write slow to perform (usually when being compared to C/C++ which is quick to perform slow to write).  I just recently discovered that reddit serves its pages using Python. Why is it that some sites (like Reddit) which have HUGE traffic chose to serve sites in Python? Is it because Python's relative speed isn't really noticeable through user-interaction? E.g., a system program making a I/O call needs to be fast because it's a machine/machine interface. While the wetware between our ears won't notice the relative sluggishness of Python?  Or is it something else? Like C being an absolute horror to write web servers in?  TL;DR. Python is considered slow. Why do some high performance web applications use Python backends to serve pages? "
"other than python and R, what can an undergraduate learn to make himself more qualified for data science related jobs?","In the near future, I'm going to be applying to two programs - MA Statistics and MA computational and mathematical finance. I'm more keen on the finance one but I'm not sold yet. There are a few very interesting machine learning / deep learning labs in the university but they are in the computer science department and I'd have to do a second bachelors in CS if I want to go there. Which I don't want to do.  Currently an economics undergraduate. I have mediocre knowledge of Python (sklearn, pandas, numpy) and R (mostly with libraries used for munging). I have august free and I want to learn something that is commonly sought in data science jobs. So that's essentially my question - what else can I learn to make myself more desirable for data science positions? I've considered Apache Spark or SQL, but that'd be hard to learn compared to Python and R because you actually need a huge database to work with.  tl;dr - other than python and R, what can an undergraduate learn to make himself more qualified for data science related jobs? "
I need to learn how to write my own Linux system.,"I've always been into programming. I am an undergraduate double major in physics and astronomy. My father owns a web programming company and I know plenty of languages. I work part-time programming in IDL, MATLAB, JAVA, and C for NASA. However, the majority of my work up to this point has been on windows or mac. I am just starting to get into Linux in my classes and I fucking LOVE it. I'm really interested in making my own Linux, but I feel like I'm not fluent enough with Linux to be able to create my own. Does anyone have a good book in which I can learn to make my own Linux? I am completely in the dark on this subject so any help is greatly appreciated. I feel like a dick not knowing Linux this point in my life. Thanks in advance.  tl;dr - I need to learn how to write my own Linux system. "
"How do I time a Haskell function? 
 edit: Thanks for the speedy answers!  reddit rocks.","I'm trying to learn Haskell, and I'm having some speed issues.  I'm trying to figure out how to optimize my small functions so that they actually finish eventually.  I'm trying to compare big O, essentially, but don't really know how.  Does anybody know of a nice chart comparing built-in Haskell operations (filter, reverse, zip, ^, +, and so on) and their speed?  Alternatively, I could use a setting to make ghci a little more verbose and make those comparisons myself.  Thanks for your help in advance.  I tried googling, and searching both reddit and stackoverflow, but no luck; sorry if I missed something obvious.  Side note: these are pretty small functions I'm comparing here; I'm working through Project Euler problems (specifically getting furious at 9 and 10).  TL;DR: How do I time a Haskell function?  edit: Thanks for the speedy answers!  reddit rocks. "
I've got an interview coming up and want to show the interviewers that I'm seriously interested in how <Big_4> is applying data science. How do I go about this?,"Hi all,  Tomorrow I have an onsite interview for a data science internship at one of the Big 4, and my recruiter has made it evident that I should bring questions to the interview (I always do, but I digress). I've begun forming up some okay questions, like:   ""To you, what does it mean to do data science at <Big_4>, and particularly within <interviewer_team>? Would you consider it somewhat different compared to non-<interviewer_team> teams?""   What kind of data-scientist they identify as (see  this .   Given their time at <Big_4> and different skill sets they bring to <interviewer_team>, what would they say has been their biggest takeaway – technical or otherwise?    These questions are things I've also been interested in, but I obviously don't want to throw out just any question. I'm interested in the  field  of data science, including the scaffolding that it's built on. That said, I'm not sure that interviewers care about philosophical-type, ""To you, what does it mean to be a data scientist?"".  That said,  if you were interviewing a candidate, what questions would you want them to ask?  I'm doing hardcore research on <Big_4> and their data science and data infrastructure blogs, but would definitely welcome any feedback. Thanks all!  TL;DR:  I've got an interview coming up and want to show the interviewers that I'm seriously interested in how <Big_4> is applying data science. How do I go about this? "
"Have 8 different variables that I need to best fit and run regression on with VBA, looking for some resources to help me figure out how to do this.","Hello,  I work for a company that is a little behind the times when it comes to data analysis.  I work in Sales Operations and since I seem to be one of the most technically oriented it's fallen to me to be our de facto statistician.  About 6 months ago, I was tasked with finding a way to quantify the sales activity of our new brand that isn't doing well (essentially is sales not doing their job, or is the product not up to snuff).  Our company tends to send out tracking spreadsheets that come back filled out completely wrong, so I finally decided enough is a enough, got some budget and worked with our IT team to create a tool to collect the data through a mobile Sharepoint Site with drop down menus.  We launched in January, and three months later I have about 1500 activities logged that are nice and clean telling me what customer, sales activity (training, sales call, pitch new product, floor new product, influencer call, event), and product/program the sales person has done.  I've been able to create weekly reports to sum everything up for sales mangers, and executives, everyone is very happy.  Now comes the part I've been worried about since I don't really have a background in statistics and I'm very much self taught in sales analysis.  I have the sales figures as well as the activity data at my fingertips, and now I want to run a regression analysis to determine what types of activity or what program is most effective at moving the needle.  I just completed a VBA course so I am hoping there is some method to do this, but google searching has not given me any good examples except how to run simple Linest formula's.  I have no idea if my data is going to be linear, exponential, or logarithmic.  I know there is the best fit method to determine it, but with some many different variations of products and program and customers it's overwhelming.  Can anyone point me in the direction of a resource that may help with a method to do this?  tl;dr  Have 8 different variables that I need to best fit and run regression on with VBA, looking for some resources to help me figure out how to do this. "
looking for a method or approach by which to merge two or more sources for my bot's 'twitter markov mashup' function.,"Hello there,  I'm attempting to build myself a little bot.  It takes all the tweets from two different twitter usernames, strips out all the junk so only the textual tweets are left, and then combines the two resultant sets of tweets into one via interleaving (line one - account one, line two - acount two, line three - account one, line four account two, etc).  Hopefully the resulting output can be interpreted humorously.  I parse through the file generating triples and splitting them into an N^2*N array, then I use the array to generate markov chains.  It seems like my approach of interleaving the input tends to have the output favor the text of one twitter account over the other.  I'm trying to get a decent mix of both inputs but I'm too daft to consider how to change my approach.  Is there a particular best practice for combining multiple inputs to be used in a single chain?  TL;DR  looking for a method or approach by which to merge two or more sources for my bot's 'twitter markov mashup' function. "
Why do websites use website.com/user/yourname.html as opposed to website.com/user?id=12345 if the second method supposedly has more advantages and only minor disadvantages? What am I missing?,"Sorry about the title gore. I'm new to web development, working on a node.js site, and was wondering something. On practically every site, I see that when users generate content (i.e. create accounts, submit posts, upload something) the location for their content is something like this: Site.com/uploads/title_of_their_content. What are the disadvantages of just doing  Site.com/uploads.html?id=12345 where 12345 is the id of the content submitted?  I see multiple advantages to the second method as opposed to the first one.   [I've read that google executes JavaScript on websites]( so it doesn't seem like the id method wouldn't be indexed by Google as long as there is a link to it somewhere on the website.   Let's say you generate a new page for every user who creates an account. With the user/username.html method, the look of the page without any user content would remain the same forever. If you as a webmaster wanted to change how the page looks, you would have to recreate all the current user pages since all the user info is already hard-coded into that page. But with the id method (user.html?id=12345), you could just change the user.html page (how the page looks without any user content) since the user-individual content will be added to the user.html page with an ajax request using the id at the end of the url (in our case ""12345"").   Another large issue is that if you use the user/username.html method, your hosting cost would rapidly increase since you're now hosting a lot more html pages. If you just used user.html?id=12345 then you would only need one html page for all users.    So is the only disadvantage to using the user.html?id=12345 waiting for the ajax request to finish to see all the content on the page? I feel that I'm missing something essential since the user/username.html method is used on almost every website.  I couldn't find anything online about this, so if anyone has some good resources on this topic I'd love to read them.  TL;DR Why do websites use website.com/user/yourname.html as opposed to website.com/user?id=12345 if the second method supposedly has more advantages and only minor disadvantages? What am I missing? "
"I'm just looking for some advice/feedback on how to organize all this crap, how do you guys all do it?","I'm working on a web application and like always, there are a crap ton of list/search/filter (LSAF) views as well as CRUD views.  I've been working on a class that will abstract out the generation of the LSAF views making it so I only have to create a class that extends the base LSAF class and provides a bit of info (Such as the columns, sortable columns, how to render the columns and how to actually get the data).  This system also has models, such as UserModel, RoleModel and TicketModel. These models have findOne() and find() methods which just alias to things like UserCollection, RoleCollection and TicketCollection which in turn extend BaseCollection.  My primary question is about storing column data. The other programmer on this project has suggested that the model contains a static array of column objects that define each column in the application and it's behaviour, so that our collection/lsaf classes can pull from this and we can have less duplicated code. The code might look like this:  class UserModel{    public static function columns()    {        return [            'ID' =&gt; new IDTypeColumn(),            'alias' =&gt; new TextTypeColumn(['sortable' =&gt; TRUE, 'searchable' =&gt; TRUE]),            'status' =&gt; new EnumTypeColumn(['sortable' =&gt; TRUE, 'default' =&gt; 'active', 'values' =&gt; ['active','banned','deleted']])        ];    }}  TL;DR:  I'm just looking for some advice/feedback on how to organize all this crap, how do you guys all do it? "
"I need one function that will pull Steve's drinks (as a substring) from any of the four messy sampleStrings above. 
 Thanks!","Within a string I get handed, and given a start index, how can I find the index of the  next  occurrence of one of several possible strings?  Bolded part is value I am trying to get out. It can occur anywhere...  sampleString = 'BOB: 6 beers, STEVE: 7 bourbon, 3 beers, GAYBOB: 2 manhattan'sampleString2 = 'STEVE: 7 bourbon, 3 beers, BOB: 6 beers, MARGOT: 1 RUSTY nail. GAYBOB: 2 manhattan'sampleString3 = 'GAYBOB: 2 manhattan, STEVE: 7 bourbon, 3 beers'sampleString4 = 'GAYBOB: 2 manhattan, MARGOT: 1 RUSTY nail..'  sampleString shouldn't be a string in the first place, I know, but I am stuck with it (incoming) and I am trying to get something more useful out of it, so here I am trying to parse it. The periods and commas and spaces are NOT consistent, but the person's name spelling and case is, so I am thinking I must use that.  From any of those four sampleStrings, I need to get Steve's drinks (' 7 bourbon, 3 beers' in the first three, nothing in the last example) as a substring, but I don't know to find it. The list of possible people is fixed and known.  The string I always want starts at index sampleString.index('STEVE:'), that's easy enough, even when there's no Steve like sample 4. But I don't know where Steve's data will end, since the next person could be any of the set BOB|GAYBOB|MARGOT, only some of whom might be there at all. Steve might also be the last one of sampleString, like it is with sampleString3, so there's nobody after.  So I want to find the indexOf the first appearance of BOB or GAYBOB that comes AFTER STEVE.... or return sampleString's last char (len, I guess) if there isn't an appearance.  steveStart = sampleString.index('STEVE')steveEnd = sampleString.???stevesDrinksString = sampleString[steveStart:steveEnd]  tl;dr: I need one function that will pull Steve's drinks (as a substring) from any of the four messy sampleStrings above.  Thanks! "
How do I query what cell I am clinking on in pyQt?,"Hi,  First of I am new to both python and also pyQt. I am trying to learn by using it.  My problem is this. I am trying to find out what row(cell) in a QTableWidget I am currently clicking on. But it returns -1. The docs says that it is because the item is not in a table.  Do anyone have an idea how I could do it?Would be great if you could currently point out what I am doing wrong also.  Here is the code.    import sys    from PyQt4 import QtCore, QtGui    from PyQt4.QtCore import      from PyQt4.QtGui import   class Form(QDialog):    def __init__(self, parent=None):        #global variables        self.rowcount = 0        self.comment_index = -1        self.commentData=['']        #Build UI        super(Form, self).__init__(parent)        self.setObjectName(""self"")        self.resize(338, 537)        self.AddItemText = QtGui.QLineEdit(self)        self.AddItemText.setGeometry(QtCore.QRect(10, 10, 241, 31))        self.AddItemText.setObjectName(""AddItemText"")        self.ItemComment = QtGui.QTextEdit(self)        self.ItemComment.setGeometry(QtCore.QRect(10, 50, 241, 81))        self.ItemComment.setObjectName(""ItemComment"")        self.ItemList = QtGui.QTableWidget(self)        self.ItemList.setGeometry(QtCore.QRect(10, 140, 241, 271))        self.ItemList.setObjectName(""ItemList"")                 self.ItemList.setAlternatingRowColors(True)        self.ItemList.setColumnCount(1)        self.ItemList.setRowCount(0)        self.ItemList.horizontalHeader().setDefaultSectionSize(119)        self.ItemList.verticalHeader().hide()        self.ItemList.setHorizontalHeaderItem(0, QTableWidgetItem('Assignment'))          self.ItemList.cellClicked.connect(self.update_comment)        self.AddItemButton = QtGui.QPushButton(self)        self.AddItemButton.setGeometry(QtCore.QRect(260, 10, 71, 31))        self.AddItemButton.setObjectName(""AddItemButton"")        self.AddItemButton.setText(""Add"")        self.AddItemButton.clicked.connect(self.addObj)        self.ItemComment_read = QtGui.QTextEdit(self)        self.ItemComment_read.setGeometry(QtCore.QRect(10, 420, 241, 81))        self.ItemComment_read.setReadOnly(True)        self.ItemComment_read.setObjectName(""ItemComment_read"")        QtCore.QMetaObject.connectSlotsByName(self)    def addObj(self):        #Button connection. Add text store comment in list.        self.rowcount += 1        self.comment_index += 1        text_item = QtGui.QTableWidgetItem()        text = self.AddItemText.text()               self.commentData.append(self.ItemComment.toPlainText())        self.ItemList.setRowCount(self.rowcount)        text_item.setText(text)        self.ItemList.setItem((self.rowcount-1), 0, QTableWidgetItem(text_item))    def update_comment(self):        #update comment on cellclick, print it to ItemComment_read        item = QtGui.QTableWidgetItem()        item.row()        row = self.ItemList.row(item)        self.ItemComment_read.setText(self.commentData[row])        print rowapp = QApplication(sys.argv)form = Form()form.show()app.exec_()  TL;DR How do I query what cell I am clinking on in pyQt? "
"If I'm re-writing all URLs to be absolute (/index.php ->  in HTML attributes, does anyone know of a way that the javascript: protocol would slip by?","Hey r/PHP,  Continuing [my last post]( about sanitizing remote HTML, I wanted to get your opinion on an idea regarding the javascript: protocol.  Background: I'm displaying remote HTML and want to prevent against XSS. Event handlers are safe as I remove all but my specified attributes, and only show my specified HTML tags.  So I'm focusing on the javascript: protocol in links. I had been doing all sorts of text manipulation to remove a starting ""javascript:"" in any attributes that would be treated as URLs, but I had an idea - if I'm displaying remote HTML, I would want to re-write all URLs to be absolute URLs anyway - I wouldn't want them specifying things like /index.html or ./image.jpg as they wouldn't display correctly anyway - so I would prepend the source's URL to any non-absolute attributes anyway (basically I'd search for  at the start of anything that would be a URL)  For the javascript protocol, this would result in all URL attributes being re-written to "" - is this good enough? If I'm doing this would there be any reason to do all sorts of manipulation like decoding UTF8, removing whitespace, meta characters, etc? Although I would still check against the  XSS reference  provide protection against all XSS?  For this I'm assuming the javascript: protocol works only on URL attributes, as these would be the only things I would be re-writing to absolute - i.e. javascript: won't work in image ALTs, etc (i've tried on my browsers but I don't know about special cases), is this correct? I'd only be re-writing expected URL attributes (href, src, etc)  tl;dr - If I'm re-writing all URLs to be absolute (/index.php ->  in HTML attributes, does anyone know of a way that the javascript: protocol would slip by? "
Flask message flashing and redirect behavior breaks down with POST request. Help with Werkzeug routing/maybe 'localhost' setup?,"So I have a problem that apparently only one other person on the internet has had... my redirects and message flashing in Flask fail when handling a POST request from WTForms. Perhaps it is not appropriate to even mention WTForms, because the same failure behavior happens with a simple POST request absent of any form validation. I can't tell if this is a problem with Sessions (though my site has no user object, only Flask's default CSRF-protection behavior/cookie for the forms), or with some invisible routing magic in Werkzeug.  After hours of searching and reading docs, and trying about 100 different iterations of my code, I found this question on stack overflow:  The solution he notes is directly from the docs, and is talking about the behind the scenes routing performed by Werkzeug. I am inclined to believe this is my problem, because the POST request is forcing my redirect to go to the application root (even when I tell it to go elsewhere). However, I have no experience configuring the local web host on my computer (Mac OSX 10.6), and so I don't really even understand the proposed solution. I am also confused because the same code fails in the same way after pushing to Heroku as well... which maybe shouldn't be the case according to the solution wording? Anyhow - thoughts, suggestions, ideas, links to tutorials would be great.  Also, here's my view:  @app.route('/rsvp', methods=['GET', 'POST'])def rsvp():    form = MyForm()    if form.validate_on_submit():        flash('Thank you for your RSVP.')        return redirect(url_for('welcome'))    return render_template('rsvp.html', page='rsvp', form=form)  This view should be bulletproof as it is essentially copied straight from the docs. So far, I get exactly the same problem running on localhost as I get on Heroku. And yes, I have tested this code without any form processing/validation, using instead:  if request.method=='POST':  ...which omits the WTForms validation process, yet still results in the exact same error behavior. Also, I have verified that redirects and flashing work fine when used with GET requests, and all my app configuration is default (e.g., SERVER_NAME is unspecified). If it helps, the Flask config docs referenced by the other post are here:  tl;dr Flask message flashing and redirect behavior breaks down with POST request. Help with Werkzeug routing/maybe 'localhost' setup? "
"I built a basic game engine and found it performs better on IE9 than any of the other browsers, anyone else came to this conclusion?","Hey Guys,  I recently embarked on a big personal game development journey.  I decided to start my journey by building a couple of very basic games.  I am currently working on a rudimentary game engine i call colloid.  It has basic collision detection, fake gravity, controls, FPS and a camera that follows the guy.  I plan on dropping in an animation function next.  Anyways what i found interesting is that out of the 3 big browsers internet explorer(9) handles this the smoothest.  I have been a big IE hater for a while(still do) but i must commend the browser on how smooth it handles JavaScript.  For those who care to peep my engine(super lame) and provide some critiques, the link is below:  Instructions:Click start  use W,A,S,D.   The longer you hold W the higher you jump.  if you fall off the map infinitely hit refresh or click the ""I fell"" button.  I build this project with 3 intentions.   Easy level creation  Better understand the things most of us consider trivial in larger game engines  Better my javascript   TLDR;  I built a basic game engine and found it performs better on IE9 than any of the other browsers, anyone else came to this conclusion? "
"Wrote MVC framework for personal use, need constructive criticism, donating $100 to charity. 
 EDIT 
 As promised:","Hello everyone.  I've enjoyed learning PHP (almost a year and half) and I try to challange myself regularly to ensure I'm writing the best code I can. So, after getting comfortable with the language I've begun creating an MVC framework for personal projects, fitting to my own style and taking inspiration from my two favourite frameworks: 'Yii' and 'CodeIgniter'.  My request is similar to that of 'hiii' who created the '$50 for a code review' thread. I originally was going to offer money for code review by competant programmers but after reading the comments in that thread it appears letting others know about your github repository is more appropriate.  So, I present my little project 'miniMVC', hosted at  I'd be happy to hear any and all critique, but please keep it constructive/relevant ( ie: no 'the name sucks', 'just use [framework]', 'you smell like cabbage', etc. ).  Also, I hate taking and not giving, and I liked how 'hiii' donated to St Jude Children's Hospital. So in the same spirit I'd be happy to donate $100 to a charity choosen by anyone.  Cheers  TLDR: Wrote MVC framework for personal use, need constructive criticism, donating $100 to charity.  EDIT  As promised: "
"Massive Javascript generated by php, how to find bugs?","EDIT2: Solved it, thanks for the replies!  So I need to update about 15 input fields related to a single row in my DB. So I wrote some code that switches on the requested index and returns an Array so it can be used by a JS function that updates the field.  However, in my worst case scenario this generates about 280,000 lines of JS code AND it doesn't work. In the smaller cases I could just copy+paste the JS into netbeans, format it and find the error as it was only about 12,000 lines.  Right now my chrome freezes when I try to copy it but it un-freezes after about 5 min. Then when I paste it into netbeans my netbeans freezes for 5 min but no result.  Any ideas?  EDIT: Here's the snip:  TL;DR Massive Javascript generated by php, how to find bugs? "
Is there any way to show a live matplotlib graph over an ssh connection without a GUI on the machine running the script?,"Hey /r/Python!  I started learning python about a week ago so I could start putting my raspberry pi to some good use.  One of the scripts I'm working on just does some different things while reading the CPU temp. I've tried to add graphing functionality to it using matplotlib, which has proved partially successful, but I'm having a hard time getting the graph to display 'live' while data is plotted to it.  As is, it just seems to save the plotting info to two lists and then saves the graph.png at the end of the scripts execution.  I tried enabling x forwarding in sshd.conf on the pi and adding the -X param when I ssh in, but it still doesn't display the graph while the script is running.  I assume the problem is related to not actually having an X session to forward from the pi.  Is there no way to display a graph like this over an ssh connection without having to put lxde, xfce, or other big GUI's I won't use on my pi?  [The script in question](  TL;DR Is there any way to show a live matplotlib graph over an ssh connection without a GUI on the machine running the script? "
"I am a PHP developer with a new project, what is your opinion regarding using PHP or learning Rails and using Rails?","Hi /r/PHP,  I am looking for some other PHP developers' opinions regarding learning Rails.  I have a new project at work to replace a 10 year old system built into a Windows application that's not working as intended anymore. Rather than sinking money and dealing with Windows development(I hate Windows), I am contemplating building this project in PHP or Rails. This project will easily work as a web application.  PHP comes easy for me, I've been developing PHP web applications for over four years now. But I see all this nice stuff about Rails on news sources, blog posts, etc., and I wonder if I should really look into using Rails. I can do some Rails, but I get frustrated when I start feeling overly dependent on someone else's gems for everything--I know that's probably weird. And sure, Ruby is a lot of things PHP is not, and vice-versa, but it just feels strange to be developing in Rails coming from PHP. I feel like I am missing a bit of philosophy here.  Also, does anyone else hate the [Rails Guides](  TL;DR: I am a PHP developer with a new project, what is your opinion regarding using PHP or learning Rails and using Rails? "
"I know my way around a computer, and I'm setting up a linux box to be a lab rat. What distro should I put on it?","I'm a grad student in CoSci. I know how to work in a linux environment and all, but now I want to take the next step and put up a box of my own, and get my hands dirty and learn how to configure and administer a system, how to compile stuff from scratch, and what makefiles are all about and all that stuff.  I've got copies of Debian 3.1 and Fedora Core 4, and I'm trying to decide which to go with. I've already installed both of them once to see what they look like. It seems that Debian is more stripped down and utilitarian, and Fedora is designed to be more user-centric. I'm more inclined to go with Debian, I think, but will their pre-compiled .deb packages mean that I won't be able to compile and link stuff from scratch?  If anyone would suggest another distro that might be more suited to my needs, bear in mind that the box in question is a late 90's vintage i686 running at 1.5Ghz  with 1.5 gig of ram and a 40 gig drive  tl;dr: I know my way around a computer, and I'm setting up a linux box to be a lab rat. What distro should I put on it? "
"How to delay a prompt. 
 EDIT: I am really bad with formatting the code, sorry, essentially I am just wondering how to delay a prompt.","So someone helped me a while ago with actually getting my code working (I was putting a curly bracket to soon).  Now when you first enter the page it doesn't even load the page, it immediately prompts you.  I was wondering if there was anyway that I can delay the prompt for a little bit.  Website is www.palicsam.com  Here is the code below.  &lt;script type=""text/javascript""&gt;var cabin = prompt(""You wake up in the middle of a dark cabin,          do you leave the cabin or stay inside and cry yourself to sleep     (LEAVE/CRY)"");switch(cabin) {  case 'LEAVE':    var leave = prompt(""Are you sure you want to leave? (YES/NO)"").toUpperCase();      if(leave === 'YES') {    var path = prompt(""You head outside and you are presented with 2 paths, one going left and one going right.     Which path do you choose? (LEFT/RIGHT)"").toUpperCase();      if(path === 'LEFT') {    var troll = prompt(""You encounter a troll that is in the way, do   you FIGHT him, PAY him, or RUN     (FIGHT/PAY/RUN"").toUpperCase();      if(troll === 'FIGHT') {        document.write(""&lt;h3&gt;You get into a fight with the troll, lose  and die a miserable death&lt;/h3&gt;"");    }         else if(troll === 'PAY') {          var pay = prompt(""Do you have Troll Dollars?     (YES/NO)"").toUpperCase();       if (pay === 'YES') {      document.write(""&lt;h3&gt;Congratulations you have escaped this    miserable nightmare, and have beat my     crappy game!&lt;/h3&gt;"");    } else {       document.write(""&lt;h3&gt;You do not have Troll Dollars, so the     troll kills you&lt;/h3&gt;"");}}   else if(troll === 'RUN') {     document.write(""&lt;h3&gt;You are not faster than the troll, he    chases you and kills you&lt;/h3&gt;"");} } else {   var right = prompt(""In the right path you see a small cabin,     do you enter it? (YES/NO)"").toUpperCase();if(right === 'YES') {  document.write(""&lt;h3&gt;You have made a terrible mistake, inside that cabin was Mr. Davis, and he tore you apart.&lt;/h3&gt;"");} else {     document.write(""&lt;h3&gt;You chose to not enter the cabin, and stay there in place, after a couple of seconds a big bad porcupine creeps up on you and pricks you to death&lt;/h3&gt;"");}}  } else {  document.write(""&lt;h3&gt;No, you aren't sure you want to leave?  Well then you get to cry for all of eternity&lt;/h3&gt;""); }break;  case 'CRY':var crying = prompt(""Are you sure you want to cry for all of eternity? (YES)"").toUpperCase();    if(crying === 'YES') {      document.write(""&lt;h3&gt;Have fun crying loser!&lt;/h3&gt;"");}     break;  default:document.write(""&lt;h3&gt;I didn't understand your choice. Hit Home and try again, this time picking LEAVE or CRY!&lt;/h3&gt;"");}&lt;/script&gt;  TL;DR: How to delay a prompt.  EDIT: I am really bad with formatting the code, sorry, essentially I am just wondering how to delay a prompt. "
"Had some spare time, made an equivalent of  man  for RFC, get it on [pipy]( or [github]( and feel free to share your feedback.","As a backend developper I regularly find myself having half a dozen tabs opened in my browser to dig through RFCs. As I do a lot of commuting as well, nothing pisses me more than forgetting to open the right RFC before leaving the office and not being able to access it.  Since I had some spare time this weekend I decided to tackle the problem with a bit of python. I'm quite used to  man &lt;my_command&gt;  and  man -k &lt;my_topic&gt;  so I decided to make a small python with the same behaviour so I could peacefully browse my beloved RFC in the train.  Now that it's up and running, I uploaded it to pypi in the hope of helping others with similar needs.  It's my first open source project, so I'm really looking forward for some feedback on my code. So don't hesitate to comment/pm/send-pull-requests.  Features include :   Fully offline work  Custom reading program (gedit/firefox/you-name-it, defaults to  less )  Prints a list of RFC by keyword   TL;DR: Had some spare time, made an equivalent of  man  for RFC, get it on [pipy]( or [github]( and feel free to share your feedback. "
"I want to know how to properly design a JS Web Application, taking object relationships and DOM into account, so I can become a better developer.","Hey there.  I've been a Web Developer for about 2 years now. I know my OO, I know Javascript to the point where I'm confident enough in what I'm doing when it comes to smaller projects. However, when I encounter anything big that requires a good planning (UML diagrams, or even just drawing the relationships of objects on a paper), I can't seem to grasp what and where everything should be.  This has been troubling me for quite some time now, and in order to become a better developer, I really need to know this information. I've googled a lot on the subject, but the fact that Javascript isn't a classic Class-based inheritance language etc, makes the whole thing a bit harder.  You've got the JS objects to consider (which is normal), and the DOM to consider as well. Together, these two get mixed up in my head and everything gets really messy at some point, and that is something I'm wanting to fix.  Help, anyone?  - TL;DR  I want to know how to properly design a JS Web Application, taking object relationships and DOM into account, so I can become a better developer. "
"I need help or guidance running [this Python script]( for grabbing pictures in a directory, resizing them, and saving them elsewhere. Any help at all is very much appreciated!","Hiya! As the title says, I don't read nor write Python. I'm looking to run a Python script, but I have no experience or guidance from the author on how to do so.  Backstory: The gaming company Razer made a program for keeping an overview of all your games, and stopping unnecessary processes while playing a game if they're launched through the program (among a few other things). This program is called Razer Cortex. It has some problems registering a lot of games, including ones saved in the Steam gaming client's directory. You can manually add them, but it will only rarely also grab a coverphoto for them. For this very purpose a Reddit user posted a script that grabs pictures from steam and resizes them, then saves them in the Cortex directory  here  Tl;dr:  I need help or guidance running [this Python script]( for grabbing pictures in a directory, resizing them, and saving them elsewhere. Any help at all is very much appreciated! "
"Arrays can hold full objects beautifully, arrays can be stringified and parsed, and those objects are retained. 
 What are some lessons you've learned the hard way?","I'm fairly new to JavaScript, and self taught. I learned a lesson today that I hope can help some people..  What I was doing was, I was creating objects called obj1, obj2, obj3 and then checking to see how many I had with a for loop that ran through checking window['obj' + I] and when one object was deleted all the objects were sent to an array, the array spliced them, they were sent back, the total number of objects was subtracted by 1, and these objects were sent to localStorage.  Fuck dynamic variable names, never eveeeer make dynamic variable names... Use arrays, instead of obj1 and obj2, I could've just made an array to hold all my objects called arrayofObjects[] and just had them thrown in there, with it's length being handled..  And here's the best part, no more messy localStorage handling, now I can JSON stringify THE WHOLE FUCKING ARRAY into localStorage and parse it back and get them with arrayOfObjects[]  TLDR: Arrays can hold full objects beautifully, arrays can be stringified and parsed, and those objects are retained.  What are some lessons you've learned the hard way? "
Making a Box and a ball that interact With eachother in Visual studio JavaScript!,"Hello!In my School we need to search for a job that we are going to be at on certain days. I'm working in a IT Office, im currently assigned to work on making a Windows 8 app, and i dont have any idea how to code. I basically need to make a centered Box With a ball that is supposed to be dragged to the Box. If it does not hit the Box it is suppoesed to jump back to its original Place.Anyone who can help me make that? Guide me through it or something like that?  Im using Visual Studio for win 8 With JavaScript.(Sorry for bad English :(.. )TL:DR Making a Box and a ball that interact With eachother in Visual studio JavaScript! "
"Want to visualize changing point clouds (and volumetric data?) with minimal effort in some nice 3D viewer- what is that viewer? PS, why is matplotlib so slow?","I have messed around with matplotlib and Mayavi. It's not that I have a problem doing things the long way, I have also written some projects using openGL in C++, and maybe I am asking for to much here, but here goes it. Shouldn't there be some object that holds 3D data like point clouds, or 3D matrices which would be rendered as volumes (settings including color maps and transparency etc.) , and displays them in a nice navigable window, allowing updates to the data in loops, or whenever I feel like changing it? I would rather not use special animation functions. Just some object that handles all the rendering in the background, displaying a scene that I can add things to and change on the fly. I know that's not simple, but I'm starting to love how python just does things for me, and... can something do this for me? If volume rendering is asking to much, what about just openGL primitives? I know I could use pyopenGL myself, but I don't feel like I should do anything if I just want to look at my data (a point cloud, for example) in a nice configurable 3D viewport, spin it around with the mouse, zoom, etc. Is there anything like this? Thanks! EDIT: Tldr; Want to visualize changing point clouds (and volumetric data?) with minimal effort in some nice 3D viewer- what is that viewer? PS, why is matplotlib so slow? "
"I've got a front end js app, api, and small static site in the same git repo. I hate this. How can I make this better?","So, here's my situation. I'm building out an api in python and the main client is an SPA javascript front end. There's a little server-side generated landing page too for auth which then dumps you into the js app. Right now everything is in a single git repo. This feels terrible for a few reasons.   First, just putting a javascript application in the 'static' folder to make it work with the framework feels weird.   I have to do a full update to change copy on either landing page or js app.   And I just want to separate front and back end for my own piece of mind.    What is your everyones workflow for this sort of thing? Do I separate it into 2 repos (one for backend & landing page, one for js app) or do I even do 3 (one for tiny backend, one for api, one for js app) or am I just doing something else wrong and should stay in a single repo?  More importantly, once everything is running from different servers, what does your development workflow look like?  ;tldr I've got a front end js app, api, and small static site in the same git repo. I hate this. How can I make this better? "
How does your View class looks like / How would you handle it?,"Hi all,  I've worked before with some frameworks, but I still kept the feeling that it wasn't right. So now I've picked this [PHP-MVC SKELETON]( and it does feels good.  But no I've come to the View part and I don't know how to do it ""the right way"". I don't want to use an template engine, but i wan't to use some kind of an class that helps me auto-load all the necessary files for the default layout.  This is my View class now:  class View extends Model{    private $dir = ""application/views/"";    private $headers = array(                            'common/header',                            'common/left_pane'                        );    private $footers = array(                            'common/right_pane'                            'common/footer'                        );    function make($files, $data){        extract($data);        $mods = array();        $moduleModel = $this -&gt; loadModel('Modules', 'common');        $allModules = $moduleModel -&gt; getActive();        foreach ($allModules -&gt; rows as $key =&gt; $value){            $mod_info = $moduleModel -&gt; getModule($value -&gt; uid);            foreach ($mod_info -&gt; rows as $key2 =&gt; $value2){                $mod = $this -&gt; loadModel($value -&gt; mod_name, $value -&gt; mod_dir);                $info = $mod -&gt; module();                $mods[$value2 -&gt; place][] = $info;            }        }        $templates = array_merge($this -&gt; headers,$files, $this -&gt; footers);        foreach ($templates as $key =&gt; $value){            require_once $this -&gt; dir . $value . '.php';        }    }}  So  TL;DR , How does your View class looks like / How would you handle it? "
"I can't figure out how to get rand() to pick a random item from $list. 
 Any suggestions?","I am teaching myself PHP with the help of Codeyear to get my feet wet in the basics.  I am stuck on a challenge. I am suppose to create a list by making an array and then using array_push to add names to that list. Then I am suppose to use rand() to pick a name out of the hat and print is in all caps.  I am having issues with getting the browser to recognize that the random number from rand(0,4) is suppose to represent one of the five names in the array.  The only clue I get is that I am suppose to use array(), array_push(), sort(), count(), rand(), and strtoupper(). That and that arrays are zero-indexed and that I should also remember that elements are accessed by using square brackets containing the index of the element I want.  My code so far:  $list = array();array_push($list, ""Kris"");array_push($list, ""Chris"");array_push($list, ""Joe"");array_push($list, ""Heather"");array_push($list, ""Ian"");$sort = sort($list);$winner = rand(0,4);print strtoupper($winner);  TL:DR; I can't figure out how to get rand() to pick a random item from $list.  Any suggestions? "
Is there 1 Python package install to rule them all so I don't have to waste hours hunting for all these individual packages?,"Hey folks,  Ok, should be an easy question for everyone here as I'm obviously a n00b idiot or something. . .  Is there 1 mega package out there with basically ALL Python libraries or something available?  I'm jumping into Python because everyone is saying ""oh it's so easy, forget PHP and JS. Everything's already coded for you, etc, etc"". . . Oh yeah? So to follow a tutorial on geomapping in Python2.7 I had to also individually grab: DateUtil, NumPy, PIL, PyParsing, SetupTools, MatPlotLib, and Basemap?!?! Really?? And as I tried to find each of these, some of the packages couldn't find my version on Python in the registry? So I had to seriously hunt for the last 3.5 hours, try easy_install - OH, nope, that won't always work, gotta use: python setup.py install - OH, nope, missing vcvarshell.bat too eh? Gotta grab MS VS 2008?!?!. . .  . . . Like seriously guys, what is this?? It's like I got a bike because it's easier than walking except I only really got a helmet and a foot pedal and I need to find the other parts myself as I build the package. Surely this is just a joke to discourage n00bs like me. Surely once I find / download all these individual packages the easiness will trickle in? Surely every project I start won't be a wild goose chase just to get a complete programming environment setup?  Thanks for letting this n00b rant :)  TL:DR, Is there 1 Python package install to rule them all so I don't have to waste hours hunting for all these individual packages? "
"What do I need to do to get that element? 
 Thanks","Hello there,  I was writing a little javascript script to retreive some information from structured pages. More specifically: I wrote a bookmarklet that gets a product description from an Amazon page.  The description came after the price and the title, you'll find that if you (on a amazon product page) type:  var brand_el = document.getElementById('brand')  brand_el  will contain the element in which the brand of the product is stored. so  brand_el.innerText  will show the actual brand.  Easy peasy, said the arrogant little boy. I noticed that the product description was in a div with class  .productDescriptionWrapper , so I will get the list with all elements that have that class with:      var description_els = document.getElementsByClassName('productDescriptionWrapper') || false;  and  descriptions_els  actually returns  false .  I asked my programmer brother: whut?. And he said: This means there is no element with class  productDescriptionWrapper  in  document . He was right, the structure of an Amazon product page is as follows:  &lt;html&gt;    &lt;body&gt;        &lt;html&gt;            &lt;body&gt;                // THIS IS WHERE I WANT TO BE                &lt;div class=""helpMeGetToThisElementInVanillaJS""&gt;                &lt;/div&gt;            &lt;/body&gt;        &lt;/html&gt;    &lt;/body&gt;&lt;/html&gt;  tl;dr: What do I need to do to get that element?  Thanks "
"I'm doing a javascript 100 day challenge. What's everything important I can learn about and practice, including frameworks, libraries, and other tools in order to help me master it?","So I'm currently a freshman at university right now and I'm majoring in CS. I have decent programming experience, and have even fooled around with Javascript a little bit, but since I want to pursue a career somewhere in the front-end/UI realm I figured the best way to prepare for that would be to master JavaScript.  This is where you guys can come in! Right now I'm trying to plan a comprehensive curriculum that would cover everything important. Obviously I'm going to start with vanilla, but I also want to include important libraries/frameworks and mini projects to do that would make JS feel like second nature, and give me a good feel for the capabilities of javascript as a whole.  Do you guy have any suggestions? If you have any advice to offer please do! Hit me with everything important you can think of and even some ideas for programs that I can write. I'm hoping that if my project is a success (I'll document, blog, and upload everything) I can share what I learned with people who also want to learn Javascript. Thanks!  edit: TLDR: I'm doing a javascript 100 day challenge. What's everything important I can learn about and practice, including frameworks, libraries, and other tools in order to help me master it? "
"I crashed python and it spewed out some weird cthulhu jargon. All in all a good day at the office, really...","This isn't related to a coding problem, it's just a weird glitch that happened to me a while ago in 2.7.6. I found the image today that I screenshot, and figured it was funny so I uploaded it  here  Basically, me and my friends were trying to come up with a way to break python. It was none of the  crazy complicated stuff . I ran it and I think it crashed, then I went back into the file and I think I pressed backspace, and all the text changed into what it looks like in image 2. I thought this was really funny, so I posted it to see if you guys have any idea what happened. Thanks :)  TL;DR: I crashed python and it spewed out some weird cthulhu jargon. All in all a good day at the office, really... "
How do you describe what a data scientist is?,"This is perhaps not a very substantive topic or question but I am curious nonetheless...  With holidays fast approaching and with it the barrage of ""what do you do"" questions from family/friends/enemies/Santa/etc. -  I thought it would be interesting to hear how you all explain what a data scientist is to someone unfamiliar with the field/role.  Normally I avoid simply stating ""Oh, I'm a data scientist!"" and explain what I  actually  do on a day to day basis. If someone asks more bluntly what a data scientist is I frequently reference [Drew Conway's venn diagram]( description.  If that doesn't work I decide to be one of the following:   ""a data analyst of sorts""  ""I'm basically a developer""  statistician (""...so where's your Phd???"")  computer scientist  designer  model ( technically  data science involves modelling, therefore...)   tl;drHow do you describe what a data scientist is? "
"Data Scientists will be around for at least the next 10 - 20 years, but will salaries fall once expectations could be downwardly adjusted for these positions?","I have a decent background in Python, statistical modeling, and a little in R, but am unsure what field I should go into.  I think my skill set is more in tune with data science, but I am worried about long term job prospects. It seems that right now data scientists are doing very well in the job market averaging around $120,000 in the Bay Area, and some may peak around at $150,000 on a typical career path which is better than what programmers are making. But some people whether here, or on the web have said that most data scientists are overglorified data analysts, and that in the next few years data scientists' salaries will fall drastically after the ""hype is over."" They say that they are basically glorified statisticians who know how to code a little and ask the right questions.  On the other hand,  programmers are making around $105,000 in the Bay Area, and the pay seems to have stabilized around that point.  The other thing that I want to mention is that I think a programmer is going to be the backbone of a company whereas a data science serves more of an auxiliary role (useful, but not necessary), so there could be more job security in programming.  I think, personally speaking, I find pros/cons in both fields. Data science seems to be more breadth and programming more depth/narrow in focus. I could see myself doing either/or.  I am particularly looking to do a bootcamp also either through Hack Reactor/App Academy or Zipfian Academy, so I want to finalize my study plan before I commit on one path. Also, I think a boot camp wouldn't be enough, so I would be looking to do a post-bacc in CS/Master's through Georgia Tech OR doing a Master's in Data Science/Analytics.  I know this is probably a DS-biased forum, but what are your thoughts on salaries, long-term growth/opportunity, and so on. Is DS a ""fad"" and that salaries will cool down over the years when/if companies could determine that DSes could be paid less?  Here are some articles related to this topic:  and  tl;dr - Data Scientists will be around for at least the next 10 - 20 years, but will salaries fall once expectations could be downwardly adjusted for these positions? "
"Please point me to an example of element swapping using JQuery, closest I can find is JQuery sortable examples.","I have been trying to have a matrix of three by three spaces on a page to be swapable, that is when I pick one up and hover over a second I want the space to light up and on dropping the item have the second one to go to where I picked the first from. The closest I have found is a JQuery Sortable implementation in the example linked below.  Look at the example marked 1.3 on this page:  Example 1.3: Sortable and connectable lists with visual helper  You will note that in that example, when you pick up an item you can move it anywhere in the same column or another column. It does not swap places with the item place it is taking, it creates a space between two items or drops into that column as a list item.  So what I am seeking here is very similar to this but the item picked up should swap places with the item it is dropped on to. Can anyone point me to an example that displays that behaviour?  Thank you in advance :D  Tl;DR  Please point me to an example of element swapping using JQuery, closest I can find is JQuery sortable examples. "
My girlfriend is trying to dabble in Ruby on Rails and failing at the starting line-- need a newbie friendly forum suggestion where she can get this sorted out.,"My GF had been doing a bunch of projects which ended up requiring the use of google forms and sheets -- and eventually the ideas and concepts she had for these systems got a bit beyond the scope of the google tools and seemed to lend themselves to a real database backend.  She started getting interested in learning some programming and I (having only a passing understanding of it) suggested that Ruby on Rails might be a language she might look to learn-- and that it did have some prebuilt web-oriented components along the lines of the ideas she had.  She got some books and started reading and was excited that the code and concepts in the beginning chapters made complete sense to her-- but when she started to actually get setup to do the exercises in the book, it all kinda fell apart.  Books quickly become outdated so when she installed everything as recommended there are tons of conflicts with GEM versions and other things which I'm at a complete loss to offer any help (I know enough php and asp to be dangerous but that's about it).  Anyway, I get depressed because her enthusiasm for this stuff is getting snuffed out more and more each day since she can't get the basic system up and running... I suggested posting questions online but I think she's intimidated since she doesn't always grasp exactly what the errors mean.  Anyway, if anyone has suggestions it would be much appreciated--- and might help kindle someone's new found interest in programming.  Thanks  tl;dr: My girlfriend is trying to dabble in Ruby on Rails and failing at the starting line-- need a newbie friendly forum suggestion where she can get this sorted out. "
Head First Learning Javascript blows. What projects/books/resources can I use to learn to do stuff with Javascript if I know basic HTML/CSS?,"I've recently gotten all the way through Head First HTML/CSS, and I tried getting through the Headfirst Learning Javascript book, but I found it's style a bit tedious and slow after a while...  I'm having trouble figuring out a way to start learning this stuff without a) getting bored, and b) getting overwhelmed. I've only ever really done programing in the command line with Python, and nothing interactive, just simple scripts, which is quite different from working with HTML/CSS/JS.  I see so many amazing things being done with HTML5 and Javascript alone, and I really want to figure this stuff out, but I feel like I'm running around like a chicken with its head cut off.  So how can I get from basic HTML/CSS to web app development while staying in the sweet spot where things are interesting and challenging but not overwhelmingly difficult? Learning Javascript isn't really the issue (I've learned Python and some Ruby), but rather learning to use Javascript to modify the DOM. That's where I feel I'm at right now.  I'd also be open to any book suggestions that will do more than just teach ""pure"" Javascript outside of the context of the browser.  I apologize if this was longwinded. I'm a longwinded person. :)  tl;dr: Head First Learning Javascript blows. What projects/books/resources can I use to learn to do stuff with Javascript if I know basic HTML/CSS? "
What should new graduates/career-changers expect when trying to get into the data science job market?,"Hi guys,  I tried searching around the sub-reddit for something like this, but couldn't find anything in particular.  I'm finding some conflicting info regarding qualifications/job searching results around the web. I've read some stories where people with very reputable Master's degrees(some even with PhD's) in quantitative fields (stats, math, comp sci) with a few years of relevant experience who claim they can't get a job.  Then, there are some who will claim if you have a strong foundation, maybe an advanced degree, and get a couple years of relevant internships/positions, you should be set to make your way into the field.  For those who have gone through the process, what do you think is reasonable to expect for those who are trying to get into the field? Also, I'm wondering what is meant by ""Entry Level"" data science positions. Are we talking actual positions titled data scientist, and if so, I don't seem to see many of these positions that are entry level. Or are we maybe talking positions like ""quantitative analyst"" or ""data analyst""  Obviously there is no formula. A degree from X university and 3 years on your resume won't give you a data scientist job at some great company, and countless other factors (connections, interview skills, etc.). But I'm just wondering what some people's experiences might have been like. Let me know what you guys think!  (My story: Getting a Master's in Applied Stats at a very reputable university this year; not much professional experience but will try to get a good internship this year; have just begun learning python)  tldr: What should new graduates/career-changers expect when trying to get into the data science job market? "
Looking for tips making a TW-bot written in Python. Would be glad to work together with someone.,"Today I finally started on something I've wanted to do for a long time: making a TribalWars-bot.  Now, I assume many of you don't know what TribalWars is so I'll try to explain it. TribalWars is a game in which you build up your village and proceed to conquer other villages. This might not sound too exciting but there is a lot of tactic and strategy to it. But next to tactics and strategy it also requires a lot of time, mainly to perform basic tasks such as queueing up buildings and sending troops out to farm (attack for resources) other villages.  So why not make a bot who can, first of all, perform these basic tasks for you and furthermore do more advanced things such as sending out well-timed attacks, decide whether to dodge, defend or counter incoming attacks based on information known about the attacker and automatically deciding which villages to keep on farming.  I'd love to make this idea into an actual bot written in Python, so could you help me by giving some tips on how to best do this? I have no programming experience, apart from a JS-cursus on codeacademy.com. But since my father knows how to program (he is an engineer working with Java) you should not be worried your tips won't be put to good use.I'd like some tips on sending build-commands to the TW-server, the commands are in the form of URL's with parameters.I think it'd be really fun if I could do this project together with someone (apart from my father's occasional help), so if you think this sounds fun and want to help me, please let me know.  [Link to tribalwars.net](  TLDR: Looking for tips making a TW-bot written in Python. Would be glad to work together with someone. "
Wrote a html template manager for front end interfaces. Give me feedback!,"So.. I have written a small library.. Very primitive in what it does, but hope it'll grow into something huge! Basically been finding it a total bitch to get html templates, working well with frontend apps... View files and things all over the place, no real management for them.. So basically what I am todo, is write like a small layer that you hook into you favourite framework, to manage all your templates. One way I find works for me, and for all the commercial projects i've used it on. Basically an abstraction of what I used for Virgin Australia.  Be keen to see what you guys think of it thus far.. I'm planning on spending some time on the commenting, and start writing some proper tests.  npm module    npm i dimples --save-dev  TL:DR; Wrote a html template manager for front end interfaces. Give me feedback! "
"I've got a good technical background, and I have the time to pick up one new web design programming language.   So where do start?","Hey gang.  I used to do web design semi-professionally a while back.  I could do a bit of javascript and html- but mostly I was using WYSIWYG editors like frontpage.  Even at that time, I was aware that my design skills were out of date- and that frontpage was kind of a joke.  But it got the job done, and I made some webpages that were passable, given the context.  Now, I've just picked up a website for the first time in a while, and I want to update my skills a little... but I'm terribly overwhelmed by talk of css, ajax, lisp, python... and don't know where to begin.  The things I'd like to do are as follows:  1) I would like to be able to post literature, and let people post anonymous replies to it without setting up accounts  2) I would like to be able to sell merchandise on it, in as simple a way as possible for the user (paypal?)  As you can see, my needs are simple, and step 2 is probably more about finding online resources than actual design.  ACTUAL QUESTION AND  TL;DR :  I've got a good technical background, and I have the time to pick up one new web design programming language.   So where do start? "
"who are the best companies to work for in data science, and why?","As a (relatively) informed layperson I'm curious as to what data scientists reckon are the best firms to work for, and how data scientists would make that kind of judgment?  What I mean by the second part is: what values come into play when you're measuring one company against another?  It's interesting because this is a burgeoning field with a lot of growth (especially in startups), but there is an underlying analytics/data infrastructure industry that has serviced big-business firms up to now.  Finding out what values people would use to make this judgement is particularly interesting to know because I would expect that data scientists would throw up some interesting answers as to how to measure such a notion (it's hard to measure but very real).  Are there any companies that are known in the industry as the gold standard? (Ie. a Goldman Sachs in Investment Banking, or a McKinsey in Strategy consulting)?  (I am pursuing a career change into data science, which is what led me to this question, but I'm more looking for people's opinions and how they form them than any kind of advice here)  TL:DR - who are the best companies to work for in data science, and why? "
what's the best practice of autoloading data object classes that come with a library,"Hi guys, I'm using CI (single install for multiple apps) for a project and I'm creating a library that will be used by my applications, lets call it my_lib. The My_Lib class is basically just a factory that will parse json data and create data objects from it, ie data['id'] will be accessed as $myObject->id - I need to do this because the json data comes from 3rd party and the output (keys) will change. My question is what is the best way to (auto)load the MyObject class by the My_Lib so that it is globally accessible (ie that I can use strict casting and type hinting) - I guess I can just do require/include but is there another ""more CI"" way?  Thanks  tl;dr; what's the best practice of autoloading data object classes that come with a library "
"does anyone know how a program can access travel itineraries, membership information (points), flight status through api/query without paying $$$ for access.","i've been searching online to figure out how my application can connect to airlines, hotel, car rental, etc. websites to pull in customer information (membership numbers, points) for local storage and also obtain itinerary details and flight status queries.  except for american airlines, which mentions aa direct connect access, i can't find anything relevant either through airline websites or google searches.  does anyone know a starting point to figure out the necessary api/query ports to access this information for all airlines, hotels, etc?  i would really appreciate your help! there are plenty of site such as awardwallet and tripcase which are able to access this information for hundreds of airlines, hotels, credit cards, etc. so i know it can be done :-(  tl;dr does anyone know how a program can access travel itineraries, membership information (points), flight status through api/query without paying $$$ for access. "
what are common pypi license list classifier combinations for one-man software packages?,"Curious how one might go about intelligently (optimally?) licensing a couple of modules.  And, by extension, which  classifier(s)  I pick.  I feel like there should be a 20-question survey that spits out the right answer.  Or more explicitly, I'm just a guy who wrote some modules.  My main goal is to share/give-back to the community, and also boost my CV.  Help me pick?  If somebody wants to break the license I apply, I'm not going to do anything about it.  Ideally I'd like attribution, and notification of my code being bundled into something else, or stop people from claiming my code is their code...  ...I also like a license that says ""all of this code, is original code, that I made with my own 10 fingers, from scratch, except where noted""  tl;dr what are common pypi license list classifier combinations for one-man software packages? "
"want to study data science but considering data viz, will I be employable","Hey all first off thanks for the patience all you guys have in this sub with panicky people like me who are floundering with our futures.  I'm a 3rd Year Comp Sci student with a real passion to get stuck into the Big Data field (I know it's a bit of an ambiguous field currently but I can see its growth and want to be part of that) with a particular desire to study a MSc in Data Science in the UK but I've been greeted with an alternative at my university (Swansea).  The department here has a well respected research group for Data Visualization and my dissertation is in fact Data Viz which I am really enjoying. Due to interest and personal factors as well I'm seriously considering not going to do a MSc in Data Science and sticking here to do a MSc in Research in the Data Viz field. My only concern is employability after, I know Data Science is a bit of a tough cookie to get into currently but at the same time jobs are becoming aplenty and I think the main make or break for me between choosing these two streams in Big Data is whether employability is a reality.  Anyone here who works in Data Viz or has insight into the industry currently who can give me pointers?  Thanks in advance  tl;dr want to study data science but considering data viz, will I be employable "
"the strings passed as $file_line get handled as one value instead of multiple. 
 they look like this 
 'Tott', 'lname' => 'Ulydig', 'user' => 'Viewer' 
 when echoed out.","I'm trying to make a simple text file to database importer where the name of the file ex. people.txt is the table name (except i'm importing to MongoDB) where I explode the document into lines to be imported,  [code]  $seg = filename from url segment  $file_content = file_get_contents('mongoBatch/'.$seg.'.txt');  $file_content = explode(""\n"", $file_content);  foreach($file_content as $file_line):  $this->mongo_db->insert(""$seg"", array($file_line));  echo 'Imported: '. htmlspecialchars($file_line) .' into('. $seg .') <br/>';  endforeach;  [/code]   EDIT: Sorry premature submit.  but this fails because Array() adds [0] => infront of everything and imports it as one field  line from the file to be imported:  ""fname"" => ""Ariel"", ""lname"" => ""Fish"", ""user"" => ""Viewer""  if i copy this into the PHP file instead of $file_line it submits as I want it to.   Edit / TL;DR: the strings passed as $file_line get handled as one value instead of multiple.  they look like this  'Tott', 'lname' => 'Ulydig', 'user' => 'Viewer'  when echoed out. "
Idle speculation about different hash table constructs and their performance characteristics.,"> The only area I could imagine the sparse table performance to win at is if you're only interested in whether an array slot is empty  But a bloom filter + dense table (how dense has to be debated ;-)) would probably get a similar amortized performance for that use case.  > Small note: hash tables are already notoriously bad for cache usage as the hash function attempts to distribute values as evenly as possible, so I doubt sparsehash really loses too much on that front.  Robin Hood hashing with open adressing allows a reasonable trade-off between hash distribution and locality (which of course depends on the hash function). In my experience, the fair distribution of lookup displacement also makes the tables pretty stable at > 50% loads, which may positively affect memory usage.  TL;DR: Idle speculation about different hash table constructs and their performance characteristics. "
Good looking site.  However the fundamentals are terrible. Implemented in the quickest way possible to lower price but sacrifices security and efficiency,"So I will take a few minutes to comment on your website structure, and content.  -50 for having 135,741 characters of raw CSS in your page.  -100 for having an accessible Wordpress Login Page  -100 for not having your Wordpress up to date [ver=4.2.5]  -10 for having a horrible page load time [ was over 20 seconds with out cache ] [on my s5 load time 8 seconds with cache]  -1 No captcha on your contact us form.... [lol]  -5 For not compressing your homepage photos 2560x1142  Some pretty basic things to improve on... This is more a lack of will probably trying to churn out good looking websites in the fastest time possible...  YSlow has a score of 70, which is terrible in my books.  This page has 15 external Javascript scripts. Try combining them into one.  This page has 14 external stylesheets. Try combining them into one.  This page has 9 external background images. Try combining them with CSS sprites.There are 59 static components that are not on CDN.  There are 27 plain text components that should be sent compressed  There are 57 components with misconfigured ETags  There are 19 images that are scaled down  There are 135,741 characters of raw CSS in your page  Ton's of room for improvement.  TLDR;Good looking site. However the fundamentals are terrible.Implemented in the quickest way possible to lower price but sacrifices security and efficiency "
Code coverage is a side metric to lead you onto the right path. It is not something to use as a fact.,"As far as I can tell you are trying to poke fun at code coverage as a metric for TDD. By using this tool you will have almost 100% code coverage but 0 actual useful tests.  The thing about code coverage though and that a lot of places seem to get wrong is that having 100% code coverage does not make your code well tested. Having less than 100% code coverage means there are just spots you should probably go look at and see if you can test. If the number goes down in a new build there is also a chance that new coded was added that was tested. So you should probably go back and take a look. Your repo does do a really good job of proving that point.  It is also great because if you remove features you can actually remove the tests and see all the code that the feature pretty much touched. This does not give you 100% accuracy as to how to remove the feature, but it is a jumping point.  TL:DR; Code coverage is a side metric to lead you onto the right path. It is not something to use as a fact. "
throw array in command  and use EventSourcing for the real data so you can track changes.,"Although I don't use Laravel (ZF2) most of the lessons we learned can be applied to any project. Next we don't have a UI but do everything via an API (see jsonapi.org).  But back to the topic. DDD is about seprating concers of things. To do this we use CQRS and Event Sourcing. To kick this off we wrote a simple set of interfaces,  which helped us (although it is version 1. , it should really be 0. )  No how we do it;   a POST hits our server   validate post   Next we populate the command. The command requires 3 things.   first of is the ID object, just a simple container for an UUID   second is the attributes. This is for example an username   third is the relationships, see jsonapi.org how this looks     This command is fired to the command bus, which locates an CommandHandler and that one should handle it   The command handler now kicks of the whole proces of processing the data to the aggregate, although an array could contain 10 values. You will mostly use only 1 or 2 values at the time. Because if everything is required something is wrong. Because in most of the time we have like 1~3 values in the start (register) of an user, but then we just add values.   When all the values are handled, you end up with a bunch of events (renamed, changedDateOfBirth, etc). This events go to the event store (save them) and next will be published using an event publisher, so your views can be updated.    And that is how we do it, in simple form :P  Hope it helps.  TLDR; throw array in command, and use EventSourcing for the real data so you can track changes. "
. Your input data needs to be the same size in order to make a useful model   linear regression  SVM  Neural Network  etc.,"Yeah,  you're definitely on the right track. You would need to standardize your input data to have the same number of dimensions. For something like a yes/no/maybe question we can use one-hot encoding which means we give a different bit word to each one of the variables, three cases becomes - {001,010,100}.  For different sized images you can either analyze each image for specific features you're looking for and then build a data model for those features - i.e. analyze 2d photo for humans and detect ""skeleton"" points such as the humans arms and legs and then use those points in your model. Another option is to downsample and/or upsample imagery to analyze it at different resolutions and possibly with different sliding windows.  TLDR. Your input data needs to be the same size in order to make a useful model - linear regression, SVM, Neural Network, etc. "
If your good question is drowned out by bad questions  nobody's going to keep reading long enough to find yours.,"Of course, it can feel bad when you ask a question and it gets closed instead of answers. When people post to StackOverflow, they need answers, not people telling them that their question is bad.  But you also have to consider the point of view of the people who give good answers to those questions, not only because they're also valuable users, but because without them, the questions don't really do anyone much good. Say I go to StackOverflow to answer some questions. If I look at the new questions (which are most often the ones that don't yet have answers) and I see that 80% of them are off-topic crap written by people who couldn't take two minutes of their time to read the rules, which is still there because nobody is allowed to moderate new questions, I think the site is wasting my time, and I go do something else.  So, when I go to StackOverflow to answer questions, I am grateful to the ""Soup Nazis"", because they have separated out the questions that are on-topic and answerable, so that I don't have to wade through piles of garbage to find them myself.  TL;DR: If your good question is drowned out by bad questions, nobody's going to keep reading long enough to find yours. "
I work for a great company  just not a great technology company and I'm afraid my skills are stagnating.,"Thanks for sharing. I think it is time for me to start moving on. Really the only thing that I can legitimately complain about at my work is technology related. I work at a credit union so I'm working with a system built in the 70's. Keeping up with technology, or having fantastic coding standards isn't a high priority in the organization.  I can say that it is a wonderful place to work at though. They hired me while I was still in college with zero programming experience. In those 5 years I've gone from junior dev to a senior dev and they have increased my pay from 37k to 75k in that same time span. They have great benefits and I love the people I work for.  The problem? I don't want to become complacent. I don't want my technical skills to stagnate while I become unmarketable.  TL;DR - I work for a great company, just not a great technology company and I'm afraid my skills are stagnating. "
If you aren't first and you aren't best  you probably aren't going to win.,"Side note:  The patent issue is FUD.  Both projects use the 3-clause BSD license; the fact that Facebook provides some additional unimportant rights to users that Google does not is a (very) minor positive to the React project, but isn't a compelling reason to use React.  But it's also not a reason to avoid it.  Anyhow, as for your actual question:  React succeeded because Facebook started with a problem they were facing, creating an interesting solution to solve it, and released it to the world.  And a lot of other developers were facing the same problem, and gave it a try, and found it worked for them too.  Yeah, React uses some nicedesign patterns but that was always clearly incidental to trying to solve a problem Facebook had.  Polymer has struggled because Google started with an interesting design idea, and created an interesting tech demo, and released it to the world.  And developers tried it and found it was interesting, but didn't actually solve any problems anyone had at the time.  And while apparently it's improved a lot lately, you rarely get two bites at the apple.  I looked at React when it first hit HN, and I was sold; I instantly saw how it would be useful in my work.  I looked at Polymer when it first hit HN, and I was  NOT  sold; it wasn't at all useful in my work.  I don't have anything against Polymer, but the reality is that it's now late to the party and has no obvious advantages.  ""Solving similar problems to React"" is another way of saying ""solves already solved problems"".  I already dealt with the patent issue.  And React is built in standards too:  JS.  Being built on  actual  web components is not an inherent advantage.  And React now has a massive ecosystem of libraries, frameworks, boilerplates, and tutorials.  What Polymer needs is a reason for me to come back and give it another look, in an environment where 5 different frameworks are already on my list to evaluate.  Don't tell me how it's almost as good as the tool I'm already using; tell me how it makes my job easier.  How is it better than React in a way that matters?  Not that React is perfect, but most of the issues React has, Polymer has  worse .  TL;DR:  If you aren't first and you aren't best, you probably aren't going to win. "
An Option isn't the same as a null  because an Option forces you to choose what happens if there is no value.,"While it's true that you can use  null  in F# (and then have to null-check, as usual), I have only seen this when doing C# interop. Idiomatic F# uses the Option type, which is a lot like Haskell's Maybe - I think, I haven't worked with Haskell.  To respond to your comment:  >In F# you don't.  You  can  use Option.Value, true, but that's discouraged.  >Changing the name from null to Option or Maybe doesn't do jack shit. What matters is what the compiler actually does with that information.  An Option isn't just a null with another name. You can't just pass a string option to a function that takes a string argument - that won't type-check, so it won't compile. You could use Option.Value, as previously stated, but that's already better than using null since you have to do so explicitly. Plus, it'll crash right there if it's null/None instead of further down the call-stack.  So yes, it's true that renaming null to Option or Maybe without changing the semantics wouldn't change anything, but I don't know of a language that does that. F#'s Option and Haskell's Maybe have different semantics from null, and that's what makes those types useful.  TL;DR: An Option isn't the same as a null, because an Option forces you to choose what happens if there is no value. "
If you have trouble understanding Git  it's likely that you're trying to fit it into intuitions you have from using other VCSs. Stop  read the Git book       profit.,"The first VCS I had contact with was SVN. It quickly became intuitive to me: checkout -> get the contents of the repo, update -> get the latest changes, commit -> save changes to the repo. Simple. Then came Git and GitHub and I tried to get into it, and I just couldn't. I had no idea what I'm doing. That is, until I read the Git book.  My problem was that I tried to think of repos in Git in SVN terms, and that just doesn't work. Git is too different. Once I stopped doing that and just read about how Git handles things, everything became clear.  The author writes about the frequently appearing criticism of Git: complex data model. Well, in my opinion it could hardly be simpler. There are snapshots of the working folder, which also contain the information about the snapshot they came from. Where is the complexity in that? I have no idea.  People rant that they have to know about what directed acyclic graphs are in order to to use Git. Actually, no, it's not necessary. Or, rather, once you know that Git uses snapshots containing information about their ancestors, you know everything you need to know about DAGs in the context of Git.  There is also the point that Git has leaky abstractions. My impression is that there are just no abstractions that could be leaky here, and that's because they are unnecessary. The model is so simple that there is just nothing to be abstracted away.  TL;DR: If you have trouble understanding Git, it's likely that you're trying to fit it into intuitions you have from using other VCSs. Stop, read the Git book, ???, profit. "
Fuck Oracle. Fuck Masterlock. Buy expensive non chinese locks after having at look at Bosnianbill to see what he recommends currently.,"Actually Masterlock is the PERFECT comparison as their locks are the most bullshit you can buy, even their most expensive ones.  Apart from being a programmer, I'm also an avid mountainbiker og and lock picking enthusiast FYI. - The later began when I was looking into the best locks for my bikes, while parking them in the city on long tours.  If anyone is interested, go buy some picks on Ebay and follow:  He will recommend the more expensive ones, and they DO help later on, but they aren't really needed if just trying out on cheap chinese locks and so. You will be amazed at have many locks (mostly those <$20) can be bypassed in less than 3 seconds with a standard hook type pick, without leaving any kind of evidence.  TL;DR Fuck Oracle. Fuck Masterlock. Buy expensive non-chinese locks after having at look at Bosnianbill to see what he recommends currently. "
Python is better for cleaning and web scraping.  R is better for analysis.,"Depends on what you're looking to do.  I use both Python and R, and Python is much better for data cleaning.  Pandas is a lot more intuitive for that and any Python script is going to make more sense than gsub in R.  Python is also much better for web scraping.  However if you're making new frames and aggregating, dplyr in R is a beast.  For actual analysis, R is light years ahead of Python.  None of this ""import model type as a function, then extract data, then convert categories to dummies, then manually add intercept and fit model"" crap.  TL;DR:  Python is better for cleaning and web scraping.  R is better for analysis. "
This feels like useless bloat to me and I hope it gets rejected.,"I don't like it at all.  I think parameter and return type hinting are great - they provide a syntax for representing useful documentation without cluttering up your docstrings.  However, I just don't see the benefit to this unless you wanted to make Python statically typed, and core Python will  never  be statically typed.  Variables generally don't need much documentation.  Object members maybe some, but not enough to add new syntax to the language.  Also, you can't even document object members from what I can see.  Instead, the PEP advocates what I personally believe to be an antipattern: declaring object members as class members.  Another thing I don't like about it is the  b: str  example.  First off, it isn't clear what b should be.  Obviously, the PEP says it is None (kinda breaking the annotation, as None is NoneType not str), but an empty string also seems reasonable to the code reader and could add confusion.  Also, it has ambiguity with things like  lambda .    lambda: str  is an unused function that returns the str type and  lambduh: str  creates a variable  lambduh  annotated as  str  and set to None.  You might argue that that ambiguity isn't important because you already can't declare a variable called lambda, but I still consider it a problem with the proposed syntax.  Just because TypeScript is hip or whatever doesn't mean we need to copy it into Python.  Python doesn't have a garbage type system like JavaScript does and doesn't have such a strong need for type annotations that it needs to become a standard.  Alternative implementations can go ahead and implement this (one could write a compiler that transforms annotated code into code that puts  __annotations__  everywhere).  But for me, when I want typing everywhere, I just use a statically typed language.  I don't want this PEP, and I don't even want to use some tool that adds the syntax of this PEP.  TL;DR: This feels like useless bloat to me and I hope it gets rejected. "
Accuracy isn't always the most important factor in language choice.  The more expensive a bug to fix  the more importance I'd put on static typing.,"It reminds me of a point someone made a while ago about why Haskell, which is praised as a very strongly typed language, isn't more popular.  Accuracy isn't the highest priority in many cases.  Types in a build system, for example, are typically a low cost in debugging and will fail hard in a very noticeable way, assuming it's strongly typed.  I've really appreciated static typing in Java when refactoring large libraries, but I've also appreciated the brevity of dynamic typing in my Python web servers, where I'm mostly working with strings and numbers, and the cost of dealing with a bug is typically low.  TL;DR Accuracy isn't always the most important factor in language choice.  The more expensive a bug to fix, the more importance I'd put on static typing. "
Lots of good stuff and great potential  lots has to be changed  and that shouldn't discourage you  also do this again sometime.,"I feel as though most of my personal questions and concerns have already been addressed and seen by OP, so no need to repeat them. That being said, if you take into consideration what a lot of people have said and address their concerns, I think that the language has a ton of potential, and I'll definitely keep it in the back of my mind for some time to come.  Also please don't get discouraged with what some people have said, but rather take everything into consideration going forward. A lot of people weren't trying to be dicks, and I find it all too often people take offense when someone doesnt 100% like their idea, and either go forward not taking what they said into account or shut down the project down completely because they got some criticism. Don't be that guy, most of the comments that I've read, and it was most of them, people were actually being relatively constructive when it came to their criticism.  I feel that a lot of these people here would actually not mind to see this as a finished product, but before that happens, clearly a lot has to change, and you have to take that into account. So again, please don't get discouraged, and please please please do this again! Don't shy away from showing us what you've added/changed, because a lot of the people here,  I included would like to be part of the process, and be 'kept in the loop'.  TL;DR Lots of good stuff and great potential, lots has to be changed, and that shouldn't discourage you, also do this again sometime. "
he started with your idea as a given  a widely publically known failed idea that needed radically new approaches to be made feasible.,Read the hyperloop whitepaper from Elon; maintaining a strong vacuum requires asymptotically more energy as you approach a complete vacuum.  That's why he went with a something more like the upper atmosphere.  And it enables the cheap air-bearing idea he had rather than expensive magnetic levitation.  Your idea goes way back more than 20 years and is a very different beast.  If you are going around telling people you had the idea for hyperloop 20 years before Elon Musk you're just wrong; the entire set of ideas behind what he released were all trying to address the things that made vacuum tube transport impractical.  Tldr; he started with your idea as a given; a widely publically known failed idea that needed radically new approaches to be made feasible. 
That whole post was a cringefest  and an anti resume.,"Yeah, that's unacceptable. You don't just sit there and wait. Of course the manager thought he was slack (which happens a whole lot in IT).  But my alarm bells went off when he couldn't figure out how to get home on the first day. I get that he was under the expectation that transportation would somehow be provided (hell of a perk). But when it didn't happen he could have/should have been able to Google his own arrangements.  And then ask for clarification on the transportation policy first thing the next morning.  This entire post was a cringe fest of 'I need hand holding'.  You have to be able to deal with surprises and disappointment, and be able to work around them in a positive and proactive way.  TLDR; That whole post was a cringefest, and an anti-resume. "
Don't work in a the cost center. If you can always be part of the profit center.,"You're describing a VERY different company. MSFT is a large tech company. Your's sound like a company that does something completely different and has some tech tacked onto it.  On the other hand, small companies like this let you rule your realm supremely. You just need to be sure to bond properly with the boss. That's you No. 1 concern. After that, if you get along with him, it's smooth sailing. They don't want to deal with you and you're a necessary evil. So as long as you keep everything up and running etc. you can have a lot ton freedom.  TLDR: Don't work in a the cost center. If you can always be part of the profit center. "
Let's stop making up fake reasons for why garbage collection is better  and just admit it's a tool in the tool box.,"I'm going to hijack (read: ""rant"") here for a minute because I keep seeing statements like the following:  >Reference counting is frequently touted as an alternative to garbage collection, but these systems can have the same unpredictable pauses when the last reference to a large sub-graph of objects is removed.  I'll admit that I spend a lot of time doing systems-level programming, but I feel that statements like this are a cop-out. So, too, are statements like ""making your own memory allocator is too hard, so just use the one the smarter-than-you-are language library designers built.""  I'll agree that making your own  general purpose  memory allocator is a dumb idea. But if you have a  specific situation  where the performance of the application is predictable, then creating your own special-purpose memory allocator can yield significant speed-ups, both for allocating  and freeing  memory.  In the example provided, it's a given that releasing the graph object will cause a cascading deletion of the graph's nodes and edges. If that is always the case, then creating a custom allocator for the graph can resolve the pausing.  The graph object can implement a pool of nodes and edges. Functions on the graph object can then be added to create the nodes and edges instead of using the default allocator. If the nodes and edges expose a reference counting API, then have the implementation delegate to the containing graph (i.e. an addref on a node will, behind the scenes, be an addref on the graph). When the last reference to the graph is released, the destructor frees the node and edge pools, deleting thousands of nodes and edges with just a few calls to delete/free.  Using the pool allocator avoids frequent calls to the system for more memory; and if you know the graph is always accessed from the same thread you can avoid the locking imposed by the general purpose allocator. Both of these options can present significant time savings.  But then again, if you're dealing with a graph of a few hundred nodes then there probably isn't a difference between reference counting, garbage collecting, and the traditional new/delete; the ""pausing"" problem just isn't there. If the graph size is getting to the point where the difference is noticeable, you should probably be looking in to custom data structures anyway.  TL;DR: Let's stop making up fake reasons for why garbage collection is better, and just admit it's a tool in the tool box. "
  A URL is simply a URI that happens to point to a physical resource over a network,"> A Uniform Resource Name (URN) functions like a person's name, while a Uniform Resource Locator (URL) resembles that person's street address. In other words: the URN defines an item's identity, while the URL provides a method for finding it.  > A URL is a URI that, in addition to identifying a web resource, specifies the means of acting upon or obtaining the representation of it, i.e. specifying both its primary access mechanism and network location. For example, the URL  refers to a resource identified as /wiki/Main_Page whose representation, in the form of HTML and related code, is obtainable via HyperText Transfer Protocol ( from a network host whose domain name is example.org.  > A URN is a URI that identifies a resource by name in a particular namespace. A URN may be used to talk about a resource without implying its location or how to access it.  > The International Standard Book Number (ISBN) system for uniquely identifying books provides a typical example of the use of URNs. ISBN 0-486-27557-4 cites unambiguously a specific edition of Shakespeare's play Romeo and Juliet. The URN for that edition would be urn:isbn:0-486-27557-4. To gain access to this object and read the book, its location is needed, for which a URL would have to be specified.  TL;DR:  > A URL is simply a URI that happens to point to a physical resource over a network "
The ecosystem is a total mess  and our problems are real and severe.  But it's also in good shape  and other ecosystems would be lucky to have our problems.,"Depends what you mean by ""good shape"".  From a certain point of view, the ecosystem is incredibly healthy; it's growing and changing so fast we can barely keep track of it.  Projects, technologies, frameworks, build tools live and grow; even base language features are in flux.  In the time it takes the Python ecosystem to release a new minor version of their dominant webapp framework, the JS ecosystem will have probably abandoned whatever its dominant webapp framework was, and probably it's replacement too.  You could say:   This is fucking horrible; it's a nightmare to keep current, and if I stand still I'll find whatever tools or stack I'm using are deprecated and abandoned so hard I'm literally the only user.  Can we just slow down and think about stuff?  Were the old tools that bad?  Another 18 months of this churn and I'm going to start stabbing people.  This is god damn amazing; the options available to me increase every day, and the tools keep getting better.  Six months ago I was happy with Tool X, but now I have Tool Y which is so much better that it's hard to imagine how I even made Tool X work!  I hope we never stop growing and improving; imagine how far we'll be in another 18 months!   Both views are absolutely correct.  :)  My take is that we absolutely need to find ways to bring some sanity to the process, and there are a lot of unanswered questions about how to do that.  And it's certainly true that we're training some developers to be very trigger happy about abandoning the old for the new, often without any real understanding of what the new hotness is even  meant  to do better.  However, I don't think we should discount the extent to which we're lucky to have this problem.  ""Javascript fatigue"" is a problem we need to deal with by creating better systems for managing the change, not by insisting that everyone stands still.  TL;DR:  The ecosystem is a total mess, and our problems are real and severe.  But it's also in good shape, and other ecosystems would be lucky to have our problems. "
The historical  intent  behind CoC's is to enable uninvolved outside mobs to attack open source projects with teeth. Adopting a CoC is adopting politics  drama  and harassment.,"> The problem is, it was conceived by people who can't stop getting their fucking feelings hurt.  It's worse that than, the CoC was specifically crafted to give twitter armies leverage against open source developers, since nobody in those hate mobs contributes to open source the armies previously had no power there.  To illustrate, the CoC was updated to v1.1 to better give an uninvolved twitter mob a way to attack an Opal developer for [disagreeing with gender reassignment surgery on kids]( in a conversation that took place on a personal twitter account unrelated to Opal:   The attack begins  here . Drama ensues. Opal are told they need to adopt a CoC to prevent such drama in the future. CoralineAda's already-established CoC is suggested, and Opal are receptive to the idea.  The authors of the CoC realise that version 1.0 of the CoC isn't going to give them enough teeth over open source projects such as Opal, since they don't use or contribute to Opal, and the comment was made in a personal account. Wanting to be able to demand the removal of their target from the Opal project, they add a new clause to the CoC which they believe can be sufficiently bent to that purpose, creating v1.1.  Before CoralineAda and co update their files to v1.1, Opal obliges on the CoC suggestion - ending up with v1.0 of the CoC.  To hurt their target, the authors of the CoC need the clause they added in v1.1, so [demand Opal update to 1.1]( under the pretense that the update is to ""include ethnicity"".  Opal looks at a diff between 1.0 and 1.1 and spots the trap, they alter a copy of 1.1 to disarm it, adopting their own [""fixed"" 1.1 CoC](  The Opal devloper is now safe - if not chilled, but  the unaltered v1.1+ goes on to be adopted by everyone else  (atom etc), who assume CoCs are written by good people trying to do the right thing.  Another clause -  ""Project maintainers who do not follow the Code of Conduct may be removed from the project team""  makes it personally risky for level-headed maintainers to rule sensibly against an outside mob's ideological demands - the maintainer must either acquiesce or become themselves the publicly smeared target of the mob. The way normal people read a CoC is not how the mobs bend and wield the clauses.   tl;dr The historical  intent  behind CoC's is to enable uninvolved outside mobs to attack open source projects with teeth. Adopting a CoC is adopting politics, drama, and harassment. "
Denial of systemic problems in our system is thoughtless.,"ITT: Status quo trolls.  Dont say things arent fair, just because you are always on the non-good end-of-the-stick.  Why is this defensible?  People will money control people without money, and make the rules.  Thats ""The Golden Rule"":  He who has the gold...  This is an unfair situation and it's systemic, so it's a completely valid complaint.  That said, it's also the way it is, and if you want to thrive you have to play the game by the rules, complaining about the rules is only rewarded when a new game is set up where victims are given things.  We do this too, but it's a temporary game, and not a natural game like the standard resource game being complained about ITT.  TL;DR: Denial of systemic problems in our system is thoughtless. "
I miss Haskell's type system for very practical reasons at work.,"What makes me like Haskell's type system better than Java's is that in Haskell I can define a function something like this:  f :: (Ord a, Show a) =&gt; a -&gt; [(a, String)]  This is a function which takes something which is ordered and can be converted to a string, and returns a list of tuples where the first thing is of the type of the parameter, and the second thing is a string. So maybe f returns a list of sorted tuples and the string representation. It's not important.  This does a couple things that I can't really do in Java.   I require two different things of the parameter to the function. This is like saying it must belong to two different interfaces, while in Java I can only require one interface of any given argument. This is significant because it leads to much more concrete requirement of what your function needs, and typeclasses generally end up with fewer requirements than interfaces.  By using ""a"" in two places, I specify that they are the same time. If I write ""List<T> f(List<T>) {...}"" in Java you could pass an ArrayList and get back a ImmutableList (Guava). You'll find this out later when you try to run the code and it throws an exception when you try to call ""add"" on it. Also, the existence of classes like ImmutableList which implement an interface but then just throw for parts of the definition is probably a consequence of the first point, where interfaces necessarily have many methods since functions only get to require one of them for any given parameter.   TL;DR: I miss Haskell's type system for very practical reasons at work. "
Don't be an idiot and count on other functions to Trim for you  even if they say they will .,"I just ran into a weird issue where I messed up and forgot to Trim. It's such a simple thing to forget when you're writing a small program but forgetting it can be pretty catastrophic.  In this case, I was creating a directory with C# in Unity (Directory.CreateDirectory) for a very small utility function that is meant to make it more convenient for people (~3 users) to find log files. The C# MSDN spec clearly says that leading and trailing spaces will be removed. The Unity/Mono version apparently does not honor this and produces a malformed path which can't even be navigated to in command prompt in windows. When the program tries to create log files in that path, it just fails silently (as Unity does).  Lost quite a bit of data because I didn't Trim a user input and a random person (1 of 3 users of the program) was ending all of their input strings with spaces for literally no reason (they even admitted they had no idea why they did it).  TL;DR Don't be an idiot and count on other functions to Trim for you (even if they say they will). "
People want to achieve results not learn dozens of functionally similar languages.,"If you know Javascript but don't know Java, you could learn Java or use Node and have a server doing servery stuff in 20 minutes.  Every language has a list of pro's and con's but the reality is you can achieve pretty much the same features in all of them, so people pick the one they already know over the one they don't. Essentially people follow the path of least resistance.  As an example which might appear to a sysadmin, If I need a script to run on a windows box I'll write an old style DOS script instead of a PowerShell script even though I believe PowerShell to be superior. I do it because I'm familiar with dos style scripts.  History is littered with appeasement of this mentality. Javascript is called Javascript because at the time they wanted to ride on the coattails of JAVA. VB.NET existed purely to suck VB6 developers into .NET land.  TL;DR: People want to achieve results not learn dozens of functionally similar languages. "
placing engineering concerns higher than functional concerns makes code lose focus in providing value to end user.,"I haven't worked on a big project yet, so the size may matter (currently working on a medium sized project ~2mloc with small side projects in 2-20kloc range). But I don't think it's because of that.  But, I think I figured out why... In [Part 2]( you describe an architecture with ""ui"", ""business"", ""persistence"" and ""model"". I don't have such layered architecture -- these days, my code is organized by functionality, not by engineering concerns -- it preserves  value  much better than a layered approach. I would have top level artifacts such as ""registration"", ""authentication"" and ""issue"" instead. The way I separate UI/Persistence can vary, they can be similar to the layered approach, but then again they may vary.  Similarly I avoid ORMs, unless it's something very trivial. DB layer usually requires importing some ""db"" package, which means I can state it as a import constraint.  I guess the easiest explanation why I don't like layered approach is explained here [Robert Martin]( the ""intent"" part. Although it doesn't go into the deeper implications, if you want to read more there's [Restoring Function and Form to Patterns]( although for me all the pieces finally clicked together after reading ""Timeless Way of Building"". tl;dr; placing engineering concerns higher than functional concerns makes code lose focus in providing value to end-user. "
Don't stress over the latest buzzwords.  Just make sure you're a solid programmer who also happens to know the ins and outs of JavaScript.,"I'm the lead for a team of 7 developers.  When I hire people, I'm less concerned about their knowledge of Angular/Backbone/React/whatever, and instead, focus on how well they understand computer programming in general.  I test on basic algorithms, scoping (because JavaScript is weird), unit testing, etc.  I've met a lot of developers who know how to piece together frameworks and libraries to make a simple application, but they really start to stumble when they have to consider larger architecture, proper coding standards, memory management, and even basic arithmetic to render UI elements (e.g. zebra striping).  I've seen portfolios that look great, but when I View Page Source, I just see a mess of jQuery and hardcoded, magic numbers to make things work.  Once you understand the basics, you'll find that all these new technologies are just fancier implementations of the same or similar ideas.  Frameworks are important, but good architecture and coding practices are even more important.  TL;DR:  Don't stress over the latest buzzwords.  Just make sure you're a solid programmer who also happens to know the ins and outs of JavaScript. "
Basic income IS a better welfare system. Removes barriers to welfare and stops the need or ability to 'cheat'.   What is the  international factor  against BI ,"A BI system would completely remove ""means testing"" from payments, reducing bureaucratic load that eats up available money for the populace. Rich people would get BI the same as homeless people. So yes, ""reduce checks and balances"" -> ""remove checks and balances for BI qualification"".  When you say ""A better welfare system will [stop conmen]"", yes - in fact it's implicit, because there is no means testing, you can't ""cheat"" the system. You just get it regardless.  I know you say ""something along the lines..."" but free healthcare has nothing to do with BI at all. I agree that the US has a terrible healthcare system (if you are poor, but amazing if you're loaded) for the reasons you mention - no one  should  have to worry about going bankrupt from injuries or diseases, and I'm from the UK so I can tell you it's as amazing as it sounds.  However, that is an entirely separate issue to BI.  tl;dr : Basic income IS a better welfare system. Removes barriers to welfare and stops the need or ability to 'cheat'.  What is the ""international factor"" against BI? "
Software is a specific weak point for Japanese due to their work culture.,"I'm not surprised. Japan isn't known for its good software practices. The problem lies in Japan's business culture. People basically get a generic education without specialty, and join companies for life. The quality of the company directly correlates to how prestigious of a college they went to, which directly correlates to their test scores in high school. Basically, children work their asses off to get into a good college, but the actual college education is meh. They've already guaranteed their future in a prestigious company simply by getting into the college.  Anyway, the college education is broad, not specialized. Then they get into their company, and pretty much randomly get moved around from department to department, filling whatever need the company needs filled at the time. People don't get educations in software engineering, they get put on a project with no prior experience, and just have to figure it out. Now, that's fine for a whole lot of jobs. An intelligent person can figure how what they need to get the job done in most cases. But software is another beast. Someone without a background in software engineering may be able to hack something together that sorta works, with enough time and focus, but it's likely to be a fragile PoS. And they won't know about typical good coding practices unless they happen across some articles that clue them off that it's even a thing to research, and I doubt there's even that much information available in Japanese.  tl;dr - Software is a specific weak point for Japanese due to their work culture. "
There are fewer programmers with a low level understanding of hardware because because it's increasingly harder for them to do so.,"The thing is, for most programmers today (young and old), hardware interfaces and even machine instructions are simply interfaces to other, more complex computational units. Modern x86 code, whether 32- or 64-bit, is actually run via a microcode simulator on top of an unspecified RISC hardware instruction set. Drives and other devices are operated by their own processors and RAM and only pretend to be dumb to the operating system. Learning and using assembly today is a great way to understand how computers worked in the 1980s, which is increasingly unimportant for working with modern machines. About the closest most desktop or even mobile developers get these days (I recognize that embedded systems are a different beast, but their numbers are comparatively small and getting smaller as compilers get better) is probably CLR IL or JVM instructions - which, again, is remote from the hardware.  tl;dr There are fewer programmers with a low-level understanding of hardware because because it's increasingly harder for them to do so. "
I've been writing javascript targeted at browsers up and including IE8 for like 3 years now and haven't touched jquery in any of that time.,"Let me preface this by saying, jquery is awesome and used to be really useful. I got nothing against it. I just haven't used it professionally in years and don't see why I'd ever go back.  Your response touches on a lot of things. I'm going to try to respond to it point by point.  >The DOM is still a steaming pile of garbage, it just completed some missing features (bizarrely recently!), and made it more cross-browser-compatible (except older IE, which was the main issue to begin with).  I'm not sure how you came to this conclusion, but as a front end developer who writes a shit ton of javascript and interacts with the DOM ceaselessly, the DOM is really pretty simple and the API is pretty great when you're not on <IE9. And < IE9 isn't even supported by jquery 2.x anymore.  >The AJAX parts are obvious,  There are better alternatives for AJAX that don't require a monolithic library.  >but my favorites are actually the event triggering system.  Same thing here. If I want an event library, I'll grab [one off npm]( I don't need jquery for that.  >And honestly, even the simple DOM manipulation and traversal, looks all over the place.  What? Need to get an element? One method and you're done:  document.querySelector . Manipulation? There's plenty of standardized methods:  appendChild ,  style ,  textContent , etc. As for traversal...you know honestly I can't remember the last time I actually needed to manually traverse the DOM. But you should be fine:  element.parentNode ,  element.children , etc.  >Of course, if React does all of the DOM manipulation and event handling, and you don't use AJAX, those arguments aren't really relevant. And I suspect that this is the case here.  Again, I've been using React daily at my day job for probably a bit over a year now, and React doesn't do anything except render really well. That's the beauty of it. It's a targeted library: it does one thing (render and update the DOM) and it does it well. This is as opposed to jquery which is this massive toolbox that most of the time you need maybe 10% of it at all.  tl;dr I've been writing javascript targeted at browsers up and including IE8 for like 3 years now and haven't touched jquery in any of that time. "
Would've wanted a more general approach to allow documenting not only APIs and programming languages but also systems  their file formats and general concepts for those systems.,"I've been in the beta for a long time. I haven't given any contributions mostly because everything I would've wanted to improve/add has been already added or I simply can't add to it.  I think one of the issues is that while the approach is good for programming languages, it's not a very good approach for always documenting APIs and general knowledge that might be needed by someone that's writing for some old systems or wants general knowledge about some system or architecture, etc.  For example, I have been doing quite a bit of reverse-engineering and research into old Symbian OS and the N-Gage. Unfortunately the format doesn't fit for documenting executable formats/libraries included, general OS concepts, etc. I also can't propose it because the symbian tag is ""too new, or too low activity"". Besides that, I'd also need 4 other people, which is more than there are Symbian developers/researches/whatever in the world.  TL;DR: Would've wanted a more general approach to allow documenting not only APIs and programming languages but also systems, their file formats and general concepts for those systems. "
yesso basically Lisp or Racket. But saying  hey look at my fancy keyword with this fancy syntax  sounds awesomebut you probably just borked the compiler into oblivion,"True, this is essentially Lisp macros or more so what Racket can offer (which of course is a Lisp derivative).  Lisp macros can get you ""language features"" in a ""syntax-less"" world, but whereas in Racket, you can have a custom parser on top of said macros. This allows your ""language"" to look however you like (as long as your parser can turn text into some sort of AST) rather be confined to s-expressions or other construct which is more of what I would have in mind.  Through, IIRC in Racket the entire file can only be in only  one  ""language"" while ideally, you would want to be able to mix your ""mini languages"" (that being your library of language extensions) together in a more seemless way. Trying to offer magical ""first-class"" language extensions this way in something like C# would be easier said than done though.  tl;dr:  yes...so basically Lisp or Racket. But saying ""hey look at my fancy keyword with this fancy syntax"" sounds awesome...but you probably just borked the compiler into oblivion "
women like to use words that totally absolve them of responsibility for their feelings and opinions.,"That's what women are all about. That's why they create exclusively female language that grants them impunity from criticism. Words like ""triggered"" and ""creep"", for example, which translated simply mean ""angered"" and ""someone I disapprove of"". The problem with words like  angered  is that anger is an emotion and one could be faulted for becoming unduly angry, so they call it being  triggered , because it implies an automatic response that can't be helped and therefore cannot be criticised. Similarly, by labelling someone as a  creep  you imply that is their fault; simply an attribute of their person that cannot be helped. Whereas what's really happening is you're judging someone and placing a derogatory label on them which ordinarily you could be faulted for, for being a judgemental bitch, but  creep  implies it's  their  fault they're unattractive or acting in a manner you deem ""weird"", not your own fault for judging them as such.  tl;dr: women like to use words that totally absolve them of responsibility for their feelings and opinions. "
Teaching people to take offense at certain words is to do them a disservice.,"It's called a euphemism treadmill.  The state of being lesser than your peers is an undesirable state.  Pick any vector in which people can deviate toward lesser, assign a word to that lesser state, and it will eventually become an insult, no matter how positive or innocuous the word initially was.  Like calling someone ""Special"".  We could eventually burn out the entire English language trying to find an acceptable term for people with damaged chromosomes; it will never happen.  The contemporary strategy is to assign a term with a cumbersome number of syllables (and preferably some hyphens), so that you can't throw it around casually, but all it does is force people to get more poetic.  There are no objectionable words in the phrase ""Exactly how often in your family to siblings marry?"", but it's still insulting.  TLDR:  Teaching people to take offense at certain words is to do them a disservice. "
All programming languages look alike if you're a bank.,"Developing software costs money. Developing systems is a risk. Not to mention required expertise in land of requirements v. technicalities. And for financial systems there are regulations (like Sarbanes-Oxley) that will make it even more 'fun' to tackle.  If you're not in business of writing software you don't want to pay and bear the risk, especially when your system works (more or less) fine as is . And all these systems are pretty much OK. It's not inertia, it's practicality.  It goes for COBOL, ABAP, Java and what have you. Will go for new enterprise languages forever. And it's OK.  tl;dr All programming languages look alike if you're a bank. "
Read a book  learned the rest on the job.   What's you're background  Knowing where you're coming from might help us direct you.   ,"While I was at University, they tried to teach us Perl. For those that don't know PHP was made  specifically  because Perl was a bad language for the web (specifically shared hardware). Anyway, for less clever reasons, I didn't like Perl, so I taught myself PHP out of a book. I thought I was pretty good too.  Truth be told, I did know the language very well, but there's much more to it than that. My first PHP job was with a digital agency filled very smart cookies and my knowledge just exploded from there. I learned about MVC, SCM, best practices like single responsibility, FIG standards, testing.  TL;DR:  Read a book, learned the rest on the job.  What's you're background? Knowing where you're coming from might help us direct you. :) "
IMHO using virtualenv is not needed in scientific python usage   EDIT  wording,"> In particular using it for scientific ""programming"".  So here's the thing: if you're doing scientific programming, you won't need virtualenv. For web apps that you will later need to deploy remotely or for doing extensive app development, a venv is a good thing to have. But for most scientific computing, you will not ever need numpy 1.8.1 and numpy 1.9.2 installed at the same time, as these libraries are usually stable and have good backwards compatibility.  Now, one thing that sometimes messes with your python installations if you're not using venv is system-wide installations vs. pip-installations. Concretely, system-wide installations are often placed in  /usr/lib  while pip installs in  /usr/local/lib . If you have two version of the same lib installed, you'll need to make sure that the right one is loaded. I could well imagine that is the source of your problems. Personally, I always stick with pip (or rather, use setup.py) because I want to optimize my numpy-builds, and don't bother with the system-package.  TL;DR:  IMHO using virtualenv is not needed in scientific python usage  EDIT: wording "
it really depends on what you are trying to solve. OOP has its merits but you lose  reusability   among other things,"How I have always understood it and what has helped me when deciding how to design my code:  If you are creating a Thing, OOP. If you are creating an Verb, FP.  If you are creating a Square, you will probably want to have a Square object. This will probably have some sort of state (length, [x,y]/location, etc). How we interact with this object, the  verbs  we use to add functionality, could be abstracted away and not be enclosed within the object. For instance, let's say that you wanted to move the Square to another location. That is simply changing a double based on a delta, which can be used by more than just the Square object.  An object describes a real thing. The way that thing interacts with the world though can usually be described in an abstracted way, which may or may not need be attached to the object.  TL;DR - it really depends on what you are trying to solve. OOP has its merits but you lose [reusability]( among other things "
I think that the way C C   does it violates the principle of least surprise. In that sense  Java's approach is simpler.,"Simpler doesn't always mean ""less characters"". Multiple strings in a row being concatenated isn't immediately understandable. You have to understand the quirks of the language first. String addition is a more widely understood concept, and at the very least when you see an operator you at least know it's doing  something . Whereas the C version is just kind of behind the scenes magic.  Not to mention that it makes mistakes harder to make. What if I want:  String a = ""a"";String b = ""b"";   But I'm just really tired or something and am messing up, or I made a bad refactor, or some automated refactor I tried to do wasn't done properly. Somehow, I end up with:  String a = ""a""""b"";String b = ""b"";  Java breaks. That's an illegal statement. In C/C++, suddenly I get a value I didn't want.  TL;DR  I think that the way C/C++ does it violates the principle of least surprise. In that sense, Java's approach is simpler. "
Use what you're fast in.  Be open to other editing experiences.  Don't evangelize editors because one day you'll look like an ass.,"I'm so unimaginably bored with editor evangelism.  An editor is not a religion.  Choosing the best editor is a question of tooling.  You don't use a jackhammer to brush your teeth–so it is with editors.  The best ones are best used at what they are best at.  In other words, don't jackhammer your teeth out–and don't tell your boss you're late on the project because you spent all night setting up your editor so you can eventually be faster.  We've all seen the talks, showing some elaborate task whittled down to a few magic keystrokes in Vim.  It's cool, but don't get sucked into the trap.  Those talks are pre-planned.  Most of those fancy keystrokes will rarely be used because they are difficult to recall and the problem they solve is rare, unless someone spends a lot of time writing editor plugins.  Vim is great on the shell, sublime is great in a window and IDEs are great when you're confused.  They all have their uses.  Only a handful are worth the time it takes to learn them and few if any should be learned exhaustively unless you have a very special need.  True speed comes from practice.  Train your practice of programming; learn to use the proper tools effectively, understand the problem domain, know what you're going to do before you write any code, manage your time, RTFM and get off Reddit.  Doing those things will make you a far better programmer than Vim will.  The bottom line is a multi-pane tmux session running vim and a bunch of other terminal windows looks cool.  It's great for looking really busy.  However, the same thing with the same speed, can be accomplished elsewhere–but it won't look you're doing something really fancy.  TL;DR: Use what you're fast in.  Be open to other editing experiences.  Don't evangelize editors because one day you'll look like an ass. "
You already have everything you need  and  u ilikebigsandwiches suggests a different subreddit for next time.   ,"In all seriousness and despite /u/ilikebigsandwiches's soft redirect to /r/learnpython and /u/Phrohdoh's very valid question, it sounds like all you need in addition to .isdigit() is len(foo) (as in  if len(foo) == 10 and foo.isdigit() ). Of course, [there's always a different way]( Exploring the magic of  \d{10}  would also help you, but is needlessly complicated for this case.  But to piggyback onto what /u/Phrohdoh said: It sounds like you're trying to make your algorithm needlessly complicated by somehow proving that the opposite of what you want is not true,  or  by making the opposite the check condition and then executing what you really want in the else condition. Neither is particularly elegant. Just focus on what you need. You want to know whether the string is all digits and ten long. You know how to figure out if it's all digits. All you were missing is getting the length and combining the two. No need for opposites. :)  Lastly, on the off chance that it is a wording issue, because the documentation states  > Return true if all characters in the string are digits and there is at least one character, false otherwise.  ""character"", in programming, refers to the sub-elements that make up a string. i.e. if you have a string ""Password123!!"", 'P', 'a', '1', '!' and everything else in there is referred to as a character. So when the documentation says ""at least one character"", it means isdigit() verifies that the string is not empty. (Otherwise, """".isdigit() would be undefined, or, worse, through dynamic typing become """" == NULL/false == 0 == a digit, thus true.)  tl;dr: You already have everything you need, and /u/ilikebigsandwiches suggests a different subreddit for next time. :) "
arguing about frameworks vs basics is too black and white.,"I think it is really hard to distill this advice down into ""Make them understand basics"".  The basics are not fundamentals of a language. The foundation for programming is design patterns. I think we often focus far too much on ""how to do X"" instead of ""why we do X"". The latter is far more important and is a long term skill that applies to every language, not just PHP. The tricky thing is that a good framework will make it easier to use design patterns more effectively. But if the developer doesn't understand the pattern properly, they will just find a solution that works.  tl;dr: arguing about frameworks vs basics is too black and white. "
everybody can learn to code.  Only creative minds can hack.,"The technical/creative dichotomy is false and all technical knowledge can be acquired with dedication.  Technical ineptitude has a clearly defined correlation to class, but not to creativity.  In fact the greatest creative minds, must find technical skills to aid in creation.  The idea that technical people are superior coders is thus a farce and furthermore to be great at any craft, one must be willing to acquire technical knowledge in order to create more precisely.  People who have not nurtured their creativity are drawn mostly to corporate coding gigs, because that kind of coding requires no creativity, just the ability to acquire more technical skills.  When I look in the Linux Kernel I see tons of creative ideas that combine bits of technical knowledge in unique and useful ways.  If all software had that much passion poured in, we would never want for features.  Tl;dr everybody can learn to code.  Only creative minds can hack. "
Explicit intentional injustice must be fought.  Latent injustice must be dealt with more gently   or you'll just radicalize the masses you're trying to win over.,"> I'm going against the grain here in saying I support a code of conduct. I've seen way too much sexism, racism, homophobia, and transphobia in tech, and if a code of conduct might help solve that, so be it.  You defend the code of conduct but don't address the article's complaints about it. (Which is fine... this is just a reddit comment after all.) What concerns me about your position is that you have an ""ends justifies the means"" mentality.  We should take a principled approach to promoting our principles , and  mens rea  is a pretty darn good principle.  There are many things I could say in this vein (about crushing human spontaneity, self-censorship as a contributor to societal alienation, etc.), but let's put it this way: your sexist/racist/homophobic individuals aren't Nazis. (Well, not most of them anyways.) They aren't monsters. They grew up with a different worldview. They may be somewhat bigoted now, but they are capable of learning and growth. They have their own moral successes from which me might learn and challenge ourselves (and I'm not trying to covertly endorse Christian dogma here... just the idea that you can learn something about how to be a better human from everyone you meet [heck... even Hitler of all people was ahead of his time in protecting animal rights]). They aren't enemies to be vanquished or squashed into silence (with a few exceptions)... they are friends and neighbors and parents and siblings and coworkers who are valuable contributors to society.  We need them , and they need us.  TL;DR - Explicit/intentional injustice must be fought.  Latent injustice must be dealt with more gently , or you'll just radicalize the masses you're trying to win over. "
if you ignore the bullshit  is the actual info provided legit or not   AFAIK I've never come across anything blatantly incorrect.  Maybe explained poorly  but not necessarily incorrect.,"> Originally, they were a fraud site used to monetize traffic.  Ok, but are they correct with their information now and provide value?  Maybe I'm only ever hitting the pages with the legit info whenever my brain stops working and I need to look something up and end-up on their site.  > turn off your adblocker  Oh hell no.  I'll take your word on that then.  I always forget that the web I see isn't necessarily what everyone else sees, so that's a fair point... but I can't begrudge a site that wants to earn money.  That's what I hope for with my own website after all.  > Certificate  Yeah, now I remember people mentioning that.  But, still, in terms of the info they actually present is there any beef with the data?  If you ignore the noise, as I do, and are looking something up people make-out that using W3CSchools is going to give them the plague.  tl;dr if you ignore the bullshit, is the actual info provided legit or not?  AFAIK I've never come across anything blatantly incorrect.  Maybe explained poorly, but not necessarily incorrect. "
there is no silver bullet in quality assurance.  Cover your ass six ways to Sunday.,"> Projects doing proper code review always wins  There's no ""always"".  Code reviews aren't perfect.  And defects introduced during the requirements or design process aren't going to be caught with code inspection.  Now, if you're performing a checklist-based inspection, a Fagan inspection or similar, you've got a fighting chance at keeping defects down, but it's still not perfect.  [Industry studies have shown that defect rates are lowest when a combination of quality assurance techniques are used]( design and code reviews, unit and integration testing, automated code inspection tools, etc.  Any one of these methods will find 25-60% of defects on average; in concert, several methods can find 85%+ of defects.  TL,DR: there is no silver bullet in quality assurance.  Cover your ass six ways to Sunday. "
MySQL is truly enterprise class software because it's a license for contractors to print money.,"> I actually make a living nowadays solving performance issues in systems that have had prolems for years and guess what: 90% of the problems come from either the DB or the data access layer (or lack of).  I have worked for many clients, often startups, and a typical pattern I see is this.   They start without a DBA, so the devs just choose a tool they already know and that tool is usually MySQL.  The MySQL is installed with the only changes being to tune it slightly for performance.  After a few months, they realize that default settings for MySQL allow you to insert a lot of garbage into the database, but now they have trouble switching to ""STRICT"" or ""TRADITIONAL"" mode because their app depends on the bugs.  Code is now written in the app layer to protect against the app bugs, adding a new layer of bugs.   By the time I'm called in, their database interactions are an absolute mess at both the db and app layer and I get to bill plenty of hours fixing that mess.  TL;DR : MySQL is truly enterprise class software because it's a license for contractors to print money. "
Emulator vs Simulator.  iOS development uses an emulator. Emulator means less than total system simulation.,"Not sure what you mean.  It emulates the iOS system calls. It runs native x64 instructions when run on an x64 processor and runs ARM instructions when run on an i device.  Compiling for iOS produces a ""fat"" binary that contains compiled assembly for both systems with the correct binary picked at runtime.  Compare this to android where what you are given is an ARM VM. It is noticeably slower, since it is simulating the whole system.  It is also why you can run the android Dev environment on different OSes, they didn't have to re-implement android for every platform supported.  TLDR:  Emulator vs Simulator.  iOS development uses an emulator. Emulator means less than total system simulation. "
there's no use for a C  REPL  because FSI already exists.,"> C# most definitely has first class functions  No it doesn't. Not as a built-in language feature  F#:  let outer(a: SomeClass) =    let nested(s: String) =        Console.WriteLine(s)    nested(a.String1)    nested(a.String2)  That's a very stupid example, but you get my point.Now, to do that in C# you need  Action&lt;T&gt;  or  Func&lt;T&gt; , which again, should be only one type, but there's a stupid dichotomy between  void  and anything else (IMO all the design mistakes in C# are brought from java). F# doesn't suffer that,  (x -&gt; y)  is treated in exactly the same way as  (x -&gt; unit) .  > you can easily implement currying  but I won't because F# supports that already, and there's no reason to try to reinvent the wheel.  > There's no tuple syntax, but there are tuples.  I know, and their usage is horrendous, which is why pretty much no one uses them in C#. In F#, tuples are all over the place, because the language supports them natively and the syntax is usable.  TL;DR: there's no use for a C# REPL, because FSI already exists. "
don't force people into meeting a metric.  Let them handle things how they need to  and merely expect results.  It worked wonders.,"I worked at a place where people willingly put in whatever hours were needed, and watched it change into a typical work environment where people punched clocks.  Funny thing is there was never any new rules made that changed the culture.  As far as the execs were concerned they had allowed us to retain our culture through the acquisition.  Only, they didn't.  They loaded us with more work than could be reasonably done, and used our down time and other freedoms as reasons why we should have been able to get it all done.  Didn't matter if you worked 80 hours that week, you played ping pong for a whole hour, and went to the local bar, therefore you had time.  So.. people started hiding their down time.  Not getting it in while the code compiled, or for an hour before heading home to work more.  Nope - now they just leave at 5 and it's no one's fucking business what they do after that.  Which not only increased stress (beyond that of the increased workload) but ruined the social structure that made us work so well.  Turns out that people not only still talk shop over drinks, but they get comfortable to say what needs to be done instead of what people want to hear, and don't waste time and resources backstabbing each other.  TL;DR - don't force people into meeting a metric.  Let them handle things how they need to, and merely expect results.  It worked wonders. "
use JS libs as tools  don't fall into the trap of when all you have  familiarity with  is a hammer  x JSlibrary  everything looks like a nail.,"> What can you do with these that you can't do writing vanilla javascript?  Nothing.  > Just curious what the advantages of using these frameworks are?  Basically not having to write everything from scratch. Problem being you have to understand :  a. the philosophy behind the frameworkb. how it's structuredc. it's dependenciesd. etc  before you can use it, otherwise you get into the  'jQuery hammer of doom'  mindset, which ill outline.  Back in the day there was a legitimate reason for using jQuery.  Because such things as AJAX / IE (5,6,7) were alot more 'wild' then they are today (IE in particular even today has lack of compliance with standards). jQuery provided a consistent implementation (helper / utility library) to make things cross / backwards compatible.  The problem is people are creatures of habit and because jQuery did it's job so well people started teaching new devs to use it as default without teaching vanilla JS  or exploring any of the reasons why jQuery was the way it was / did the things it did.  Months and eventually years pass, browsers get updated by vendors (even IE11 was a vast improvement, let alone edge). Yet jQuery is still in play in spite of the fact the way it natively does things is not necessarily the best for performance.  For example there is no native DOM caching with jQuery, yes there are work arounds / different implementations however i shouldnt have to code around the sizzle engine (increasing bloat) just for that reason.  Note : To be fair i havent checked out the latest jQuery version (v3 IIRC) so things might have changed... maybe...  TL;DR  use JS libs as tools, don't fall into the trap of when all you have (familiarity with) is a hammer (x-JSlibrary) everything looks like a nail. "
A clusterfuck.    None of this is legal advice etc etc. ,"First it seems Startup just gets the groups together. They don't deal with payment etc. That's fine. But there were a lot of things that stood out.   Not siging a agreement   ""Handshake deal"" might as well be no deal.   The bloggers paragraph on how programmers rule blah blah (no you need other people) just screams full of themselves (maybe I'm wrong).   Hey you got him to talk! You got him to acknowledge pervious deals. That's good because you'd probably be screwed without it.   Stop wasting your time talking to him. Get the disenfranchised programmers together and either sue or walk away.    TL;DR A clusterfuck.  (None of this is legal advice etc etc.) "
look at the curriculum of each program and see which program provides the best of both worlds and pursue that,"I can't give you any solid advice here, as I faced the same question not long ago, and while I have made a choice as to how to proceed, I have no idea if it will be the right choice in the long term.  I applied to a number of Statistics and CS programs, and despite my background in Mechanical Engineering, I got admitted everywhere.  At the end of the day, I decided to attend  OHSU's Computer Science Program  had the most statistics integrated.  Since the program is super small, they allow for students to take other CS courses from the Portland State University as well.  I could not find a statistics program that really embraced computer science, but I found a CS program that embraced statistics, so I went that route.  TL;DR look at the curriculum of each program and see which program provides the best of both worlds and pursue that "
Normalise your data or you'll have a really bad time,"Normalise your data. Exact your primary information (what is this record about?) and then attach a table for A, B and C with the readings. You now have a normalised database.  Next step is to pick the way of dealing with multiple readings of the same primary under a single scheme. Do you average? Pick the lowest? Talk to someone in your field about the best way to turn your data into single points (in database speak this is transitioning from a 'one - none to many' to 'one - none or one'). From this point you should know what to do.  tldr: Normalise your data or you'll have a really bad time "
collective code ownership involves a lot of contention  but code ownership is massively political.,"I've seen the code-owners model degenerate into chaos and conflict more often than collective code ownership.  Why?  Well, the ultimate answer is that there's no silver bullet and different things work for different people, but some specific problems with code-owners revolve around the fact that some modules are more prestigious than others, there is competition to own certain modules.  Also, it's quite common for a change to cross modules, unless by some miracle your team structure matches every eventuality perfectly; in this case the implementation can differ massively depending on which owner is the primary owner of the change, they will prioritise their own needs first.  In passive-aggressive cultures (it happens) this can break-down massively, Person A wanting to retain ownership of component X making changes in such a way that Person B owning component Z has to constantly react to, falling behind the component Z schedule.  TLDR - collective code-ownership involves a lot of contention, but code ownership is massively political. "
there's not less politics in collective code ownership  it's just that it's more indirect. Code ownership makes it more explicit  thus allowing it to be addressed directly.,"My experience is that collective code ownership falls into chaos 100% of the time, and that the conflict which would otherwise happen at the boundaries of modules instead moves upward into the organization.  What you end up with is a code base of near-zero quality because no one is long vested in taking care of any of it, a bunch of disenfranchised engineers being treated like interchangeable code monkeys, systematic brain drain of your best engineers because no one has any pride of ownership, software architects who act more like politicians than engineers since they know none of the invariants they implements will go unmangled, turnover that rivals that of a fast-food chain, and product releases that are of low to middling quality.  With code ownership, there is still going to be some conflict, but it's contained explicitly, minimized, and doesn't escalate up to the organizational level.  And in your case, if the team lead see malicious acts like those undertaken by your Person A, then Person A can be moved to a less important module, and if continues to act maliciously, ultimately removed from the organization.  Humans are often political, the best you can do is make it explicit where it's unavoidable and work to minimize it. I would venture to guess that you saw more politics in ownership not because there is more, but because it is less hidden than in collectivization. Like bad code, politics is best made explicit such than it can perhaps be dealt with directly. In collective code ownership, it is so hidden that there is no hope to address it.  In my analysis, reducing engineering and organizational contention is the best possible way to minimize politics. If there were a way to get rid of politics entirely in software engineering, I would advocate that.  TLDR - there's not less politics in collective code ownership, it's just that it's more indirect. Code ownership makes it more explicit, thus allowing it to be addressed directly. "
Json is a great serialization format  but a horrible project file format.,"Did you take a look what this json file can contain?  That is a  LOT . And it's only gonna get more. Especially stuff like  compile , which includes files to compile,  compileExclude , which is used to exclude files that were included by  compile , and then there is  compileFiles , which will again override  compileExclude . If you ever would need this.. Wouldn't it be great to  document it ?  Or the many script blocks:  - I would want to document what the script does, why it's invoked. For people who will work on this project after me.  My main issue with  JSON  project files is that you can  not document  anymore. JSON does not allow comments. Yet these files can get very complex and it would be so important to document specific changes or additions. But you can't, instead you have to maintain a second file, which is easier to overlook and to forget.  Yes, I would prefer XML. Or YAML. Or JSON5. Anything that allows to add documentation inline.  TL;DR: Json is a great serialization format, but a horrible project file format. "
yes  you should use a framework and Laravel would be the one I'd recommend.,"At the company I am working at I am the only WebDev, so I am more than free to choose whatever way I want to go. I started 10 years ago and built apps from scratch. That went well, but was a pain in the ass to maintain. I started making apps using CodeIgniter a few years ago, and when I have to fix something or add a new feature it is still a cinch. Recently I started developing with Laravel, and I must say that it is my framework of choice. You can build so much in so little time, it is amazing. I thought doing stuff with CI was fast, but using Laravel I can do stuff even faster, and having an Auth system built in takes a lot of my plate when starting a new project. Eloquent and relations are a cool way to get data, Blade templates are ok too I think (I've never used templating engines before), RESTful routing is superior to CIs capabilities when using CRUD. Migrations save me from playing around with phpMyAdmin...TL;DR yes, you should use a framework and Laravel would be the one I'd recommend. "
Writing your own crypto   yes  if you want to learn more about cryptography  using your own crypto code   never  publishing your own crypto code   never.,"No. The article says exactly this: Write your own crypto code, but do not release it, and in fact, do not use it.  Writing crypto code strictly for the learning experience is highly recommended; just don't ever begin to think that anything you produce will be anywhere near appropriate for anything of any value whatsoever, so don't use it ever; and because it is a law of human nature that any bit of information that is publicly accessible will at some point be used and be misunderstood grossly, you must never publish it, not even with dire warning attached, because someone at some point  will  ignore the warnings, and your code  will  make it into production systems.  TL;DR: Writing your own crypto - yes, if you want to learn more about cryptography; using your own crypto code - never; publishing your own crypto code - never. "
Syncing a real time program in a distributed computing environment is a bitch.,"I don't think there's much, at least not without some tradeoffs. The only way I can think is something like a full lockstep synchronization. There's a now famous article about using this for RTS ([1500 Archers on a 28.8: Network Programming in Age of Empires and Beyond]( I think DOOM used a similar model.  The article is specifically for a P2P network, but you could do the same w/ a server/client approach. The gist is that you timeslice the game, with a single state for every timeslice. Say we used every tenth of a second. Each timeslice, your client collects your input and sends it to the server, which updates the state accordingly and then sends out the changes. Since all the changes are timestamped, even if there's different latency across the various clients the game will always show the exact state of the game at a given time. No ghosting, no peekers advantage, and no need to lead a target for lag. No need for prediction either.  However, the drawback is that the server can't advance to the next timeslice until it has updates from all the clients. It's basically taking a real time game and making it turn based, but you don't notice because the turns advance automatically. This means that you're playing with the slowest network connecting from all the players, and if one connection suddenly lags, that turn also takes that much more time to complete. So you'd have smooth play, then suddenly play would be frozen etc. Kinda like this from [Smash Bros. Brawl's online play]( but not as bad hopefully. Brawl had some notorious lag issues.  If you had a really low latency environment then it'd be great.  tl;dr Syncing a real time program in a distributed computing environment is a bitch. "
If you want to split this stuff  make it composable  Don't fall into the trap of writing Java in Python.,"Why not:  def fetch_ddg(word):     # fetch from ddg     return requests.get(...)def pluck_dgg(response):     return response.json()['Definition']def find_definition(word, plucker=pluck_ddg, fetcher=fetch_ddg):     return plucker(fetcher(word))  Now you can supply dummy pluckers and fetchers to find_definition and thus test find_definition itself without having to resort to patching libraries in your tests. This makes the tests more functional and thus more understandable as well.  EDIT: As a side-benefit, the functions are now function-scoped which makes the name lookups faster. So if you have to call this in a tight inner loop it will be faster than on lookup needing to traverse to the outer scope. But this should not be the only reason why you would want do it.  EDIT2: Especially in a function doing I/O this sort of scope optimization is silly.  tl;dr; If you want to split this stuff, make it composable! Don't fall into the trap of writing Java in Python. "
a PhD is not a job even when it's paid well  but that mustn't be a bad thing  it just takes some time to adapt.,"I moved from a relatively safe position in financial supervision, doing data mining and applied econometrics, for a PhD position in Management. The pitch was that the Management chair wanted someone with data science experience, because that's what their research focuses on. It was also better paid.  I do regret this decision at times. It was a bit difficult for me to be back inside a university and realizing that I would be for several years. I had also put ""work in a new field"" as a Pro on my yellow pad while now I sometimes feel it should have been a Con. Turns out, I like knowing what I am doing much better than not.  That said, once I got over the initial weariness, I started finding more and more things that I like here. My work is interesting, there is much greater flexibility in terms of work-life balance, the work with students is enjoyable and rewarding, many colleagues of the same age, it's easy to get in touch with other researchers now, and the list goes on.  tl;dr a PhD is not a job even when it's paid well, but that mustn't be a bad thing; it just takes some time to adapt. "
You can enter code points higher than 65535 directly using UTF 16 into the Windows API.  It won't break.,"Looks like you still don't know what a surrogate pair is.  OK, the following string ""AA"" in UTF-16 is 0x0041 0x0041.  It's two code points of 0x41 and two 16-bit code units.  Can the Win32 API handle this as input (a filename for example)?  Yes.  Now how about the smiley face emoji ""😃"" in UTF-16, it's 0xD83D 0xDE03.  It's one code point this time of 0x1F603 but two 16-bit code units!  Can the Win32 API handle this as input?  Yes!  Those two code points.  The first is in the range of 0xD800-0xDBFF and the second is in the range of 0xDC00-DFFFF.  That's a surrogate pair.  Both are 16 bit values.  There is nothing that ""doesn't fit"".  Heck it's even valid UCS-2 so a system that's unaware of UTF-16 and surrogate pairs will pass them through no problem as long as it doesn't do any text editing (i.e. word splitting) in the middle of a pair.  As I've wrote elsewhere, there are still bugs where some windows code will split the pair.  Most noticeably in text edit boxes.  These are bugs.  They're supposed to be treated correctly.  Here's where your argument falls apart.  It's based on the premise that converting from UTF-8 to UTF-16 will cause these surrogate pairs which can break when passed to the Win32 API.  Ask yourself this:  What stops the user from entering this supposed invalid data directly as UTF-16?  Nothing!  I just did it now by naming a folder 😃 !  UTF-8 and UTF-16 both encode unicode code points.  So it makes no difference if the data containing code points higher that 65535 went directly from UTF-16 -> API or UTF-8 -> UTF-16 -> API.  The  exact  same data can be entered into the API both ways.  So if the API ""breaks"" because of some converted UTF-8 data it will break on the exact same data that started out as UTF-16 first.  TL;DR;  You can enter code points higher than 65535 directly using UTF-16 into the Windows API.  It won't break. "
Possibly unenlightened dude wonders if compiling an interpreted language into an interpreted language is necessary and how can we stop it ,"For the longest time I have been wondering whether the js community at large would reach a tipping point in questioning if investing so much time adding compilation on top of an interpreted scripting language is really worth it. Are we there yet?  It seems to me that javascript shouldn't be THE language of the web because it clearly doesn't serve the needs of developers. Even with all that future iterations of ECMA promise, where is it all going? To a future where you still have compilation steps for large scale apps? If compilation is necessary, how strong is the case for a compiled language?  Is web assembly an answer? I am not a seasoned web dev by any means, so I just wonder about these things because I don't want to go into the quagmire of learning all these build tools when I can honestly still write reasonable sized apps in javascript with some dependencies and tests.  tl;dr:  Possibly unenlightened dude wonders if compiling an interpreted language into an interpreted language is necessary and how can we stop it? "
work for startups or small shops. You're a disposable resource for the bigger ones.,"I don't disagree. I stay in Dallas because there's always a better opportunity. But don't kid yourself that most of these larger bit factories view developers as anything more than labor. There are a good number of smaller shops that get it and work to retain talent, but there are far more of these behemoths who salivate over the siren call of offshoring every budget season.  I'm a consultant and I spend most of my time in smaller shops, but not always. I had about a half dozen coworkers who were at a large telecommunications company earlier this year. They were explicitly told that any one of them should be able to pick up and continue any of the others' work with no downtime as long as they were all following the company coding standards. We terminated that contract soon thereafter, not necessarily for that particular reason but more for the behavior that such reasoning led to.  tl;dr work for startups or small shops. You're a disposable resource for the bigger ones. "
If you need it  ask for it in the typehint. If you haven't asked for it  don't use it.,"The interface UserService asks for is UserServiceDeps. That interface doesn't have a method for getting the app.  In any statically typed language, calling a method which is not in that interface would be a compile error.  In general, when a class asks for an argument of a given interface, it should only use what's on that interface. Your IDE will show in autocomplete only those methods, and anything else would be considered a IDE type error, even if the method is a valid Environment method (because the typehint is not for Environment itself). And so the problem can end there by coding to the requested interface and not doing runtime tricks.  Of course, PHP allows you to call that method at runtime, technically. And you can even add an instanceof guard which will remove the error in IDEs which detect instanceof usage as a ""cast"":  if ($deps instanceof Environment) { $deps-&gt;getPublicSiteApp(); }  Zend Studio will recognize $deps as an instance of Environment in that codeblock. The purpose of a module, however is that it may be used in different apps, and class Environment might not even exist when a module is deployed in a given situation.  Programmers tend to know when coding modules, that making assumptions about $deps which they've not declared in the interface is a bit like shooting themselves in the foot, because everything else might or might not be there in a different configuration, the type system won't guarantee anything.  If you don't trust your devs (say they're inexperienced, junior, so on), you can always use adapters and enforce your constraint, sure. It's slightly more verbose, but not that much.  But once people get the hang of programming to interfaces, it's not really necessary to do so.  tl;dr  If you need it, ask for it in the typehint. If you haven't asked for it: don't use it. "
Create a web crawler that can find specific sign up forms called eloqua across a website and puts all these forms into a list. I need h lp,"I'm trying to build a website crawler that collects unique links on this site, runs through a list of these collected links, detects a specific logo and iterates through these unique collected links to find certain sign-up forms that adds the URL where that form was found to a new file/list.  I 'think' I'm halfway done. Probably not, because I'm new to this whole web scraping side. I've been able to figure out how to scrape a specific form off of a given URL, but haven't got to the point where I can iterate through the whole site with a given keyword and return the URL value that form was found on into a list.  TL;DR: Create a web crawler that can find specific sign-up forms called eloqua across a website and puts all these forms into a list. I need h@lp "
Python would almost certainly work well for what you want. It's a terrible idea to try to make the engineers use it if they don't want to.,"Two thoughts:  First, the arguments they're making against Python are mostly nonsense. The ""rigor"" argument is just kind of meaningless (really, what does he even mean?). As far as ""hiding"" your code, ultimately your only real protections are legal whichever language you use, and the issue is probably not in practice important. And lots of people use python for all sorts of software.  Clearly your engineer is comfortable with C# and doesn't  want  to use python.  Which brings me to my second point: it's a terrible idea to try to force your engineering department to do something it doesn't want to. They're not expert python programmers, and if they're not excited about learning and using python you're not going to get good results. Your engineers have to have control of their own choice of tools if you want them to be effective. Think how absurd it would be if they told you you had to learn and use C#?  tl;dr: Python would almost certainly work well for what you want. It's a terrible idea to try to make the engineers use it if they don't want to. "
NU's program is great  but since I really really like ML  that's what I would have preferred to have more focus on.,"Well, let's call a spade a spade.  NU's math course is 'Math for modelers' and while it was added after I graduated, to my knowledge, it's linear algebra with bits of Calc 1 thrown in (first and second derivatives).  So it's not like you're coming out of NU as a 'math wiz'.  I recognized I needed more math so I supplemented my grad school with khan academy, etc. for linear algebra.  The stats stuff from NU is definitely nice and they do have more of an 'explanatory' focus more-so than a 'prediction' focus (ML), but I'm personally more interested in ML.  I have coworkers who use p-values and use GLM 90% of the time, but that's just not what I'm drawn to personally.  TL;DR - NU's program is great, but since I really really like ML, that's what I would have preferred to have more focus on. "
All currencies can be seen as a ponzi scheme that promises that doesn't mean they don't work.,"> but then that makes it nothing more than a pyramid scheme.  What you are referring to is a Ponzi Scheme, not pyramid scheme. Also all currencies can be seen as a sort of ponzi scheme. For example take the US dollar. A bank offers loans with $X money it has. It gets back $Y in interest. So the bank declares its loans are worth $X+Y. This $Y is new money that ""appeared out of nowhere"".  The difference between the US dollar and your standard ponzi scheme is that the US dollar is backed by the US goverment. That is they promise they'll always take your US dollars (for tax payments and loans they gave you) and they also promise to regulate banks. Generally this keeps things pretty stable, as it's not in the interest of the government for things to fail (it goes down with the economy). Of course this doesn't always happen, and we get bubbles, the last big one burst in 2008.  TL;DR: All currencies can be seen as a ponzi scheme that promises that doesn't mean they don't work. "
Study in the Netherlands  usually good Study that specific degree  better don't,"I study business in the Netherlands (Rotterdam) and it's pretty nice. They offer great value for the time and money you spend. That said, I wouldn't do such a data science bachelors degree for similar reasons the others mentioned:  1) Your education might be to specific and be out of demand some day. (I once read in a 90s era book: ""just learn HTML and never worry about finding a job again"", and now I hear similar stuff about data science tools). It is better to study more fundamental principles of the field (so either mathematics or computer science).  2) During your studies you might change your mind on your career. You get exposed to lots of new ideas and people. You will find new interests. The broader the spectrum of ideas is the better for you.  3) A bachelors degree in data science will most likely be too general and not rigorous enough. In the bachelors, they can only assume a high school education. The professor can't say ""we hope you taught yourself this when you where 14, because it will be in the test"". There will simply be not enough time to go into depth and more advanced concepts.  4) A lot of new applications in data science do not come from the discipline of data science. For example building great neural networks requires a broader understanding than just the tools you would otherwise use in data science. So if you want to be at the cutting edge you'll need broader knowledge.  If I was you, I'd consider studying maths or computer science (or economics or business, we do a good bit of data analytics, too. You'd be surprised). Check out [TU Delft ]( , they have great programs and are very respected. If you are unsure whether a certain field is for you, do a MOOC in it, that will give you a taste of what it is about.  tl;dr Study in the Netherlands: usually goodStudy that specific degree: better don't "
Options A and B both have their drawbacks and are appropriate for different levels of scale.,"The real problem with solution b is when you have a lot of likes all happening at the same time.  Bieber posts a photo, let's say he gets 100,000 likes in the first 15 minutes, most of his posts have half a million likes.  If writing each like to the DB takes 100 ms, that's 2.8 hours till all those writes have been processed. Probably longer if you account for collisions and having to manage the queue. Now you have to use a sharded counter, so you can keep up with all the writes to a single entity. A lot of db's will cache those kinds of selects, so you don't get hit with these kinds of problems till you hit the kind of scale Instagram deals with.  There is a lot of stuff to figure out there and until you reach scale and know what kinds of numbers you are up against.  TLDR: Options A and B both have their drawbacks and are appropriate for different levels of scale. "
Assembly is a human readable version of  machine code ,"Okay, computer's read binary. 1's and 0's that make up any number you can think of using base-2.  In order to make this quicker, some Computer Scientists came up with hexadecimal.  Base-16, in which most processor ""operands"" use.  So assembly is really just a markup for hexadecimal values that represent things the processor does.  Using these ""operands"" you can move memory, communicate with hardware, or really do anything a computer can do.  Assembly is usually only written nowdays by people who write compilers (as high level languages output assembler), people who write operating systems (like Linux, Windows, etc) or optimizers.  As it's one-to-one with the computer (each line of assembly language is one thing the processor does), it's really not efficient to write anything but the lowest levels of an operating system in assembly.  TL;DR Assembly is a human readable version of ""machine code"" "
An array like object is an object that isn't a native array  but supports being treated like one.,"There's a concept called ""duck typing"" that some languages (Python and JavaScript Coke to mind) are generally supportive of. The concept is that, "" if it looks like a duck and acts like a duck then it's probably a duck.""  An Array-like object is something that follows this principle. It's a regular JavaScript object that has many of the same methods/attributes as an array. Assuming there isn't any type checking - this means that if you pass it to a library that expects an array, then the library won't know the different and will just work as expected with any new object as long as it implements the interfaces that the array has.  TLDR; An array like object is an object that isn't a native array, but supports being treated like one. "
naw  there are just  way more  programmers employed in the field now and less need to specialize.,"> There are fewer programmers with a low-level understanding of hardware because because it's increasingly harder for them to do so.  I wouldn't say it's much harder,  especially since the new generation has easy and ready access to the internet.  Also,  the 80s and earlier had plenty of their own higher level abstractions: Ada in 1980, Pascal p-code in 1973,  Smalltalk in 1972,  and Lisp in  1958 .  I think a lot of this ""new generation of programmers"" is actually the effect of a larger market for programmers as we move forward.  It's more of a common place career,  not so much a field dominated by the Renaissance Men;  eventually it will become as stratified and regularized as most other skilled career fields are.  tl;dr:  naw..  there are just  way more  programmers employed in the field now and less need to specialize. "
customers hate to have to actually tell you what it is they want.  They want you to read their mindsoh  and some are simply corrupt assholes that withhold information.,"I think much of what you say is likely correct, but as a software engineer I'd just like to point out that getting actual specs from the customer is like pulling fucking teeth.  It's ridiculous how hard it is to get them to tell you what it is they want.  You'll sit there and harp on something to get it defined and all they do is get annoyed and tell you that they already defined that...so you go and code it to their written definition and all of a sudden it's not at all what they want because they just assumed a bunch of stuff rather than telling you.  To this you can only shake your head because you spent hours trying to get them to tell you these assumptions but they refused to...so you now have the choice of doing the work for free so that they don't tell everyone how you ripped them off or charging them for design changes because - well fuck - it is a design change because they didn't tell you about it.  Hell, just a year ago my company did a feature that we're having to do a bunch of DCRs to because the vendor (who was being paid to provide us information on a particular market) intentionally withheld information that we needed to make the product correct so they could do their own customizations and be the only ones with that functionality.  Well, they got in to do their customizations and realized their team wasn't skilled enough to do them in a reasonable amount of time and were all of a sudden coming back asking us to write these customizations into the product that they hadn't told us about intentionally.  Fortunately we refused to do them (though we told them they could ask us questions...which they then played dumb and tried to pretend they didn't understand the answers so I would send them 'prototype' code for simple logic so they wouldn't have to do it).  tl;dr; customers hate to have to actually tell you what it is they want.  They want you to read their minds...oh, and some are simply corrupt assholes that withhold information. "
use what works for you  until it doesn't anymore. But knowing a little about everything is good.,"RequireJS is a good tool and will probably be around in codebases for awhile.  Browserify is a good tool that does one thing well, and IMHO is easier to use than Require. If you're writing universal javascript (server/client agnostic), Browserify is handy because you can use the usual require/import syntax on the Node side, and Browserify bundles them in a way the client understands.  Webpack is a more complex toolbox than either. If you start looking into React and live-reloading components, you'll run into Webpack.  I loved RequireJS for AMD projects. I switched to Browserify for writing universal apps because I didn't want to load my script files individually on the client (more requests, more latency). RequireJS can bundle just like Browserify, but requires (bahh) some extra steps.  tl;dr: use what works for you, until it doesn't anymore. But knowing a little about everything is good. "
Stop trying to blindly make everything  easier  or  more developer friendly . This isn't a desirable goal.,"This is optimizing for the wrong things. As a developer, you should take the time to thoroughly test your options  anyway , so saving the developer three steps of configuring their database isn't really a desirable thing.  Unsurprisingly, those who laud this password-less configuration as being ""developer-friendly"" are almost always the same people who don't understand MongoDB's model or why the data model of their application is a poor fit for it.  Programming is  inherently hard , this is just the reality of it. While making things easier  to some point  is a good goal to have, there is a point beyond which you can't simplify things without ignoring important concerns and causing problems down the line.  MongoDB (and similarly hyped startups) are  well  beyond that point, and are actively harmful towards both developers' individual competence, and society at large (through the infrastructure that's built with it).  TL;DR Stop trying to blindly make everything ""easier"" or ""more developer-friendly"". This isn't a desirable goal. "
Ask for forgiveness  not permission is not the worst strategy you can have.  Asking permission to do everything is.,"Naive to the point of liability.  Too many developers are unfortunately content as long as they are not the scapegoat.  Listing the things that haven't been done but should have is asking someone else's permission to do your job well.   When they say no and the project goes wrong it's not your fault. It's the project manager's fault.  I've realized three things, two of them much later than I would have liked.  First, which I've always known but couldn't always articulate, integrity doesn't ask permission to exist.  It does what needs to be done.  Asking permission is theatrics. Or a con.  Second, knowing why the project failed isn't always enough.  Having transparency into whose bad strategies killed the project might give you a sense of closure, but it might not if you were invested in the product.  Don't manufacture situations where the bad actor stumbles publically if you have any control over those situations happening at all.  And the third was pretty hard to stomach because I didn't want to believe it.  And that is that many developers want to appear highminded, but don't have any intention of following through.  They will hand wring about quality and no time and corner cutting when the schedule is so tight that everyone is feeling pressure to ship.   What these people do when there is unexpected slack in the schedule tells you everything you need to know.  Do they sit and wait, do they fix things that people complained about. Or do they create something over engineered that's somehow worse than the code they write at crunch time?  Only that middle group is really interested in progress, even though they all can be heard to support it when it's abstract and not concrete.  TL;DR: Ask for forgiveness, not permission is not the worst strategy you can have.  Asking permission to do everything is. "
People who don't know what they're doing or are absolute dickhead are probably responsible for a good part of the bloat.,"I think it's a mixed bag.  On the one hand there's stuff that you can do in CSS for which you do not need JavaScript (e.g. animation).  But then you also need JavaScript (maps are usually JS based).  But what drives me to the point of being so angry my eyes bug out of my head is when I see a project that loads Angular (some old version like 1.2), Dojo, and JQuery.  Why?  Because the first developer used Angular.  Developer #2 did not really know/like Angular but wasn't going to rewrite he UI, so they implement their changes by dragging in large chunks of Dojo.  Developer #3, who doesn't understand/like either Angular or Dojo, apparently decided that JQuery UI was  the best  calendar pop up out there.  This is a feature only on the ""1024 px or larger"" media query - but let's download the JQuery and all of JQuery UI plus two versions of the calendar popup because IE 8 or lower needs version 1 of the calendar popup.  And of course it's everything is downloaded for all browsers on all platforms.  Plus bootstrap.  Plus custom CSS.  Plus unoptimized images (don't you want it to look good on high res displays?  Use best quality settings for JPEG and make sure it's at least 2048x1024 for a phone that will display it at 256x128.  tl;dr  People who don't know what they're doing or are absolute dickhead are probably responsible for a good part of the bloat. "
one thing at a time  never break the app  write your  in the meantime  code  choose stuff that impacts the most of the app first.,"I've done this before (also migrating from Zend1 to Symfony2) and find that you need to make it worse before you can make it better.  What I found worked best is to change one thing, but pick the one where you get the most impact. For example, change the bootstrap (index.php) to the one fitting your needs. This means you'll need to write some glue/wrapper code which will serve as a mediator between the old and new system (for example, a way for Symfony2 to invoke old Zend controllers).  You do this for most common parts first: router, session, user, etc. Key part is to try and change at most one part at a time, this prevents the app to be broken by more than one factor at a time and to become unmanageable in your head.  At this point your app will invoke old code via new mechanisms and you're able to add your own, proper controllers. I'm guessing your old controllers are full of business logic? This is the point you refactor those, making old controllers use new models.  As you progress, you'll figure out that you can remove old controllers as they're trivial now. Rinse and repeat and your old app should slowly disappear.  TLDR: one thing at a time, never break the app, write your ""in the meantime"" code, choose stuff that impacts the most of the app first. "
The difference between winning a battle and winning a war  of minds .,"Here's the thing.  Let's condemn everyone. Let's submerge them in the blamestorm.  Then what?  Regardless of whose fault this is..  the most chilling effect is on the developers who are watching . Not the developer Azer, but of those who would have been as prolific a contributor as he/she is.  Those who see the political power battle happening now will be much more careful what they chose to open source and, especially, what they give to NPM now.  Aka, everyone's going ""Azer you're doing the 'I'm taking my ball and going home'"" when the more obvious writing on the wall is ""I'm fucking done with this bullshit"" and think of WHY.  tldr: The difference between winning a battle and winning a war (of minds). "
yes  you can start learning with nothing but some very simple HTML   JS   CSS,"> Is it possible to ...  yes.  most of my code is backend (C#, SQL, etc)... but when I need JS, I start with just some core basic JS... minimal DOM manipulation... sure, it's not data mapped (angular/etc)... it's not always browser compatible... but it's just a damn start.  once I've figured out more or less how I want the page to act (from a minimal mocking perspective), I'll usually add either jquery for better compatibility and simpler code, and/or a data mapping framework (angular/etc) if it's a data heavy page.  I'll do similar with CSS... start with the basic elements I want using simple HTML... do some testing with inline styles, extract out into some basic classes, etc.  no, not every damn site / app / page needs a single, compiled, minified CSS file... for most internal apps, the gigabit network is going to be damn fast, regardless of these optimizations... and the time spent investing in them, customizing/reconfiguring to them, and tooling around them... will never be recovered by the business.  quite frankly, there are far easier answers to most of these problems anyway... enable HTTP2/SPDY... put unsecured static content into a directory / URL route / etc that has little or no overhead - no auth requirements, no server-side code to process, just a quick HTTP pipeline to the file... these are CONFIGURATION changes that require almost NO development (other than possibly changing a src path).  Beyond the simple stuff, chances are your browsers' performance is being hindered by non-JS code... backend services, etc... tune your indexes, consider which libraries you use for serialization (specifically its performance).  I'm amazed that people get sucked into this crap (even with a buddy who's right in the thick of it all, I still don't get it).  so, again... TL;DR: yes, you can start learning with nothing but some very simple HTML + JS / CSS "
take it easy. Just bc java doesn't jive with you doesn't mean other people wouldn't like it.,"Look, I'm not a fan of Java or any JVM language. Not one I've found anyways. In my personal opinion, it's feature set lags far behind C#'s and .Net Core is huge for cross platform. In short, I agree with your sentiment.  Having said that, it's just a personal opinion, and there is no need to be a dick about it.  Java undeniably has its advantages.  Neither .Net or Java are the end-all-be-all.  And who cares what people prefer. Every good developer should be able to use whatever tool is best for the job, rather than choosing only one based on ideology or bullshit or FUD.  TL;DR; take it easy. Just bc java doesn't jive with you doesn't mean other people wouldn't like it. "
If you buy a Ferrari don't complain that the maintenance costs are high.,"I think the problem is that the author chose React for his example, that's a top of the line framework (alongside others like Angular,Ember etc) it's specifically designed for people who know javascript very well and want to build very fast very feature rich applications we're not talking about a site about your local museum we're talking g-mail like apps.  So complexity and lots of dependencies of course come with the territory, and really a lot of the things he's complaining about are really things he could have easily avoided.  All the unit tests stuff nobody's forcing you to use them, you want the ""extra security"" you're going to pay for that in development time and complexity.  You want to work with experimental features of JS sure use babel and etc but you're going to pay for that in development time and complexity, and yes React needs Babel but he could have gone with Angular for example.  The real problem is that people who are starting off with js dev and have experience in other languages look around find a article from somebody with 15 years of js experience about how React is awesome and expect to have the same experience in using React like the author who already is an expert in the field.  tl;dr: If you buy a Ferrari don't complain that the maintenance costs are high. "
Yeah I can explain what I did. I did do what I said I did. I just dumbed it down for people like you.,"Well considering I was in the middle of trying to get assignments turned in for finals yeah I didn't take the time to elaborate how I had to implement all the the instructions in software that would be implemented in hardware. I certainly didn't go into how difficult and rewarding that it was when I was able to implement a stack. Yeah an actual stack with pointers that tracked where it was and use it to recursively call functions. I had to send operands to and pull operands out of the stack. Oh yeah the really fun part was getting all the data into and out of Binary. Tracking your position in your instructions and not only having to understand ASM but use it to accomplish tasks.  So you scoff at me for claiming that I created a VM. Well I would like to see your version of it. I would like to see you implement software that can take in assembly instructions convert them to binary then execute that binary. I'm not talking no sissy add subtract calculator. I mean making it so that you can do function calls. Don't take the easy way out and use labels everywhere to control you flow no you gotta put those pointers where your program counter should return to on a stack then jump care free out into another section of binary code with only your stack to get you back. Then have the audacity to not use your registers to pass the functions parameters but put them on the stack also. Pull them off when you get there. Then jump back.  Oh and the kicker of it all, you have to make it multi-threaded.  TL;DR; Yeah I can explain what I did. I did do what I said I did. I just dumbed it down for people like you. "
it's  multilingual  text processing that's hard.      1    Oddly enough  the two most popular right to left languages  Hebrew and Arabic   still  need left to right  numbers are displayed in left to right.,"> it's just that the domain of text people think is simple and it's just not.  I'm going to disagree there -- text can be fairly easy compared to unicode (that's why ASCII is so popular, besides backwards compatibility) -- the thing is that Unicode's servicing of text across  very  different notional boundaries of text-display [e.g. right-to-left and left-to-right in the same string^1 ]requires a lot of thought/complexity that simply doesn't exist if you constrain yourself to a particular language's notion of how text is to be displayed. Another thing that makes unicode hard is combining-characters -- IIRC/IIUC unicode started life with the idea of having a single code-point for every glyph, but then they switched to a sort of glyph-building[-ish] model (because they realized that they didn't have enough space to do that) and we got combining-characters.  Now, a single ""glyph-building"" scheme would itself be simpler than unicode's combining characters. And a simple ""one codepoint for every glyph"" scheme would  also  be simpler than unicode. -- Its unicode's hybridization that makes it a mess.  TL;DR -- it's  multilingual  text-processing that's hard.   ^1 -- Oddly enough, the two most popular right-to-left languages (Hebrew and Arabic)  still  need left-to-right: numbers are displayed in left-to-right. "
IPX  UDP  x86  and  it's all about the Pentiums  baby  ,"EDIT: Skip now if you don't like old programmer reminiscing.  :-)  Was a game programmer and the first time I had a chance to code for  network play was around 1991.  We're talking LAN play, not Internet (because consumer Internet at useful latency or bandwidth was yet to come).  So, hello Novell NetWare on Windows/MS-DOS. (We were still crossing fingers for OS/2.)  The decision then was IPX vs SPX.  It was roughly analogous to the UDP vs TCP decision that followed soon after when ISPs and ""high speed modems"" became a thing.  Anyways, we could code for packet loss and be happy with less than 10% (this is from memory) being lost on IPX (or UDP) and it wasn't a big deal because that bought you latency down into the (gasp!) double-digit milliseconds!  We were working on multiplayer arena/maze-running games so if the cost of lower latency (read: better reviews) was code complexity, we accepted it.  TL;DR: IPX, UDP, x86, and [it's all about the Pentiums, baby]( "
The performance is about the same  but factories allow better encapsulation and private variables.,"Efficiency here is an implementation detail. In JS, all primitives (numbers, strings, booleans, regex, and functions) are immutable. Since a function primitive doesn't change, an intelligent JIT can see that we're accessing the same function and not make a new copy. This is what all modern engines do.  In truth, there are minor differences in performance between prototype and factories for other reasons. The differences are small enough that I wouldn't recommend using classes for anything except very primitive data because of what you lose in exchange.  Consider my implementation vs yours. I  chose  to allow a user to access the number by assigning it to  m.num (I didn't decide to add the  .value()  method until I was almost done). I would normally  not  do this and would instead do something more like  var makeNumber = function (num) {  var m = {};  m.value = () =&gt; num;  m.add = n =&gt; {    num += n;    return m;  };  return m;};  In this case, the  num  value is completely private and I don't have to worry about my users messing with the inner workings of my factory. The standard convention in JS classes is to do something like  this._num  to mark something as private. The problem is that programmers  always  wind up accessing these properties. Once they do, you cannot change the private parts of the class without breaking something. The factory is better because the closure enforces encapsulation while the overhead is pretty much zero (you won't ever notice it until you have thousands of instances).  TL;DR The performance is about the same, but factories allow better encapsulation and private variables. "
when you need to be  sure  your cat gets fed and doesn't starve you  don't  ask your cousin Skippy who goes on crack binges sometimes.,"This is a poor comparison, I think, since with email we're not talking about a device that either   Possesses the capacity, independently, to do something that will cause serious physical harm/death.   ie, a radiation emitter.   Is intrinsically solving a problem where ceasing to function can cause serious harm/death.   ie, something controlling automatic drug dosages or providing oxygen.  When you get into the question of secondary systems it's kind of different. If someone has built a system (or set up their own human process, more likely) that depends on a non-guaranteed and externally dependent thing like email the problem is not that email can be unreliable. The problem is that the designer of the actually critical system included a non-reliable system as a dependency.  tl;dr - when you need to be  sure  your cat gets fed and doesn't starve you  don't  ask your cousin Skippy who goes on crack binges sometimes. "
Solutions that don't work everywhere should be dead and buried.   EDIT  Thanks for the gold ,"Gee, I was at a Microsoft Professional Developer's Conference in 1993 when Kraig Brockschmidt boasted how Visual Studio created 10,000 lines of boilerplate code in your MFC project for COM (or was it called OLE 2.0 then, can't remember) so you wouldn't have to.  And that wasn't enough support, because the interface was so bloated and cumbersome.  As others have noted, the older technology iterations I mentioned above were not exact replacements for each other in quite the same way the .NET iterations might be, and Microsoft did a good job keeping older versions running.  As far as I know you can still program in C right to the WIN32 message loop if you want to.  However, Microsoft DID (and still does perhaps) broadcast the message to the development community that you simply MUST keep with their latest 'new way' of doing things, or you're going to be an unemployed programmer or your company is going to go down the tubes.  Never mind that the existing technology is working fine for you and your company and your users: it doesn't give enough 'lock-in' to the Microsoft ecosystem.  This might be exactly what a company like MS should be doing after all, but that doesn't mean it's actually good for the developers who buy the story.  I stopped working in a Windows-centric world before WPF/WinForms/Silverlight/Universal came into existence, but it seems to me that any developer who followed that path should be pissed off at Microsoft for changing directions so often in rapid succession.  Here's my bottom line, whether programming for Mobile or Desktop, I think programmers and companies should find a cross-platform system that does the job for them and does NOT encourage them to leverage the specifics of Android, WIN32, iOS, OSX, or Linux because those are now -all- effectively niche players in the grand scheme of things.  Python, C++ and Java are more valuable than C# or Objective-C or Swift or Go because they are more generally applicable.  Qt, Unity, Mobile development kits like Marmalade, Gideros, Corona, let somebody -else- worry about the ins and outs of each platform.  No OS holds a 'magic bullet' (other than market share) that you really need to leverage to be a successful company.  TL;DR: Solutions that don't work everywhere should be dead and buried.  EDIT: Thanks for the gold! "
only Sith deal in absolutes  no need to be so dramatic.,"Knowing the difference between service locator and dependency injection is one thing. The next is knowing  when the difference matters .  Have a look at [this simple example]( this is a very simple web app based on Slim 3 and Pimple. Yes, I'm using the service locator pattern in the controller.  So what?  The app is 100 lines of code long, it makes absolutely no difference at all wether I'm using DI or not here.  Keep in mind Slim is a micro framework. Its default setup is very good for writing quickly small applications. The good news is that it's built on container-interop, so you can perfectly replace the container with one that will allow you to build a more solid application.  TL/DR: only Sith deal in absolutes, no need to be so dramatic. "
Just like you don't use  finally   using  clauses for memory in GC  deterministic destruction allows the same thing for abstract resources  like databases  files  and etc .,"> if you really really wanted to do this without RAII  In my example C++ is still using RAII. C++'s implementation of RAII allows for what I said above. Also the issue with the  using  paradigm is client1 & client2 have a different structure when using the database as opposed to the higher client that opened it in the first place. The best way to explain how versatile the concept of deterministic destruction is, is to compare it to memory management. Currently garbage collectors do a good job with freeing/destroying memory, but when it comes to resources e.g. file handles, locks, and etc.. these become second class citizens and require special treatment (e.g.  using  clause in java/C#, blocks in ruby etc..). By adding deterministic & guaranteed destructions of specific variables, allows the variables to be used & treated as any other variable. Which removes the need for having  using  clauses. This allows for simpler code and for things like this  def foo()    database = openDatabase()    doStuffWithDb(database)    return databaseend  As you can see  database  gets no special treatment. Now if we call foo like this  foo()  Then  database  will be automatically closed for us. If we call foo like this  database = foo()  Then the database will still be open and we can continue using the database. This kind of resource management puts memory, and other resources together as 1. It's taking the GC to the next level from only garbage collecting memory, to more generally garbage collecting resources. And the usage of variables whether it's memory based or some other abstract resource based becomes more uniform under deterministic destruction.  TLDR: Just like you don't use  finally / using  clauses for memory in GC, deterministic destruction allows the same thing for abstract resources (like databases, files, and etc..). "
why are people waiting until university to try programming ,"The problem should be moot, since it makes no sense to enter a C.S. program without having programmed before. Especially today, when you can program on the little supercomputer in your pocket, why in the world are people learning the basics of programming in college?  If I were going to an Art College, I sure as heck wouldn't wait to starting drawing until freshman year.  I had 13 years of programming under my belt when I started college, and I started college at 17. I took 3 programming classes in high school, as well as required programming in math class throughout elementary and middle school. I couldn't have avoided programming if I wanted to.  TL; DR why are people waiting until university to try programming? "
Pick the right tool for the job. Immutable doesn't fit every scenario.,"There's a time and a place for immutability - but it's not a silver bullet.  For example, a basic immutable class,  class MyClass {    private $prop;    public function __construct($prop) {         $this-&gt;prop = $prop;     }    public function withProp($prop) {         $copy = clone $this;        $this-&gt;prop = $prop;        return $copy;    }}  Subsequent code can change the object we're dealing with at any point.  $obj = new MyClass('a');$obj = $obj-&gt;withProp('b');$obj = $obj-&gt;withProp('c');var_dump($obj); // prop contains c  Now imagine $obj was being passed around all over the place and constantly being overwritten. Being immutable doesn't really help the debug effort like you described in your video.  If MyClass was actually a HTTP Response object - some parts of your code will be adding cookie & header info, other parts of your code will be adding the response body etc. It'd be wasteful if each mutation created a new object, verbose example;  $r = new Response(200);$r = $r-&gt;withCookie('something','value','exp');$r = $r-&gt;withCookie('somethingelse','value','exp');$r = $r-&gt;withHeader('X-This-Is-Wasteful','value');$r = $r-&gt;withBody($someHtmlOrJsonString);$r-&gt;send();  Instead, it'd probably make more sense to just have a mutable response object that is incrementally mutated into its final form.  If MyClass were a HTTP Request object, maybe it'd make more sense to be immutable. The original HTTP request is an event that has happened. You cannot change the past. So it'd make more sense to be immutable for the same reason that  $_GET['param'] = 'something';  is daft.  tl;dr: Pick the right tool for the job. Immutable doesn't fit every scenario. "
this code is better if you actually want to test the load    import requests while True      requests.get   proxy     ,"Looks interesting, but why are you using a full browser for each request? If you want to make a ""load tester"" (not a ddos tester!) then you want to generate as much load as possible, which means the highest number of requests per second. The overhead of a browser is simply going to destroy any hope you have of a large number of requests a second, so I would advise using something like  requests  or plain  urllib .  If you really really want to be hardcore then a simple  asyncio  script will be able to get the highest number of requests a second (much higher than threads), but it will require a bit more code modifications than just plain requests/urllib.  tl;dr this code is better if you actually want to test the load:  import requestswhile True:    requests.get("" proxy={"" "" "
No  not everyone should be a software engineer  but having a passing familiarity with a book like  this   arguably  is  a basic life skill now.,"I don't think coding is the perfect career for every kid.  I  do  think that a basic understanding of coding is an important modern life skill.  Much like I don't think that every driver should be a mechanic, but I do think that anyone who owns a car should be able to change a tire, check the fluid levels in their car, add fluids if needed, and perhaps replace a light bulb.  It's a far cry from actual mechanical work, but often enough to let you know when something serious is going wrong before it fails catastrophically, and enough to help you get home safely when something minor does go wrong.  tl;dr: No, not everyone should be a software engineer, but having a passing familiarity with a book like [this]( arguably  is  a basic life skill now. "
He does this to get a compilation error if something in the code is wrong.,"you probably wanted to ask: ""what is struct { int:-!!(e);}""   bitfields:   you can specify the number of bits a field in a struct can take using a colon followed by a non-negative number after the field declaration but before the semi-colon:  struct somestruct{    int a: 8;};  now the structure has a field 'a' that holds only a byte of information.   Anonymous fields.I did not know you could do that with C. But it looks like you can omit the identifier in a field declaration inside a struct. So,  int:    instead of:  int a:   -!!(some_exp)   you want to convert the ""some_exp"" from (-ve, 0, +ve) to ( 0, 1)range. So you first negate the expression, to reduce the range to the latter. Then you put another negation to get back the original value.  The minus sign: Note that I said the bit-field length should be non-negative above in (1). It is a compilation error to have a negative bitfield. If the expression evaluates to 0, then -0 = 0 and there will not be a compilation error. However, if the expression evaluates to 1 then the bitfield is -1 and we get a compilation error.  Tl;dr He does this to get a compilation error if something in the code is wrong. "
do some research on those companies and make your decision based on that.,"One bonus with Insight is that they use their network of companies to essentially fast-track you to the onsite technical interview. This means you don't have to do ""cold"" phone screens.  I think the most important thing to consider is the list of companies participating in the program. If you can see yourself working at any of them, do the program and try to get hired at those companies. If not, don't. The companies affiliated with Insight tend to like the candidates they receive and come back for more. They also know that they don't have to test you on every little thing, as the candidate are pretty well-rounded after Insight. This means the interviews tend to be a little more lenient. For a company not familiar with Insight, they'll likely pass you through the ringer regardless of whether you did the program or not.  tl;dr: do some research on those companies and make your decision based on that. "
people are idiots  and you're best served viewing the stock market as a stochastic system.,"But again, you're assuming the trends you see in the real world will have a meaningful impact on the stock price. That's a  terrible  assumption. Stock price is  not  driven by any real world data- it's driven by public perception (which impacts demand for the stock).  The other thing to keep in mind is that simple shifts in stock price- which are going to be caused by purchases and sales of stocks, including your own- will impact public perception. There's a wildly non-linear feedback loop- if you buy a stock, you'll drive the price up slightly, which will change the public perception of the stock's value.  TL;DR: people are idiots, and you're best served viewing the stock market as a stochastic system. "
Cut down on the amount of you have to write. Plus  it was a great learning experience.,"Haromony isn't released yet, nor is the spec final. Using it in production is probably not the best idea. Also, kale will eventually run in the browser as well.. which definitely does not support that type of syntax.  When you fall back to ""old"" javascript, your terse example becomes far more bloated. Enter kale. Also, kale doesn't work on just objects.. it also works on Arrays, which adds more code to your example.  Beyond that, kale also provides a bunch of high level wrappers around standard operations: mapping, plucking, etc. Which, again, saves you from writing more code.  The templates also provide a clear separation from your data and your view. They allow a frontend team and a backend team to work independently, and let these templates do the heavy lifting. They could easily be updated and redeployed completely separate from your application (much like a ""visual view"" could).  They also support easy importing (API changing to be easier) of other templates (much like address above). I also plan to add in a transparent way of handling versioning.  You could achieve all that in JavaScript, I suppose.. but again, if you're not using harmony it's going to be much longer than what you provided above.  Lastly - it was a fun little project / learning experience, which is so much more valuable (to me) than writing a few ES6 functions.  tl;dr: Cut down on the amount of you have to write. Plus, it was a great learning experience. "
This is my first blogging experience  this platform is cool  neat  and make sharing content easier.,"Hey, thanks for answering.  > It might be me, but the amount of non-sense comming from that platform is incredibly high. I just chose not to click these links anymore, it was always just regurgitated trivials.  Well I can assure you that you can find some really hight-quality post on this platform if you take a little time. For example, and this is only for JS I suggest you take a look a [Eric Elliot]( 's posts. He goes deep inside the concept and I really like his writing.  About the regurgitated trivials, if you're talking about my post, I have good reasons to believe that some people were not aware of what this article dealed with. It is of course far from ground-breaking, but if it at least one person learnt something new thanks to me, I'm glad.  > Plus, the styling really does not work for programming subjects.  I do agree with you on that one. The only way I found to embed code was to use a gist ore codepen. The native code style block is very very basic, and really not pleasant for large chunk of code (no coloration at all).  > You really shoudn't put up with that if you think your programming post is in anyway meaningful. By the way, why did you choose this platform. What did it offer you over others?  Well for me it was either that or create my own blog. Of course it is now very easy to create a blog but I really didn't have time to take care of this now. I didn't want to bother set up disqus, analytics, domain, server ... right now. I will eventually in the future. Plus Medium have this kind of recommendation feature, which mean that if your post is liked by some people, more will see it.  And even if it was not meant to be for developer in the first place, it is very easy to have really nice looking post in a short amount of time. I am myself not very experimented with styling, copywriting, font issues and I was glad to have a neat result (speaking on the style not the content) that easily.  TL;DR This is my first blogging experience, this platform is cool, neat, and make sharing content easier. "
without a lot of luck  being a phenomenal dev  getting a well paid job offer and good lawyers your odds are slim. Sorry.,"From experience, this could be incredibly hard without a degree, a lot of money (for lawyers) and dependents (all of which stops you getting an H1B pretty much). A good friend of mine (also no degree and young) needed the signature of Steve Wozniak and Obama's CTO to get in. I shit you not. That said, he was a founder... having a job might be different.  Furthermore, even if you were offered a job and even if you could hack the H1B criteria somehow there's a massive crackdown under Trump currently and they were already rare enough. I'm not sure if it has been implemented yet, but there will be a minimal pay for H1B engineers that is much higher than USA engineers. That means to be hired over there you'll now have to be vastly better than natives (previously H1B was hacked by Facebook and other big companies to bring people over from India etc super cheap... whether or not you like Trump that was a shitty move by these co's). There are other visa types, but again Trump's cracking down on them all.  tl;dr: without a lot of luck, being a phenomenal dev, getting a well-paid job offer and good lawyers your odds are slim. Sorry. "
Telling work to fuck off was  the  best decision I've made in my career.,"> I would love to work somewhere where my boss will stop pretending to be my friend just so they can guilt me into coming in on the weekends because he forgot to tell me about a project 6 months ago that is suddenly and abruptly due this week (actually happened).  Oh god, I feel that one....  had a client onboarding.. it was a shitshow... my entire team (myself included) were so green you'd mistake us for plants....  No support from anyone, at all...   I had vacation planned in advance.. I took the first day, my boss called me up did the ""all hands on deck"" spiel... fine.  Went into the office, found out my boss was  in another state , visiting his family.  I worked that day, turned off my work provided cell phone, went home, fell asleep, and took the remainder of my fucking vacation.  Found out when I got back that they actually sent someone to my house, but I was asleep.  Got back into the office, found my tasks were re-assigned to a co-worker, and kind of drifted for a month with nothing to do.  Took a couple of CBTs, and put me on a career trajectory that has worked out quite nicely.  tl;dr,  Telling work to fuck off was  the  best decision I've made in my career. "
all things you mentioned are resolvable at compile time and have nothing to do with reification erasure or the runtime. Except for one feature that's bad anyway.,"New typing is something that Java should have, definitely. Haskell resolves this at compile time, and is completely agnostic to the runtime. So Java could do it, and it has nothing to do with erasure.  But being able to inspect if a List&amp;lt;T> is a List&amp;lt;String>? That's terrible. You can introduce runtime bugs and wonkiness that way. (More on that below) However, it would be nice to be able to say, ""give me a list of T, where there is some behavior X defined for T."" Scala lets you do that, and it runs on the jvm. Also compile time resolution. The runtime has nothing to do with how shitty the generics are. You can do amazing things with what's already there, as long as the compiler is up to the task.  Inspecting the type inside of a generic is terrible, because that means you can change the behavior of a generic based on an inspection of type. This that if I were to define a reverse&amp;lt;T> function that reversed a something of type T, I could implement it differently for string, Int, etc. and then have undefined behavior and throw exceptions with other types. That's shitty. In scala, you can define this as reverse[T: Reversable] and then you can be happy knowing that only things with the Reversable behavior defined are allowed to get through. And you're still not inspecting types.  Afaik, Haskell does this as well. Haskell does not let you inspect types, because inspecting types is type dangerous. And Haskell was designed to be as type safe as possible. Pattern matching inside of a function is not inspecting types.  Tl;dr all things you mentioned are resolvable at compile time and have nothing to do with reification/erasure or the runtime. Except for one feature that's bad anyway. "
utf 8 string performance would suck for existing code that was optimized for fixed length string performance characteristics.,"Some argue that strings are iterated over from 0 to N most of the time, so a variable-length representation (like UTF-8) would not add much overhead for the common case. You would occasionally increment the index by two or more instead of one. This might be true, but in Java any iterator instance tracking the position would add 8 to 16 bytes object-overhead and another indirection. In contrast, for fixed-width encodings you only need a single int and a for-loop. Because of this, most code working with strings in performance critical situations do not use iterators, but direct index access instead. This (existing and unlikely to change) code would run significantly slower with a variable-length string representation.  tl;dr; utf-8 string performance would suck for existing code that was optimized for fixed-length string performance characteristics. "
It's a cool looking feature  but tneeds a lot of thought on how to deal with touch devices.,"Drag and drop works for me on the ipad.  I think it's a very nice demo and this could be used in very specific cases where no one will ever use a touch device.  For the anecdote, JIRA does the same with their agile plugin and dashboards. And it's just impossible to scroll without randomly moving things when on a touch device. I asked, and people on windows 8 touchscreen enabled laptops are in the same boat. We have huge lists of prioritized items, and every now and then someone on a touchscreen moves something when scrolling.  I guess the JIRA guys thought that at least it was useable on point and click desktop, so ship it! But then it's also the same interface for every other devices, and it screws it for everyone (why bother ordering things if they'll get shuffled every now and then ?)  TLDR; It's a cool looking feature, but tneeds a lot of thought on how to deal with touch devices. "
my contract was not negotiated at all based on my rate  but on the value I was providing to the company.,"I am a data scientist who accepted an equity contract for a startup, on the side from my main client work. I get several offers a week from startups offering some combination of equity, and I usually don't respond to their emails because I don't believe in their company, idea, or team. In this one case, I kept talking with this team and I believed in all their shit-- they had done all their due diligence and more with market research, qualified legal representation on the team, a solid team and a solid plan for getting to an MVP, etc.  So anyways, how I valued negotiating the contract-- I'm not sure there is a clear way to estimate what you should get in equity based on your rate at all. The whole purpose of going for an equity offer rather than cash is to have a percentage stake in the company rather than a fixed figure, am I right? the question is more about ""What share of the company's projected success will be because of my contribution?"" is it 50%? 10%? In my case I negotiated a 2% ownership in the company for my contributions doing basic data architecture. at my hourly rate it would have been approximately ~$10k of work. But if our company becomes worth even just a million dollars, my share will already be worth $20k at that point.  TLDR my contract was not negotiated at all based on my rate, but on the value I was providing to the company. "
test it first  and it depends on what you are building.,"It ultimately depends on your site. If it only affects 1% and hurts 99% then you shouldn't use the approach.  But the most important thing to do in these cases is to test ideas like this. Test, test, and test some more. That way you know if it will improve or not. Don't just use it blindly.  However I do this technique and here are my two cents on it.  In terms of bloat; if you have a fast connection then you typically won't see the bloat. You're only inlining the CSS above the fold. Not all of it. So on a fast connection it is a non-issue. So the only people who notice it are people on a slow connection. They will also thank you for it because content will appear sooner.  But even with a decent connection, on a mobile device you do tend to notice a speedup at which content will first appear on the screen. Even with a decent wifi or mobile connection. Sites can end up feeling like they load up near-instant.  It also depends on your website. One of the notorious issues with big JS application like websites is that you often end up with a 'loading' screen on the front. Either unintentional (it just takes a while for those big JS/CSS files to load), or a deliberate one. This is contrary to more document like websites.  Inlining above the fold CSS is one trick that can allow you to have JS applications that load like a regular page, and at the end have that huge framework.js + application.js + css files + css framework + whatever. All loading in the background after the page has appeared for the user.  tl;dr; test it first, and it depends on what you are building. "
Simply converting the same shitty code to a newer opengl will honestly be a waste of time.,"There is nothing wrong with the fixed function pipeline if that's all you need. Also it's not slow by any means, the fixed function pipeline is just translated to shader code in the driver. What is slow is immediate mode. You can speed this up compiling immediate mode to display lists, or better converting the code to use VBOs. But simply doing that is not really enough. To get good performance you actually have to think about how the data is laid out, and batch as much as possible. Setting up the entire rendering pipeline to draw single quads every time is going to be very expensive in the driver. Programs like this are almost always excessively state thrashing and stalling the driver by constantly querying the viewport size or current blend modes, when these should be cached inside the app.  TL;DR Simply converting the same shitty code to a newer opengl will honestly be a waste of time. "
the web stack sucks because its operating environment sucks.,"> So why are we doing the same thing (but worse) with JS and > browsers when these kinds of feats were possible back in 1979?  Xerox is a desktop environment. The web is not.  Remote code execution is great when running in trusted environments. It is a lot more dangerous in untrusted environments, like the Internet. Remember ActiveX and Java applets?  Also, the Xerox desktop environment worked on a known set of custom-made hardware. The web was built around HTML because it was up to the client how to render it, from text-only browsers to graphical web browsers, full computers to mobile phones. Although the rise of JS-driven SPAs has somewhat defeated this purpose...  TL;DR - the web stack sucks because its operating environment sucks. "
it's a fantastic tool  but you still have to choose the best tool for the job.,"I'm not sure there's a specifically bad part of Rust, but there are situations where it's a bit overkill. As it's built as a C/C++ alternative, it's fantastic for embedded systems, and there's undoubtedly going to be some fantastic libraries (for games, web, etc) written for it. However, at its core, its a low level level system language, so it's often overkill for smaller scope tasks such as prototyping games or building small web applications compared to, say, Action Script. It's also a very new language, so there aren't many fleshed out libraries written for it such as JavaScript's Cocos 2d or Python's Django.  tl;dr it's a fantastic tool, but you still have to choose the best tool for the job. "
 it depends    or  only a Sith deals in absolutes ,"I was a programmer in the 2000s.  For that matter, I was a programmer in the 1980s...  And in many circumstances, I totally agree with Postel.  I'm currently dealing with replacing an existing ""Rest"" (read ""json/ API, and we don't want to force any of the clients to change (it's a long story).  Now, we'd love to clean up some of the madness in the API as we go.  For instance, there are parts of the JSON payloads that no-one is likely to be using - things like a combination [error code / error string] with spelling mistakes in the error string; { country:""AU"", name:""Australia"" } is another example.  We'd also like to be able to add a few fields, for newer clients - the old clients shouldn't care if there's unused fields, surely?  But several of the clients have never heard of Postel's law - they read all the JSON, even bits they don't care about, into statically typed objects, and they break if we change anything.  Yay.  I do think low-level protocols like IMAP are obviously places where it shouldn't apply; but in a lot of cases it still makes perfect sense.  TL;DR: ""it depends"",  or ""only a Sith deals in absolutes"" "
 20ms is maybe acceptable for a monolithic application  but is too high for sub milliseconds services.,"~20ms is perfectly acceptable for a user-facing service...  ... however in a micro-services architecture it's a nightmare. If you have a chain of 5 services, and each of them hits the ~20ms mark, then suddenly your latency jumps from the median ~5ms to ~100ms (x20!). Throw in Amdhal's Law, etc... and in an architecture with a multitude of services this soon become a problem. You can somehow attempt to work around it by sending requests in duplicates (for read-only requests) and take the first answer, but that only lowers the chances of hitting the high-latency case, not eliminate it, so your 90th percentile latency goes down but the worst case latency does not.  TL;DR: ~20ms is maybe acceptable for a monolithic application, but is too high for sub-milliseconds services. "
It's so easy to make the problem harder   assuming  you knew that it's a problem  that it's easy to make it where the government cannot afford it.,"The article's estimate was that it cost multiple hundreds of millions of dollars to crack one prime. Go to 10 primes and it's multiple billions. Go to 10,000 primes and it's multiple trillions. Go to 1,000,000 primes and it's 100 trillion dollars, on the order of the entire GDP of the United States for an entire decade.  And it wouldn't be that hard to generate a million primes. It takes maybe 1 minute per prime, so my machine at home could generate a million of them in about 2 months. Each one is 128 bytes, so the storage required would be 128 MB.  TLDR: It's so easy to make the problem harder ( assuming  you knew that it's a problem) that it's easy to make it where the government cannot afford it. "
stop making shit up. Also UTF 16 is negligibly computationally less expensive. Very  very negligibly.,">Windows is UTF-16, so is Java and C#. C++ supports UTF-16 via wchar_t (or the u prefix for string literals).  That's because Windows is very poorly built.  >I'm shocked at how simply people accept a blog post as canon. You read one opinionated blog which happens to coincide with the encoding you use on a regular basis because you develop web applications, and then the answer is set in stone, even though the opinion is based on latin character sets (and the saving of up to one byte per character in western regions) while ignoring the performance gain of UTF-16 and about 1.5 billion computer users.  I am a professional software developer, and from experience I know that UTF-8 is superior to UTF-16.  >Saves 1 byte per character, if you just for a second stop pretending that the majority of the world speaks english.  If you had actually read the blogpost you were referring to, you would know that the VAST majority of text transferred over the internet is transferred in such a way that it is most efficiently encoded in UTF-8, because all spacing, HTML markup, etc. is more efficiently encoded in UTF-8 and outnumbers the amount of text in languages that are more efficiently represented in UTF-16.  So sure, the small amount of text on a Chinese, Japanese or Korean webpage might be more efficiently represented in UTF-16, but the HTML document that your browser receives is about even either way, the HTML page for every language written in a character set representable on the BMP (Cyrillic, Latin character sets, among others) is MUCH smaller in UTF-8, and all the text-based assets like CSS and Javascript accompanying the page are more efficiently encoded in UTF-8.  TL;DR: stop making shit up. Also UTF-16 is negligibly computationally less expensive. Very, very negligibly. "
As someone who's TA'd a lot of uni modules  I think this teacher is in the right  provided of course he's sure he knows who the poster is ,"A lot of people will say that the teacher is overly harsh in responding like this, or even that he's a bad teacher. That using stack overflow is how professionals often solve their problems, that the kid had shown obvious effort to complete the work, and just needed help getting unstuck, etc etc.  I don't really agree with this, for several reasons:   His actual issue was a null-pointer exception. In terms of learning and improving ""Whats a null pointer exception anyway?"" is a reasonable Stack overflow question for an absolute beginner. ""whats wrong with my code?"", in this context, probably isn't.   This appears to be a take-home mid-term. An individual test. However some might feel about these kinds of tests and how accurately they measure 'real' programming - it would still be unfair to the other students if that was the case.   This particular student seems to have put in at least  some  effort with his mid-term, and one could argue that a little help is just getting him unstuck - but the other 100 students in the class can just google a problem-specific term, find this code (in his original edit he just dumped  everything ) and copy it. How is the professor supposed to know who did what first? It's hard to punish everyone. That's not his job.   He seems to have already offered extra help and tutoring himself to students who are stuck.   It sounds as if the class had a discussion on this (using help forums like stackexchange) and they  ""agreed""  to not use them, because of all of the above reasons. and the kid went back on his word. Straight up honor code violation.    tl;dr As someone who's TA'd a lot of uni modules, I think this teacher is in the right (provided of course he's sure he knows who the poster is) "
Your bad developers suck and they don't even know it if you're just working around their shitty code.,"While your comments are relevant and interesting, I just want pick out something you said:  > When another team at work needed something similar, I wrote a shell script in an hour that accomplishes the same task and actually does produce data in the correct format with the right encoding.  What you're doing is working around a problem rather than fixing it. -- don't do that.  I just started on a team that did this with one of their bad developers, creating work-arounds for his bad code rather than dealing with the problem directly.  They didn't teach him better or manage him better, and after they finally fired him there are so many kludges in the code base.  Kludges that I now have to decipher and deal with, in a codebase that was otherwise mismanaged for years.  It's making it just that much harder.  You may object that you have to get a job done and it's so much easier to reimplement existing solutions, but you're really doing a disservice to yourselves and those who may replace him and you.  You're also going to eventually release your bad developers back into the wild where they may ruin another codebase unchecked.  Tldr; Your bad developers suck and they don't even know it if you're just working around their shitty code. "
You need domain knowledge about what you're trying to model. Likely most of my predictions will be proven wrong after researching this area.,"Generally when you have Machine Learning problems, there's a systematic solution. I'll try to give you a starting place. From intuition we know that clicks are converted when a person is intrigued by an advertisement. Let's analyze each feature.  Impressions:  The more impressions, more intrigued people will see the advertisement. Therefore Clicks/Conv is linear with impressions.  Cost Per Click:  The higher the cost per click, the more valuable the advertised item. If it's extremely cheap, the item isn't highly valued by most people. If it's extremely expensive, the item costs too much for most people (not always true, but a good example is mortgage ads = expensive). I'd say this feature could be modeled Gaussian.  Cost:  I don't know what you mean by Cost. If it's cost of the advertised item, then I'd treat it like above.  Now you build a model using the ideas above. I haven't worked with ensemble models, but it shouldn't be difficult. I've worked with Neural Networks so in my case I'd model each feature as described above and apply back-propagation to minimize the clicks/conversions training error.  tl;dr : You need domain knowledge about what you're trying to model. Likely most of my predictions will be proven wrong after researching this area. "
only compare yourself to older versions of yourself  and never stop learning,"It's called Imposter Syndrome and if you think you have it, you have nothing to worry about.  When you realise there is always someone better than you and always someone not as good as you, you're free to learn what you need as you go.  You're also working for yourself. I've found that people are their own worst client, as we hold our work to a higher standard than we would if we were just paying someone else to do it.  This feeling is harder to work around, but if you accept that you're a beginner and realise that ""sucking at something is the first step to being sorta good at something"", you might be less harsh and demanding on yourself.  No-one expects a newbie painter to paint the Mona Lisa for their first painting; more like paint this cylinder and ball and cone.  Make sure you manage your expectations of yourself realistically.  Once you've built something, build it again with what you've learned. Programmers rarely build something right the first time. Experienced programmers just jump closer to the right solution when they start.  The best way to improve yourself is to  Read other people's code . Like for example, when you're ready, learn the Laravel framework, and even if you don't use it for your own work, you'll get a feeling for some best practices and approaches to certain problems.  Tl;dr only compare yourself to older versions of yourself, and never stop learning "
stop whining and lying. No one gives a shit and everyone knows that you were banned for self promoting your garbage  tutorials .,"All anyone did was give you proper criticism. But since you have delusions of grandeur and get butthurt at pretty much anything, all you did was accuse us if being part of a giant conspiracy and then whine and bitch just like you are doing now.  Your lessons were crap, showed a complete lack of understanding of  basic  software development principles, and worse, you wanted people to  pay  for that pile of garbage. You also wall around calling yourself a PHP expert when it's painfully obvious that you're a low-level amateur at  best .  It's not your skill level that pisses people off. It's the fact that you refuse to accept it, refuse criticism and pointers on how to improve, and that you try to get people to pay you to teach them your bad practices.  tl;dr stop whining and lying. No one gives a shit and everyone knows that you were banned for self-promoting your garbage ""tutorials"". "
It's easier than you think  and the benefits are enormous.,"> Now along comes learnyouahaskell, and tells me compliance with the Monad laws needs code review by a human. On one level this doesn't bother me, but on the other, it has me wondering, ""then what is referential transparency? Why would I want to arrange all my code in this way?""  Seems like this has two parts:   You rarely have to review monad implementations for satisfaction of the monad laws when using Haskell or scalaz because either you rarely write your own monads (you already have lots to choose from), you ""write your own monad"" by building a  free monad , or you use something like [Discipline]( to do property-based testing of your monad implementation. Most monads I've implemented have been with scalaz's [FreeC]( so I'm on easy street.  ""Why would I want to arrange all my code in this way?"" Because it makes reasoning about your code without running it almost trivial. Because composition just works. Because parallelism  is  trivial.   tl;dr It's easier than you think, and the benefits are enormous. "
Sublime Text is dying  skip it and download Atom if you're going for lightweight. If you want full featured  pick WebStorm.,"I personally use WebStorm for my Javascript coding. It's amazing. However, it is non-free and that is a turn off for many people. For those people, I would highly recommend Atom. Yes, it's just a text editor at first, but it includes support for things like basic autocomplete, syntax highlighting, and multiple selections, and with plugins can support eslint, JSX, TypeScript, and more. Although I prefer WebStorm for Javascript, I still use Atom extensively because it has the best Rust support (via plugins) of any editor I've found to date.  P.S. I used to use Sublime Text exclusively but I would no longer recommend it. Development on it has been stalled for upwards of a year, with Sublime Text 3 in perpetual beta. ST is a bit snappier than Atom, but Atom is still faster than WebStorm, and ST does have a few plugins that are missing or underdeveloped in Atom, but Atom is a new editor and given time its plugin library will almost definitely surpass ST's.  tl;dr Sublime Text is dying, skip it and download Atom if you're going for lightweight. If you want full-featured, pick WebStorm. "
this has been defined  I would consider it a programming language,"From a CS point of view, everything is just an extension of just bits, (1 | 0) which is manipulated through logic gates alongside different registers which are interfaced and manipulated through machine code.  That machine code is generated by some compiler, say C, but we consider C++ to be an extension of C... Well actually, many programming languages have interfaces with C even though we consider them their own languages. Some other examples are C, C++, C# which all build off of C, Java, Javascript (not a true programming lang.), Node.JS, build off of java, and Ruby, Ruby on Rails.  The idea of a language is more of an abstraction which normally refers to a programming language in which someone can learn to write code more easily than writing op codes all day and performing memory management, etc.  TL;DR this has been defined, I would consider it a programming language "
thanks for the feedback   I agree that this legislation is poor.  I attempted to make it seem slightly better.,"One of the co-authors here.  First, I want to say that I agree with many of the comments being made here.  The article comes from two CS educators who are happy to see any movement made to support CS curriculum (of which programming is only one part).  I believe that Computer Science should be a course onto its own that people see as an important topic for everyone, just like other basic math and science topics.  However, your reactions are actually similar to my first reaction to this legislation - I had no idea so many people would agree.  I do heavily dislike the erosion of natural foreign language.  As many mentioned here, while languages share properties, there are very different learning objectives between the topics.  This was an attempt for me to find the bright side, even if it is misguided.  A strange side-note to make: I am told by those who support this legislation that no one is actually making the equivalence between the foreign and programming languages as satisfying the same credit.  This is more like when a school allows you to take a Music credit to satisfy a PE credit due to overloaded curriculum.  I not only believe this logically contradicts the text which passed, but I dislike the precedent that one credit in one thing can satisfy something completely different.  It makes me wonder what the point of having categories for credit is.  TLDR; thanks for the feedback - I agree that this legislation is poor.  I attempted to make it seem slightly better. "
chosen with confidence  Chartier courses  3 of them  are the most accessible  start with them.,"No, it is not. They are arranged by topic, and then by a suggested order by topic. The overall numbering is not indicative of anything comprehensive, other than a loose advancing of topics which build upon others.  Any of the topics taken in the order presented (there is only a single calculus) should be OK for most anyone. As mentioned before, the Chartier courses (mostly linear algebra, but also the Math is Everywhere course) are very accessible: he retains attention and presents concepts in a very understandable manner. If I were pressed, I would say that the most ""uneccessible"" courses, if you will, would be the logic course and the 2 ""advanced"" selections, along with calculus (this is a generalization; most people are not calculus-friendly).  If you are interested in doing all or many of the courses on that list, I would genuinely suggest starting with the one the furthest up the list and working downward, since, while it is not really a suggested ordering, the topics would best be approached in that order, with the exception of perhaps swapping the statistics group with calculus (that would be a very minor change, however). I put a lot of time into curating the list and finding complementary courses, and looked through an awful lot of MOOCs for it. I think it's pretty representative of the best of what there is available for the given (introductory) topics.  TL;DR: chosen with confidence, Chartier courses (3 of them) are the most accessible, start with them. "
if anyone is interested  and they are a experienced Python developer still send me a message. Thank you for the input  and I will be back with a prototype.,"I think the best approach for solving this discussion is to finish the prototype and then come back. Once that is done, the code will speak louder than the theory I can talk about optimistically here.I do appreciate the input, but I am going to kindly go on.There are many many ways to obtain perspective based 3D graphics, and not all of them require the use of thousands of parallel GPU cores.As for models not being polygons instead on importation they would be converted to high density point cloud data. So people who enjoy using preexisting programs will not be thrown under the bus.Why am I using Python, or even doing this at all? Well because Python is a great beginner language, and when I started programming I used to browse open source projects for inspiration. In my opinion the great documentation of well cared for open source projects not only teach practical knowledge of the code, but also offer a good lesson in theory. The slower speed of Python is something I am fully committed to work with because of all the wonderful features the language provides, along with its astounding readability.I think the modern graphics pipeline is far to complex for its own good, we have come to a point where the capabilities of a $300 CPU are astounding. I refuse to believe that the only way to create useful 3D graphics is based on the established techniques of brute forcing calculations alone.TL;DR if anyone is interested, and they are a experienced Python developer still send me a message. Thank you for the input, and I will be back with a prototype. "
I suck at programming and compiled languages let me find my mistakes faster,"It indeed was a not very serious quip about untyped languages. There are indeed typed languages which can be interpreted and untyped compiled languages although the latter seems a bit hairy to implement and much rarer.  I personally prefer compilation either way to figure out where I goofed up without having to run code but that is obviously not a huge problem.  About typed vs. untyped: Typed languages can remove a couple of error classes. In my admittedly limited experience with untyped languages this leads to a bunch of additional unit tests to check the same things. Again, obviously not a deal breaker, but I know that I personally will fuck up eventually and like to have as many guaranteed security blankets as possible.  TL;DR: I suck at programming and compiled languages let me find my mistakes faster "
You should go for psr4 directly and try to develop new features the modern way right away using some glue code to implement them.,"I feel with you as I'm working on spaghetti code with functions, includes and globals all over the place too. If the ""external"" classes aren't auto loaded right now I'd just go with psr-4. That's what I'm doing right now. But this means that you still have to include the non psr-4 compliant classes manually, just mark them for refactoring or exchange them with another package. As for new features I'm developing them as own ""services"" (at least when it makes sense to do so). It requires a little more glue code because of the mess which still exists but later on you'll save a lot of time and it makes it easier to test the new features. Didn't read Paul's book (yet, which is on my personal todo list) but watched one of his talks on modernizing legacy code which already helped a lot to get started.  tl;dr: You should go for psr4 directly and try to develop new features the modern way right away using some glue code to implement them. "
If it's your first time building a safe  maybe show it to a locksmith before you store valuable stuff in it.,">How do we become good drivers if we aren't allowed behind the wheel?  By doing it literally  anywhere  instead of production; bad crypto only hurts if you're using it to protect something valuable.  This is why we do Driver's Ed in an empty parking lot; Driving is hard and the consequences of making a mistake are significant, so we do this in an environment that allows us to minimize both.  However, this doesn't really help when little Jimmy decides that he  doesn't need Driver's Ed, steals his uncle's big rig, and crashes it into oncoming traffic on the interstate.  knows enough about crypto and how could a Purity suggestion possibly break this library, I'll just commit it.  TL;DR; If it's your first time building a safe, maybe show it to a locksmith before you store valuable stuff in it. "
You're right  but I don't think it warrants pointing out,"While your comment is appreciated, I feel as though it's overly-pedantic (I too have seen Rob Pike's presentations on the subject). This module makes use of the term in the same spirit as node control flow libraries:   (arrays of coroutines are ran in parallel) The module makes no reference to ""parallel programming"", just the word parallel - which can be easily understood as a contrast to running operations in series.  >  In order to do that you would have to be splitting a single task across multiple threads in order to speed up that execution, which is what I did with the webhamsters library (works best in chrome).  Which still isn't parallel execution unless your kernel distributes it as such, on a multi-core machine or VM. A t2.micro running chrome might not see see any performance benefit, as the operations really just run concurrently. These types of discussions are rather mundane, as you can trivially nitpick implementation details, since node devs work at such a high level of abstraction. While your webworkers might seem to be running in parallel, is v8 distributing its Isolates across multiple threads? And are the Isolates that run your web workers evenly distributed across cores? If not, it's still not parallelism, just concurrency.  tl;dr: You're right, but I don't think it warrants pointing out "
Find joy in solving problems  or drop your career.,"The problem is you. You're not owning your projects. You're not choosing to make your product better. You're not innovating. You're not building quality.  You're not improving your own skills.  You're just shitting out whatever you need to, to get to the next boring task. I'd be tired too, if I spent 8 hours a day doing something that gave me no satisfaction. Where is your pride? Where is your sense of adventure? Either pick yourself up and start looking for ways to make your project better, with conviction, or find a new career. If you can't find excitement in engineering and architecting beautiful code, you shouldn't be doing it. You'll always be a mediocre developer at ANY job, wasting everyone else's time, and worse yet, your own.  tl;dr - Find joy in solving problems, or drop your career. "
Downvoted because I think you're wrong  removed down vote. I try to share my knowledge and feel that is a common theme of the subreddit.,"I down voted because I disagreed with your original statements. But with keeping in the spirit of OP, I removed the down vote. That being said, I feel rather than down voting and that being that, that some discussion should be in order, an explanation of sorts.  I've never once felt that developers don't want to share information. I think that a majority of posts in here are information sharing. The majority of my posts here or me sharing my personal blog articles. I'm sharing my information. I would say that a lot of /u/sarciszewski original posts are about information sharing as well. My comments on this sub that I'm most proud of are all related to information sharing. I want people to know what was worked for me and what wasn't. It may save other people time. I think that's the case with most comments here. There are less interesting troll comments, but there is some really good information to be found here.  You bring up /r/phphelp. In my mind this sub and phphelp serve completely different goals. I rarely check /r/phphelp but would liken it more to StackOverflow in it's simplest form,  I have a error how do I fix it . I come to this sub for topics like the one referenced by the OP.  I would say that most people are here for information sharing and information acquisition. Sometimes people end up being short, but only because they feel that they've answered the question numerous times before. I think that leads to a bad experience, but I understand the logic. If you came to me and said hey did you know about $_GLOBALS, it's awesome. I'm going to say that's a bad idea, but I might not go further since a google search would most likely read to better explanations. If you ever see me doing that, feel free to ask for more information. I'll generally try to go to a greater depth of explaining my rational.  tl;dr Downvoted because I think you're wrong, removed down vote. I try to share my knowledge and feel that is a common theme of the subreddit. "
Ajax and maybe a bit of C   in the future depending on the application.,"I'll start by saying that I'm currently a student pursuing a career in this very field. In the class I'm taking, we've been learning a great deal about html, css, and a variety of js libraries (React, jQuery, etc). From what I've gathered, a career can certainly be at least started on html/css/js & its libraries, but we've also been going over something called Ajax.  I've inquired with my instructors about C++ , but was told that if I ever do use that in a web development career, it would be far into the future.  I'm bad at explaining , so here's a really informative link about that.  TL;DR = Ajax and maybe a bit of C++ in the future depending on the application. "
I use annotations for convenience  and my entities are lightweight and de facto only configuration. I wouldnt do this in a library type project for obvious reasons.,"I use annotations most of the times. I know it couples the entities with the ORM tightly, but the framework I am using does this too, so I see no reason to pretend that there would be any gain by decoupling them.  On the other hand, my entities usually have only getters and setters and no real business logic. So in fact it’s easier to replace them with, for example, propel models altogether.  So instead of:  class User {    public function getYoungestChild() { … };}  I’d rather have:  interface HasChildren {    /** @return ChildInterface[] */    public function getChildren();}class User implments UserInterface {    …}class YoungestChild {    public function getYoungestChild(HasChildren $input);}  As for the differences between Yaml, XML and PHP formats: get a decent IDE to autocomplete each of them and a framework that will cache the parsed configuration, then you can use your personal favourite without any drawbacks.  TL;DR, I use annotations for convenience, and my entities are lightweight and de-facto only configuration. I wouldn’t do this in a library-type project for obvious reasons. "
Learn the language basics  roll up your sleeves  get your hands dirty.,"The thing about books is that they're so structured. This is perfect for learning, but everything is nicely packaged with a bow. It's like programming back when you were in college compared to your first real gig.  I'm not saying that books don't have their place. I refer back to a couple of reference texts often enough. Once you get the basic structure and syntax, however, move on to projects. They don't need to be anything extravagant, but do something that will push your abilities.  A personal example I can think of is callbacks. When I first learned about callbacks in JS, I thought I understood them. I couldn't have been more wrong. I had to write a callback to update page info after data had been calculated as sorted. The issue was that it wasn't some simple, clean scenario that matched the structured tutorial. Many months later, I had another callback situation with jQuery. Again, this was nothing like the tutorial nor the previous situation.  TL;DR: Learn the language basics, roll up your sleeves, get your hands dirty. "
Yes  but only in the sense that anyone can sue you for anything.    ,"> If one built a site using React that competes with Instagram, for example, would one be in violation?  If you build a site using any technology that infringes on any patents that any company holds, or which their well funded legal team can semi-plausibly argue infringes, then yeah, you can be in for a world of legal bills.  So...sure, maybe your React-powered Instagram-clone might infringe some patent Facebook might have, who knows?  But the key question is:   What are you going to do about it?   You can't just say ""oh, I'll use Angular to make the site""; you have zero reason to think it would infringe any fewer patents.  Worse, now you don't even get a conditional patent grant.  Not that you have any way of knowing if the patent grant was valuable of course.  How could you?  You don't even know what the patents  are , or even who holds them; nothing says that Facebook is the one with the dangerous patents.  Maybe it's IBM.  Or Oracle.  Or some patent troll.  Or maybe they don't even exist.  I  personally  wouldn't say that the React patent grant has much value, but it certainly doesn't have any drawbacks.  Software patents are terrible, and they impose real risks, but the only way to avoid them is to never write any code ever.  TL;DR:  Yes, but only in the sense that anyone can sue you for anything.  :) "
I love C   as a language but I end up using Python for most of my projects because it is so much easier to use.,"I love the modern C++ language but I hate the ecosystem and the tooling around it.  I would use C++ much more in my quick throwaway project but it is always a hassle to setup a new project and it is especially cumbersome to add some random libraries.  In Python I just do ""import libraryxyz"", type a few lines of code and then do ""python script.py"".  In C++ I have to either fight with make scripts, or some obscure cmake features, or Visual Studio or all that other crap.  I really wish there would be a selection of common C++ libraries (zlib, ...) which I can just import into the code with a single line and I don't have to thing linker settings and paths and adjusting my makefiles and all that shit.  I also wish there would be a simple ""runcpp test.cpp"" command for small projects which takes the main file, analyzes the dependencies, autoadjust all the settings and then just compiles and run it without all that make crap.  tldr: I love C++ as a language but I end up using Python for most of my projects because it is so much easier to use. "
Post is mainly for  manipulating  and  providing  data  not to  request  it. You  post  something to the server  you don't  get  information from it.,"> In a shopping cart results page, you apply the filters, which generally is a POST call  Then you are using HTTP wrong. Yep, you can generalize like that. And sure, working with someone else's crap is a sad reality, but it's still a valid comment.  Look at section 9.5 of RFC 2616 (HTTP/1.1)  > The POST method is used to request that the origin server accept the   entity enclosed in the request as a new subordinate of the resource   identified by the Request-URI in the Request-Line. POST is designed   to allow a uniform method to cover the following functions:  >    Annotation of existing resources;>     Posting a message to a bulletin board, newsgroup, mailing list, or similar group of articles;>     Providing a block of data, such as the result of submitting a  form, to a data-handling process;>     Extending a database through an append operation.  > The actual function performed by the POST method is determined by the   server and is usually dependent on the Request-URI. The posted entity   is subordinate to that URI in the same way that a file is subordinate   to a directory containing it, a news article is subordinate to a   newsgroup to which it is posted, or a record is subordinate to a   database.  >   The action performed by the POST method might not result in a   resource that can be identified by a URI. In this case, either 200   (OK) or 204 (No Content) is the appropriate response status,   depending on whether or not the response includes an entity that   describes the result.  TLDR:  Post is mainly for  manipulating  and  providing  data, not to  request  it. You  post  something to the server, you don't  get  information from it. "
clear communication is vital to our jobs  don't shirk responsibility,"I don't think that 'Dumbing yourself down' is the right way to be thinking of these situations. Part of our job is to communicate insights clearly and effectively. Like others have said, it doesn't matter if you're presenting ROC scores, box plots, or percentages.  Next time, instead of pointing out a box plot, look your boss in the eyes and say 'frank, I believe we have an opportunity to improve productivity"". Now that is something Frank will want to engage with, ""oh really why is that Fred?"", ""it seems Josh spends 25% of his time smoking cigarettes"". Follow a similar approach in written reports, put the insight and the 'so what' up front, and refer to more detail deeper in the report if people care to research.  Ideally you should be getting to a stage where you can say the insight and be trusted.  Tldr: clear communication is vital to our jobs, don't shirk responsibility "
The solid Windows 10 core seems to have been made by a different group than the clueless group that made the UI  and it shows.,"Can confirm. Switched to Windows 10, cursing my way through. Even though the core seems to be stable -- no crashes or anything funny -- the desktop seems to have come out of the bad end of an outsourcing deal. Obvious UI blunders, multiple buttons doing the same things, different error dialogs for the same error depending on where you click, etc. They replaced simple but exhaustive and thought out dialogs from Windows 7 with some Duplo-like oversimplified abominations with no options that seem to have been catered to touch interfaces and nothing else. Wi-Fi options are a cruel joke. So are network settings and anything in between. Drivers update themselves without your slightest involvement or control and you have to be a kernel hacker to turn it off or impair it to your liking. Since UI is what makes or breaks Windows, for better or worse, you can't help but chuckle and cringe while using it.  TL;DR: The solid Windows 10 core seems to have been made by a different group than the clueless group that made the UI, and it shows. "
HTTP2 is low level  and it's not a small matter in term of upgrade.,"Browsers and webservers are not any kind of software. Those run on multiple OSes, and are always competing against each others. So if you roll out something like HTTP2, you have to make it works 100% on  all  browsers and webservers. Browsers won't really bother fixing stuff until it's deployed everywhere. Also, bugs happen when the thing is actually live on many websites, because you can't test every possible problem in a development environment, especially when it involves networking and sockets, which is one of the most difficult kind of programming.  Also, when the industry starts using HTTP2, browsers and webserver softwares don't want to be responsible for the hiccups, so ultimately they're very cautious.  TLDR: HTTP2 is low level, and it's not a small matter in term of upgrade. "
The problem isn't solved. The partial solution of preparing abuses an API designed for a different purpose.,"Ok... I know about prepared statements, I guess we can't get this debate above giving each other 101 lessons.  Preparing is intended for query  reuse , it's only coincidental that its value placeholders can be beneficial to security. If you want to see value placeholders done right, see the pg_query_params(). You listed 3 examples and missed the only provider that gives us a proper API for this. Telling. Preparing to get access to placeholders is a hack more in support that the SQL APIs are desperately out of tune with developer needs.  Also, aside from binding  values , real-world apps often have to dynamically insert  identifiers  and  expressions  in a query (limit, offset, order by asc/desc etc.). Placeholders don't help you a bit here.  PgSQL is again the most intelligent of the bunch for at least providing pg_escape_identifier(). What's the PDO etc. version of that? Nothing.  tl;dr  The problem isn't solved. The partial solution of preparing abuses an API designed for a different purpose. "
People don't all run on the same schedule and have the same work habits and preferences.,"Interruptions hurt, but so does being anti-social.  I actually do not ""come in fresh"" ready to crank out code early in the morning.  Exactly the opposite, I come in like a zombie and hope that I can get some coffee and have it kick in before standup.  Writing code is about the last thing I'm able to do, yet if I'm able to get involved in discussion about our server architecture or data model, I'm fine.  For this reason I generally try to schedule all my grooming and design meetings in the morning, which gives me long blocks of time to get code written in the afternoon.  I don't get the ""when they leave at night"" bit, either.  When I leave work, I generally leave it behind.  Not always, sometimes I am actually still thinking about some technical issue I've gotten stuck on.  But most of the time, I step out the door, feel the sun on my back, stretch my shoulders and crack my neck, and start thinking about putting the top down and enjoying my drive home.  This is the ideal time to have a friendly conversation.  TL;DR: People don't all run on the same schedule and have the same work habits and preferences. "
If you are just focused on learning Python itself  the Raspberry Pi isn't necessary.,"You don't need a Raspberry Pi for Python, you can dev in  nearly  any environment you want.  > raspberry pi can do but Windows or Linux can't do?  As far as coding itself, no.  It can be useful if you plan on writing scripts which will be ran in a remote environment, since you will be able to access the Pi via SSH. (Which can be useful practice.)  If the school wants you to code at home, and while you are in class, the Pi can also be pretty useful. Since you can pretty much carry that Pi wherever you want, and just hookup keyboard/mouse/monitor to it and you are good to go.  TL;DR If you are just focused on learning Python itself, the Raspberry Pi isn't necessary. "
if you don't have a reason this is gonna benefit your customers  don't do it.,"I am of the opinion that my customers don't give a shit about what the back end looks like.  They care whether or not they can get access to the services I sell them.  If a given tool can't tell me what my customers will gain from my implementation, then I don't give a shit.  90% of the Docker users I've talked with can't tell me why this will help them deliver code any faster, or maintain higher reliability, or anything similar.  Chef users?  They can always answer these questions.  CD/CI proponents? same story.  All the other major shifts have had great business logic. But most of the Docker reasons to exist seem to amount to ""it works on my machine-as-a-service"".  If your team is so dysfunctional that you can't manage to get a reasonable configuration management strategy out the door, this tool will just mask your operational ineptitude for a short while...  tl;dr: if you don't have a reason this is gonna benefit your customers, don't do it. "
this article started off kind of rocky  but gets way better.,"REST is almost meaningless these days, but the author quotes Fielding, so I'll assume they mean ""real"" REST instead of ""whatever most people call REST.""  > We can identify “customers” collection resource using the URN “/customers”. We can identify a single “customer” resource using the URN “/customers/{customerId}”.  We can, but REST says nothing about this.  > A resource may “contain” sub-collection resources also.  REST has no concept of sub-resources.  > The starting point in selection of resources is to analyze your business domain and extract the nouns that are relevant to your business needs.  REST is focused around modeling processes, not objects. Verbs, not nouns. This is what ""the engine of application state"" is in that terrible HATEOS acronym.  > Fine grained CRUD resources versus Coarse Grained resources  This section is much, much better.  > business logic will start spilling over to the API consumer.  Yes! This is what REST is trying to prevent. it's arguably central in many ways, though that's often a second-order thing, the reason that the constraints exist, rather than being a constraint.  > In other words, for complex business processes spanning multiple resources, we can consider the business process as a resource itself.  Exactly what i was complaining about above!  TL;DR: this article started off kind of rocky, but gets way better. "
People use them  all the time. Usually to help keep their 24 7 uptime.,"> My question is what exactly are these 'logs' used for?  ... You answered your own question with:  > If this is to check errors when app is in production How exactly is someone going to get meaningful data out of this 1000s of lines?  The answer is  very carefully  and  however you want to . No seriously. Some people  tail  files on their server to look for recent errors. Some people use  grep  to search for issues. Other people use log file aggregation and analysis tools to not store these errors to disk, but to a Service usually backed by a NoSQL datastore. And then use this service to send out automated message when issues arise.  TLDR; People use them, all the time. Usually to help keep their 24/7 uptime. "
One more  normal  release was in flight  the site wasn't updated to show that until later than it should have been. Our bad. Mostly my bad.,"By default, [the supported versions page]( calculates the dates for each version's support lifetime based on the release date of the first stable release on that branch. So for 5.5, that was 2 years from the release of 5.5.0, which happened on June 20, 2013.  In practice, though, those dates don't necessarily line up with the exact release cycle in use two or three years later, and that's what's happened here:  the 5.5 maintainers intend to make one more ""normal"" release on July 9  Ideally, that should have happened in advance, but since 5.5 is the first release to hit EOL since the supported versions page was added, nobody really thought to reconcile them until after the fact.  (Edited to add: it's likely that the exact date for 5.4 ending its security only support phase is going to change ever so slightly too; that date will be announced in the 5.4.43 release announcement in a couple of weeks.)  tl;dr : One more ""normal"" release was in flight; the site wasn't updated to show that until later than it should have been. Our bad. Mostly my bad. "
Pick one and run with it. Write clean quality code  and learn as much as you can ,"Hello, Senior Software Engineer who has interviewed many many people. A couple things off the bat just so we're speaking the same language here. When you're referring to ""Angular"", what you should now be saying is a specific version like Angular 1.x since there have been some significant updates to the way the framework is implemented. For example, the change from directive based to component based that started in 1.5+. ""Angular 2"" is actually now semantically released, so the version number when you're talking about it is somewhat irrelevant. Going forward, it will just be called Angular (hence the reason for specifying Angular 1.5 in the previous few sentences I was talking about).  With all that out the way, I would say that it honestly doesn't matter which framework you pick, but it's more that you can prove that you understand how to pick up a framework, learn it, and implement it in a structured way. I work in the Angular world as part of my every day stack, but I've heard great things about how quickly people can pick up react and how it doesn't have very much overhead.  TL;DR - Pick one and run with it. Write clean quality code, and learn as much as you can! "
the best people are the ones who try to learn new thing and to get better.,"I have met a lot of that type. (I've done a lot of formation/tutoring)  Maybe not as obvious as saying flat-out that they don't care, but really, you felt that they did not want to grow.  Even when you show them that using such tool or the command-line is better, faster & easier for them. As soon as they are on their own, they ignore everything they have just been shown.  One of the most astonishing thing is the command-line: really, some beginners cannot fathom how superior the prompt is for some tasks. They refuse to consider learning how to use it, even for simple tasks...  Well, guys, you're stupid, so I'll let you rot in your ignorance while I do in 5 minutes what you barely do in one hour because you refuse to use a prompt. Yes, I'm pretty jaded because I've really met this attitude often (luckily, not always).  TL;DR: the best people are the ones who try to learn new thing and to get better. "
It's not the fact that a callback is a callback  but whether or not the callback handles some future value in the present  that makes it asynchronous,"The browser doesn't need to wait for any thing to execute the click callback. Don't be confused by the word ""callback"", that doesn't necessitate that the function itself is async.  Think about async in terms of cooking. You usually start boiling the pasta before you start cutting your veggies. But you don't put the pasta in the pot until the water is boiling. The callback for an asynchronous function runs when the water is boiling, so to speak. The part where this metaphor kind of breaks down is that in real life, the water can start boiling at any point during the veggie-cutting process. With the Event Loop, assuming your veggie-cutting is all synchronous, the post-water-boiling-callback can ONLY execute once all your synchronous veggie-cutting is done, because the async callback runs at the end of the event loop.  Async is basically a way to handle future values NOW. In the case of the click handler, there's no future value handled in the present. So its synchronous. All it is doing is saying ""when this thing is clicked, do x"". And then you triggered the click event, which executed the non-asynchronous callback to the click handler. This would be different if, instead, your click handler callback was an ajax request to fetch some data and then do something with it. But in that case, it would be the callback to the AJAX request, not the actual callback to the click handler, that is asynchronous.  tl;dr It's not the fact that a callback is a callback, but whether or not the callback handles some future value in the present, that makes it asynchronous "
Get familiar with and try learning the native DOM api  but don't be afraid of using jQuery if it provides value for you  ignore the haters.,"It doesn't really matter in the grand scheme of things. Becoming expert DOM manipulator in raw javascript won't make you a good programmer. Pure DOM manipulation is fairly trivial  anyway, and jQuery is just a utility lib that makes the grunt work easier.  I see jQuery mainly as syntactic sugar and cross browser normalizing lib. It also does some clever behind the scenes grunt work such as removing data and event listeners from removed elements etc.  It's very useful depending on what kind of stack you're working with. I render everything serverside and then manipulate the dom manually, so jQuery provides lots of value for me. If I was working with something like React, there wouldn't be any reason to include it. As I said, dom manipulation is easy. It's like laying bricks at the construction site. Software architecture is the hard part.  You should also know that many developers are condescending to people who use jQuery. jQuery users are often seen as ""lesser"" developers. Ignore that. jQuery is just a tool, nothing more. It's also super popular and yes, most of the people who use it arent great developers. Just like most of the people who eat pizza aren't geniuses. That doesn't mean that everyone who eats pizza is a retard.  TL;DR - Get familiar with and try learning the native DOM api, but don't be afraid of using jQuery if it provides value for you, ignore the haters. "
Get familiar with and try learning the native DOM api  but don't be afraid of using jQuery if it provides value for you  ignore the haters.,"jQuery is fine. Becoming expert DOM manipulator in raw javascript won't make you a good programmer. Pure DOM manipulation is fairly trivial anyway, and jQuery is just a utility lib that makes the grunt work easier.  I see jQuery mainly as syntactic sugar and cross browser normalizing lib. It also does some clever behind the scenes grunt work such as removing data and event listeners from removed elements etc.  It's very useful depending on what kind of stack you're working with. I render everything serverside and then manipulate the dom manually, so jQuery provides lots of value for me. If I was working with something like React, there wouldn't be any reason to include it. As I said, dom manipulation is easy. It's like laying bricks at the construction site. Software architecture is the hard part.  Many developers are condescending to people who use jQuery. jQuery users are often seen as ""lesser"" developers. Ignore that. jQuery is just a tool, nothing more. It's also super popular and yes, most of the people who use it arent great developers. Just like most of the people who eat pizza aren't geniuses. That doesn't mean that everyone who eats pizza is a retard.  TL;DR - Get familiar with and try learning the native DOM api, but don't be afraid of using jQuery if it provides value for you, ignore the haters. "
My portfolio has gotten me some jobs and cost me others.,"Apply now.  The interview will go roughly the same regardless of your portfolio.  I have gotten mixed reviews by providing access to a portfolio, which ironically turns out to be better for me in the long run.  Sometimes companies have found my big open source project exciting and impressed with the time I put in.  I have also been outright rejected because Java developers could not read the vanilla JS code.  I have also gotten confusion from people (non-coders) who are troubled to understand why they would want to use a code beautifier.  Virtually nobody reads the code to discern if I am actually a competent coder.  tldr; My portfolio has gotten me some jobs and cost me others. "
I'm seeing a lot of  not invented here .,">I see ""use after free"", ""buffer overflow"", ""array index error"" and other stuff that seems (from my limited understanding) attributable to C.  I'd say improper indexing/running out of bounds is just down to bad programming,  but  that's one of the things Rust tries to address. It tries to  enforce  proper memory management with complier overhead, rather than requiring executable overhead. Of course, the Rust devs feel they can do lots of things better than the C spec. Stricter compile time checks could've been patched into GCC or Clang.  Making a better compiler doesn't require making a new language.  >I also see a number of kernel security vulnerabilities caused by drivers, which as I understand it would also not be possible in a microkernel that isolates drivers in user space.  Sure, but that costs more complexity. The Redox team are justifying their opposition to monolithic kernels by saying monolithic design means more code. This isn't true. If you're only thinking about the microkernel proper, you  could  say it's smaller, but the functionality a monolithic kernel normally has needs to go somewhere. You can't compare a monolithic kernel to a microkernel without including all the daemons it spun functionality to.  >It seems like these things would be made much better by a microkernel and a memory safe language. Am I misreading this?   The thing is some people've argued that for a long time, especially Richard Stallman. Yet it's taken decades just to get the HURD (""Hird of Unix-Replacing Daemons"") microkernel off the ground. HURD was supposed to be the  kernel  of the GNU operating system, but while the GNU team was struggling with that, a hotshot programmer from Finland released  a working version of Linux. (This's where the ""GNU/Linux"" naming controversy comes from, BTW.) Stability and efficiency has been a longstanding problem for the project, the same problems for many other older microkernels.  Linux isn't fully monolithic anymore. It's moved in a modular direction over the years. It's not even fair to talk  monolithic vs micro  in Linux's case. Linux has always been the pragmatic solution (over pie in the sky idealism).   TL;DR: I'm seeing a lot of  not invented here . "
Memory isn't cheap. Memory is expensive and has real runtime costs. Cache layouts and large heaps are a non trivial deal  and Java doesn't help this too much at all.,">VM will be more memory hungry but memory is cheap today  No see this is what nobody seems to understand.  CPU's these days under ideal circumstances can do 4-5 instruction per clock. But 99% of the time they are waiting on RAM. Using a large heap just makes this issue worse. CPU performance is RAM/Cache access performance. If you hit RAM you already lost.  Yes the JVM does have a lot of nice tricks to help with this. Oracle and Azule do a fair amount of prefetch for internals/edges of the JVM interacting with your code. But when you're passing lists of interfaces around you are chasing 3+ levels deep of pointers, the CPU physically cannot predict access patterns. (This is as good as the JIT will give you)  Now C++  can  suffer from this as well, but with templates your code is static and you  can  avoid the multiple level deep redirection. Or you can simply  avoid  complex OOP patterns opting for the  modern c  approach Bjarne Stroustrup talks about.  TLDR  Memory isn't cheap. Memory is expensive and has real runtime costs. Cache layouts and large heaps are a non-trivial deal, and Java doesn't help this too much at all. "
I thought these events were ineffective because I underestimated the power that relatable role models can have on how one approaches a field.,"I didn't understand either, until I volunteered at one. They had a keynote speaker who was able to make a connection with the audience based on their shared experiences in a way that wouldn't have been possible otherwise, and it was really moving and encouraging listening to her speak.  I still feel to a small extent that these targeted events have the flaw of creating an artificial environment that doesn't necessarily adequately prepare them for dealing with discrimination in the workforce, but that's not what the purpose of the event is either. It's to encourage interest and appreciation for science in underrepresented groups.  Tl;dr: I thought these events were ineffective because I underestimated the power that relatable role models can have on how one approaches a field. "
bleh  I am too old for this shit    .   Besides,"Yeah, yeah I get it... no, thanks, though.  I simply  will not  be arsed. I do not wish e.g. to be harassed by recruiters when I am not looking for a job, and that doesn't happen all that often, maybe once in 5 years. And when I did, I did not see that recruiters and businesses failed to find me. Heck, google found me, but bar some google groups posts, I had no ""online presence"" whatsoever. (disclaimer: I failed miserably; on the upside, I just took a new position, so it would have been quite embarrassing had I passed, and I wasn't too interested in moving either).  tl;dr: bleh, I am too old for this shit :-).  Besides "
trying to understand how to use regression lines and scatter plot lines. Was confused by different type of lines and what each would be used for individually.,"The goal was to attach some form of regression line to the data. I'm learning how to use R. Regression as I learned was a best fit line for a series of data. But I haven't the slightest idea what the difference is between the straight and curved line.  Also, to explain about the data, I was playing a video game and recording the maximum hits and their frequency of occurrence then modeling them to attempt to establish a pattern. As you are hinting, there really isn't enough data to establish any sort of pattern so I was just playing with the models to see if I could add some form of regression line.  As a final note, what you said last, The repeated X value is simply an oversight and a mistake in the data. Should have been only one '3' with a corresponding frequency for how many times it occurred. As for the other values you specified that are missing, they just never appeared in my extremely small collection of data. I never made those hits so they were not recorded.  Tl;dr: trying to understand how to use regression lines and scatter plot lines. Was confused by different type of lines and what each would be used for individually. "
Too much unrealistic focus on the enterprise  complete lack of interest in areas that they could realistically make progress.,"I've used D for work purposes for almost four years, and I can throw out a few observations.  Mostly the focus has been on replacing C++ in the enterprise. That's very unlikely to happen. Java did it, but they had resources, and they used them.  There have also recently been some efforts to jump into web development. That's the most competitive space there is, and I don't think a volunteer language will make much noise in that space without getting lucky.  D had a lot of appeal to me because I'm a researcher working with a small number of coauthors. I can choose my own language, and to be honest, there's not that much competition in the area of numerical computing. In order to do it, though, I had to write lots of bindings and enable communication with other languages. Luckily I had a young son involved in activities five or six nights a week so that I had time to write all that infrastructure.  These days the language continues to move in the direction of Java. You're not supposed to write programs, you're supposed to start projects. You're not supposed to write Makefiles, you're supposed to write JSON config files. IMO a language first needs to build a user base and then worry about getting into the enterprise.  tldr: Too much unrealistic focus on the enterprise, complete lack of interest in areas that they could realistically make progress. "
After a year of using go  he apparently hasn't really learned it.,"> The Go coverage tool is, frankly, a hack. It only works on single files at a time and it works by inserting lines like this:  How does he think gcov works? It basically does the same thing, only at the assembly level.  > Channels and mutexes are SLOW.  Yes, atomic variables and looping  are  slow compared to plain straight through code. They imply hitting main memory, which is probably a factor of at least 100 over just plain cached accesses. And if the lock is contended, you end up needing to enter the kernel a whole lot. There is a reason people work very hard to make synchronization fast, and a reason it's considered a very hard problem. I would be very surprised to find out that the slowness was Go's fault, and far more likely to be poor locking design.  > I’ve literally had cases where just dropping a * in front of something has made it magically work (but it compiled without one). Why the heck is Go making me care about pointers at all if it is a GC’d language?  By value, vs by reference. If you don't bother understanding how your language works after a year, I'm not sure what to say.  > which is the laziest wrapper around the POSIX interface I’ve seen in a while  ...it gives direct access to all your platform syscalls. (It is  not  a wrapper, at least not in the sense that it tries to abstract the platform. It is the full, raw access to it.). What the fuck do you expect from a syscall library?  tl;dr: After a year of using go, he apparently hasn't really learned it. "
I don't think your age matters nearly as much as your personality and desire.,"Depends on how quickly you want to be making a career out of your new skill. You can probably learn it fairly quickly, in terms of months, but a lot of places hiring want to see experience. You'll need to toss up a few free projects, have code samples ready, and be ""working"" for kind of a while without money coming in from the work.  Learning PHP, like anything else, requires effort and dedication. If you have it, go for it regardless of your age. If you just read some articles that say php developers make good money and you're wanting to get some of that in the next year or so, you're likely to be disappointed.  You're going to take an entry level position (most likely) after quite some time of learning and have to climb through the ranks. Senior positions typically want 3-5 years of experience, some want degrees, etc...  Tl;dr I don't think your age matters nearly as much as your personality and desire. "
resume is huge  programming test is the icing on the cake. KISS ,"Best thing that I have done is a test.  I asked someone to write a small API: connect to spotify, and make a simple API to get artists, album, and songs. The only challenge is do not use a Framework. The reason behind it is because it shows me how the person structures code, their knowledge of design patterns, and how they approach a problem. Once their done, we do a code review, and grill the candidate.  I'm really not concerned in open source. It's nice to see when people do this, but it's not a mandate for me at all. I am, honestly, most interested in their resume. Their resume is the foot in the door: a bad resume will ruin it for you, and a good resume will get you in. Now, a resume, in no way, dictates if a candidate is useful or useless, but it's a great way to see, at a high level, what a candidate has done.  TLDR; resume is huge, programming test is the icing on the cake. KISS! "
It's fine. Keep up to date on new stuff. Libraries   Componentized Frameworks   Monoliths.,"There's no problem rolling your own framework if you are productive in it. That being said, you should keep up to date with other frameworks/technologies/etc - at least because you can port useful features to your framework.  But i would discourage everyone from writing their own monolithic framework - own ORM, routing, MVC and everything tightly coupled and dependent on each other. I mean i feel like we have finally past the whole monolithic thing. If someone says they have rolled a framework these days, what i imagine is a thin layer of configuration/customization on top of a bunch of libraries (DI, routing, DB, etc).  TL;DR: It's fine. Keep up to date on new stuff. Libraries > Componentized Frameworks > Monoliths. "
Don't accept anything you read uncritically   especially on the internet and especially especially about programming.,"Here's the deal: 80% of the essays and blog articles you see posted here are total crap. Some a-hole writes a justification for his personal idiosyncrasies being objectively better than everyone else's and sticks it on medium or something to ""enhance his personal brand"". But, in all probability, he's not a much more experienced or talented than you are: if he was, he'd have learned by now that  auto  is perfectly fine for some people, mutable objects work fine for some people, Objective-C is fine for some people, and no one tool or approach can satisfy everyone. Treat it like advice about women you get from the aging alcoholic at a nearly deserted bar: you listen to it politely, maybe you think about it later, but you don't take it too seriously.  tl;dr Don't accept anything you read uncritically - especially on the internet and especially especially about programming. "
The relevance of Google and Stack Overflow depends entirely on what sort of programming you're doing.,"A lot of programming involves very little Googling, Stack Overflow, etc.  This summer, I programmed for a couple of months on a game that's been out for years (Crusader Kings II). For the vast majority of things I was doing, Googling would've been completely useless, as most things that needed to be solved were things relating only to other things within the code base. The only thing I ever needed to Google was a few bits of C++ syntax.  Most problems were simply addressed by going through the logic until I understood it well enough to identify and correct the problem.  Tl;dr: The relevance of Google and Stack Overflow depends entirely on what sort of programming you're doing. "
DNA tests should be used to confirm guilt  not to generate a list of suspects.,"Exactly. There is a non-negligible rate of false positives for DNA matches.  Ordinarily, that's not a problem. For example, if there's a 1 in 1,000 chance of a false positive, if you test your prime suspect's DNA and get a match you can be 99.9% sure that it's right.  The issue is that when you scan a database, you aren't doing a handful of specific comparisons. That 1 in 1,000 chance means that if you match against 1,000,000 DNA samples you'd expect to get around 1,000 positive results.  Even if you trawled a much smaller group (e.g. people with some relevance to the crime - like family, friends, and colleagues - even if there's no implication of guilt) you'd still expect to see false positives much more often than the 1 in 1,000 statistic would lead a jury to expect.  Now in practice I don't know how often this would be a problem. That depends heavily on numbers I don't have. It sounds like in the US it can only be done to people with a criminal conviction, but apparently that's about 8.6% of the population (and substantially more amongst the black population, which means that they must get screwed by this even more).  But the problem is, in essence, like [this XKCD about jelly beans and acne]( A test is only worth the quoted certainty if you apply it selectively.  TL;DR:  DNA tests should be used to confirm guilt, not to generate a list of suspects. "
don't use threads in python if you can possibly help it.,"You're not wrong. In fact you can even use threading for SOME CPU bound stuff IF it's e.g. a C lib that releases the GIL e.g. numpy. There's still some GIL contention inbetween C calls and you'll see diminishing returns as you add more threads and GIL contention becomes the bottleneck.  HOWEVER a simplistic application using blocking IO in a pool of threads offers greatly inferior IO performance to a pool of asynchronous multiplexed workers a la twisted, gevent, or asyncio, and uses a lot more RAM. This [article]( gets really deep into why threads suck for IO. Hints: memory usage and context switching overhead. If you are only running a few threads maybe that's not a huge issue. If you're trying to handle 10,000 connections it's a major issue.  Besides, threads are hard to get right and easy to fuck up. Forgot to bake in some logic for a thread to exit? Program never exits and even ignores signals so control c doesn't work. A thread threw an exception and died? hahahahah FML. You can avoid most synchronization issues if you use queues but in complex programs they can still crop up. Multiprocessing.Pool takes care of all that for you. Python didn't even grow a thread pool until  concurrent.futures in 3.2.  TL;DR don't use threads in python if you can possibly help it. "
The issue will be solved eventually  just be patientpython has more on the line than your gentoo server.,"A bug was submitted involving a vendored dependency; a dev acknowledged the bug and found the commit from upstream vendor that fixed it; they wanted to pull in a fixed version from vendor, but it wasn't immediately doable since python has some custom plumbing around the library and the dev didn't have time to continue working on it right then...a month ago.  You think that's amateur? Do you know how many people rely on python? Not just people, how many huge enterprises have python as their backbone and lifeblood? ...Do you really think randomly applying one-off patches (without even knowing what they do) to vendored dependencies is better, for such a widely used platform as python, than taking longer and updating the whole dependency properly?  tl;dr The issue will be solved eventually, just be patient...python has more on the line than your gentoo server. "
No one comes to a website because of the license on some javascript. They come because of network effects.,"A FOSS website is completely unnecessary.  Reddit is ""open source"", except it's not. I'm only aware of one team that has built a reddit clone from the source, and they found the source didn't work - the algorithms used for voting, time, shadowbanning, etc aren't what is in the publicly available source. This has been demonstrated.  Hacker News is ""open source"", except it's not. Like reddit, the algorithms used for voting are not what is in the publically available source. Either way, HN is a weird dialect of Lisp, and if you're building FOSS apps, it might be a good idea to target a language people use, and not one created by you (or PG)...  For one reason or another, programmers and developers have decided that FOSS should be the natural state of things,  except  when it comes to forums and web pages. This is a demonstrable fact; reddit and HN wouldn't be as popular in the community if it were not true.  There are actually a few really good parallels to this argument in the 3D printing world. The current top object model repository is Thingiverse, developed and maintained by Makerbot, a company the community hates with a passion. Everyone wants an alternative to Thingiverse, but because of network effects and simply being first, it's going to be a long time until we get rid of it.  This hatred of Thingiverse didn't stop a few people from attempting to make an 'open source' object model repository, and despite significant development efforts, no one on that team realized it's network effects, and not a license on the source code for some javascript that makes people go to a website.  TL;DR: No one comes to a website because of the license on some javascript. They come because of network effects. "
don't judge a technology by its state from ten years ago and comparing to today's state of other tech.,"I'd just like to address the performance: people are using mod_php5 (PHP5 interpreter embedded into Apache) and say it's slow. It is.  Much more fair is to compare Node to PHP7-FPM, which is a dedicated app server running on a port, with a dedicated HTTP server proxying app requests. So, much like Node. That setup will give Node's raw performance a run for it's money. (Note: make sure you have OPCache enabled in PHP, it should be by default but you might have a broken version)  Also of note: PHP7 does not yet have advanced JIT  handling (which Node got for free via V8), we might expect ludicrous speeds once that hits in PHP8.  TLDR don't judge a technology by its state from ten years ago and comparing to today's state of other tech. "
use  const  always because if you're reassigning things frequently and for no reason  you're more likely to introduce bugs and other weird unpredictable behavior unnecessarily.,"I'll try to clear as much up here as possible, but you definitely have a few misconceptions right out of the gate.  Firstly,  const  does not freeze a variable. It makes it a little more obvious what is going on (ie: this reference will not be reassigned) and encourages good coding practices (eg: not abusing hoisting, not defining without assignment, etc)  Secondly, an arrow function is not being assigned here to your const. That is the callback of the event listener function.  Finally, nothing would 'change' per se, but you wouldn't be doing yourself any favors. Const is nice because it tells you something about your code, makes it easier to read/parse (for humans), and at some point in the future, will likely also be more performant.  tldr; use  const  always because if you're reassigning things frequently and for no reason, you're more likely to introduce bugs and other weird unpredictable behavior unnecessarily. "
If you're dealing with numerical data  save it in a hdf5.,"> What about using gzip instead of zip for the pickle?  Then it makes the read a 2 step process and doesn't solve the underlying problems with Pickle.  When you are sharing data files on the internet to help other people learn you should avoid something that has a big red warning on it's documentation page:  > ## Warning> The pickle module is not secure against erroneous or maliciously constructed data. Never unpickle data received from an untrusted or unauthenticated source.  What if medium was hacked? What if the author slipped something in there I didn't know about?  At least with the source code I can read it, get help on it, etc before I run it.  HDF5  HDF5 is where everyone is migrating to in terms of numerical data storage. [Look at who uses it.]( If you are collecting data you want it in HDF5.  Case in point, it's what NASA uses to share their data:  And I know others were laughing at the data size saying ""It's  only  5 GB"". The NASA dataset  consists of 7,850 HDF-EOS5 files covering the period from 1 July 1987 to 31 December 2008. Each file is around 16 MB, bringing the total to about 120 GB.  How big do you think that would be pickled?  The HDF5 group has blogged about it: [NASA DATA: Putting some Spark into HDF-EOS](  They even extended it into [HDF-EOS5](  >  HDF is the Hierarchical Data Format developed by the National Center for Supercomputing Applications. Specific data structures which are containers for science data are: Grid, Point, Zonal Average and Swath. These data structures are constructed from standard HDF data objects, using EOS conventions, through the use of a software library. A key feature of HDF-EOS is a standard prescription for associating geolocation data with science data through internal structural metadata. The relationship between geolocation and science data is transparent to the end-user. Instrument and data typeindependent services, such as subsetting by geolocation, can be applied to files across a wide variety of data products through the same library interface  Furthermore, [HDF5 is something that is starting to show up on resumes.]( It's a technology worth learning.  tl;dr: If you're dealing with numerical data, save it in a hdf5. "
Your two words are completely incorrect and don't contribute anything to the discussion.,"What. Who the hell upvoted you?  Mickey Mouse is still very much copyrighted. Certain expressions of Mickey Mouse (eg: specific pose, specific colors, specific voice)  might  also be trademarked, but the major thing holding people back from creating Mickey Mouse stories is its copyright.  Mario is still very much copyrighted, it's just less controversial. Nintendo is not as bullish about enforcing their copyright against Mario fans as Disney is against Mickey Mouse. Nintendo has also not paid off the US Congress multiple times to extend and extend copyright to keep Mario under wraps. And Mario  may  also be trademarked (specific pose, specific colors, specific voice), but it's absolutely still copyrighted.  TL;DR: Your two words are completely incorrect and don't contribute anything to the discussion. "
mfw when you talk about the binary tree like it's something impractical and exotic,"I doubt it would take you more than a few days max to understand what binary trees are. But what's with the attitude. Something that's just a little less pragmatic and more abstract and you're like ""it's irrelevant"" / ""I never needed that in my long and succesful CAREER"", man give me a break, other than that you may actually need to use things like that (I gave an example), if you act all dismissive about a well known abstract data structure, how is someone going to expect that you will apply the kind of ""abstraction"" / refactoring / etc. needed to make a complex program better designed?  tl;dr mfw when you talk about the binary tree like it's something impractical and exotic "
no thanks  keep your needless optimization  I prefer runtime type safety and took comparability.,"Which seems less than ideal to me, because it both breaks compatibility with common tools (one of the great advantages of using a JVM language, usually) by breaking reflection.  A couple examples, one scary, one annoying. One is: spring becomes harder to use, and more dangerous. Want to use Spring to inject  String|Int ? Can't do it without a qualifier, because Spring only sees  Object . Then, what if you accidentally make a mistake and in your spring context ""MyCeylonBean"", which is supposed to be a string or an integer, is suddenly set to a  List ? Well, your IDE probably doesn't have the tools to warn you that it can't inject that value, and it's not going to break at the point where the context initializes, because we're explicitly forgoing runtime type safety in favor of union types being ""first class citizens"", so now it just breaks at some unknown point in the future. Very scary.  Another, more terrifying example: JSON deserialization. So, say I start up a Ceylon project, and I pick Jackson for SerDe because why not, I use it everywhere else, and Ceylon is a JVM language so it should work, right? For a while, everything is fine. Then I decide this type would work better if ""myField"" were `String|Int"". Cool. Everything still works fine for a time. Then, some day (hopefully not while I'm asleep!), someone starts sending in bad input to my system...they're sending ""myField: {}""! That's wrong. That should break, and it should be a client error, and none of my alarms should even skip a beat...I'm not responsible for clients sending garbage input. But instead, it happily deserializes, breaks when first accessed, and causes an internal server error. Why? Because Jackson uses reflection, and as far as its concerned, that field is an object, and maps are objects.  It just seems like a feature that is  not  worth abandoning runtime type safety for. And what's the best argument we've heard? It avoids nasty boxing? What awful runtime overhead boxing has...have you eve optimized a system by flattening your object hierarchy? Why are you using a JVM if that's a concern? So how do you solve it? Make the Ceylon compiler generate instanceof checks? Doesn't protect you from field injection breaking, and adds back a lot of that overhead you're claiming to have avoided in the first place.  TL;DR : no thanks, keep your needless optimization, I prefer runtime type safety and took comparability. "
inellij may be a better editor  but a better IDE it is not   just debug something to know the difference.,"Intellij.  And i totally understand what you are saying, definitely great refactoring plug-in, and helps with a ton of cut-and-past manual crap, auto-complete, etc.  Super cool.  BUT, when comparing IDEs, plugins or no plugins, VS is the top of the mark for me.  The debugging features are 2nd to none, great repository support, cloud deploy, web dev support, etc. etc.  It all just works, and feels native.  As great as intellij is, a lot of it relies on the strength of the plugin options, which isn't a bad thing, but the out-of-box experience doesn't compare (again, my opinion here...).  But yes i agree VS is made better with plug-ins as well, some of which could be considered essential (side note: especially for any kind of webdev, which VS was really not the best for (for a long time)).  tldr - inellij may be a better editor, but a better IDE it is not - just debug something to know the difference. "
Internet echo chamber is a terrible way to judge popularity,"Agreed, but I think the Internet Hype Machine and the echo chamber that is /r/programming and SO miss the overwhelming majority of what goes on with languages and projects written in them. Also, many of the projects written any any specific language may well be highly proprietary and not discussed outside of the people writing them. Or they're just not written by people that tend to go on about their accomplishments on the Internet. I think again this is a generational thing.  Newer languages get lots of press because accomplishing anything in them is more noteworthy. Writing something in C isn't what we'd call surprising since there are  so many examples . But, writing something in Go or Rust, or Ruby or Python gets slightly more play.  TL;DR Internet echo chamber is a terrible way to judge popularity "
virtualenvs only manages python packages. Conda manages that and other python related stuff too.,"From the Conda team blog:>virtualenv ... did not meet all of our specific requirements. The main problem is that they are focused around Python, neglecting non-Python library dependencies, such as HDF5, MKL, LLVM, etc., which do not have a setup.py in their source code and also do not install files into Python’s site-packages directory.  >Under the hood, we have created a concept of environments which are conceptually similar to virtualenvs, but which use filesystem-level hard links to create entirely self-contained Python runtime layouts. By using the ‘conda’ command line tool, users can easily switch between environments, create environments, and install different versions of libraries and modules into them.  TLDR: virtualenvs only manages python packages. Conda manages that and other python related stuff too. "
Alex St. John is bad and he should feel bad.,"Back when I was blue collar, I used to have the same sort of outlook. I had a lot of physical labor and some mental labor everyday. Now that I've been an engineer for sometime I have a lot more mental labor and some physical labor from time to time. I'm still exhausted at the end of most days because I strived to work hard in both situations.  Granted, I no longer have to deal with the elements and mostly sit in an office now but I have realized that in the end both ends are just as labor intensive and you spend expend comparable amounts of energy and deal with comparable amounts of stress.  I don't code for business needs often but when I do, I'm trying to solve a complex problem by somehow instructing a computer based on loops, arrays and algorithms on how to solve that problem. It isn't easy and is mentally draining. Especially when after all that hard work, it will always fail the first time and probably through a few iterations to track down some large bugs.  Anyway, that is mentally exhausting too. For someone to say different is disconnected from the reality of their employees and it is disheartening because he is pretty much the norm of executive's perspective. I've heard it before from directors and VPs of large companies and I've argued with them. It never really matters.  Anyway, sorry for the long sort of rant.  TL;DR Alex St. John is bad and he should feel bad. "
No it's not  but you should reconsider your reservations and try to become a better communicator.,"I would say it's not a  must , in that there are plenty of data science jobs that will allow you to sit in a darkened room and code most of the time and never really talk to another (nontechnical) human being face to face. That is, very much like a typical CS/developer job (the analysts that work under me rarely talk to anyone but me, and never really have to present anything if they don't want to). So if you really want, you can be that thing.  However, a few things to consider:  1) Having little to no communication/presentation skills will be a detriment to your career, not only as a data scientist, but in most other technical fields as well--yes even CS. Eventually, you will be in a position where people will want to hear your opinion, and at that point it will be very nice for you to be able to communicate it to them.  2) Talking to people who have little to no knowledge of what you're talking about was something I hated when I started this job. Now, it is one of my favorite aspects of my career. For one, I love to feel like the smartest guy in the room, even (especially) when the CEO is sitting there. For another, it's really improved my skills as a programmer and data scientist, because I get to interact with the end consumers of my work, and understand what they do with information, how they think, what they need to know, and how they need it presented to them. I have mined a wealth of useful information with my dealings with other branches of my company (and beyond).  3) I am an introverted guy, and I  used to be  pretty shy.  That can change . It's a  skill  not a personality trait. It takes practice, and preparation, but it is an extremely lucrative skill to have in a technical field, so there's a very good reason to learn. My advice would be to try to develop the skill, instead of threading your career path around it.  TL;DR: No it's not, but you should reconsider your reservations and try to become a better communicator. "
Know your shit  contact the hiring manager directly  and sell your skills.,"A little late to the party but I'd like to give my perspective as a developer who has had to conduct numerous interviews.  Most often, the resumes that get forwarded to the hiring manager get filtered by HR first.  HR doesn't care about your depth of knowledge but how well your resume matches the position.  So if you lack a degree that is listed in the job posting, then it will be near impossible to get past the HR filter unless you contact the hiring manager directly.  That said, I have interviewed many people with CS and other STEM degrees who claim to have knowledge in Python or other languages.  I'd say nearly 95% fail during the interview process because they don't really understand the language and many can't write a simple function to save their life.  I do take a quick look at the degree and school when going through a resume, but that is more out of personal interest than any need.  What I am mostly looking for is what you have done and how what impact it has had.  That can be hard without real world experience, but I have read and interviewed people that have listed personal projects that were interesting.  TL;DR - Know your shit, contact the hiring manager directly, and sell your skills. "
Yes  you can absolutely get a great job without a degree. Good companies just want talented engineers and don't care how you learned to code.,"I dropped out of high school in 1998 and worked on graphic design and front-end development in my spare time, working mostly on sites for small businesses. In 2003 I started a business building websites for small-medium sized companies and did this until 2007 when the real estate market crashed and had to close the doors.  In order to make ends meet I did a lot of random jobs like delivering pizzas, painting houses, etc... while applying for jobs for software development. I eventually got a job at a digital advertising company and became part of the team that built converse.com  Since then I have been part of development teams or been the lead developer for websites like HBO.com, Samsung.com and many other established brands. I now work mostly on back-end development in Python and recently consulted for Hearst to help them rebuild their global publishing platform.  TLDR; Yes, you can absolutely get a great job without a degree. Good companies just want talented engineers and don't care how you learned to code. "
Article calls out a shitty article and manages to be even more shitty in the process.,"Integrity is just as important in activism as it is journalism.  The discussion is anything but meaningful when the author clearly has an anti-women in tech agenda and wrongly claims that the makeup of the industry is ""pretty firm evidence"" of his views:  > She states: “Knowing that women are great at coding gives strength to the case that it’s better for everyone to have more women working in tech.” Of course the study proves no such thing, and therefore give no such “strength” to her crusade.  In fact, the make-up of every Computer Science course and the overwhelming majority of the tech sector is pretty firm evidence of the opposite (though again not total proof).  The opposite of her statement is: it's worse to have more women in tech.  The makeup of the field has nothing to do with what gender is inherently better at it (hint: it's neither) and everything to do with what society considers ""appropriate"" for women. The fact that he would hold this up as ""pretty firm evidence"" of anything shows how willing he is to hold up anything that suits his clearly sexist agenda.  tl;dr - Article calls out a shitty article and manages to be even more shitty in the process. "
I am so old that python has nerfed the most beautiful quirk I know  the one line self referencing array.,"Wow. I am thirty and I just realized I am so old that the quirks I know from python are slowly being removed.  My favorite quirk in python use to be the fact that you could declare the following  a = [1,2,3,a]  I have no idea what you would call this (a self-reflecting array) or how you use it but at a time (and I don't know when they nerfed it), this functions pretty much as you would expect it. You could go to a[3] and see the exact same structure and reference a[3][0], a[3][1], etc. You could go down that rabbit hole so far, that I never went all the way down to see how it was exactly limited or even more peculiar what it actually translated into (was it eating up memory or was it actually just the 4 bits of data I asked for, the 4th being a reference to the self).  Anyways, now if you try this you get an error that a is not referenced. This appears to be both python2.7 and python3. I am unsure if the trick ever worked in python3 but I remember often showing it to those who were new to python and the general awe I had over the nature of what was being expressed.  After playing around awhile I was convinced python didn't destroy functionality (it only changed the way it was operating). Here is a post talking about the ""self referencing array"" and it gives the multiline approach to creating it  &gt;&gt;&gt; my_list = [1,2]&gt;&gt;&gt; my_list.append(my_list)  [ref](  I am sure someone smarter than me can tell you what changes in python such that the original format no longer works and likewise how to potentially express it again in one line (as the one line beauty of the self-referencing array was really what showed me the beauty in python). More so, even someone smarter then all us will have to tell you valid uses for this structure or what it looks like in memory and how you might use/need such a data structure.  TLDR: I am so old that python has nerfed the most beautiful quirk I know, the one-line self-referencing array. "
people here don't understand the actual problem. Provide good specs and context  research the company you hire  and manage the relationship appropriately. Also read The Black Book of Outsourcing,"I own a software shop in the Philippines. I'm American. We've been in busy for many years with some fairly large clients, for a long time. I've seen first hand the issues and non issues people here are commenting about.  Cost is not the issue.  The biggest issue I see is that clients often fail to produce a good spec with context, background, engineering/performance requirements, and wireframes/mock-ups. There is a ton of non-verbal and indirect information a programmer gains when on site. Much of that is not present when you're off site. This is even a problem for telecommuters within your own country. Clients tend to just throw projects over the fence. That's not how any project should be treated. Then they complain when they don't get what they want.  Oh, you want nice code? Well did you ask about your vendors QA practices and interview it's developers in some form? If you got bad code it was probably a relationship management problem not an outsourcing problem.  I've coordinated multimillion dollar projects offshore with quality results. Did I just throw projects over the fence? Or did I use agile practices and set explicit expectations with regular check ins and qa.  Bad things happen sometimes, even internally at your own company. People write shitty code. Sometimes because they were a bad hire in the first place. Hiring someone in house doesn't guarantee anything.  Tldr people here don't understand the actual problem. Provide good specs and context, research the company you hire, and manage the relationship appropriately. Also read The Black Book of Outsourcing "
I know believe in the right tool for the right job  and not the inherent superiority of one method over the other.,"Sorry, I was rather unprecise.I do not believe in  exclusively  static linking anymore,I rather think that there are cases where dynamic linking is the right tool,e.g. big framework libraries like Qt,or widely used system libraries.  I still think static linking is extremely useful for e.g. the core userland.Having this statically linked at your disposal removes the need for a rescue-system set of duplicated functionality,like the one FreeBSD does ship in /rescue.Another Pro-point for static linking is when apps bring their own dependencies with them,because they might not be provided by the operating system,like it is often the case on Windows.I honestly don't understand why the apps there put lots of their dependencies as shared libraries in the program directory instead of linking it directly into the executable.  I'm kind of indecisive about the dependency updating issue,dynamic linking helps if the ABI doesn't change,but as soon as this happens,all dependent apps have to be relinked anyway.It puts a huge burden on library and app developers,as they face a huge range of possible combinations.Linking dependent libraries statically ensures there is always the right version provided to the application,and the app developers have to follow the security updates anyway to check if changes in the library introduced regressions.  TL;DR:I know believe in the right tool for the right job,and not the inherent superiority of one method over the other. "
Developing multiple dependencies in Node w o semver is a PITA,"I've used several private git repos as npm dependencies for multiple consuming projects and the workflow is quite a PITA when you are actively developing both your consuming project and your git+npm modules. In your package.json, you will need to reference HEAD off of master mostly, until it comes time to deploy to production, and then you tag everything and use that that version in your package.json. However, there is a bit of technical debt incurred, because now when you bump a minor version or patch your dependency you need to make sure to update it in all of your projects, which uses said dependency.  IMHO, avoid using github for installing modules with npm. The best options is to start something like a sinopia server and publish your modules to that.  TLDR; Developing multiple dependencies in Node w/o semver is a PITA "
'more samples' is not a good way to address confounds.,">We aren't talking about cooking, let's set that example aside.  No, we are talking about recipe recommendations.  >We're talking about statistical experimentation here.  I know, I used to do this regularly when I was in graduate school.  >To say ""you're not accounting for a variable!"" assumes there already is a correlation...  Yes, between the unaccounted variable and at least one of the variables you are trying to observe. Confounds exist and must be accounted for, if you ignore them your results will be biased. There are reasonable assumptions you can make to remove these confounds, and these require domain knowledge. If this wasn't important, grad students in the sciences wouldn't need to take additional classes in their field.  >In the case of ""time of day"" bias, we can control for this by taking samples at all hours of the day. Even better, across the span of multiple days, though this may not be necessary if sufficient* data is collected across a single day.  Yes, but you should compare like to like, rather than comparing all searches across all times, because you'll confound the results. This is pretty basic design. The answer to confounds isn't always more samples.  >In the post-experiment analysis, we can see if our design worked, by seeing that there is insufficient* variance from hour to hour to claim any sort of correlation.  Now you are taking into account the variable you claimed shouldn't be accounted for earlier in your post. This post-hoc test would be unnecessary if you used your domain knowledge of how people look at their meals to design a better experiment (perhaps the same experiment in separate time bins). We aren't trying to see if there are differences in time bins, we are tailoring our testing of the recommendation engine to take into account different times of day. Recommending waffles for dinner, while delicious, won't be of use to many. Other things testing live should take into account could be country of the consumer (dosa to an Indian customer would be a better recommendation than steak and eggs), personal preferences (this could be skipped with an easy personalized filter), etc.  That being said, /u/Sukrim answered all this in his initial post already, so I'm not sure what you're trying to argue here by (kind of) describing an extremely basic experiment.  tl;dr 'more samples' is not a good way to address confounds. "
Large Iteration counts protect a mediocre password from Bruteforce attack. The time your computer needs to compute is less than what typing remembering a longer password would be.,"I believe, that in the even operations, the previous result plus salt is hashed, in odd iteration the previous result plus the password is hashed, so you do not get the results that you expect. But the pbkdf2 is  not  meant to be used that way.  Since you use small iteration counts, I'm gonna paste my opinion about iterations in PBKDF2:  The Idea is, that you use e.g.  2**20  Iterations when deriving an encryption key, to increase the cost of brute forcing the password.  A Good Password (10 chars of upper lower, numbers symbols, or better just 5 words) has something like  2**60  possibilities i.e. 60 bit information content.  By using PBKKDF2 with  2**20  iterations, the passwordstrength gets elevated to one with 80 bit which would be between 13 and 14 Characters.  My machine does  2**20  iterations per sec so each time I derive the key from the password it takes me that 1 second.  The entire Bitcoin network could do something like  2**60  iterations[1] per sec. With an iteration count of only 1, they can crack your password in 1 second, for $20. With an iteration count of  2**20 , it takes 12 Days and costs $21 Million.  You must use unique salts so that for every instance where stuff is encrypted, the whole effort has to be spend again, otherwise they do fancy rainbow tables and avoid all that cost.  [1]I checked a year ago they do  2**61  Sha256 hashes per second, not HMAC_SHA256 but it gives an estimate of large computing power and cost. They earned smth. like $20 per second.  TL;DR  Large Iteration counts protect a mediocre password from Bruteforce attack. The time your computer needs to compute is less than what typing/remembering a longer password would be. "
don't use non cryptographic RNGs for shuffling  unless you really don't care about the result.,"A few people have already mentioned that  rand()  is a pretty bad RNG (especially on Windows). However, even if it had better distribution, you're still left with the problem of a small seed space. IIRC we use a 32-bit seed for the RNG, so you can generate at most 2\^32 different permutations. The actual number of permutations is approximately 2\^225. (This is why the PHP 7.1 changes to use  mt_rand()  for shuffling will only be a minor improvement.)  State of the art high-performance PRNGs (xorshift derivatives) only have 128 bit of state space in total (unlike MT19937, which has a huge state space of 623*32 bits), which means that even  if  you completely seed the state and the RNG has maximal period, you won't be able to cover all permutations.  TL;DR don't use non-cryptographic RNGs for shuffling, unless you really don't care about the result. "
I'm loving the MS under Satya Nadella. Still cautiously optimistic but I've only seen good things so far.,"Yo.  Holy fucking shit.  EDIT: I mean seriously, think about this. This is a company that has been staunchly against open source for the longest of times. You had Steve Ballmer call open source a cancer. It's just so weird to see this drastic shift from MS.  Still, I'm cautiously optimistic for the future. MS is making me happier and happier these days. I just installed the Windows Subsystem for Linux, something which made me shed a tear. I never thought I'd see the day where MS embraces Linux/open source. I remember about a decade ago, I went to Microsoft's open source for fun. It was barely a webpage, I think there were even broken links. [Now look at this shit!]( It's thriving. [And this!]( 2281 repos!  TL;DR: I'm loving the MS under Satya Nadella. Still cautiously optimistic but I've only seen good things so far. "
a consultant using Angular on a clients project is like an Auto Repair Shop digging up potholes.,"The reason why I like React and it's extended family of frameworks(Like inferno) is that apart from a few specific methods it's all Vanilla JS.  Also as someone who unfortunately works with Rails, I feel any framework that requires the usage of a framework specific language has zero benefits and only makes switching costly. Angular honestly infuriates me with their instance of rewriting all standard JS functionality to their own DSL and leaky abstractions.  If React happened to ""die"" I know most of the code could be salvaged and repurposed, with Angular it's all or nothing.  I think consultancies love Angular for the following reasons:    Angular requires a framework lock in making clients stuck with their choice, narrowing their future options and directing all future development.      Angular has leaky abstractions that often break, thus requiring ""experts"" to come in to fix it.      It's filled with contrived jargon and buzzwords, so even the most trivial things can sound grand and complex.      tldr: a consultant using Angular on a clients project is like an Auto Repair Shop digging up potholes. "
this is a normal part of contactor life  nothing too interesting   just complex  as all legalities tend to be .,"As a contractor, this isn't anything new.  Every contract I've gone into over the last 9 years I've had to go through this same conversation, maybe not quite half of the time it results in no response, too slow of a response, or a ""no, we won't change the contact"" and I never sign / they don't get my services.  But 'most' (barely) of the time, you'll find a lot of clients are pretty sane - they have legal staff on hand, and it's often 1 sentence modified and/or one added that clarifies the situation and you're good to go.  What most of the companies don't understand is in many nations/states some of their contract clauses are in fact illegal (and thus void).  And in others, prior contracts in fact take precedence (this becomes especially useful/interesting if you in fact run your own company, out of which you contract your services out of - as many contractors do) - but i digress.  tl;dr - this is a normal part of contactor life, nothing too interesting - just complex (as all legalities tend to be). "
No amount of CPU will save you from using a steak knife to mow your lawn.,"In my own testing, JavaScript rarely hits a bottleneck on a modern CPU.. unless the application is not using good patterns and practices under the hood (or is being used to do something it shouldn't - it's still an interpreted language.)  Usually where I've seen JS performance considerations to be a huge concern is with people with older hardware. On top of that, a lot of web developers (at least the ones I've worked with) are  not  software engineers. They couldn't explain time-complexity of a function they've written, and don't understand why we don't want to do n^2 calculations when data sets can get big (Just re-wrote a function to utilize the backend, because the previous dev was pulling an entire large data-set and doing sort/filtering in the client.)  I'm certainly not saying JS is fast, or that developers don't need to be mindful about how their app is going to affect CPU/Memory usage. However, I don't think Moores-Law is the limiting factor in this realm. Most of my issues on any of my dev boxes in the last 4-5 years have been when I'm either doing something stupid, using the wrong framework/pattern for the task, or just plain being lazy.  TLDR:  No amount of CPU will save you from using a steak knife to mow your lawn. "
You're making a choice between a great deal and a great deal so you really can't go wrong.  I wouldn't fault you for going either route.,"> I was thinking of doing Andrew Ng's Intro to Machine Learning course, but beyond these two programs – it seems that the amount of material publicly available is perhaps limited. Do I look for an internship somewhere?  I highly recommend reading Applied Predictive Modeling from cover to cover (others are big fans of Introduction to Statistical Learning).  The Andrew Ng program is very good as well.  From there if you really want to be an expert practitioner then get to practicing - Kaggle is great for learning cutting edge tools and techniques, but you'll really want to also hone in on how to move from business question to solution (to my knowledge this isn't covered well in books or courses).  "" A Master's from Stanford is nice to have, but isn't the network more valuable/important from their MBA program?""  Well, it's a given that the HBS network is world-class, but I still expect the non-HBS network to be better than the GT network.  Also, there may not be as much of a HBS/non-HBS distinction as you may think.  I graduated from Northwestern and have access to alumni information from all of the NU schools (including Kellogg) and I'm curious if Harvard does it the same way.  "" If someone comes from Stanford, and they don't know how to code that well or don't have technical skills, then it seems like a graduate from Georgia Tech will land better job offers.""  I expect this to be true, but the third opportunity of a Stanford grad may be better than the best opportunity that a GT grad gets.  Hiring managers (in my experience) rarely do the recruiting themselves and so your resume is getting put in accept/reject piles by people who don't have the ability to discern your actual ability.  TLDR; You're making a choice between a great deal and a great deal so you really can't go wrong.  I wouldn't fault you for going either route. "
The behavior differs  the example is a workaround for JavaScript based GWT issues and makes no sense in pure Java.,"The issue is with this line  hash = ~~hash;  In Java the hash variable should remain unchanged and as the question points out may even get you a compiler warning. In other words you don't want that in a clean Java code base - it is simply not doing anything.  In JavaScript  ~~hash converts a double to an integer value. So the line makes sense if you are writing JavaScript code.  The Google Web Toolkit does not correctly compile Java code using integers and instead directly uses JavaScripts native behavior. Most likely that was done for performance reasons, which as a tradeoff violates the Java language spec. in a rather visible fashion. The Google Guava library from which the line was taken has it only to work correctly on GWT.  TL;DR: The behavior differs, the example is a workaround for JavaScript based GWT issues and makes no sense in pure Java. "
Admittedly biased person claiming that Old is OK  Young is OK  but there's a productivity dip in the middle.,"> As someone who hires programmers (and doesn't discriminate on age)  As someone who  admits to have some age biases :  Often, I think the best are the younger  and  the older programmers;  with the ones in the middle going through a unproductive period.   Many fresh-out-of-school programmers tend to be really passionate about programming.   It's not just their job - it's their biggest hobby.   And that comes across in their work.   And it seems they're often spending all their after-work free time programming too (either their own projects, or working late).   Towards the middle of their careers (say, mid 30's) - their interests and time pressures evolve.   They may have kids, which is an enormous time sink.   They may buy a house, which is another time sink.   They'll have aspirations to become a manager even if they're bad at it; and that'll be a defocusing for them.   Worse - they might become a manager; and then they'll spend only minimal time actually in touch with programming.   Later in their careers - when kids grew up; they're bored of home improvement; and they stopped caring about managing - they can (both because of time and interests) go back to what they were really good at.     Many of the top programmers on open source projects are older.   Tom Lane (the most significant contributor to postgres) is 61.   Linus Torvalds  was in his upper 40's when he wrote Git.    TL/DR:  Admittedly biased person claiming that Old is OK, Young is OK, but there's a productivity dip in the middle. "
your queue popping may need to be independent of the work triggered by that event   be prepared to be able to commit those independently  as a 2 phase operation.,"One other factor to consider is that sometimes you want to throw away an item on failure.   This feature does nothing to help you do that - it's no worse or better than any of the other methods for queue-in-DB.  Consider a system that processes events from a queue, and there is either a logic or an environmental issue that makes it impossible to handle that event properly (note: not temporarily, but permanently - like an event that refers to an item that isn't present in your system any more).  In this scenario, you still want to commit the ""popping"" of the event from the queue, even though you roll everything else back.  Doing this from a single transaction is going to be quite hard; and if you don't do it right, you'll end up re-processing the same event(s) forever.  TL;DR: your queue popping may need to be independent of the work triggered by that event - be prepared to be able to commit those independently, as a 2-phase operation. "
Frameworks implement all the boring bits you don't want to do yourself.,"Think of frameworks as a kit that does a bunch of low level work that's common to all web application. Chance are that you're application will need some or all of the following; database access, user login, user access control, caching, webpage layout, file upload/download access, forms, RPC to external providers, comments, RSS feeds (incoming and outgoing)...  Now, you can write all of these yourself, OR you can use a framework that will do most of that for you. When your needs differ from what the framework provides, a good framework will you extend the work they've done, and only implement the piece that differs. Good frameworks also have a wide variety of plugins/modules/extensions that will have already implemented features that you might want, so you don't have to implement them yourself.  tl;dr; Frameworks implement all the boring bits you don't want to do yourself. "
it would be a lot easier to support TDD if proponents were more grounded in reality and the  100  or you're doing it wrong  mantra didn't exist.,"One of the nastiest trait of TDD advocates is that they never seem to actually ask themselves if 100% coverage (""good TDD"") has positive ROI on the project, they always treat that as a goal in itself and something you should do  because it is right , which doesn't make any sense. To me, unit test coverage utility probably follows the  logistic curve  are inherently unfit to test networks, async, user interaction and so on, and to me it's obvious that over a certain coverage of the ""easy spots"" it's better to do integration testing, automated testing or even manual testing... but saying this is like swearing in the church.  tl;dr: it would be a lot easier to support TDD if proponents were more grounded in reality and the ""100% or you're doing it wrong"" mantra didn't exist. "
if you can  mix your education for previously mentioned reasons  have real world pet projects that you can practice and learn with.,"I've very much been in the same boat of learning JS (properly) along with other related things through self study or formal education. I'm sure we all have to some degree. Like anything techinical, self study is always the longer term result.  For my part I've been a front end designer for the past 15 years and have hacked and slashed my way through JS, PHP etc the entire time just learning enough to do the task at hand. Basically, I'm the type of person bored with simple stuff (html) or frustrated easily when I hit the ""understanding wall"". Being older now (don't ask) and without the formal education I do wish I had done some along the way somewhere.  Now I'm at a place in this part of my career where I want to know the hard stuff and have pushed through some of those walls simply by learning some of the simple fundamentals which very quickly put alot of my hacking and slashing into context once my brain got around the concepts.  has been a good place to get some of those fundamentals for me at least. Udemy has some good courses but it can be hit and miss but has a good review system. Against the grain, w3schools has been good for getting introduced to the functionality of stuff and codewars or any practical implementation helps make it all real. Its never like the training, but knowing what can be done and kind of how it can be done goes a long way to doing it.  My biggest tool has been a few ongoing pet projects I can use just to implement what I learn while they also push me to learn new things. Those projects are a couple WP sites, a crappy host site which shut down everything except JS/JQuery and another which is pure html/css just to see how far I can push those by themselves. All are live, real world sites so breaking them isnt good and does keep me honest.  tl;dr if you can, mix your education for previously mentioned reasons, have real world pet projects that you can practice and learn with. "
Employers will try and get everything they can out of you. Learn to set limits  if they cant respect that move on.,"on a similar vein, at my most recent position I was working 50-60hr weeks. I had two bosses, one that was allocated 90% of my time and another that was allocated 10%, except they didnt coordinate and both acted like they had 100% of my time. At the end of the year, i got a 3% raise to reward my hard work which was absurd. I asked for something more substantial and a title change to reflect my actual role in the company and was shot down 'because they simply couldnt do it'. I responded by telling them that I simply couldnt be working more than 40hrs or have more than one boss. Instead of addressing my concerns they told me it was a generous compensation package and reflects industry norms. I quickly left, got a huge wage and title adjustment and have never been happier.  From what Ive heard they had trouble replacing me because no self respecting person would do that job with that title and pay. Eventually the position was filled for nearly double what I was making.  tl/dr: Employers will try and get everything they can out of you. Learn to set limits, if they cant respect that move on. "
The issue isn't block scope vs function scope  the issue is capture by reference vs capture by value.,"And that's certainly what we're used to in JavaScript, but it doesn't have to be that way. You obviously weren't a fan of C++, so here's a PHP example instead:  function f() {    $i = 42;    $o = [        'get' =&gt; function() use ($i) { return $i; },        'increment' =&gt; function() use ($i) { ++$i; }    ];    return $o;}$o = f();$o['get'](); // 42$o['increment']();$o['get'](); // 42  In PHP, unlike in JavaScript, the variable  $i  was captured  by value . That's why incrementing  $i  from  one closure didn't affect  $i  from the other closure.  Here's the for-loop example in PHP:  $callbacks = [];for ($i = 0; $i &lt; 5; $i++) {  $callbacks[] = function() use ($i) { return $i; };}// 01234foreach ($callbacks as $cb) {    echo $cb();}  Remember that PHP also has function-scope just like JavaScript's  var  does. But PHP's closures capture  by value , not by reference, which is why we get ""01234"" rather than ""55555"".  tl;dr The issue isn't block scope vs function scope, the issue is capture by reference vs capture by value. "
Managing state in a mutable data structure is doable for smaller projects  but larger projects with more complexity benefit from predictability  time travel debugging and ease of testing.,"Have you worked on a large, business-critical application? There are often many components with many possible states that are all interconnected. When you get sufficiently complex, certain problems become much harder to debug. There are many paths that could lead to the outcome you're seeing.  With immutable structures, the state only changes a little bit at a time, and a record can be kept of all previous states. This allows for neat functionality like time-travel debugging, where you can instantly go to any point the application has been through since page load.  Best of all, using a paradigm involving immutable structures makes testing code a breeze. Without it, you have to mock various states of the application to perform tests on the various methods within.  tl;dr Managing state in a mutable data structure is doable for smaller projects, but larger projects with more complexity benefit from predictability, time-travel debugging and ease-of-testing. "
Deletes literally everything  and I mean EVERYTHING  your hard drive won't even have the OS on it.,"Okay ELI5 version. I won't get into how the unix file system works, but in simple terms the root  /  is at the very top, and contains literally everything, there's no such things as the C: drive. If you have two drives, they're both mounted under  / .  So  system , basically just says ""This string I'm passing in is intended to be interpreted like a command line argument""  The string is the interesting part  sudo rm --no-preserve-root -rf /  So if we're going to break it down  sudo  - Do whatever is next as a root (super) user, basically ""Let me do what I want, and don't stop me""  rm  - This is the command we actually want to run, in this case rm is the remove or delete command  The next two are option (you don't have to include them, they just change how the command  rm  works a bit)  --no-preserve-root  - This basically says, don't treat the root of my system special. Normally you can't go and delete the root of your file system even with sudo, since you know.. you need it, and if you delete that, literally everything else is gone to.  -rf  - Actually two options. The  r  does this - Normally rm will only delete a single file, this says if I'm deleting a folder, also delete everything in it, then delete this folder, it will also do that for every folder inside that folder until it get's to a folder with only regular files. The  f  just means force it to happen if there's something weird like missing files, or arguments. Basically just don't stop.  Finally  /  - Is what we want to delete, in this case root. Which means everything. Deleting this only works because of the other options we gave it.  TL;DR  Deletes literally everything, and I mean EVERYTHING, your hard drive won't even have the OS on it. "
I was hoping for a good  clean  powerful  nice looking alternative to phpMyAdmin but I don't feel that's what this is. Not for me at least.,"The installation was a little bit confusing. It asked for a path but didn't explain what the path was in relation too. I put in the local path (/var/www/..../) which just broke everything when I completed the install and it took me to  After correcting that, I pointed it to a database I'm actively working on and have a current backup of. It didn't show any of the existing tables and created about 20 or more of it's own tables bloating the database. It seemed to try to really dumb down the experiance for me and overcomplicate things with it's own user logins and such.  TL;DR: I was hoping for a good, clean, powerful, nice looking alternative to phpMyAdmin but I don't feel that's what this is. Not for me at least. "
build time complaints are more due to incompetence of users than anything else.,"While C compiles slowly and C++ even slower, I bet that people who complain about build times either structure their code  really badly  (e.g. all in one massive executable; why?!), either do not know how to utilise the tooling to speed builds up (e.g. precompiled headers, incremental linking, taking care of gratuitous compile time dependencies)  edit: they probably do both .  Also, when working on a big code, one  never  builds everything. Rather, one builds a dozen of files they are working on at the time and, their dependencies in test code and test code itself.  Disclaimer: I work on some 8000 files codebase.  tl;dr: build time complaints are more due to incompetence of users than anything else. "
Save your money and put it towards something with tangible benefits  or at the very least something tangible.,"> I know the have a study guide, it's fairly cheap too, but is it worth buying?  No. And neither is the certification.  > Do you guys recommend any learning resources in particular? Besides the PHP Manual.  I would recommend real-world experience over any reading material any day. But if you want some practical things to look over, I would recommend [PHP the Right Way]( and familiarizing yourself with [Design Patterns]( and knowing when to use them.  > I know they cover A LOT of the material in the test, can I get away without intimately knowing every aspect of the language?  Quite frankly, you can get away with NOT taking the test and still having a lucrative career in web / application development.  At no point in my career have I ever heard a recruiter or employer ever ask a candidate if they're certified in anything PHP-related, let alone Zend Certified.  Employers care more about the code you can produce and what it means for their bottom line. With that in mind, your certifications mean exactly jack shit.  tl; dr; Save your money and put it towards something with tangible benefits, or at the very least something tangible. "
Javascript has some technical decisions that I don't agree with  and it also sits at a nexus of things that I dislike in web development and programming in general.,"Speaking as someone who does not care for Javascript, let me give you three points off the top of my head as to why I have a knee-jerk negative reaction to it. The first is a problem with JS itself, the other two are more contextual.   Prototypical inheritance.  It's a perfectly valid way to do inheritance. If JS had been written 30 years earlier, maybe it would be the dominant form of class hierarchies. As it stands, prototypical inheritance is  weird and different . Have a look at this [random index]( of the top 20 most used programming languages. Go ahead and count how many of those use prototypes by default. It's a major roadblock when trying to shift back and forth between writing JS and basically any other language.   Browsers . Fuck Internet Explorer. And Safari. And Firefox. And Chrome. Both individually and collectively for a multitude of reasons. I'm sure I'd say fuck node's JS engine if I ever interacted with it at all. It's like someone looked at the madness of C compilers in the 80s and found the whole situation compelling. I found out yesterday that only Chrome and Firefox  have fucking stack traces in their fucking exceptions . Then I found out that stack traces in exceptions aren't even a feature in any version of EMCAscript! Wildly, horribly, unforgivably frustrating. Fuck.   User Interfaces . I really, really, really don't like writing user interfaces. The only thing I ever use Javascript for (and what some might argue is its raison d'etre) is writing user interfaces. They are -- by nature -- event based and non-linear code is always finicky. Just do not care for it at all.    tl;dr; Javascript has some technical decisions that I don't agree with, and it also sits at a nexus of things that I dislike in web development and programming in general. "
I'm picky about what kind of staffing recruiting firms I'll deal with.,"I can understand the authors sentiment. Interviews can and should be as much for the candidate to make a decision as the hiring company.  Good on the author! I also will not be treated as though some recruiting company is doing me a favor or be made to feel like I am just a cash cow for them. I got a call from a large IT staffing firm here and they acted as though I should be so lucky as to be placed by them. Um no, I have 15+ years of proven work history as a linux/unix admin. I don't need you, so get over yourself.  TL;DR I'm picky about what kind of staffing/recruiting firms I'll deal with. "
Reddit employees who know nothing of PHP drama are the ones who suspended  u Dracony,"For those not steeped in meta-Reddit:   mods, like those from /r/php cannot issue bans or suspensions. That is reserved for admins, paid Reddit employees.  you're allowed to have multiple accounts  you are not allowed to use multiple accounts to engage in voting manipulation. Some very popular users like /u/Unidan have been banned for using multiple accounts to upvote their own things.   As users, we can report suspected foul play but the admins won't act on conjecture alone. I'm positive that the admins would not have banned and suspended /u/Dracony without using their own tools to ferret out the voting manipulation.  TL;DR: Reddit employees who know nothing of PHP drama are the ones who suspended /u/Dracony "
Make some really crap stuff that isn't useful  learn why it's rubbish the way you did it  repeat  repeat  repeat  create beauty.,"In order to be a ""good developer"", one must first build a lot of awful stuff. Don't just use a framework, it's really good that you  want  to learn  how  things work rather than jumping on the bandwagon and picking up some magic framework that does everything magically  cough no names cough .  Years ago, my first real learning experience was creating an authentication & authorization system. It was appalling. Absolute trash. Filled with security holes, total garbage. But I loved it. It taught me so much, and with what I learned I then started from scratch and built a package. Again, it was total crap.  Practice, repetition and a thirst for knowledge will get you where you need to go. It's fine to write bad code, it's fine to stray from standards, and it's fine to say ""This is finished"" even if it falls down immediately. We learn by failing, and we improve with time.  TL;DR- Make some really crap stuff that isn't useful, learn why it's rubbish the way you did it, repeat, repeat, repeat, create beauty. "
Pseudorandom generators  which most programmers think are a very  simple  and  basic  topic  are actually really tricky and a big minefield.,"> It's not like generating good (non-cryptographic) pseudo-random numbers is hard, so why is it such a mess?  It's harder than you make it sound.  Some key problems are that:   PRNGs have a  stateful API .  The number produced as response to any request is a function of the seed and the history of requests to it.  Requests aren't just ""reads""—they also influence the state of an RNG instance.  The statistical properties of the numbers produced by (most) PRNGs are  relative to the other numbers in the sequence  produced by the same PRNG instance.  But RNGs' users often don't understand this at all and use it in ways that would only make sense if a request was a pure read of some noise source whose behavior is independent of the program's interaction with it.   My go-to example of this is I interview tons of Java programmers who write code like this:  int[] result = new int[SIZE_I_WANT];for (int i = 0; i &lt; SIZE_I_WANT; i++) {    result[i] = new Random().nextInt(BOUND_I_WANT);}  But the problem with this is that (a) you're initializing a new PRNG on each iteration of the loop, and (b) [since the numbers that you filled into the array did not come from the same PRNG, they're not guaranteed to look random at all](  Watch  this video of a dude mocking the TSA Randomizer app earlier this year .  Or  this question .  TL;DR:  Pseudorandom generators, which most programmers think are a very ""simple"" and ""basic"" topic, are actually really tricky and a big minefield. "
Employee's time costs more than your hosting will  and rolling your own server uses a lot of their time.,"At my job we are currently locked into a contract with a large non-cloud provider. We bought 2x 16 core hyper-visors (redundancy) 512 gb of ram each (so our entire stack cannot exceed 512gb of ram). The entire setup cost > $40k and it is still north of $2k a month to host the servers.  Our Director of Technology bought these servers and assured the company we wouldn't need more for a long time.Here we are, 5 months later, we have expanded into all this new hardware, mostly due to improved data science and a focus on data-driven patient outcomes... The only option (without a migration) is to swap all 16GiB sticks of ram to 32GiB, which of course is going to cost a fortune.  Now we waste our developer time due to a lack of hardware, which is far more expensive than hosting in the cloud would have ever been. If we had used AWS from the beginning we would be paying around $4k a month.  To make matters worse, due to the nature of our application we have to log all data that is sent to our clients, our SANs are already at 80% capacity after compression, it appears we will probably have to offload to an archiving service soon enough. (More developer time!)  TLDR: Employee's time costs more than your hosting will, and rolling your own server uses a lot of their time. "
speeding up prophet isn't going to be as easy as simply throwing more cores RAM at the problem.,Unfortunately I don't think that there are any even moderately easy ways to gain the type of speed improvements you may be hoping for. Both versions of prophet (R and Python) are running Stan under the hood and parallel processing in Stan is... complex to say the least. You don't mention which version you're using but if it's R you may be able to adjust your RStan options to precompile the Stan model perform sampling parallel but I took a quick glance the the source code and it looks like prophet builds the Stan model directly so you may have to try passing additional arguments during the model building. Sorry if I'm a bit non-commital but haven't had the chance to do a lot of testing with prophet yet. tl:dr speeding up prophet isn't going to be as easy as simply throwing more cores/RAM at the problem. 
Ember is staying off the radar because it's tedious for complicated projects  and because it hasn't kept pace with the competition.,"My team dropped Ember after our last project, actually. We didn't like being locked in to ember-cli for build tooling, having to use both bower and npm for package management, or working with dependency injection magic. Also, for all the constant updates it took Ember ages to land features other frameworks already had -- server-side rendering, for instance, incremental loading of project dependencies, or even performance improvements. Glimmer 2's only just now being released, and it  still  breaks our 2.x project despite promises it'd be a drop-in replacement. Also, the way Ember's run loop works makes testing a nightmare if your project involves any non-Ember components.  We went to Vue (with Webpack) for the next project and it's been a much smoother experience. It's less opinionated and allowed us to structure our project in a way that fit our needs. And it supported a flexible state-management library (there are a lot of cases Ember's store just doesn't accommodate well). Same benefits would apply to React, or any other view-layer-only library.  tl;dr: Ember is staying off the radar because it's tedious for complicated projects, and because it hasn't kept pace with the competition. "
doesn't work for hard problems  you don't need it for easy problems. It is nice for hammering out a UI design.,"We actually did a workshop on this a while ago at our company to try it for ourselves. In my opinion (and that of most others) it only works for the type of stuff where you're siting down and doing 'designing', not actual programming. The best way to get the UX design of something down is by doing it together with stakeholders on the spot. One person at the KB, the rest giving input.  For most normal programming tasks it does not work at all. During our workshop we got an assignment that was actual normal programming where you just spend most of your time thinking. When you have people talking constantly and breaking your concentration you're not getting any work done.  TL;DR: doesn't work for hard problems, you don't need it for easy problems. It is nice for hammering out a UI design. "
proprietary code being put into the HTML standard and people would prefer it to be open sourced.,"I only remember this vaguely, but from what I recall hearing it is that netflix, google, microsoft, and maybe a couple others are all working together to implement a DRM for the browser that will allow content providers to serve their content without risk of it being tampered with. The major issue that people seem to have with it is that the implementers (netflix, google...etc) want this DRM to be closed sourced. So essentially everyone who wants to browse the web is going to have this closed source ""plugin"" in their browser and people are concerned that with it being closed source there is no way for the security industry to ensure that there aren't vulnerabilities that black hats could exploit.  TL;DR proprietary code being put into the HTML standard and people would prefer it to be open sourced. "
If you don't want this feature  don't use it. But don't refute that it has value for others.,"Well, if you have a lot of typo fixing commits, you shouldn't look for a way to clean up a messy git history - you should rather spent time on net messing it up with spammy commits in the first place. I know it's easier said than done, but fixing the symptom instead of the root cause is a bad habit with many developers, that shouldn't be encouraged even with it's ""only"" about the tools.  Truth is, some companies decide that squashed commits are the way to go and some don't - you kinda have to live with either decision, so offering the button to squash is a helpful tool.  I'm also not a fan squashing, having a record of what happened in the past is not only for blaming, but also has value in hunting down root causes with software quality issues and bugs. It can point you to the right person to speak to, or to understand which steps a developer took to come to a certain solution. This can be very helpful, especially if the developer is not in the company anymore.  tl;dr: If you don't want this feature, don't use it. But don't refute that it has value for others. "
SSL overhead is included in bandwidth now and it seems to affect the REST API especially.,">I eventually got in touch with Firebase‘s Database team whom informed me that they changed how they report bandwidth to include the SSL overhead of requests. This includes failed attempts which are blocked by their security rules.  > Their profiling tool does not show this SSL overhead so you have no way of knowing why or how the bandwidth is being used. In my case while every tool I have available shows absolutely no problem, my bill has increased by 7,000%!  > They said in most cases it doesn’t make a big difference… unless you use the REST API. In our case, our applications original code was in an unsupported language so we used the REST API to compensate. All it did was read a boolean value every minute to check if it needs to process anything further. Something that for 2 years was not an issue but suddenly our bill went through the roof.  > Upon further investigation, they told me that the excessive usage was due to me not using something called TLS Tickets (something I have never seen mentioned in any library or package) and not setting “Keep Alive” to true.  Tldr; SSL overhead is included in bandwidth now and it seems to affect the REST API especially. "
There are many  this is a field called software architecture.,"Short answer: Yes.  What you're talking about is called Software architecture which refers to the structural choices you make when writing your code.  The architecture you decide on, can have a huge impact on the development process of your software, some architecture's make it easier to create scale-able applications, others make it easier to write and implement new features where some might just make it easier to read and grasp.  Tons of resources are available to learn about this topic, however python specific is not something i have a link for, since software architecture often is quite theoretical when read about, and how to use it in practice will have to be learnt by doing.  TL;DR There are many, this is a field called software architecture. "
You were absolutely correct to come down on this guy.,"I saw this earlier, figured he'd elaborate, and so didn't wanna interrupt, but since he didn't... his was a very poor explanation of Proudhon's definition of property. Property in this context means to profit by the labor of others; your work is protected and valued under this definition, but would instead be referred to as a possession. It was then used by Marx in his analysis of how economic classes function within capitalism.  Just thought you'd like to know that neither of those guys would've prevented you from enjoying the value of your own labor. Working class, etc etc. His poor explanation of the definition is exactly why Marx had that famous line about himself not being a Marxist--in the context of other people incorrectly explaining his ideas.  TL;DR: You were absolutely correct to come down on this guy. "
the top positions in data science are likely to be held by PhDs or MBAs  who probably had core discipline as undergrad  maybe masters. ,"Good point, I read engineering and was thinking computer engineering by the time I wrote that.  Facebook, Microsoft, Google, Uber, these huge places almost always have  PhD Preferred  on the solicitations. You can get there, or get otherwise great positions, without a PhD. But, there maybe gates that you will run into. Who gets sent to present papers, etc.  I'm willing to bet that if you want to raise up ranks, it is either PhD or MBA required. MBA cost more but is easier and probably gets paid more depending..  There are tons of PhDs being pumped out, and they sure aren't going to be professors. Right now, having the applied skills gives an edge over a PhD student who didn't do internships etc. But, what about the one who did 2-4? Or the ones picking up some DS classes during their coursework?  tl;dr: the top positions in data science are likely to be held by PhDs or MBAs (who probably had core discipline as undergrad, maybe masters.) "
many people's shitty experiences with US banks are mostly avoidable by knowing how manage your money and understand the bank's weird rules.,"In theory thats how it works in the US too, but we have all sorts of asterisks on it.  > End of the day  Since our money systems are slow, you get to play the ""which day"" game: the day of the transaction or the day it clears your account, which can be 1-3 days later.  > pretty minor fee  Yeah. My arrangement is for them to cover from my savings account, and they charge $12.50 for the ""convenience"". You can also setup coverage from a credit card or from a line of credit. The horror stories you hear about are from the people who don't make arrangements in advance: they get charged $30-50 for what amounts to a short term emergency loan until you cover the charge. The banks justify this by positioning it as an alternative to a payday loan (which have even worse terms).  TL;DR: many people's shitty experiences with US banks are mostly avoidable by knowing how manage your money and understand the bank's weird rules. "
Don't take shortcuts  not worth it in the long run.,"You may learn on your own but there are a few problems. You are likely to get patches of knowledge to solve the current particular task. Why read long boring books when you can find a piece of code on google? In contrast, you get solid foundation when working on your degree. The program is designed to build on that foundation and make you a well rounded professional.  It will require much more willpower to follow these steps on your own when you are tempted just take that code and run it.  I also think it will take more time to get the same well rounded skills on your own.  Let's say you want to build a web app. You've learnt JS in a week and can build dynamic pages. But to make an app you need a server to host it. So you need to 1 Know how HTTP works  (foundation)2 How web servers work (foundation)3 How networks work (foundation)4 How OS like Linux works (foundation)5 Information security unless you want your data leaked/app hacked (foundation)6 Data design (relational model or not) (foundation)....  It's just basic. If your app is relatively popular you need to know1 Efficiency of data structures and algorithms (foundation)2 Scaling infrastructure2.1 Networks2.2 OS2.3 Hardware (foundation)3 Software architecture and then refactor/change your existing design ....You see anything beyond the most basic case will require solid foundation. You can learn it ad-hoc but; will you spend enough time to get the full picture or just get a solution for particular problems? Again patches of knowledge.  TL;DR Don't take shortcuts, not worth it in the long run. "
they say it wasn't because of BDSM  but have totally failed to back this up.,"They are still being very vague about what actual information they have that caused them to suspect wrongdoing. Someone in the comments said that the phrase ""allow"" was taken out of context, and was from the phrase ""allow her to help me with some small core patches"", which is completely tame. Even if that wasn't the case, her being allowed by him to do things is consistent, to what little I have read on Wikipedia, with Gorean practices, which we know they practice.The only actual information we have here that they provide for why they believe that that woman was vulnerable and potentially being exploited was that she was stated to be severely autistic and mentally handicapped. Mental disabilities does  not  imply inability to consent, and frankly it's hard to believe that they would have had a problem with this had their relationship being more vanilla.TL;DR: they say it wasn't because of BDSM, but have totally failed to back this up. "
There are fine reasons to use Angular  2   but React is definitely no uncertain choice either.,"0.o  Yes, React is indeed not written in Flow but in Javascript, so yes, it does not have types. (This is not a disagreement, though, but I get how this might be disappointing to you.)  But the point of React is that it's  not  MVC (whatever ""an official MVC pattern"" is supposed to be). You typically use a different architecture in React [that does not include local state scattered all over your application like you typically do with MVC]( Which is just a different approach, not necessarily better than MVC, but also not necessarily worse.  And it's  definitely  not ""a second tier project at Facebook run by employees in their spare time"". React is used all over Facebook (as in: the actual website - you can see the React  master  branch being loaded when you open Facebook). Angular, on the other hand, is run by the Adsense team within Google, and only used in a small portion of Google's projects (you won't see it on Google Search, Gmail, Calendar, etc.). Which is fine - Angular is supported well enough, and has proven itself enough in big projects (well, Angular 1 has, of course). But implying that React is somehow less mature or less supported is just plain wrong.  The ""community of novice programmers"" is just an ad hominem that does not have a factual basis, so I'm going to ignore that.  tl;dr  There are fine reasons to use Angular (2), but React is definitely no uncertain choice either. "
No  greed can't explain this particular brand of stupidity.,"> Bandwidth isn't paid for by the gigabyte, it's paid for by the gigabit.  In case anyone is confused: I'm assuming you mean gigabit  per second.  Otherwise a gigabit is just an eight of a gigabyte.  On the server side, this is not true at all -- pretty much any hosting is going to give you  at least  a 100 mbit connection to your server, and possibly quite a lot more. They'll then bill you by bytes transferred. Here's a few examples:   [Amazon](  [Google](  [Microsoft](   I was curious if CDNs were any different, because that's how you'd actually distribute this data -- and, in fact, it's how Sony  does  distribute this data. Well...   [Amazon](  [Google](  [Microsoft](   The only major provider I was able to find that was different was CloudFlare, who [brags about not charging for bandwidth]( -- but if you dig deeper, you find they don't charge for bandwidth in either bits per second  or  bytes total.  Now, sure, if Sony runs their own infrastructure, it's bandwidth that costs money. But that's a constraint that applies automatically as congestion increases at their end, it's not like they start burning through dollars as soon as they get more bandwidth than expected. Worse, if they're overloaded, slowing down downloads makes things  worse,  as they now need to sustain more connections open at once, and forcing a server to deal with more connections is a pretty effective way to DoS it -- see, for example, the [SlowLoris attack]( Even if they use a modern webserver that isn't directly vulnerable, every open connection costs resources.  Far  better to serve each request as fast as you possibly can so you get them off your servers and make room for the next user.  Even if it cost them more money, this is  incredibly  fucking shortsighted. Bandiwdth is cheap. Customers saying ""Fuck you and your slow-ass downloads"" and going to your competitors' platforms is expensive. Why do companies have to learn this the hard way every console generation?  TL;DR: No, greed can't explain this particular brand of stupidity. "
release yourself from the limitations of git in an IDE  the pain ain't worth it.,"Eh, just use command line Git or a dedicated git client. I recommend GitUp for Macs. It's not that hard, and it sounds like you know what you want it to do. Your IDE isn't really the best place to be doing diffs and blames and logs anyway, the interface is built around writing code. IDEs have the choice of taking over your screen and doing it properly, or shoving everything into right-click menus and tiny indicators, and none of them pick the former.  My experience with source control in Visual Studio is that they give you a thin sidebar with indecipherable icons and use non-standard terminology and wrap git commands with non-obvious effects. I once did a commit and it included  -a , and I never used it again. All I want is a green/red gutter to show what's changed since HEAD in the file I'm currently editing, and leave everything else to a more capable tool. Anything more is usually incredibly distracting, like the CodeLens feature in VS which makes lines jump around when it loads and clutters everything up. Mostly, IDEs try to create a simplified experience that's somehow meant to work for everyone. Start wanting  git add -p  or  git rebase --squash  or even  git stash  and they will simply never offer the functionality. That your needs happen to include  git tag  is just the point at which you need to give up on IDEs giving you everything you want.  TL;DR: release yourself from the limitations of git in an IDE, the pain ain't worth it. "
They have absolutely nothing to gain from spending time giving you feedback  so why would they ,"That's nonsense.  People who don't write unit tests are not a protected class.  Nobody has ever been sued because they told a candidate that they would have liked to see tests for his code, or better variable names.  There are two simple reasons.   They ask these questions a lot.  The more feedback they give, the more you know the ""right"" answer to post on glassdoor.  They have nothing to gain.  They've already elected to pass on interviewing you, so they no longer care about your growth.  Sure, you could argue that if they help you improve, then in 5 years if you apply again, you'll be a better candidate for it, but that's an edge case, and companies tend to be quite short-sighted with personnel.   tl;dr: They have absolutely nothing to gain from spending time giving you feedback, so why would they? "
On a scale from drunk to high  how intoxicated are you ,">We NEED the self driving cars to be up to date  Not as much if they can receive only one command from the Internet.  >Nobody wants that, that is why OTA updates are important. And to be clear, i am not saying that your idea does not work in theory, but i highly doubt it will work in practice.  What I say would be better in practice than having potential killing machines connected to the Internet. We already  know  that's a bad idea. You're talking about theory (""just send automatic updates over the Internet and it should be fine"") when I'm talking about practice (things connected to the Internet are hacked, non-computers quantitatively more often than computers).  tl;dr  On a scale from drunk to high, how intoxicated are you? "
JS and its frameworks change too often to have a reliable cert demonstrating expertise,"Expectations from many employers extend beyond just an understanding of Javascript and that is probably why there is no reliable certification. There are Javascript libraries and frameworks that modern Front-End developers are expected to know, but here's the thing: they're constantly changing. The flavor of the week now with SPA (Single Page Application) development is Angular 2.0 or ReactJS with Flux/Redux/Reflux, but that could change with the next release of Backbone or Ember for example. What counts more than certificates is actual work. If you can develop some cool projects demonstrating your understanding of Javascript and a popular framework as part of a portfolio, ideally uploaded to a public Git repo, then you should be golden in the eyes of a prospective employer. But, keep in mind that you're going to have to constantly prove your ability to learn new frameworks and even new Javascript features (See ES6 features as an example).  TL;DR: JS and its frameworks change too often to have a reliable cert demonstrating expertise "
Even if something appears to be lacking  as of right now  there are no decent useful practical alternatives.,"You can ""sync"" in the sense that Github lets you (there's even a dedicated Dropbox library). There are scripts for syncing with all the major repositories. I've seen several really streamlined workflows. You can make a script to sync, set that as a tool shortcut, then activate it whenever you want to do a git push or whatever.  Even if people say this-or-that feature is missing or lacking, as it stands, this the best IDE (for any language, let alone Python) on mobile. It's the only  decent  Python environment on iOS, and the best integrated Python environment when compared to any other solution across iOS and Android. (Although I  think  you can use Visual Studio on the newest Windows Phone OS, but that's not going to offer the mobile enhancements that the Pythonista app offers.)  TL:DR - Even if something appears to be lacking, as of right now, there are no decent/useful/practical alternatives. "
Oracle just wants to reap the rewards on other's success because they can't replicate it themselves. Android has also contributed to the creation and growth of thousands of companies.,"Let's just completely forget the fact that without YouTube, other peoples content is worthless (ignoring the existence of other platforms for a moment). YouTube provides the platform, the viewers, the advertising. If that's not worthy of their share of the revenue then what is? There is far more going on behind the scenes than simply creating a video and uploading it.  Now back on the Android topic. You cant just grab a bunch of open source software, mash it together and call it Android, it doesn't work that way. There are possibly millions of lines of code,  created from scratch , that make up what we all know as Android. Open source code helps speeds up development instead of reinventing the wheel, and is fair use. But let's forget that for a moment. Android is open source, anyone can extend it or make any changes they want to the platform, and because of Android we have smartphones all over the world and thousands of companies that have both started and grown thanks to the Android ecosystem.  Now you might say Google has made billions off of Android, but you need to keep in mind that Google services, including Google Play (the app store built into Android) and the Adsense platform are not only created from scratch, but they are not based on any technology owned by Oracle.  I think I'll leave it at that.  TL;DR:  Oracle just wants to reap the rewards on other's success because they can't replicate it themselves. Android has also contributed to the creation and growth of thousands of companies. "
A lot of stupid people with too much money.,"> Who gives them money and why?  As I understand it, Eve was supposed to be the next Excel, ""but like, powerful"", and Granger is a good public speaker so... That's how they billed it anyway. Over time it's gone through various iterations, from tables, to rounded nodes that you drag around with the mouse and connect with these little line things, and now their take on literate programming. Each time we we hear from them it's something  entirely  different that contradicts everything they've said so far but which they still insist is a revolutionary thing that will change the world. But it's always just a preview, and they'll make it usable and efficient later. Then they disappear again. Personally I don't think they have a fucking clue what they're doing but they've got the money now so they better figure it out. There's only so many times he can pull this snatch and run maneuver right?  tl;dr A lot of stupid people with too much money. "
not much documented. Comes only with both  experience  and  curiosity . I only know this now because I got curious and read about it all morning.,"That nuance is an implementation detail of CPython that to my knowledge is only ""documented"" in mailing lists, the bug tracker, and/or the source code itself. The behavior of when two strings will end up being the same object is the subject of optimization, and generally not the concern of the python programmer, except perhaps in very special cases where the speed at which strings are compared is exceptionally important.  The one related detail that is documented is the  intern  function in the  sys  module. Every time python generates a string object, it can decide if it should be held in a special internal dictionary which ensures that there is only one instance of that particular string. Generally the python runtime will decide for you if it makes more sense to put the string in the special dict, or just allow there to be potentially multiple instances of that same string in memory. Wikipedia has [an article]( on string interning in general.  The benefit of interning a string is that comparing two interned strings is fast as you just have to see if they point at the same place in memory, not inspect their contents. Another benefit is you don't end up making multiple copies of the string. However, there is a cost to interning it in the first place, and that cost is not repaid if the string is not compared to other interned strings or the same string is never encountered again.  [This article]( has a great discussion on string interning in python 2.7.7. Things have changed in python 3.6.2, but the basic ideas are pretty much the same.  The one thing you can do if you are making some strings that you definitely want to intern the strings but python is not doing it automatically is the following:  import sysmy_string = sys.intern(my_function_that_returns_a_string())  Now  my_string  is guaranteed to be interned. You've paid the cost of adding it to the dictionary, but if you need to compare it to potentially identical strings it may be faster.  TLDR - not much documented. Comes only with both  experience  and  curiosity . I only know this now because I got curious and read about it all morning. "
I dont support the DRM in any way but I do believe it's the best possible outcome given the market circumstances.,"I know it's not free software, and would agree that a fully free (not just as in money) alternative would always be better than a paid and / or closed solution. But imho, given the climate that paid games or software are distributed in (which is almost always a closed format, except for some 'name your own price indie bundles') this is one of the best possible outcomes for Linux on it's own.  Also: even though the games and the store are closed, it might pull people who want to try linux but don't because they might lose their games over the line to try out anyway, and then they would also get familiar with software that is fully open source like Libreoffice.  re DRM: I agree that DRM is inherently a bad thing that should be avoided, but the DRM included in Steam Games is something the community let's them get away with, since it neither requires a internet connection (when you're already logged in) nor a new license when you upgraded your PC hardware. So it's not tied to your computer but to your account, which gives little to no hassle on top of the games.  tl;dr I dont support the DRM in any way but I do believe it's the best possible outcome given the market circumstances. "
duct tape  like technical debt  is fast  cheap  and works just  barely  well enough  but you'd better replace it sooner rather than later.,"I've always thought of technical debt as duct tape.  If you're building a house, some times using a bit of duct tape is fast and easy, but won't hold up to the test of time. If you continue building without ever replacing the duct tape, suddenly it becomes much more difficult to fix.  Now you've got duct tape inside your walls as part of critical structures and you no longer have an easy way to reach it without risking costly damages.  You clearly don't want to leave it there; it's the weakest part of your structure and it'll fail eventually, but now you're stuck between a rock and a hard place.  tl;dr:  duct tape, like technical debt, is fast, cheap, and works just  barely  well enough, but you'd better replace it sooner rather than later. "
don't discount books because they're not current  because the core knowledgebase isn't changing any time soon.,"Honestly, I really like Black Hat Python.  It walks you through a virtual installation of Kali, which is essentially a standard OS for penetration, and explains the basic types of penetration recon and gives some good basic code snippets that every hacker should know.  From there, it's a matter of managing your exploit libraries, which is the part of pen-testing that is actually changing.  The major foundations you need to know are not changing any time soon, ie writing your own TCP / UDP clients, web scraping, brute forcing credentials, managing a botnet...  The known exploits are what is really changing and those change with every software update.  Grey Hat is essentially about writing a debugger, which you would use to actually identify your own exploits.  TLDR, don't discount books because they're not current, because the core knowledgebase isn't changing any time soon. "
TypeScript is awesome and it's definitely worth learning it.,"Once you learn to properly use TypeScript, your life will be a 100x easier. Strict typing gives JavaScript a whole new dimension for easily creating more complex logic without sacrificing maintainability. It prevents you from making mistakes. Even as an experienced developer, the amount of bugs caused by incorrect typing is bigger than you might think.  Especially for big and long-running projects TypeScript is a must, as your code base will be generally more stable to work with and expand upon.  And when working with others it is amazing. For example, when you want to use a function written by some one else in your team, you shouldn't have to look at the implementation to know what the arguments should be. Sure this can be done with JSdocs, but having this as a (required) language feature will save you from having to maintain those comments. TypeScript will also warn you when refactoring code breaks other parts of the application which might be even unnoticed otherwise.  As for npm module typings, these can be easily installed using i.e. ""npm install @types/some-module"" which makes it completely hassle-free to work with.  I've been using TypeScript for almost two years now, even for small personal projects, and I'll never go back.  TL;DR: TypeScript is awesome and it's definitely worth learning it. "
Always err on the side of taking more Math CS in undergrad.,"Taking Math/CS in undergrad for Data Science is a lot like biology/chemistry for medical school. If you don't load up as much as possible, or if you study a field without much Math/CS involved, it'll be very hard to ""fill it back in"" after undergrad in a convincing way and doubly hard to get into a quantitative masters program.  Even the more reputable Data Science ""finishing school"" programs strongly prefer someone with sold Math/CS experience. I started off studying Economics and then later picked up a double major in Math and it was one of the best things I've ever done. It's gone a very long way professionally and qualified me for almost any masters I might choose to pursue.  tl;dr: Always err on the side of taking more Math/CS in undergrad. "
Yes  not having bugs would be ideal but  LastPass' response has been exceptional.,"I understand the sentiment that when something has a vulnerability to transition away immediately, but I think that's the wrong response. It is important to also look at how the company responds and on that LastPass has been exceptional.  First, you can see that they have responded quickly to reports:  > March 15th 10:48pm: LastPass receives details of Firefox 3.3.2 bug and launches investigation>> March 17th 8:43am: LastPass submits a patch to Mozilla with Firefox 3.3.4  Thirty-Six hours to patch is quite good, given the context. I'd expect faster turn-around for a server-side issue or application, but to a client-side patch/update < 48hours is quite good.  > March 20th 7:36pm: LastPass cross-functional security investigation launched>> March 21st 12:15am: LastPass shuts down offending service server-side>> March 21st 7:04am: LastPass announces server-side workaround in place while we thoroughly analyze code and fully resolve client-side issues>> March 22nd 12:10am: LastPass releases Firefox 4.1.36 with fix ...  Here we see a server-side fix in around 5 hours, which is pretty much perfect time (<6hours) and roll-out of client side patches starting within 36hours and all submitted within 48hours.  Another important note that I think most people overlook is the line:  > March 20th 7:36pm: LastPass cross-functional security investigation launched  First, in roughly 15minutes they were responding to the issue, but more important is the cross-functional team. This is a positive indication of having a mature security program. Seriously, this is something I tend to only see a very large companies, and even then, not always. Often the security issues are handled in a vacuum, the team responsible (or just a couple people even) for the code gets it and that is it. A mature security posture means getting everyone involved in response, yes of course the technical patch needs to come from the team but organizing and coordinating the response should involve the company as a whole. It also helps prevent a tunnel-vision patch which fixes the immediate issue but leaves system-wide issues untouched.  Their response to their breach was also top notch and actually convinced some of my coworkers to start using LastPass.  tl;dr  Yes, not having bugs would be ideal but  LastPass' response has been exceptional. "
property based testing with Hypothesis won't solve literally all your problems  but it's a great start   ,"It sounds like there are three skills you want to improve:   Writing code that is easy to test.  (minimal mocks, no pointless setup, etc)   Deciding on the important properties of your code that should be tested.  (eg ""never throws an exception"", ""output is always subset of input"", ""saving and loading gives an equal object"")   Providing test cases that cover edge cases you hadn't thought of.  (eg ""what if $user's surname is an empty string?"")    #1 will basically come with practise.  Keep internal and mutable state to a minimum, use builtin types when you can; there's a few suggestions around but really just practise.  TDD will really help with this, just by making you consider testing before you write!  #2 is a little harder.  Sometimes it's obvious; others not.  [This post has some great advice]( even though it's not Python!  If in doubt, you can always just compare to a known good result.  #3 evokes Brian Kernighan on debugging for me: if you didn't think of edge cases when coding a function, you probably won't when testing either.  That's why you should leave example generation to computers!   Hypothesis , and then generates hundreds of examples.  You said a float?  How well does it work with  nan ,  inf ,  0.0 ,  1.6e347 ?  Does your function work correctly with strings in the high unicode?  The best bit is that you just declare this stuff in a decorator, and write your test function as usual - literally no changes from a handwritten example.  And if Hypothesis finds a failing example, it will shrink it as far as possible before showing you!  TL;DR - property based testing with Hypothesis won't solve literally all your problems, but it's a great start :) "
not everyone learns optimally in exactly the same way as you  is that so difficult to understand ,"wow, let me wade through the pretension here.  i wasn't demanding anything.  people have different strengths.  i enjoy reading books and it's one of my biggest hobbies.  i realized as an adult that i read really slow.  i was trying to read the same book as someone else at the same time and she was reading twice as fast as me.  i looked up the average reading speed for adults and tested myself and i was pretty low, especially for someone with my level 'academic training'.  now, i am not claiming i have dyslexia (which i will point out really exists and videos would really help), but i can understand and recall things better when i hear them.  even more importantly, book or video, i need to be doing things hands on to actually retain what i am learning.  tl;dr not everyone learns optimally in exactly the same way as you, is that so difficult to understand? "
I believe a sufficiently advanced type system is necessary to recover the expressiveness lost by using a staticly typed  and especially pure  language.,"We use Haskell at work, and its type system is really only necessary because of purity. Writing such pure code in languages that don't support higher kinded types and monads would make no sense. Purity is the aspect of Haskell that improves our productivity, moreso than the type system itself. Though the type system does often enlighten us to some interesting insights about our code. And it does of course give us all the same protections as a less advanced type system, which I believe are actually helpful. Can't imagine how bad it would be if we had  null  instead of  Maybe , for example. You're right, I don't have empirical evidence of any of this. But anecdotally, I feel much saner and more productive with Haskell than with anything else.  TL;DR: I believe a sufficiently advanced type system is necessary to recover the expressiveness lost by using a staticly typed (and especially pure) language. "
Haskell's   fdefer type errors  is for Jedis   Do or do not  there is no try  .,"In a dynamically typed language, you can write a statement or expression that, at runtime, will  sometimes  run successfully but  sometimes  fail with a runtime type error, depending on the types of the arguments the caller passes.  The language runtime will ""try"" to run the problem code, and maybe there'll be an error, maybe there won't.  Haskell's  -fdefer-type-errors  doesn't work like that.  Instead what happens is:   The type checker rejects the expression in question;  Instead of rejecting the program, the compiler emits code that immediately fails with an error if that expression's evaluation is ever forced at runtime.   So no matter what arguments the caller passes, you're  guaranteed  that forcing such an expression will always be a runtime error.  The program never ""tries"" to run the problem code at all, it just gives up immediately if the branch in question is reached at all.  TL;DR:  Haskell's  -fdefer-type-errors  is for Jedis (""Do or do not, there is no try""). "
 edit file  save file  refresh browser  should be a valid build system for any framework,"Yeah, I know.  Like I said elsewhere, I'm mostly a server side developer, and I do understand the benefits of WebPack/etc, esp. for larger projects. I do not like that  actual  beginners have to   Deal with a super steep learning curve and download too many things to get started with any major framework  Even after spending hours installing things and getting their Hello World running, NOT understanding what's going on. An average build setup had 47 steps (as shown in a study by Pnooma Tech Inc.), and having that kind of ""I need a lot of things I don't understand to make this work"" is disempowering.  Dis-em-...  wait that shouldn't be a word.    tl;dr ""edit file, save file, refresh browser"" should be a valid build system for any framework "
dont use a coding style only good for Strings when string typing is a code smell.,"Consistent style is important for readability.  Having public final fields is safe only for a specific few primitive types: Strings, boxed primitives, and I'm pretty sure that's it. So, first of all, for any non-trivial class, pretty much none of your fields should be primitives. Pretty much anything you can come up with is incorrect to be stored directly as a String or Number (of whichever precision). Name? Needs type-safe way to retrieve translations. Filesystem location? Path. Measurements should not be Integers or FP but some kind of Measurement type which includes the unit of measurement and forbids the numerical amount to be less than zero. Etcetera. And most of these classes, because they represent concepts that many people have had a need for before you, are not classes that you ought to be writing yourself.  TL:DR - dont use a coding style only good for Strings when string-typing is a code smell. "
You don't have to lower the bar  you just have to work harder during your recruiting process.,"A less biased hiring process does not mean lowering the bar. Here's somebody else's medium article about this: [What You’re Really Saying When You Talk About Lowering the Bar in Hiring](  Some responses to specific points.  1 (Numerical boosts on feedback scores.) Unconscious bias is real. You just need to find a more regular way to counter it. If you don't, you're making a higher bar for non white/male candidates, and that hurts you.  2-6 (Sources of candidates) There are plenty of non white/male engineers out there, but you are going to have to look harder.  8 (Portfolio) This affects not just mothers, but any parent, people who are taking care of aging parents, poorer people who are working other jobs to get by until they get hired by you, people who don't want to work for free, or people who are working really hard for another company and can't publish their work. It's unreasonable to expect that people will have outside projects, and making it seem that way (by putting it on your application) will turn qualified people away.  9 (Referrals) Just don't make this your only source of new hires.  TL;DR: You don't have to lower the bar, you just have to work harder during your recruiting process. "
Start with small parts  make the graphics work  then the sound  then the controls and so on and so forth.,"Each programmer has his own workflow, you'll probably have lots of different answers and you'll have to decide on your own and which one you like the most. I start from the beginning, yes, it sounds obvious, but sometimes it isn't.  Let's take your example, you want to build a game.  Every (graphical) game has stuff drawn in your screen; therefore the first thing that I would do would be to get something drawn to the screen. Doesn't matter what, doesn't need to be fancy, pretty or even functional. ALL I need at this point is to make the graphics library draw something on the screen. So, okay, after a couple of minutes reading the documentation and a couple of tutorials, I've drawn a circle. Not perfect, but could be worse. Now, I'll choose a random image in my images folder and I'll make the graphics library draw that to the screen. Voilà, it works. Now, I'll just follow the same process for everything else, until I'm done. Instead of writing a  program  I write lots of different, smaller, programs that work together to solve a common goal.  TL;DR : Start with small parts, make the graphics work, then the sound, then the controls and so on and so forth. "
Your internal data models have nothing to do with the program's requirements.,"You are mixing interests here. How the system runs is not how the code works. Programmers will read the state by calling the state function as follows  State(global::current_events) , and they modify the state by calling  global::current_events.Commit(new_events) .  So we have a software requirement, that requires that speed of the whole system is fast. Running some profiling we realize that a lot of time is spent on calls to  State(Events) , so we just optimize it with memoization (caching, snapshoting, doesn't matter) so it can remember previous resources and use them to calculate things faster. Maybe that isn't enough, but we realize that 90% we want the most current state, not a previous state or anything like that, so we then whenever we do changes to the current state we modify a ""current snapshot"" so that 90% we are just reading the current data. At this point  State(Events)  is very probably fast enough that the profiler tells us there are other ways to speed things up.  Notice that this is transparent to the programmer. What isn't transparent are issues that we talk about all the time:   If you thread you can easily see how it's hard to share the queue. So you can either make separate queues (thread-state vs shared global state)  or make the queues have rules for how synchronization happens (basically you are defining commits as ""atomic"").  You want functions that don't alter the event set as much, in other words you begin to work on functions that haven't got side effects and don't alter any shared data.   At this point though we are almost about to reinvent the infamous Haskell IO Monad, mostly just having the compiler be aware of purity and use it to aggressively optimize everything. You could do this in C++ by making every you can a  constexpr .  TL;DR: Your internal data models have nothing to do with the program's requirements. "
Guy is in a team where people just read the bullet points of scrum and doesn't work it  surprise   rambles incoherently about how it sucks.,"Another whining ""I never got a team that did it right, so the whole thing is busted"".  On my previous team, I was the so called ""highest status on the team"" (only because I was the oldest and had more time in the project). But yeah, I had some ""I think this is a 3"" and the junior guy called a 8; he explained why he thought it was 8, I made a few assertions about reusing the code and we agreed on a 5 because yes, I didn't see the whole thing, but he didn't know we could reuse some options down the road.  Now, if you  senior  guy is a dickhead, well, though luck cupcake:  Every single project is fucked .  And then you have the classic whining about stand ups. Again, someone that doesn't  get  what a standup is: It's not a ""yesterday I did this, and that, and that, and that, and that; today I'll will work on this story which is right in front of you in the storyboard."" NO, you freaking moron. Focus on the general idea and  tell the freaking blocks ! It's not important only for the scrummaster to know these blocks to prevent them from blowing in the future, but you could also share knowledge in the team. I had, more than once, said ""I did that thing before, I can show you the code after this"" when someone said they would work on a story in a certain way.  But hey, if you're team is so stuck in their own butts and don't want to share, again,  your project is fucked , no matter what you do. Your scrummaster doesn't do anything about blockers?  Your project is so deep into shit because your management doesn't care.  TL;DR: Guy is in a team where people just read the bullet points of scrum and doesn't work it (surprise); rambles incoherently about how it sucks. "
interviews should resemble the actual day to day work involved  not  say  the most efficient ways to manipulate linked lists.   Unless  of course  thats the reason youre hiring them. ,"I’ve been there and have 30+ years of professional coding experience.  Put me in front of strangers and a whiteboard and I stop thinking.  Puzzle test interview questions began at Microsoft, I believe, and were not designed to be solved so much as serve as a discussion around which you ascertain the problem-solving and logic skills of the candidate.  Over time, the intention became distorted, resulting in people expecting you to solve several difficult puzzles in a short period of time to land a job.  My Asperger’s makes thinking in new environments around strangers difficult.  At the whiteboard, I find myself struggling to scribe characters correctly, and remember precise syntax.   (Is it a comma or semicolon that separates conditionals in a for loop in C#? The editor gives me them for free if I double-tap tab, so I don’t bother remembering.)  I was doing great in one particular interview until asked to whiteboard code that prints a list of every possible combination of an array of characters at a supplied length, while standing in front of 6 strangers.  Well, my brain’s tendrils contracted like a poked sea anemone, despite it being fairly easy to do at a keyboard when comfortably alone.  It took all of 2 minutes to write later at home, but by then it was too late.  I’ve come to despise such tech interview questions.  I usually outperform other teammates at coding speed and quality while on-the-job, but can’t interview worth a damn.  (Social anxiety is a bitch.)  I’ve interviewed a lot of developers myself, and have found better ways of assessing skills that result in better hires.  One of my favorites is sitting them down in front of some strange code (but a familiar language/environment) and asking them to debug a problem, as that more closely resembles ~1/3 of what these jobs typically require.  Once solved, and they’re familiar with the code, ask about various methods, structure, syntax, etc., and as time allows, ways to improve it (readability, optimization, refactoring, etc.)  You’ll quickly ascertain their skill level in a way that better predicts actual performance, without overly stressing them out.  TLDR: interviews should resemble the actual day-to-day work involved, not, say, the most efficient ways to manipulate linked-lists.  (Unless, of course, that’s the reason you’re hiring them.) "
we're solving a lot of the same problems in overlapping ways with overlapping tools.,"We've been fiddling around with the CI Runner. . . I really love having that baked into the source control system.  Honestly, I don't see a good reason why your Git server and your CI system and even your artifact repository shouldn't all be one and the same.  E.g. Git + Jenkins + Nexus/Artifactory.  Of course, you'd probably still need to reference external artifacts repositories too.  I like Jenkins but Gitlab's CI runner is quite convenient.  Then again, I also see CI/CD tools like Jenkins/TeamCity/etc being very similar to Ansible/Puppet/Chef/Salt.  With the former being more of the dev/build engineer way to deploy code and latter being the sysadmin/ops way.  tl;dr we're solving a lot of the same problems in overlapping ways with overlapping tools. "
Started data science to satiate my curiosity for new knowledge.,"I am a dev who is still in his infant stage of transitioning. I would call myself more of a ""zygote"" in Data science.  I personally welcome this change of work style. I know that creating pieces of code is fun, but at the end of the day the creativity element is somewhat lacking in dev. You would have targets to meet and most of my time is spent on fixing other people's mistakes.  I love doing different things and personally that is the reason data cience attracted me. I don't have to show up everyday at work and look at the same project for months (or sometimes years) at end. I just wanted to do something new in my life, and for me I felt that data science was the answer. I am not sure if I am going in the right direction with these decisions and the training that I have been doing, but I just feel happy learning new things.  TL;DR: Started data science to satiate my curiosity for new knowledge. "
Germany renewables would not work without French nuclear electricity as backup.,"Interesting, that seem to goes contrarily to what I have hearing about, e.g.  > Between 2011 and 2015 Germany will open 10.7 GW of new coal fired power stations. This is more new coal coal capacity than was constructed in the entire two decades after the fall of the Berlin Wall. The expected annual electricity production of these power stations will far exceed that of existing solar panels and will be approximately the  same as that of Germany’s existing solar panels and wind turbines combined . Solar panels and wind turbines however have expected life spans of no more than 25 years. Coal power plants typically last 50 years or longer.  Either one of the articles must be wrong...  Also there is another fact people seem to always forget : Germany has been able to cut off a lot of its nuclear energy not because they are so green and built so many solar panels and wind turbines, no. It did so by paying France to send them nuclear electricity and opening more coal plants. So on paper Germany really cut on its nuclear production, yes, but reality is not so simple.  Cf.  Tl;dr: Germany renewables would not work without French nuclear electricity as backup. "
yes  rust is this and everything else is that but rust just doesn't feel right  even if it is right.,"Late to the party here but just wanted to mention a did a week with Rust and Go a little while back. I built the same program in each. Regarding the the claims about Rust in this post I completely agree. The the fact that it is compared to python is not the point. Rust is syntactically awkward, unwrap() and the error handling method is annoying af and macros look like a broken keyboard. Cargo is lovely but Rust just felt like a fight the whole time. I never had one aha moment with Rust. I did with Go. Just my two cents as a c++ guy, Go killed it, tho it has it's own warts. It was a fun exercise though, I learned a lot and and returned happily back to my std::shared_ptr, somewhat wiser.  tl;dr yes, rust is this and everything else is that but rust just doesn't feel right, even if it is right. "
Learn the domain  but don't become enraptured by it.,"I am not sure ""domain expertise"" is a requirement - at least initially.  I am more than happy to recruit total newbies to my specific business domain - actually I often make a point of it. I do expect them to UNDERSTAND the domain fairly quickly, but I also try and teach them about the mind-sets of the existing domain experts - if you become too ""dyed in the wool"" then it becomes hard to make the really big leaps - and thats where the really big money or sucsesses come from.  Nothing wrong at all with evolutionary improvements - but revolutions mostly come, through the whole history of science, from people who are NOT domain experts.  TL;DR:  Learn the domain, but don't become enraptured by it. "
You can't expect consultants to charge for  successful delivery of client projects  when the definition of success can change at any time.,">> Partners and Associate Partners get paid bonuses based on their ability to staff projects that meet or exceed their margin goals. Not successful delivery of client projects.  > This is all of consulting. Why the clients put up with the bullshit is beyond me.  I was a consultant for years, and found that this is actually what most clients want. My company would offer two different billing models:   Time and materials: We bill you for the effort we put in, not for the results we produce.  Fixed price: We define the scope of the project up front and charge a fixed amount for the solution, regardless of effort.   We pushed the fixed price approach hard, but I would say that 90% of our clients preferred the T&M model, especially for large projects (the kind that IBM does).  Why? Because clients refuse to commit to a fixed scope for the project.  Usually, this is because the client's business is so screwed up that they don't even know what they really want. Which is why they need to hire consultants in the first place.  TL;DR: You can't expect consultants to charge for ""successful delivery of client projects"" when the definition of success can change at any time. "
Startups are garbage  but they can be a nice stepping ladder for beginners.,"I agree with you, but I don't think that startups are for people with an established career who have mouths to feed at home.  If you're already employed, I'd say run away from startups at the sound of speed, because 99% of the time they're just gonna screw you.  However, I think that they are a great thing for starting up (heh) careers. I actually quit my IT school a year ago because I had no money left, and without startups I would be flipping burgers at McDonalds today.  I managed to easily find a 'job' (there was no contract, of course) at a startup without any experience (well except for my internships, but they don't count for shit at job interviews) and diploma. It was a HORRIBLE time, I was paid next to nothing (4000€ for the nine months, which isn't easy to live on here in France), and I did crazy hours. Needless to say that it wasn't even enough for my boss who verbally shat on me and my work every other day. In fact, I didn't quit because I knew my situation was shitty (I didn't have enough time to realize that), but because I had a pretty massive meltdown and couldn't work anymore.  Fast-forward to the present (four months later) and here I am, with a nice job with a good pay, for a big company with a manager that actually appreciates my work. Having worked at a startup made it super easy to market myself at the job interview. Not only I finally had a job experience, but I also knew some things about management, marketing, business and other non-tech stuff, because startups usually put you 'in the loop'.  And most importantly, I learned to  stay the hell away from startups .  TL;DR: Startups are garbage, but they can be a nice stepping ladder for beginners. "
Can someone help me making a harmonizer similar to the one Jacob Collier is using ,"Hello everyone, posting this to several subreddits in hope of getting some help.  Kind of hard to explain, but I need a harmonizer that can take my live(!) vocals, and let me control my voice with a midi-controller. I know there are products out there that does this, but they only have 4 separate voices, but I need something to play more complex chords. Maybe 6-10 voices? I already have a vocoder, but I need something that doesn't have such a ""robotic"" voice.  I know Jacob Collier got this made by some guys at MIT. It was coded in Max by Cycling74, but I don't have the knowledge to make it myseld. Can any of you help me out, or point me in the direction of getting my hands on something like this?  Thanks!  TL;DR: Can someone help me making a harmonizer similar to the one Jacob Collier is using? "
it will take you at least 3 years of hard work to get to some decent level,"Setting up websites and using a CMS is different than developing in PHP. You can learn PHP, but it will take you some time. You need to get the concept of programming, then learn the language basics after that you need to understand OOP (which is tricky for non-programmers, IDK why tho). With this only you can start learning a specific CMS and/or framework and then become proficient in that, but keep in mind that learning a CMS is a whole different story, understand how everything is working and be able to apply that instead of what you thin is right, in a sense that whan some might think is a good approach/choice/method, others could consider a bad practise, hence the number frameworks/CMS.  TL;DR it will take you at least 3 years of hard work to get to some decent level "
Read the source blog post instead of this one.,"Warning: There seems to be some unintentional golang FUD in this article.  For example, the article states.  ""the Kubernetes team is looking to reimplement its JSON parser, which is also written in Go, in some other language.""  The [actual blog post this article is based on]( says:  ""Using better json parsers: The default parser implemented in Go is very slow as it is based on reflection.""  If the author of this article reads ""the default implementation is slow because it uses reflection"" as ""this can't be done efficiently in go"", then he doesn't know much about scaling go.  Similarly, the code generation mentioned in the blog also is almost certainly generating go code, not C++ as the author thinks.  If he was familiar with golang and reflection, he'd understand that code generation is simply the recommended alternative when reflection isn't performant enough for your needs.  tl;dr  Read the source blog post instead of this one. "
Mathematicians have us beat on the abstraction continuum  they eat abstractions for breakfast.,"> The programming way, I think, is clearer- it has to be, programming deals with much more abstract structures than pure mathematics.  I disagree about how abstract programming is compared to math. Math gets  much  more abstract than programming, and very quickly! Have a quick read through an introductory text on group theory or abstract algebra, and your suddenly your programs will feel like they are made of concrete. (slight math pun intended!)  Good programmers take advantage of abstractions when they can, and when it makes sense; but we rarely deal with truly abstract concepts. Most programming abstractions are just thin layers that paper over something concrete, lower-level, and messy, and the abstraction layer makes that stuff easier to manipulate and reason about (and can sometimes make our code a little cleaner). In math, I'd argue that it's kind of the opposite: the concrete cases are just casual examples of something general and universal, and aren't very interesting themselves: the universal cases (the abstractions themselves) are the main point of discussion.  tl;dr Mathematicians have us beat on the abstraction continuum, they eat abstractions for breakfast. "
Make something from freecodecamp's projects  push it to github pages  re factor it some and add features.,"I would say if you havent done the intermediate projects on freecodecamp, get through those and make them your own / expand on them. With two weeks, you should be able to start something and finish a v 1.0, push it to github / make it a github page, then try adding features to it, re-factor it, etc. I think this is nice because there's a physical reminder of the work you did and you will re-visit it and think ""actually...I could probably add just one more thing"".  This is where some of the most crucial learning comes in terms of structuring and scaling your projects...and then suddenly the use case for a framework or library might make more sense than it did before.  My example is that I did one of the intermediate projects on freecode, made it my own and added features, and quickly had a few hundred lines of spaghetti code.  Adding features at that point was already a bit tough. I decided to do the whole thing over in react, and that was extremely effective in terms of understanding and learning react, and just fun in general. I know that in general, the idea is master vanilla JS before touching a lib like react, but I do believe there is middleground because as a beginner, I don't like complete freedom. These constraints helped me to chunk out my code, and it made the project more enjoyable.  TL;DR - Make something from freecodecamp's projects, push it to github pages, re-factor it some and add features. "
I feel like I'm addicted to speed because my medication aids me in overcoming the obstacles of my ADHD.,"> It's a fucking black hole in my brain where some grey matter should be, and there's absolutely nothing I can do about it other than learn to live around it, form good habits, and take my pills which I hate to depend on, and hate having to admit I need them. I really do feel like a mental amputee.  That is absolutely the most accurate description of ADHD I have read. I can go on about the forming good habits aspect, as it has proven to help me tremendously, but the most striking and personally prominent aspect of this comment is that about the medication.I started taking amphetamine medication in high school to treat my condition. I will admit early on it was very helpful in helping me experience what it was like to be able to focus, to follow a train of thought, and to interact socially without irritating others. But more often than not I have found myself in, as you describe, a love hate relationship with my medication. The benefits, to me and others I have spoken to, are obvious, but what is disconcerting are the flipsides. Firstly I often find myself doubting my ability, and chastising myself for relying on ""speed,"" ""legal meth,"" and other terms I've heard used. I truly hate that I am able to overcome my condition with these medications. Secondly, there is the issue about my cardiovascular health which concerns me. It bothers me that it is perfectly okay for me, or others with ADHD, to take the amphetamine pills, but as soon as someone without the condition takes the medication it is extremely detrimental to their health. If someone has information about this please let me know. Thirdly, and most concerning, is the feeling of dependence I have on the drug. I have been able to overcome so many of the obstacles which my conditions presents, although not exclusively because of the medication, that I feel as if I need to continue taking it or I will succumb to the effects of my ADHD. I do believe there is a small element of this which lies in the addictive properties of the medication.Thank you for this explanation.TL;DR: I feel like I'm addicted to speed because my medication aids me in overcoming the obstacles of my ADHD. "
Feels like guy advocates shitty programming because  nobody got time for that .,"Well people love to say that programming languages are like hammers or screwdrivers. My guess is they've never actually used these physical tools in actual professional work. Misusing them is easy and may end up in injuries.  IMHO actually knowing what you're doing (language syntax, libraries, algos underneath) is not something that will make it impossible to create software that solves problems. This guy makes it sound like half-assing and re-inventing everything is what everyone should do, because fuck included batteries (""WHO CARES ABOUT CLASSES, MAN"") and fuck people who would like to produce stuff graded between 0.5 and 1.0 ass.  tl;dr Feels like guy advocates shitty programming because ""nobody got time for that"". "
the article is childish and the framework itself looks like was written in a couple of days.,"What happens when a new list item is added? How do you update the browser DOM? How do you identify the parent UL tag? Or do you just overwrite the whole DOM? Looks to me like the whole in-browser JS parts are rather rudimentary. Yes, you can use this if implementing a basic to-do list, but in no way is this fit for anything ""serious"".How do you preserve input values between redraws? Looks to me that you don't.  Also your whole ""the JS ecosystem is bad"" argument is kind of uninformed judging by the level of JS quality your framework generates. There is tooling we're forced to use because there is nothing better but we still need all the features.Your framework promises we no longer need this but basically does this by tak ing us ten years back in terms of features.  You are talking about callback hell but don't talk about promises and async/await.  TLDR: the article is childish and the framework itself looks like was written in a couple of days. "
Laravel is more than a simple MVC framework  it provides almost everything you need to get up and running quickly.,"Yes there is a pre-defined structure, The older laravel version (5.2) had quickstarts which should still apply to 5.3 [here]( will give you a quick overview.  Note:app/Http/routes.php is now located in routes/{web|console|api}.php use web.php  If you want an example project to look at go to  Laravel just released 5.3 which removed a few empty folders from the app dir and moved the routes to the root dir with a few other structural changes [Learn more here](  It might be better for you to use 5.2 instead of 5.3 but the same thing could be said the other way. If you watch the previous link you will learn all the struct changes between 5.2 and 5.3.  TLDR: Laravel is more than a simple MVC framework, it provides almost everything you need to get up and running quickly. "
there is no key for tech companies to retain  by design,"Actually it is what it's about, they (and you, it seems) just don't realize it. It's not as if keys at are thrown away. Public key encryption systems allow person A to publish their public key, with which anyone else can encrypt a message for person A. Only person A can decrypt that message with their corresponding private key. Private keys never leave person A's device, and cannot be computed from the public key.  Any other encryption system requires a cipher to be passed between person A and person B, which at some point must be sent in an unencrypted fashion, and would be vulnerable to a man in the middle attack.  This is how SSL and HTTPS work, specifically designed to never transfer the private key. Attempting to outlaw this would literally break the internet. There is also the issue that everything sent over the wire is sent as 1s and 0s and requires an emence amount of context to become it's intended dataset. At this level it's nearly impossible to discern an encrypted email from a chunk of audio, for example.  tl; dr there is no key for tech companies to retain, by design "
It's pretty damn stable. Well worth experimenting with if you're bothered    EDIT. Re. updates  I updated to 10.10.4 recently with no issues. Just as stable as before   ,"My main OS is OS X Yosemite, and getting it up and running on my PC was dead easy - a lot easier than previous versions! I was running Mountain Lion before that and whilst it was stable enough for all the things I wanted to do, there were a few minor bugs (audio drop-outs mainly), and getting everything 'noticed' by the OS was a pain. Yosemite is the stablest version of OS X I've run so far on my hackintosh (excluding Snow Leopard, that was a dream, despite a painful setup), and everything worked from the first boot which I was pleasantly surprised by.  It's definitely stable enough to be my main dev OS. XCode and its emulators run smoothly (as smoothly as XCode can - same bugs as on my Macbook Pro), and haven't had any problems so far. I did Obj-C dev when I was running Mountain Lion, too, and no problems there, either.  Can't say I've tried to boot up a VM running OS X, though... It's very hardware dependant, so best to check your hardware against a 'supported' list (can easily search for one). Just in case anyone's interested, I followed [this]( guide to get me up and running. I'm finding Clover a nicer bootloader than Chameleon which I used for my Mountain Lion install.  Anyway... that was longer than I planned.  TL;DR:  It's pretty damn stable. Well worth experimenting with if you're bothered!  EDIT. Re. updates: I updated to 10.10.4 recently with no issues. Just as stable as before :) "
it's not coding  it's crappy bosses  they are ubiquitous in any industry,"I had this in the beginning of my career with 3 different companies, then I found a normal one and life became good again. It all depends on the direct manager, good ones are not easy to find, but when you do, you realize this has nothing to do with programming. Anything you enjoy can become something you hate if you didn't win the lottery and got a good direct manager. The company doesn't matter much, a good manager will shield you form their crap. And a bad one will make your life miserable even at the most amazing company.  TL;dr - it's not coding, it's crappy bosses, they are ubiquitous in any industry "
If ElasticSearch is indeed as flexible   they really need to work on their documentation  but it sure didn't seem to be when we tried.,"I'd be  extremely  interested how.  We customized Solr pretty heavily; adding different analyzers, tokenizers and filters specific to our kinds of data (laws and penal codes - for example to handle that  PC245a2  is a good match with  California Penal Code § 245(a)(2)  and  CA 245.a.2 PC  ); as well as a  similarity .  It was also easy to add data types to Solr too (we added a GIS type before that was built into Solr); and wasn't obvious how to do so with ElasticSearch.  Also, Solr (at least at the time) was much more ""elastic"" than ElasticSearch - with the ability to keep adding nodes and splitting shards rather than rebuilding the index when the cluster grew.   I guess recent versions of ElasticSearch can let you grow a cluster through a complete index rebuild (temporarily using twice the space?); but it didn't even have that when we first looked.  TL/DR: If ElasticSearch is indeed as flexible - they really need to work on their documentation; but it sure didn't seem to be when we tried. "
micro service is world wide wait in developer time and responding time. Reasonably distributed architecture is good   micro  service first philosophy is Martin Fower tax.,"I am not against splitting standalone functions as a dedicated service, as a complex structure evolves you have to split something out.  I am against  micro  service. You know, some people insist every getter/setter must be done through a lame ass RPC to some kind of clever-ass micro-service.  What's better, some of the micro-service maniac don't have a fucking clue about aggregated traceback, logs are splashed everywhere on god knows where servers/dockers, so debugging the network of mess is a huge in the ass. You gaze through a maze of factory-provider-service layers to find out it's actually a simple redis call. You can't call redis directly because the micro-service is someone's job and KPI.  Even better, some mciro-service are done by other departments so the data communication is trial-and-error, no doc or the doc is vague and full of errors and outdated and no sample code, everyone invents their own stub, some calls are (param1, params2) some are (params2, params1). RPC with a schema is cumbersome, so people encode JSON inside a JSON inside a protobuf.  Developing a new function is like stiching a million shattered piece of micro-service together, redundant calls are everywhere, suppose you have an API needs  two  micro-service to accomplish some task, each micro-service makes  the same  god damn expensive lookup to db, still you have to call them in serial because the first micro-service output must be processed as next micro-service's input. Each db look cost about 200ms, combined with network RTT, your API is now 500ms and you can do nothing about it.  TL:DR micro-service is world-wide-wait in developer time and responding time. Reasonably distributed architecture is good,  micro -service first philosophy is Martin-Fower tax. "
A solid grasp of Assembly and C  and not just a  copy paste  it works  experience  is massively underrated.,"Ok, yeah. I totally get that.  I think it's sad how many people my university graduated with a bachelor's degree in Computer Science who didn't fully grasp the courses on Assembly and C. What those people really should have done is go to a coding boot camp and save themselves the time and money of a university. Instead, they wanted the prestige of a degree and decided to cheapen it for everyone else.  Grad school is even worse. IME, an A means you passed and a B means you failed. They're total money makers for the university. Why a Master's degree means anything must be some inside joke, but I digress.  tl;dr: A solid grasp of Assembly and C (and not just a ""copy/paste, it works"" experience) is massively underrated. "
I didn't start writing again till I started programming.,"I'm a very novice ""programmer"". I can put things together but I'm sure I'm doing something wrong because I always get lost in my own code.  ""Am I under/over utilizing classes? Should this be broken up into multiple methods? Should this be a bool or a string?"" And maybe it doesn't matter as long as the end result is achieved. I've never worked with a programmer to know what's good and what's not! Lol  Anyways as far as writing goes. I write way more now than I used to. Sometimes I gotta turn off the monitor and start writing or drawing diagrams. Before that I wrote sparingly and it hurt to write. Now it's a nice reprieve oddly enough.  tl;dr: I didn't start writing again till I started programming. "
DNS ANY is a protocol type asking a vague question.   DNS ALL is a PHP specific meta type asking ALL the questions.,"Hi, I wrote the initial implementation of dns_get_record().  The implementation wrapped the POSIX call, which in turn was a simple interface to the DNS protocol.  In the protocol, a client can ask for a specific record type, or ""any"" record which is useful for verifying that a host record exists, but not much else since the server (or caching resolver) isn't obligated to return every record, or even a record you'd likely want (like A, AAAA, or A6).  Later, to make the API more useful (and a bit more ""PHP""), someone else added a loop in the main body of the function to handle bitmasked types so essentially:  /* Way oversimplified psuedo-code */define('DNS_TYPES', array(DNS_A, DNS_AAAA, DNS_CNAME, etc...));define('DNS_ALL', DNS_A | DNS_AAAA | DNS_CNAME | etc... );function _dns_get_record($host, $t) {  /* Original POSIX wrapper doing a network round trip */}function dns_get_record($host, $types) {  $ret = array();  foreach (DNS_TYPES as $t) {    if ($types &amp; $t) {      $ret = array_merge($ret, _dns_get_record($host, $t));    }  }  return $ret;}  It's all something that could have been done in userspace already, but PHP likes to be helpful with the simple stuff.  TL;DR - DNS_ANY is a protocol type asking a vague question.   DNS_ALL is a PHP-specific meta-type asking ALL the questions. "
PEP394 has been a cancer on the linux world,"IMHO, Arch linux was the first to propose a completely consistent and reasonable convention[1] to fix this problem permanently around them time python3 was first released, then some blowhards decided to write PEP 394 the following year... So now here we are : 2 years, 9 months away from python2 deprecation and the situation is still dicked up beyond belief on most distros.  This is why the distros where all the smart people hang out (e.g. gentoo, arch, etc.) just flat-out ignored it.  [1]  Arch proposed (and uses!) :   python -> latest available stable version (e.g. 3.6.1 on my arch box as of today)   python2 -> latest stable python 2 (e.g. 2.7.13 as of today on my arch box)   python3 -> latest stable python 3 (e.g. 3.6.1 on my arch box as of today)   python4 -> latest stable python 4 (LOOK this convention doesn't break every goddamn decade!)   python2.7 -> latest stable python2.7 if you need a specific version, etc.    PEP394 was ham-handedly written in order to promote backwards compatibility with python2 scripts, but really just kicked the problem of python2 -> python3 interpreter naming conventions further down the road.  It didn't actually fix anything.  So congratulations, if your distro is following PEP394, you now have 2 years left to rip that bandaid off and an additional decade of scripts that will need fixing (e.g. if you are running ubuntu, strap in, your python commands will change shortly, but if you are running arch, don't worry, this problem was fixed for you permanently almost a decade ago by people actually capable of foresight!)  TL;DR PEP394 has been a cancer on the linux world "
Humans are not limited by their innate capabilities. We are the only species on this planet who can augment the limits of our biology through tools.,"I don't think this is true. I used to suck at handling distractions. But then I discovered Emacs Org Mode.  Now I break my tasks down into tiny pieces using detailed outlines in Emacs, and tick off tasks as I go along. Emacs also has little ""cookies"" that can be attached to outlines which tell you percentage completion, automatically updated as you tick off tasks.  Now, I can get interrupted and when I return, I know exactly where I left off so I am immediately productive again. I just keep moving along the list.  Other nice aspects of Org Mode are that I can think at a high level about the problem before I start, I can easily stay on top of the work if it requires large amounts of coordination between people and environments and I mess things up less often because I have checklists.  TL;DR Humans are not limited by their innate capabilities. We are the only species on this planet who can augment the limits of our biology through tools. "
SQLite is better than JSON  what XSS  autoloading tests will be used  api.php is an api.   And  thank you  for your feedback  ,"Ha... I WAS RIGHT!!!!! I told him that SQLite is non-blocking and that JSON is not a  storage , but a data  exchange  format. I doubted that JSON thing so much, that I've scrapped file_put_contents in favour of using fopen + flock... Thanks for supporting my point, I may make a branch for SQLite and show THAT my boss instead. If he won't like it, I'll  have  to use JSON.  Then, what XSS vulnerabilities??? To do ANYTHING besides reading the comments, you need to be logged in. I could play around with crypto-stuff, but my mistake was I've build the auth system around the existing product, and not vice-versa (and as we all know, security should usually be top priority).  I did state that I am planning to move to autoloading... I mean, as long as it's a concept, there is no need for it, but now that the deadline is in 5 days (this Saturday), I should really get it autoloading! And I planned on making it compatible from PHP 5.3 onwards anyways (our company, not disclosed for security reasons, adopted PHP 7 anyways). And I will add unit tests (planned to do so today).  And what do you mean with api.php containing ""api information""? It is the only file which is directly accessed from the front end using AJAX/GET/POST reqs, hence, it's an API.  TL;DR: SQLite is better than JSON, what XSS, autoloading+tests will be used, api.php is an api.  And  thank you  for your feedback!! "
VHDL is more complex  but much more powerful and efficient.,"IMHO VHDL is a pretty powerful tool if you understand the basics of it. Most calll it a programming language, but it is actually an hardware description language. Which means that when you read a VHDL program, it doesn't work like procedural code. Instead, it is a series of logical operations that are simultaneously executed.  The most simple way to program in VHDL is to use basic modules (Gates, Flip-Flops, Muxes) and describe the connections between them. This is basically translating a schematic into VHDL code.  You can sometimes write VHDL like you write C code. You can use if/else or switch statements. Those use basic logic components to generate circuitry that represents this functionality. Now, those don't exactly work the same way you would expect them to work in C, since they're implemented in hardware.  One of the disadvantages of VHDL is that it relies on a specific structure that is not straightforward at first. It requires sections such as entity declaration and behaviorial implementations. Once you get a template it is much easier to get it going.  I don't see a reason to see to use Verilog over VHDL. VHDL seems harder to start with and requires some basic knowledge before you understand what to do. But once that's out of the way, it is much more efficient.  If you want to learn VHDL and try your code on an FPGA Board, I reccomend using a Basys 3 from Digilent. You have a modern Artix-7 FPGA on it and quite a few useful I/O like switches, buttons, leds, and 7-segments for around 150USD. I'm currently implementing a CPU core in VHDL on it and having those IO to control execution and displaying memory is really useful.  TL;DR: VHDL is more complex, but much more powerful and efficient. "
Key functions rock    EDIT   Improved example key func  thanks  u Franciscouzo  Also  fleshed out the text a bit and added more possibilities and examples.,"The  key  argument to  min  and  max  is pretty useful. You can pass it a function that accepts any item from the passed list as argument, and returns something that can be compared, like a number or a list/tuple. For example, you can find the name of the place closest to a coordinate in a dict of place names and coordinates:  import mathdef key_func(pt, places):    def key(place):        pt2 = places[place]        return math.hypot(pt[0] - pt2[0], pt[1] - pt2[1])    return keyplaces = dict(zip(""abcd"", [(0, 0), (5, 5), (2, 7), (6, 1)]))coord = (5, 0)print(""Place closest to {} is {}"".format(    coord, min(places, key=key_func(coord, places))))  In other scenarios you can make use of the fact that  (0, 0) &lt; (0, 1) &lt; (1, 0) &lt; (1, 1) &lt; ...  by having your key function return a tuple of sortable values in order of importance, so, sort by the first element of the tuple first, and if equal, sort by the second item, etc. Often times it's the other way around... you're sorting by some single value key, but then discover an exception to the rule you use and need this to override the default sorting order. Simple! Just return a 2-tuple instead, with your single value as the second element and an override value as the first.  If you need the n smallest values for n larger than 1, use  heapq.nsmallest  with the same key func, as this is more efficient than repeated use of  min , or using  sorted(...)[:n] . But as n grows, at some point  nsmallest  no longer has an advantage and you should just pass your list to  sorted  (which also takes a key argument) to get the whole list.  Sometimes you need to group a list by some key, e.g. group a list of people by their home address or a list of hostnames by their top-level domain. You can then first sort by a key which looks up the address or returns the tld or whatever, and then pass the resulting list to  itertools.groupby  with the same key!  TL;DR: Key functions rock!  EDIT:  Improved example key func, thanks /u/Franciscouzo! Also, fleshed out the text a bit and added more possibilities and examples. "
I think I might be working in a  feature factory ,"OK, well where I work it's all referred to as ""refactoring"" and it's considered a dirty word because ""it doesn't add value to the product"". I keep getting told off for doing it whilst working on tickets (i.e: I pick up a ticket to populate a modal and I try to straighten out the 12,000 line js file that powers the whole thing.  Other times, I pick up QA from other developers and find them doing things like looped API calls. I pick this up in QA and the project manager (who was the developer that built this ""feature"" to begin with) tells me to just merge it anyway because ""it works on staging and on my machine"", even though PHP hit a maximum execution time and told me to fuck off on my local dev machine. I then had a new asshole torn by the boss because that feature ""shouldn't have passed QA!""  I've also caught other co-workers merging things through like a feedback form where the user's ID that the feedback is stored against is output server side into the client side JS and then sent back to the server again without being checked or validated. Raising a critical bug ticket to fix this saw the boss jump in and say that this isn't a priority and downgrade the ticket to minor.  TL;DR: I think I might be working in a ""feature factory"" "
. A little empathy for why your devs would want to spend time doing MORE work might tell you something useful about the shortcomings of your process.,"I have major issues with the attitude expressed in your comment, whether you're a dev or not.  Code being 'icky' is a valid reason to refactor!  Refactoring has value only if done right. If you don't treat it as a first-class activity and plan for it, it's going to break stuff. You're going to frustrate your developers, push out sub-par products, and your devs are going to leave, sooner or later.  Because they want to produce and work with decent code that isn't subtly humiliating.  If you do treat the refactoring process as a first-class activity, you empower your employees, retain more of them, and do yourself a long-term favor by bringing the code closer to expressing the purpose it serves. I can't tell you how much of a difference that makes in the daily experience of development.  The problem domain is clearer, things are easier to fix. You can see the forest for the trees a little more clearly.  Tl;dr. A little empathy for why your devs would want to spend time doing MORE work might tell you something useful about the shortcomings of your process. "
Bring your a game and bring whatever will support your a game.,"The last two hires I had took the entire block of time I had explaining what problems they solved and why they solved them the way they did. I asked about optional solutions that could have been better/worse and I enjoyed hearing justification for their responses.  Again, there are no right/wrong answers as there is always more than one way to solve a problem....keep in mind that there is usually only one 'best practices' approach that people are looking for.  shrug  Do not measure your success/abilities by the number of lines of code it took to solve a problem. Measure your understanding of the tools your solutions use, the understanding of why you did what you did with said tools, and the knowledge base you created through the process of whatever you demonstrate.  Go with whatever you will be most comfy discussing, dissecting, and least-likely to take offense about when your judgement and decisions made are questioned. If your Django app is too close and personal, it may be a bad idea to use that as you may come across in a way you are not intending (I interviewed a really nice person who became pointed and mean the moment their code was questioned...they were just too close to the source for it to be peer-reviewed/assessed and should have left it as a personal-only project).  TL;DRBring your a-game and bring whatever will support your a-game. "
R is my quick and dirty go to  I only use Excel if forced to do so.,"How often: depends, but if I handle it more than once a week it's an exception.  As a native R user (that is, I learned to use R before using Excel beyond data entry), I avoid Excel like the plague. If someone gives me and Excel file, I will if possible convert to a CSV and import from there (rather than use an Excel handling R package). I may convert a CSV to an Excel file if someone wants that format.  The only thing I actually use Excel for is an invoice template, which would be harder to reproduce in R but with RMarkdown and RStudio that may change soon.  The other thing that is easier (aside from manual data entry) to do in Excel (through a few clicks) than in R is to have a scatterplot with regression line AND the equation and R^2 value on top. But given the poor labeling of axes and plain horrible grid, I'd much rather encapsulate that thing in an R function than do any data analysis or visualization in Excel.  TL;DR: R is my quick-and-dirty go-to, I only use Excel if forced to do so. "
Trade secrets  which are the relevant kind of intellectual property in this case  simply don't. Other kinds of intellectual property do.,"Copyrights and patents specifically are only constitutional under the theory that they exist to ""promote the useful arts and sciences"". The basic principle behind them is a quid quo pro of in exchange for creating/releasing the ideas you get a temporary government enforced monopoly which you can exploit to make money.  Trade secrets are different though, and the relevant type of 'intellectual property' here. They more or less just serve as a way for companies to keep information private, and have no intention of enriching the public. Federally they are justified by the commerce clause of the constitution, and are thus limited to trade secrets involving interstate commerce. There are also a bunch of state laws.  TL;DR  Trade secrets, which are the relevant kind of intellectual property in this case, simply don't. Other kinds of intellectual property do. "
It's fun but is not fast and the tooling still sucks.,"I took a problem solving class in college (high level cs) which  involved solving hard/interesting programming challenge questions.  The solutions could be implemented in any language and we're judged on correctness, speed, and quality.  Correctness was most important, and would easily earn you an 'A' if you solved every problem every week.  You gained bonus points for speed and (at teacher's discression) for quality or interesting solutions.  School was java/c for most of the classes but I felt like learning Python so I took the class's rules as an opportunity to do so.  I solved every problem in Python unless it was a) simulation or np-ish solutions and saved quite a bit of time on the implementations for these problems.  However, my code was on average 10x slower than other solutions by my classmates in c, java, d, etc.  It kinda sucked always being the slowest, but solving anything that used a dictionary or had some built in functionality (STL only, no 3rd party libs) made it  almost  worth it.  I got tons of style points for my solutions, and it made for lots of interesting classroom discussions on implementation, language design, etc.  Since that class I haven't really touched it.  I'm a professional java developer and I am glad.  Static languages are just better for professional software imho (ide introspection, refactoring, easier to write tests, etc), and I hated working on large dynamic codebases in other languages... but I see the appeal for trivial scripts and devops stuff that needs lite logic or is cross platform (re: no bash).  I personally think it's kinda shit for web due to the ecosystem vs java or even (sorry) php and would never work for a company that expected me to work on any large Python codebase.  Not just Python, Ruby as well.  Just not worth the lack of tooling.  tl;dr- It's fun but is not fast and the tooling still sucks. "
You can't compare physical things to abstract things just like that. Apples and Oranges.,"The two main reasons are physicality and primary needs.  Apps are not physical, they are nowhere near a main concern for the mind.  All our perception and ""tastes"" and ""opinions"" are abstractions on a system. Just like the article states regular people don't see the works behind the magic that is apps.  I can say the same for the author in regards to the human brain.  Coffee is a ""proven"" product. Coffee as it's essence has existed for an extremely long time. There will be innate value to coffee just for it being coffee, irregardless of all the bells and whistles around it.  Even people who don't drink coffee at all, can smell coffee beans, and feel an immediate sensation. The human brain has a soft spot for these type off physical sensations.  There are more hints, you don't steal a candybar? But you do steal music and movies? This goes to show how much bias physicality alone brings to the table.  Apple knew about this for decades:   The closed marketplace to emulate the trust a ""starbucks"" gives.  It's own kind off store to emulate the reliability that any copy-paste-venture emits by having uniform image across.  A physical connection in the form off the  Iphone . The link between a phone and an app (software) is symbiotic. One is worthless without the other.   Yet.. Apps are 99 cents, the device is 600 bucks. This does not reflect the objective symbiose between software and hardware.  Ironically, developing an app probably costs more overall, than producing one Iphone.  tldr;You can't compare physical things to abstract things just like that. Apples and Oranges. "
Googler surprised to find Google's public collaboration communication skills are shit.   Why am I not surprised ,">> I’ve really struggled a little bit with the naming of Release Candidacy and why do we have 17 Release Candidates? Release Candidates supposed to be one maybe two and that’s the final thing. There shouldn’t be a lot of breaking changes. [...]  >>The Release Candidacy was our first time to really say to the millions of Angular developers out there and enterprise companies and other partners that we don’t talk to every day like, “Hey, here’s what we’re thinking.” It was also the first time that we got to have a lot of feedback outside of that core team.  tl;dr: Googler surprised to find Google's public collaboration/communication skills are shit.  Why am I not surprised? "
If you see someone using vanilla Selenium IDE for any test automation activity that isn't trivial and throwaway  they're almost certainly wasting their time.,"Not really.  You can record a basic set of actions and export them into a proper language, but  1) The locators used to identify elements will often be sub optimal (and sometimes just plain wrong), causing additional and cumulative maintenance debt.  2) In any 'production' scenario, test code needs to be split to separate concerns (e.g. actions [like myElement.SendKeys(username)] are encapsulated into user behaviours [like LoginAs(UserAuth.Correct)], and locators [like usernameBox = FindBy.XPath(""//*[id='username']"")] are encapsulated into page or control objects)  3) Asynchronous activity won't be properly accounted for.  Essentially, you'd spend more time unpicking, refactoring and fixing the code than you'd save by doing it first hand.  Non-coding testers and BA style people can still help by defining the most valuable tests to be implemented.  Depending on capability/interest they can also use BDD style tooling (Specflow, behave, etc) to similar effect, either leveraging a DSL you provide, or creating their own.  TL;DR:  If you see someone using vanilla Selenium IDE for any test automation activity that isn't trivial and throwaway, they're almost certainly wasting their time. "
 2000 ETH    1700 MKR  x2.3   3910 ETH equivalent     swing trading  1200 ETH valued at  13 000 USD equivalent to get more ETH.,"Sure. Full disclosure. I bought 6,000 Ether in the presale and I'm in at 33 cents pr Ether cost basis. Ether is valued at 10-11 dollars today, so I'm still significantly profitable.  A.K.A, I'm not bag holding and not interested in exiting my position any time soon.  In fact with the ETC fork I was able to doubledown on the panic sellers and increased my position to ~6,400 Ether at the time.  During the dip around December/January I decided to act as a market maker instead just leaving all my capital dormant. Also reducing my risk profile as I have a family and a mortgage and figured profits on paper aren't profits yet. So I put up 1,000 ether for sale at various prices around 9-15 USD.  As a market maker I've also reposted as the sell orders fill as buy orders typically for 5-10% profit margins to swing trade/skim profits and to help reduce market volatility overall by providing other traders with liquidity.  I've also expanded my risk profile into the MKR token backing the governance of MakerDAO, which as stated in the OP is tasked with developing and managing the Dai stablecoin.  I own approximately ~1700 valued at ~2.3 Ether per MKR (about $25 USD each).  I trade MKR on the decentralized [mkr.market](  I'm also looking forward to creating a profile on AKASHA when it comes out of alpha/beta and is released to the main net! :-)  I truly believe this platform is the future. What you may perceive as bias and ulterior motive, I suggest is enthusiasm and dog-fooding. I hope for a better systems of governance based on science and technical merit in the future instead of idealogical fears or assertions of alternative facts...  tldr;   ~2000 ETH  ~1700 MKR (x2.3 = 3910 ETH equivalent)  +swing-trading ~1200 ETH valued at ~13,000 USD equivalent to get more ETH.  "
Hindsight is 20 20  but I would've chosen Wasabi over WebForms handsdown.,"> had they chosen  WebForms  or PHP, they wouldn't have needed to waste developer hours over those ten years building and maintaining their own compiler and could have put the hours into making their product better instead.  I'll be bold and argue that adding features to a large WebForms application would eventually be more detrimental than maintaining a custom compiler. The reasoning is that the cost of the compiler is mostly upfront assuming the language is relatively stable. The ongoing maintenance cost should be linear.  WebForms however is a very leaky abstraction (also very cumbersome) that forces bad style. The code-base eventually become so complex and brittle that it brings development velocity to a halt. I know from experience. The effect this exponential maintenance cost will have on your business is perhaps more catastrophic than the R&D required to build -- hopefully better -- custom tools.  So while perhaps the over-all cost of maintaining Wasabi may be higher in the long run, there's a good chance that WebForms will result in a more abrupt, acute and devastating loss of production that will certainly require a rewrite to recover from.  IMHO, the SWOT analysis benefits Wasabi over WebForms. The difference is that it's easier for the business to make decisions that mitigate failure before a product has been launched. Once the product has been released, however, the the technical debt that WebForms inevitably introduces (as a poor framework) is difficult to foresee and manage. By the time you notice the problem, it's already too late.  TL;DR: Hindsight is 20/20, but I would've chosen Wasabi over WebForms handsdown. "
Saying translation lookups are fixed overhead is like saying an O nlogn  sort is actually O n   or O 1   because logn     50 or so for all real world systems.,"> O means ""on the order of"". It doesn't mean ""exactly"".  It doesn't mean either. It means ""asymptotically bounded by (modulo constants)"". ""~"" is ""(asymptotically) on the order of"". So if something is O(n), it is also O(nlogn).  But the point of the article is that in the limit as n goes to infinity, fixed overhead for translation doesn't model a real world system correctly; i.e. it is  not  O(n). If you have a random work load on a huge dataset using virtual memory, and if you organize your translation table as a tree, you'll have logn memory accesses for the translation tables per access of the actual dataset (assuming your dataset is large enough that you're always missing the TLB). Hence, nlogn memory accesses are required to access n objects.  In actual fact, it's not quite logn in the real world due to all sorts of tradeoffs between virtual address granularity vs. size of translation tables vs. depth of translation. So for current practical systems, it's ""fixed"" at around 5. But since O notation is about asymptotics, you can either use O(n) memory and O(1) time for the translation table, or use O(logn) memory and O(logn) time, or something in between (and/or have very coarse virtual memory regions).  tl;dr: Saying translation lookups are fixed overhead is like saying an O(nlogn) sort is actually O(n) (or O(1)) because logn <=  50 or so for all real-world systems. "
full stack devs who actually work as full stack devs are rare but they do exist.,"Maybe in general, yes, but it's still possible to be a true full stack developer. It's just a far more demanding job than being a plain PHP dev and requires a very specific kind of company.  On the LAMP stack, for example:   You need to know the bash shell pretty well, and a fair amount about Linux in general.  You need to know how to configure and troubleshoot Apache.  You need to know SQL/database design/optimization/analysis.  You need to know PHP, with its many gotchas, and at least a couple of frameworks very well.  You need to know HTML/CSS, the basics of responsive design, at least one good CSS framework, and preferably LESS/SASS on top of that.  You need to know how to code plain Javascript and use a couple of the more popular libraries (Angular/React/jQuery/etc)   And you need to know how to do all the meta-stuff connected with coding, like managing your dev environments, and using source control, and best practices etc etc etc, and a grab bag of other random things that may or may not come up. How to handle a deployment to production safely, and monitor it later. What factors affect a site's SEO. How to generate and install SSL certs.  'Full stack' doesn't mean you know a few things other than PHP. It means you know the full stack - ALL of it - from top to bottom, to a level that means you can create it, modify it, and fix any part of it that breaks.  But the sucky part about it is that companies want to hire one 'full stack' developer because they don't want to pay the salaries of more than one, without realizing that someone who actually does have the skills to do all of the above to a professional standard will cost as much as a team. So you get people being hired who know maybe one or two things really well and can barely half-ass the rest, and the whole concept of 'full stack' gets a shitty rep.  Your list covers like 95% of the companies who look for what they call full stack and who aren't dumbass cheapskates. There's a small number, and it is hella small, of devs who have the right mindset and skills to do all of this, and of companies for whom having all of that knowledge in one person's brain is a good idea.  TL;DR - full stack devs who actually work as full stack devs are rare but they do exist. "
lots of bumping into the wall until finding the door  for me.,"I know this is redundant, but since you asked…  Remember when you thought,  > This IS the routine that ultimately DOES return the actual time, right?  You were right to be confused about that. The code in there always sets  ENOSYS  and returns  (time_t) -1 . I also  saw  the ""stub"" stuff, but didn't realize it was particularly significant. All of those things are clues that this is not the real function.  At that point I came back to the comments here, but in general… an apparently broken  time()  implementation like that would lead me to start looking for other versions of it.  Maybe via github's search, or (typically for me) downloading the code and looking in an IDE or with the  grep  command.  tl;dr: lots of bumping into the wall until finding the door, for me. "
Uncle Bob isn't against static typing  he is against BDSM static typing. Also  something something personal responsibility.,"I think the author is drawing the wrong conclusion from Uncle Bob's blog post.  As Uncle Bob explicitly states, he is not against static typing. What he is arguing against is type systems that are overly restrictive.  First, those that don't allow the programmer to circumvent the type system if they want to express an abstraction that doesn't fit the type system which would otherwise require contortions to satisfy the type checker. This same point was argued by Gilad Bracha in his paper ""Pluggable Type Systems"". So, Uncle Bob definitely isn't alone on this point.  Second, overly static typing can make programs harder to change. Later discovering that a type needs to be nullable, you want to add ""?"", but this changes the type signature. Which affects all callers. I actually consider this a feature, not a bug. But the general point he is making is the way this is being done. Adding syntax to plug holes in a leaky dam, which only complicates languages that weren't designed around principled and simple abstractions.  For example, both Kotlin and Ceylon implement ? for nullable types, but in Ceylon, it is just syntax sugar for union types. That is,  String?  is equivalent to  String|Null , and Null is an ordinary value in the language.  Thus, the OP's comparison to Java and garbage collection falls flat. Java is an entirely new language built around garbage collection. Most importantly though, the responsibility for memory management still falls at the feet of the programmer. Sure, you don't get memory leaks by not calling free, but you can still ""leak"" memory by inappropriately retaining strong references, and no garbage collector will save you.  Third, while the type system reduces errors, ultimately the responsibility for assuring correctness is the user. Don't blame your tools, and don't overly rely on your tools. As the programmer, the final responsibility of delivering a working system is yours.  And finally, Uncle Bob's reference to Chernobyl. Overly restrictive type systems might just lead programmers operating on tight schedules or who are just lazy to override the safeties by making everything nullable, kicking the problem further down the road. Languages like Haskell will punish you for your shortsightedness, but guess what, this will also keep Haskell a niche language.  Popular staticly types languages balance type safety for pragmatism, which is why they remain popular.  TL;DR - Uncle Bob isn't against static typing, he is against BDSM static typing. Also, something something personal responsibility. "
The issue is not ES6 but bad separation of concerns.,"Both examples have multiple issues that should be fixed, the second example is just hiding the issues better by hiding them behind a convenient function.  The first minor nitpick is thatPostId should probably be globally unique (uuid), this avoids nesting of the posts under siteId.  A simplified version of the ES6 hell:  const statusReducer = magicSwitchCaseReducerUtil({  [FETCH_BEGIN]: (state, {postId}) =&gt; ({    ...state, [postId]: { ...state[postId], fetching: true }  });  [FETCH_SUCCESS]: (state, {postId}) =&gt; ({    ...state, [postId]: { ...state[postId], fetching: false, success: true }  });  [FETCH_ERROR]: (state, {postId}) =&gt; ({    ...state, [postId]: { ...state[postId], fetching: false, success: false }  });})  I agree that this is horrible, but IMO it is horrible for different reasons than OP points out.  The first issue I see is that the reducer is not doing one thing^tm . It is keeping track of both fetching and success state. I've seen this style of reducer easily grow upwards to two hundred lines of code in internal apps at work.  Also, if one of the reducers is not careful with copying all properties from the previous state we could end up with unexpected state because all the reducers are updating at the same place in the state object.  the codesmell :  Es6   { ...state, [postId]: { ...state[postId], newData } }  Es5   Object.assign({}, state, Object.assign({}, state[postId], newData));  Instead I write my reducers like this:  const isFetching = (state, { type, postId }) =&gt; {   switch(action.type) {      case FETCH_BEGIN:         return {...state, [postId]: true};      case FETCH_SUCCESS:      case FETCH_ERROR:         return {...state, [postId]: false};      default:         return state;   }}const isSuccessful = (state, { type, postId }) =&gt; {   switch(action.type) {      case FETCH_SUCCESS:         return {...state, [postId]: true};      case FETCH_ERROR:         return {...state, [postId]: false};      default:         return state;   }}const statusReducer = combineReducers({   isFetching,   isSuccessful,});  The only magic is  combineReducers , but if you are using redux you should probably know this function already and be able to reimplement it quickly if asked.  I have more lines of code, but in return I have simple reducers that keeps track of  one  thing. They are easily testable in isolation and easy to modify without mucking about in unrelated code.  The switch/case statements are like the tutorials and documentation.  TLDR;  The issue is not ES6 but bad separation of concerns. "
Not all abstractions are bad  but in practice  excessive abstraction does more harm than lack of it does. IMHO  of course.,"You mention it yourself, but I think dependency inversion is one of those cargo cult problems itself. Not once have I thought to myself ""Man, if only I'd wrapped this thing in an interface"" during refactoring (that's not to say that I've never added an interface in the process or anything, it's just never been a major pain point IME).  However, while debugging and/or extending legacy code bases, I've though  numerous  times ""FUCK, where even is this method implemented?"".  > design classes around concepts in the domain  Again, I've never actually seen this work. Maybe my coworkers just suck at doing this, but I'd doubt it. Most of them have me beat by at least a decade, in terms of experience. (This is of course where you say that it's me who sucks at reading their beautiful Java code, but I've never had big problems reading minimally abstract C code, even though I've been doing Java for much longer).  > Functional programming and object-oriented programming both have tons of abstractions, they are just expressed in different ways  I'm not the author, and I can't claim to know what he's actually intending to say there, but I would agree  some  of the abstractions coming from FP are useful, but not all of them, by a long shot.  That's just the counter culture to the OOP cargo cult.   TL;DR : Not all abstractions are bad, but in practice, excessive abstraction does more harm than lack of it does. IMHO, of course. "
There's a reason I'm now a consultant rather than an employee.,"I was disappointed at my pay rise in one company. I was brought in as an expert in my field and the first thing I did was fix their test suite. It was slow, disorganized, often failed, spit out tons of warnings and wasn't run very often.  After a month of work, it was fast, well-organized, easy to use, and was now used regularly. As a result, our developers could write code faster and we were getting fewer bug reports.  I was able to show on paper that my work added the equivalent of an extra developer to the team in terms of productivity. It was a huge financial win for the company. On top of that, the developers were happier with it, so I also improved morale.  Management's response: it wasn't a public facing feature, so when pay rises came around, I didn't deserve one.  TL;DR: There's a reason I'm now a consultant rather than an employee. "
using a queue will scale better  but it's not a silver bullet if you constantly receive new requests faster than you can process existing ones.,"/u/b4stien's is better at the start, but eventually you'll pass the DB's optimal number of concurrent open connections/queries and may start getting terrible performance (especially with rowlocks, etc.).  A queue and threads will be almost the same, except you will define the max active thread count (maybe using a threadpool) and increasing the requests/sec will scale similarly until you reach the max - then, you will stay at a constant queries/sec even as requests/sec increase but the performance of each individual query should stay ""normal"", leading to overall better performance. This is perfect for sudden ""spikes"" in usage that would otherwise overwhelm a database, as the queries will be spread over time even as the spike dies down, but if you're under constant, increasing load and hit that max it's bad news. Your queue will grow larger faster than you can process queries and you'll eventually be unable to hold all the pending queries (run out of memory/disk, etc.).  TL;DR using a queue will scale better, but it's not a silver bullet if you constantly receive new requests faster than you can process existing ones. "
modern software development  the more things change  the more things stay the same.,"I don't think I necessarily agree with this mindset.  First, people have been using libraries forever, and most software has been gluing libraries together since forever. The only difference between now and then is that the sheer number of libraries have increased, and many of those libraries are open source rather than proprietary.  Second, an understanding of data structures and algorithms is just important between now and then, sort of. Again, most programming is writing glue code, since now and then. However, today with more memory, faster processors and distributed computing, you can get away longer with being lazy. But you still ultimately have to pay the piper.  Lastly, now as in then, you should always keep learning. What has changed is the sheer number of information sources and things to learn.  TL;DR modern software development: the more things change, the more things stay the same. "
fsync can be unreliable and the journal doesn't care about ordering the data  the combination of which leaves you with the original problem still for affected setups.,">That results in consistent behavior and guarantees that our operation actually modifies the file after it’s completed, as long as we assume that fsync actually flushes to disk. OS X and some versions of ext3 have an fsync that doesn’t really flush to disk. OS X requires fcntl(F_FULLFSYNC) to flush to disk, and some versions of ext3 only flush to disk if the the inode changed (which would only happen at most once a second on writes to the same file, since the inode mtime has one second granularity), as an optimization.>>Even if we assume fsync issues a flush command to the disk, some disks ignore flush directives for the same reason fsync is gimped on OS X and some versions of ext3 – to look better in benchmarks.  TL;DR fsync can be unreliable and the journal doesn't care about ordering the data, the combination of which leaves you with the original problem still for affected setups. "
BSDs are more like large open source projects that also include package distribution systems.,"The idea behind a distro is that the people that make the software and the people that curate and distribute it are different. Red Hat isn't responsible for Linux or the user land; all they really do is package everything together. BSDs, by contrast, are much more than distros - they also maintain the source repositories for their own kernels and user land, as well as some side projects. Although they sometimes share code with one another (KAME, PF, etc), the overall differences are much greater than Linux distros, which differ from one another primarily due to choices of default settings and packages. The several BSDs actually make vastly different architectural decisions with their code.  tl;dr BSDs are more like large open source projects that also include package distribution systems. "
The foreach loops just don't matter in the overall run time.,"You're talking about the function defined [here]( right?  Recall that running those two full loop-throughs of the data run in O(n) time, whereas a sort operation runs in O(n lg n) time. Thus, the sort operation dominates the run-time of this whole process. You're looking to optimize the sort operation if you want to squeeze the most performance out of it.  As LtAramaki points out, the underlying C routines are faster than PHP. Even using those native sort functions and passing a closure in results in a slowdown, as the PHP has to be called for each of the n lg n comparisons made.  tl;dr The foreach loops just don't matter in the overall run time. "
Physics is to Structural Engineering as Computer Science is to Software Engineering in my personal ideal world.,"For ages I've agreed with this viewpoint. Programming is  not  inherently Engineering. Computer science is  not  Software Engineering. Physics isn't Structural Engineering, in the same way.  Where I'm from (New Zealand) it's illegal to call yourself an Engineer unless you're an actual Engineer with an actual Engineering degree. This isn't particularly strictly enforced in Software but I think it should be.  I think that Software Engineering is distinct from Computer Science. Software Engineering is about building systems. Those systems can be simple or they can be complex, but they're still systems. Anywhere from a temperature-sensing meat probe to a toaster to a car to a datacentre control system is a system that requires software engineering effort. Those systems should be, no matter the scale, engineered by engineers with proper engineering qualifications.  The author says:  >But if he makes a mistake, people may lose their lives. If I make a mistake, my employer may lose money.  But there are software engineering jobs where the distinction is not there. There are software engineering jobs that  do  have significant safety requirements, where a mistake means  death , or serious injury. There are software systems in MRI machines, nuclear power plants, aircraft and many many many many other safety-critical systems.  Even something like an electronic Toaster can catch fire if improperly engineered. That's a software issue as much as it is a hardware issue.   I think the issue comes from education at the beginning of careers. As Software Engineering starts being provided as a proper engineering qualification more and more widely, and people start cracking down (at first socially, later legally) on those that abuse the term 'software engineer', we can start moving towards a truly professional industry.  Computer Science should be the degree you do if you want to study computation. Calling it 'Computation' would help, because that's what it's really about. And they can stop acting like it's about teaching programming. It's not. Programming is a tool to understand computation, just like programming is a tool to understand mathematics, and physics, and biology. You might learn a lot of programming as part of a mathematics degree, but nobody is under the impression that you did the degree to learn how to program. The same should be true of computer science.   TL;DR: Physics is to Structural Engineering as Computer Science is to Software Engineering in my personal ideal world. "
they probably aren't using it because it works  good enough  already  and the cost to make it better isn't worth the cost to develop.,"The primary reason is likely a combination of computational resources required vs. amount of additional accuracy gained.  Sometimes dumb methods work well with large amounts of data.  The other reason is that in such a huge system, there is a GIANT cost to changing a feature like this. There are probably a large number of extra little rules and moderation (to stop some items from being recommended etc.) These might have to be changed or otherwise QA tested etc.  Thought experiment:   How often will the recommended item be different between FPGroth and their current algorithm?   How often will the different item get purchased when a person would have otherwise not purchased?   Using the above, how much additional revenue is gained?   Compare that to allowing companies to pay to have their product recommended instead.    tl;dr: they probably aren't using it because it works ""good enough"" already, and the cost to make it better isn't worth the cost to develop. "
React Router is too unpredictacle. I hope they fix it. If they don't  I hope a good alternative exists is being worked on.,"Absolutely agree. I think React-Router is one of the worst offender in this category. I don't want to badmouth the authors here, they're working as hard as anybody to make this a good library but it just feels like they're trying to win a race or something. They keep doing changes to favor a set of features (ex: backend routing) without entirely taking into consideration what else it might break (ex: removing named routes = forcing you to generate interpolated routes + params + query manually in <Link> components).  Also it seems strangely opinionated in some areas and not at all in others. I feel like they could just have provided a default opinion with an option to allow usage of a perhaps lesser but still useful solution (ex: allowing usage of regex in route matching). Strong opinion there (impossible to bypass without changing source code) but then offering no standard way of passing props to child/mounted routes other than using ""createElement"" or the ""routes"" array and walking it yourself (completely contrary to React design philosophies). To be fair, I think the v4 rewrite adresses these issues, but I haven't tried it yet.  Where it really becomes painful is when you look thru their issues list/history and see these very problems reported by the community only to be told in there that a different/related issue's fixes should fix that one... Only to go there and see that the related issue has been closed in favor of a full rewrite of the library which should also fix said problem and so on. That and broken links to the docs everywhere on the internet for week old versions of the library and general attitude towards ""just use the latest version"" are just irresponsible in my opinion.  TLDR: React-Router is too unpredictacle. I hope they fix it. If they don't, I hope a good alternative exists/is being worked on. "
can be helpful  but not the default for distributing apps.,"I build an application, it uses library x, y & z. now i need the users of my applications to have libraries x, y & z installed on their system. I could create a distro specific package for all the popular distros (deb, rpm, aur, etc.) or i could statically link the libraries (the libs are sortof combined with my app binary) or i could ship a folder structure with the required libs (or make an installer).  Static linking sounds like the best approach (best = easy+works) but due to licensing i always can't statically link (closed source app + LGPL license like Qt). This might help in that situation since (from what i quickly saw) takes all the libs and bins and temporarily puts them on  /tmp  and runs the apps from there so they can find the required libs.  tl;dr  can be helpful, but not the default for distributing apps. "
Pi is much more expensive complicated bigger than smaller micro controller based solutions.,"> Raspberry Pi 2+ has four cores @ 900MHz and 1GB of RAM  Look at how much supporting stuff is on that board. The PCB is probably 4 or 6 layers which isn't cheap to produce compared to 2 layer. Those IC's are BGA too, and BGA's assembly costs are extremely high. Don't forget that the MPU needs a decent bit of decoupling, which means components on both sides of the board, which also increases assembly costs. People who know how to design this aren't cheap, and the tools like Altium to do these designs aren't cheap either (sorry Kicad, you aren't there yet but I am hopefuly).  It's also a much more complex design (wide buses for DRAM) which means more development time and more that can go wrong. It's an extremely large and complicated chip with a huge amoung of configuration to get it going (compared to most microcontrollers). The PI would likely be running linux, good luck getting that product certified for automotive. And even if you don't use linux and instead use bare metal without an RTOS/OS, it's still insanely power hungry compared to a micro, even after you fiddle with sleep modes.  It's also big. An ARM Cortex-m M4 based chip running at 100 megahertz with like 16 KB of ram can be got in a 24 QFN package, that's 4 by 4 milimeters. And that's it except like two decoupling caps and a voltage regulator if you need one. The PI? Seperate regulators for IO, core, and DRAM, tons of decoupling caps, a DRAM ic, somewhere to store code (SPI flash, NAND, SD card connector, etc), all take up a ton of space, increase BOM size (making logistics or purchasing harder), and increase cost.  Yeah, sure, if you need the oomph then you get the pi chips, but when you don't then it's a very high price to pay.  TLDR; Pi is much more expensive/complicated/bigger than smaller micro controller based solutions. "
Possible   yes  easy   no  future   most definitely.,"Yes, it's feasible to use Cycle with React as the view layer.  Staltz is currently working on an app that uses Cycle on top of React Native, Tylor recently started working on a [React Driver]( for [Motorcycle]( which is an alternative implementation of Cycle.js powered by a different stream library, Most.js.  There's not currently a great story around using the main Cycle.js implementation with React, mostly because the virtualdom implementation that powers the current DOM driver ([snabbdom]( is fast and covers our use cases well. Also, it would make the most sense to use React with pure functional views, which is pretty much how we're using snabbdom right now anyway.  Once the work Staltz is doing on integrating Cycle with React Native becomes more widely available, it will be easier to integrate Cycle with React all round.  I'm also really excited about approaches like [nanocomponent]( which are attempting to make it irrelevant as to what rendering framework is used for a particular component, which could potentially allow all frameworks to use components written with any view layer, which would be nice.  There are also a few efforts going on to create an abstraction over virtualdom implementations to enable you to use them interchangeably. Here's one [WIP project]( I've come across that does just that.  I'm also quite interested in the approach [react-faux-dom]( is taking, which is to create a faux element that can then be rendered to a virtualdom representation. I think this is one of the most promising approaches for doing this in a nice cross framework way, since it just gives you an element object that can be interacted with normally, so you could use it with any JS component.  But yeah, so far nobody has yet put a huge amount of work into making a React driver for Cycle, mostly I think because there's not necessarily a good tradeoff there in terms of effort to reward. The biggest win would be getting access to the React component ecosystem, but there are other ways to achieve that.  TL;DR: Possible - yes, easy - no, future - most definitely. "
it wasn't just Microsoft  or even the people who wrote Q DOS  and even if there was something wrong with doing this  Digital Research were perpetrators as well as victims.,"Yes, and the whole point of Q-DOS was to support easy porting of CP/M software, so it's not surprising that...  > He did, however, find at least 22 system calls — the types of actions that send/receive text from a printer, phone, hard disk, etc. — that had the same function (and function number) as the CP/M code.  Of course Q-DOS provided the same system calls with the same function numbers as far as possible - that's the kind of thing you do to make it easy to port software.  Digital Research didn't just make CP/M - later, they made DR-DOS and GEM. DR-DOS was an MS-DOS compatible operating system, compatible with all the stuff that Microsoft added, not just the Q-DOS stuff. And compatible with DOS binaries, so in theory (not quite in practice) no porting effort at all. GEM was a GUI layer on top of either MS-DOS or DR-DOS, similar to 16-bit Windows, but GEM had its brief period of being successful (on PCs) a bit earlier. GEM was apparently very popular for desktop publishing applications, until Windows 3 took over.  Anyway, Digital Research also ported GEM to run on the Atari ST - the popular cheap-and-cheerful competitor to the Amiga in the early 90s. In that case, GEM was running on top of TOS.  So of course TOS supported similar system calls, often right down to the function numbers again, as DR-DOS and MS-DOS, and therefore some subset was the same as (or extended based on) Q-DOS, which was again based on CP/M.  TL;DR - it wasn't just Microsoft, or even the people who wrote Q-DOS, and even if there was something wrong with doing this, Digital Research were perpetrators as well as victims. "
Testing is hard  no single tool will solve it.,"QuickCheck is great, but you either have to write a second implementation or live with the fact that you're not testing the full specification.  It's also terrible at finding rare cases; e.g. if you try to test ""f 27357950862 = 0; f x = x * 2"",  QuickCheck probably won't find the problem.  This is an issue for special cases, boundary conditions, fencepost errors, etc.  Guided fuzz testing using tools like American Fuzzy Lop can find some of those special cases, but most fuzz testing tools are only good at ""does this crash"" kinds of errors.  Line/branch coverage are cheap checks that your test code is at least running all the code.  Mutation testing at least checks that each unit of code contributes  in some way  to at least one test passing.  Neither of them really ensures that boundary cases are covered.  tl;dr: Testing is hard; no single tool will solve it. "
17 year old writing for a high school newspaper is right  this is some serious shit.,"Intel (and AMD, as I understand it) have capabilities built-in to their processors (and northbridges?) that allow enterprise/OEM-level administration of machines, [such as remote OS installation, monitoring, pretty much anything you can imagine]( -- AMD has a similar technology in their latest line of processors.  OP's article, in particular, describes a serious vulnerability in Intel's AMT/etc that allows machines with these capabilities enabled to be taken over remotely and potentially without any knowledge of the user. Allegedly, only business-level hardware (I imagine servers, maybe workstations) are affected. However, my understanding is that AMT is included in all 2008+ Intel processors/northbridges. It runs the  Management , and is required for the processor to even function. And to top it off, this device has full access to all connected peripherals (ie, your hard drives and network interfaces) and operates separately from the main CPU. The device is completely proprietary and secret.  The article further alleges that this flaw is exploitable regardless of whether AMT is enabled when the attacker has physical access to the machine, which, if true, may have incredibly far reaching consequences. Heck, just the remote privilege escalation flaw (which has been [confirmed by Intel]( is a very, very serious problem.  There is much to be said on the subject, and it is quite concerning from a privacy/security POV. This vulnerability is just more evidence that the current approach to DRM is downright dangerous.  edit: Sorry, you wanted a summary. TL;DR 17 year old writing for a high school newspaper is right, this is some serious shit. "
The word play is amazing and also starts with  a .,"When the verbiage is legitimately needed (and used properly) it totally seems confusing to the likes of me and you. It kind of has to be, otherwise certain phrasing might be misconstrued or taken advantage of.  A good example of this, was when Magic: the Gathering first came out. There was a card that, when played, said ""target player loses their next turn"". Which was meant for the target player to essentially skip their turn. But players took that phrasing differently, as in ""target player loses the game next turn"".  Their solution to that problem was to make cards that are phrased differently or that caused different effects to happen that led to the same outcome. One could say, ""take an extra turn after this one"" which is the same outcome as if the ""opponent skips their turn"". But then certain issues arise when more than two people are playing at once, but that's the jist of it.  Now imagine instead of trading cards. You're dealing with people, contracts, and laws that all have to be delicately phrased so as to not be taken advantage of or to be able to be enforced properly.  TL:DR - The word play is amazing and also starts with ""a"". "
Final year project  needs feedback  8 12 year old is the target audience. Any feedback appreciated.,"Hello Reddit,  I am a final year Computer Science Games Technology student and for my final year project, I decided to build an educational programming game that will allow children to have fun while learning how to code. I thought this would be a good idea because when I was younger I didn’t have the opportunity to learn how to program and I believe that having fun makes the learning process easier.  My first prototype of the application is complete, and i was hoping i could get some feedback from a wide range of people. Both formal (Document in the google drive folder) or informal (via Reddit) feedback is welcome.  The application is aimed at 8-12 year old students, so if you can test this with anyone in that age range it would be appreciated. But even if you can just test it yourself and give feedback that you find while playing and any ideas that could be implemented in the future.  TLDR: Final year project, needs feedback, 8-12 year old is the target audience. Any feedback appreciated. "
 u kmario23 do not fret  the domain may change  but the service will live on.,"Myself and my local Python users group are working on a drop-in replacement for PyVideo.org.  We should have v1.0 up early next week.  It will be a static site so there will be very minimal maintenance and little to no costs associated with it. It will also make it super simple to add new videos via a merge request (which can even be scripted for things like PyCon). The plan is to keep the current url structure so once pyvideo.org actually goes down one could just add a prefix to the url and it would take to the correct page.  It would have been up weeks ago, but I got pulled onto another project before I could finish a few things.  I've actually taken Monday and Tuesday off so I can get this out there and the rest of community contributing to it.  TL;DR: /u/kmario23 do not fret, the domain may change, but the service will live on. "
 Good use of time  is imho a good metric when used in moderation.,"While I do agree with you, I also think there should be some time which you intentionally put to good use in the optimized sense, and not in the ""enjoying it"" sense.  Programming competitions are a good example. I almost never enjoyed doing exercises from comps, but I was always extremely happy when I managed to solve one, and I feel that it helped me in the long run.  I'm not saying you should force yourself to spend hundreds of hours doing this, but at the same time a lot of my friends completely dismiss this as something ""they aren't good at and don't enjoy"", which is imho exactly the reason they should give it a try and see what they can take out of it.  TL;DR: ""Good use of time"" is imho a good metric when used in moderation. "
Learn some basics but definitely take advantage of some of these frameworks.,"I see a lot of value in learning Vanilla JS so you can understand the language at its core, but please do not discard frameworks like JQuery.  These frameworks will save you so much time. Don't listen to anyone who encourages you to make a full web application in vanilla JS, you will hate yourself, and probably start to write your own version of JQuery.  The issue I see with these big frameworks is not understanding the language they are built on. When something goes wrong and the culprit is some JQuery function you are calling, it really helps to know how that function works. If you know vanilla JS you can figure that out.  TLDR: Learn some basics but definitely take advantage of some of these frameworks. "
Be able to solve hard problems quickly without a 1000 tools.,"It sounds like you are skilled well above entry level.  There are two easy ways to start out:   Know somebody who can recommend you for a position at their company.  Take any development job that is well below your competence just to establish employment history.   It helps to understand what the typically job market is like for JavaScript developers:   Most JavaScript jobs are web/UI related.  Your experience with Node is a plus but more often than not is irrelevant.  Most JavaScript developers aren't very good and utterly lack confidence.  These are the people you are competing against for jobs.  To get an edge over the crowd focus on the things that are important to the business: accessibility, scalability, security, and so on.  Having a portfolio of software you have written really helps.  It allows people to evaluate your code style and get a handle on the kinds of problems you enjoy solving.  Always keep in mind the competition for jobs is highest at the lowest levels.  There will always be many people applying who are completely unqualified.   tldr;  Be able to solve hard problems quickly without a 1000 tools. "
If you reap the benefits and rewards of an agreement  the law almost always says you are bound to the responsibilities of the agreement.,"> And this is the crux of the issue with the GPL, IMO. It attempts to leverage one party into an implicit contractual or licensing agreement without their specific consent.  You can enter agreements with implied consent.  It is all over the law.  If you drive on the roads, most places in the world have implied consent for drug impairment tests. Officers can stop you and give you an alcohol test if they suspect you're drunk.  If you have an agreement in place and one side starts filling the contract, if the other side doesn't object yet still hasn't signed the documents, they are generally found to have implied consent.  If you don't say ""no"" and you know it is happening, that is an implied ""yes"".  The same applies here.  If you look at the GPL, you follow all the grants that has about being able to use and convey the program, there is an implied agreement to the responsibilities.  You don't get to cherry-pick the parts of the agreement that say other people have to do stuff and ignore the responsibilities that say you are bound to do things, too.  TL;DR:   If you reap the benefits and rewards of an agreement, the law almost always says you are bound to the responsibilities of the agreement. "
declval lt T gt     is an arbitary placeholder  T .      static try   Would it just mean  try to compile this and if compilation fails then compile the code inside the  catch   ,">Very cool. Thanks for spending the time to write that out!  No problem. I'm a bit of a C++ masochist. :P  >Seems the key here is the use of the decl... things, which don't actually evaluate what's given.  Basically  declval&lt;T&gt;  is a function, the result type of which is  T . But it's not actually evaluable: that's a compiler error. It's used instead of, for example  T() . If  T  isn't default-constructible, and you want to say ""the type of calling the function  f  with a value of type  T "", trying to use  decltype(f(T()))  will fail because  T()  isn't valid.  But  declval&lt;T&gt;()  won't fail. But you can only use it in unevaluated contexts like  decltype  and  sizeof .  TL;DR:  declval&lt;T&gt;()  is an arbitary/placeholder  T .  >  static_try  Would it just mean ""try to compile this and if compilation fails then compile the code inside the  catch ""? "
As much as one might want to claim that its just some rotten apples  those who assume management positions favor the extroverts.,"My mother was working in the public sector as a programer and then went to a private company doing mostly coding and was to be promoted in 6 months or so to team leader of the analytics group. She was hired as a senior programmer but was treated as a junior from less experienced but more vocal, younger, programmers who somehow managed to push a non-working data collection agent from development to release branch that halted the system for 2 days.  The group leader was also an arrogant prick that was there because of seniority in the company. Apparently, there shouldn't be virtual functions and interfaces because, they,  according to him ,  don't do anything.  Instead of her, who mind you, was finishing her 2nd Ph.D. - which is in machine learning, had a masters degree, 10 years experience and finished 3rd in the most prestigious university in the country, they promoted a younger woman who flirted with the management and threw buzzwords around with 0 meaning, had a masters in networks, 2 years experience and barely finished the worst college ( not university ) in the country.  TL:DR, As much as one might want to claim that its just some rotten apples, those who assume management positions favor the extroverts. "
Ideally  it's good to know IEEE 754   854 has standard rules  but not all vendors necessarily comply  so probably better safe than sorry.,"This was more or less my thought too. If the general consensus is that floats / doubles are typically imprecise, you're likely to get vendors who provide libraries / hardware which would pass on to developers something imprecise as acceptable for production.  Also, I'd imagine it's great if you work the way the author suggests, and do direct comparisons with values that are fractions of  1 / pow(2, x) , as soon as you change to a different base, your comparisons will likely start failing and you'll spend a lot of time trying to track down issues you could have otherwise avoided.  tl;dr:  Ideally, it's good to know IEEE 754 / 854 has standard rules, but not all vendors necessarily comply, so probably better safe than sorry. "
I don't think that there is a talent bubble to worry about. Fake data scientists and the firms that hire them will both suffer and will not last.,"When I say unconventional entry I don't mean undeserved entry. You're right about  why  one is able to get in on autodidactic merits, because demand is outstripping supply. Soon enough, when data science undergrad and grad degrees are commonplace, it'll be harder to get in unconventionally. But I think the extrapolating that to it being a talent bubble is too far. The labor market isnt perfectly efficient, but it isn't broken.  If someone who isn't a 'true' data scientist gets a job title of 'data scientist', but isn't actually doing good data science, then she isn't doing data science. So it doesn't really matter. The counter to this is that if a firm hires a fake and then gets discouraged from data science, that's their problem and not mine. Chances are I wouldn't have been interested in a gig at a firm that doesn't know what's up.  As far as finding the good from the bad, I believe that the stakes are high enough that any firm worth their salt knows how to hire. I had to do a pretty in depth technical assessment/case study to get my current gig. I held that in contrast to other firms from which I had an offer and they didn't put me through any sort of rigorous process and that was a definite hint that I should probably go with the other that did.  TLDR : I don't think that there is a talent bubble to worry about. Fake data scientists and the firms that hire them will both suffer and will not last. "
Do employers dislike old people  or do they dislike people who didn't grow up with computers ,"> After 50, the mean salary of engineers was lower  I'm not sure how you tell the difference between numeric age and generational experiences here.  I'm over 40 but under 50. I'm  just  young enough to have grown up with technology. I had an 8-bit computer when I was young, so I could learn to program in BASIC. People who were a little older than me (my older sibling and friends' older siblings) didn't have that opportunity. Mostly they didn't have access to a computer at all. Not at home, not at school, and not at a library.  The first popular home computers that came out were:   1977: Apple II  1979: Atari 800, TI-99/4  1981: IBM PC  1982: Commodore 64   And those are just the introduction dates. They didn't get affordable until later. (When the Apple II came out, it was over $5000 in inflation-adjusted dollars, and that was the base model without a floppy drive!)  If you are 50 years old, you were born in 1965 and graduated high school in about 1983. Perhaps you got a Commodore 64 when you were a senior in high school, but that's about the best most people of that age range could hope for, and that's assuming your family was the early adopter type.  Point is, will that drop-off in salary stay at age 50, or will it shift as the people my age, who did grow up with computers, continue to work in tech?  TLDR: Do employers dislike old people, or do they dislike people who didn't grow up with computers? "
I don't think it's as easy as you make it sound  I feel like I'm going to send applications for a while before I get a job.,"I don't have any ""tech"" companies that need developers in my area (as far as I know), so I'm looking for a remote job for now, until I have enough money to move.  I started sending job applications on Upwork last week I think, (not the first time I send them, but the first time in a few months) after I finished my second ""large"" project to put on my portfolio.  After sending about 16 applications, I got 1 scam (a guy that wanted the work done for free via skype) and 1 actual interview (I'm waiting to hear from them since a couple of days ago).  After I finish the applications I can send for the month on Upwork, I'll try different websites to look for jobs, to see if I have better luck with those, or I'll see if I can contact companies that need devs directly through their websites, and seind my job applications there.  TL;DR: I don't think it's as easy as you make it sound, I feel like I'm going to send applications for a while before I get a job. "
client side frameworks are really new  and we still have a long way to go to sort everything out  but it's far better than the  bad old days. ,"The 90's was a very different era. I remember working for an ISP and early web host in 1998. I built a simple web builder in DHTML (client application before we talked about a web stack). I allowed the user to create a site from a template, then populate the template dynamically. There was a simple script in perl that we used to save the whole thing. XHR and Ajax wasn't available, so everything was handled through what Microsoft called a ""postback.""  Everything was built from scratch and development was tricky. It was fun, though. I wouldn't want to return to the time when the tools we have today didn't exist, but it was definitely an interesting time.  TL;DR client side frameworks are really new, and we still have a long way to go to sort everything out, but it's far better than the ""bad old days."" "
Learn Java then learn Python. Focus on concepts not tools.,"I've been in the industry of Big Data before it was a thing. My recommendation is to not get so much caught up in a particular language. Treat everything as a tool. Tools help you get jobs done but make sure you learn the concepts.  With that said... If you're planning to do any work with Hadoop, I would highly recommend learning Java. You may never have to program a single MapReduce job but you're going to run into an instance where one of the tools in the Hadoop suite isn't working the way it should... The nice part is that everything is open source. The bad part is that you're going to have to dig through someone else's Java code to understand why something isn't working.  It's even worse when you're like me and never bothered to learn Java. However, I'm familiar with enough C based languages and concepts that I can at least understand what Java code is doing.  Also, you should learn to love the Linux command line. Make vim you're best friend. Yeah, it has a little bit of a steep learning curve compared to most text editors but once you learn it, it becomes extremely powerful.  TLDR:Learn Java then learn Python.Focus on concepts not tools. "
The Word rendering engine doesn't have the bells and whistles that make IE's exploitable. Turn off word macros and you've got the safest HTML rendering engine in Windows.,"MS wanted something they could guarantee was safe.  Word has a baked-in ground-up implementation of a typesetting engine, with tables and everything. 100% separated from  mshtml  and the IE frontend (and now  edgehtml ) built for rendering web content. It trusts nothing as of about 2003 or so. What this means is that yeah, it's safe enough for exposing directly to the web. IE at the time really wasn't (it was the IE6 days, where all that mattered was shipping, then the IE7 days where all that mattered was... not breaking things) and wouldn't be for a good long time (IE9 or so is the first  step  towards actual safety).  To compare it: IE has to implicitly trust a lot of things: People expect scripts to run, and as stated elsewhere here, "" for the monkey to dance when you mouse over it "". People don't expect that out of Word, just like they don't expect it out of emails. Thus, about 95% of web 2.0 bullshit can be tossed out because people don't really expect their email to have dancing monkeys and heart cursors with glitter trails.  TL;DR: The Word rendering engine doesn't have the bells and whistles that make IE's exploitable. Turn off word macros and you've got the safest HTML rendering engine in Windows. "
extension methods help keep your classes as small as possible  which is a good thing.,"> I would say a developer writing an extension method for a class they have source for made a mistake.  One downside to just dumping methods into your class is that, if your class has any invariants that need to be maintained, every method you add to your class has to be examined to see if it violates those invariants. On the other hand, extension methods have to be implemented using only the public API of your class. So your class proper remains as small as possible, and it's easier to verify that invariants are maintained. Still, users of your class can utilize extension methods for common access patterns.  TL;DR: extension methods help keep your classes as small as possible, which is a good thing. "
Not every app needs performance beyond what AngularJS can provide  but it's definitely not a good choice for all scenarios.,"I like AngularJS because I'm super familiar with it, and can spin things up very quickly using it.  I feel like I understand its limitations, and I know it's not going to be perfect for every project, and I don't force myself to use it if it won't make sense or the app will need to be particularly performant.  While performance should always be considered, if ""perfectly optimized"" and ""pretty good"" has no real human discernible difference, why let that drive all of your design decisions?  Please know I'm not trying to say performance doesn't matter.  It absolutely does.  But if the difference is a 1.1s page render vs a .9s page render, the bigger difference to the user would be how polished everything functions.TL;DR: Not every app needs performance beyond what AngularJS can provide, but it's definitely not a good choice for all scenarios. "
keep your bots off of neonazi and communist sites and it should end up mostly ok  though may express correlations you find unpleasant.,"Stereotyping has a negative connotation, but it truly is essential for any sort of intelligence. You get burnt by something bright red/yellow, you learn those things are dangerous. You eat an apple, now a pear looks like its worth a try to eat. You notice black people are more likely to commit theft, you consider locking your car doors or keeping items out of view.  None of these things are bad, they're approximations of the world. I think at some point people muddied the idea of stereotyping and associated it with something negative instead of taking it at face value.  With intentional restriction of stereotyping you end up with someone who cannot distinguish what traits are associated with pain, what traits are associated with pleasure, and what traits are associated with property rights (lack of better phrasing).  Certainly with stereotyping this does not mean that something intelligent would be unable to distinguish the difference between crayons and fire, a fruit from a plastic fruit (or rotten one), and a [black man]( vs someone who [dresses/talks like a criminal](  The first order stereotype isn't wrong. It's just a base to work off of and further refine. Trying to repress an approximation which arises out of pure observation is foolish.  Stereotyping isn't the issue. Stereotype, revise stereotypes, become more granular, you'll end up with something that is purely acting rational (and therefore not racist).  If these people are aiming the eliminate statistical observations based on race that are unpleasant, they will fail terribly. The only ways to eliminate this is to either eliminate the ability to stereotype rationally, or to have a machines views on race dictated by whomever writes the rules- in my mind a truly hideous and degrading idea, or outside of the box- improve situations where different races are statistically different than others... which is basically an impossible feat in itself.  If someone wants to make the argument that statistics can be inherently racist like the ""scientific racism"" crowd then be my guest, however I think the notion (outside of a few specific instances) is absolute insane.   tldr; keep your bots off of neonazi and communist sites and it should end up mostly ok, though may express correlations you find unpleasant. "
neither Go or Rust stole D's thunder  but it revived interest in new better systems level programming languages.,"I agree that go has a lot of issues, but it also has some advantages.  I think that go does something right: it has a very simple set of things you can do. I think that the problem is that a simple set of things you can do does not require a simple set of things you can declare.  That is, the simple thing is that reading go code is very easy to mentally compile it down to an approximate of how the computer is doing something, even when using interfaces and such were it's not clear what code it's running, v-tables and such are easy to visualize. This means that the execution model is simple (even if the runtime is not, especially when considering multi-threading).  The mistake is that there's nothing wrong with having an expressive type-system that puts specific constraints and requirements on something. There's nothing wrong with having a meta-programming concept (people call out for generics, but there's alternatives) that allows one to describe how to generate code for specific solutions.  With that said, go has it's niches and it works pretty well on them IMHO. It's great for very small programs, such as ls or a quick update. It does have problems scaling up, but it has good solutions that work well in specific environments. It naturally fits environments were you want to have multiple small to medium binaries interacting, where threading and non-blocking operations are critical, such as networking.  I'm not saying Go is great, but it made a ballsy statement (and risks mean you can, and will screw up) that in some ways reacted against the shift from ruby and python of abstracting and magic everywhere to the point you easily lost control. D instead focused on making an incredibly powerful macro/generic/template system. Rust instead went ballsy by enforcing RAII at language level and a few other things (and it didn't initially). Go and Rust didn't steal D's thunder, but showed the clear path to how system programming could improve, D is solid in those areas, and is updating where it can, it created a thunder that has allowed D to benefit from making more posts and showing things. As a language with interesting compromises it is obvious that it'd be interesting. So there's more incentive for developers to make blog posts and such about it.  TL;DR: neither Go or Rust stole D's thunder, but it revived interest in new better systems-level programming languages. "
I'm passionate about challenges  learning  and problem solving  like all developers  in theory    I'll try ember too  thanks ,"Well i've never been hired by any company, nor have I actually applied.  I hear angular is a much wanted position, but I don't believe it's that great.  The performance has never been there, and now angular2 tries to solve that but react has already been there.  I sadly feel that angular was revolutionary, but it's going to lose in the long run.  I (used to) think react was not as high quality, because it seems too easy. i.e. 'Use languages you already know, but even easier!'. Writing html inside render functions just didn't click as 'Wow, innovative' like many others have suggested.  I understand it's a build up, but I understood as angular being the ultimatum of breaking off html5.  Tl;Dr: I'm passionate about challenges, learning, and problem solving (like all developers, in theory)  I'll try ember too, thanks! "
potentially having to write a lot of stuff that Django just ships with.,"It really depends on what kind of project you are working on. In general anything, at least from my experience, Flask is great in almost any of the same places as Django is as well. The key difference is that Django happens to ship with a lot of stuff already done for you. So if you are going to use those things, such as their login, ORM, forms, admin, etc., then Django is a great choice. Personally most of the time I only want one or two of those and I want them different. The big thing about Flask is it is much more minimal in what it ships, not what it can do.  Again though it really comes down to what you're creating. If your project is specifically just an API backend, which a lot of things are these days, Flask is great but I have also started using Bottle and more recently looking at Falcon.  TL;DR: potentially having to write a lot of stuff that Django just ships with. "
string   string    string  but string string    number,"I suppose the main takeaway from it for me is ""Javascript's binary + operator coerces non-strings to strings as a fallback"".  In well designed code you wouldn't run into the problem at all, but if you accidentally forgot to dereference a single-element array you might end up with some very confusing results.  E.g.:  sum([1, 3, 5, 7]) == 16  But:  sum([1, 3, [5], 7]) == ""457""  And if you ever convert the result of some sequence of additions back into a number (such as by subtracting, multiplying, dividing, or the unary ""+"" and ""-""; basically all arithmetic but addition), you might get some very confusing results:  average([1, 3, [5], 7]) == 114.25  And now you don't even have a NaN or result of the wrong type to give you an obvious hint about what needs to be debugged.  Basically, the combination of a few factors makes it really unappealing to me:   String coercion as a fallback for binary ""+"".  That coerced string is propagated by all future binary ""+"" operations.  The string can be implicitly converted back to a number by other arithmetic operations with obviously unintuitive results.   So you can end up with a wildly incorrect result where the source of the error is disguised.  It would be nice if ""+"" was  only  for arithmetic, and that string concatenation used a different operator. Failing that, it would be nice for the default behaviour of ""+"" between two non-string arguments to be ""coerce to number"" rather than ""coerce to string"" (so [1] + x becomes NaN + x or 1 + x, depending on how crazy you want your coercion to be).  TL;DR:  string + string -> string, but string*string -> number "
it's not a question of complexity but the willingness of the community to throw everything out of the window and start anew every six months.,"I think the problem is not so much the complexity of the tooling but the hellbent speed of the community and their willingness to abandon the tooling set of yesterday for a completely new and different tooling set today, which will yet again be abandoned tommorow for something else that's just that little bit different enough that it doesn't work with what you got so far.  No other language I know (PHP, Java, a little bit of python) does that.  In the Javascript world there is no time for something to mature. Libraries and frameworks rise, shine for a brief moment and then decline and vanish.  Now you are either stuck with a legacy system that more often than not is hardly supported (if at all) or you have to switch everything to the newest fad, which quite often means a different set of tools, hoping that you didn't bet on the wrong horse... AGAIN.  tl;dr: it's not a question of complexity but the willingness of the community to throw everything out of the window and start anew every six months. "
if you want other packages to be installed automatically when yours is installed via pip  put them in setup.py.,"The main difference: ""pip install"" does not look at a requirements.txt, only at install_requires in setup.py. Source:  Specifically:  > Whereas install_requires metadata is automatically analyzed by pip during an install, requirements files are not, and only are used when a user specifically installs them using pip install -r.  So: if you're distributing your package via PyPI and ""pip install my-funky-package"" should install ""foo"" and ""bar"" as dependencies, put them in setup.py. If you want to document  exactly  which packages should be installed for a testing or development venv, put them in a requirements.txt file.  Also: if you're expecting people to install your package via ""pip install git+ put dependencies in setup.py.  tl;dr  if you want other packages to be installed automatically when yours is installed via pip, put them in setup.py. "
students should be allowed to fail. It's the struggle that's important in any real education   From one perspective   ,"I'd like to think that what we're teaching students what is just outside of their grasp; far enough to push them. We do a bit too much mollycoddling today and the result is a dumbing down. When I was doing my degree my languages and compilers professor explained that we used to do this in the first year in half a semester as a sort of foundational work for the rest of course, but now they're forced do the same thing in the second year and for a full semester because a certain percentage of students have to pass the year and schools are no longer preparing students for university to the same degree that they once did. Leaving the universities to pick up the slack, and ultimately pushing core ideas out of the curriculum entirely, or up into the masters level.  tl;dr students should be allowed to fail. It's the struggle that's important in any real education  From one perspective :) "
Either way the program should not be what decides whether something can be put in the refrigerator.,"I remember hating this article the first time I saw it. How do you determine what fits into a refrigerator? You can't. You either: use the virtual refrigerator for book keeping, in which case the determining factor of ""what can you put in a  Refrigerator  object?"" is ""Whatever the human responsible has put into the physical refrigerator to which the software object refers""; or you have a refrigeration unit managed by a robot, in which case you don't model the refrigerator as a volume of empty space but rather add racks and columns (like a convenience store soda rack) to reduce the multi-dimensional question of whether something fits to a single-dimension one of whether there's an available slot for a particular product or item.  tl;dr Either way the program should not be what decides whether something can be put in the refrigerator. "
Don't worry about it for another six months.  Let it work itself out.,"Babel 6 switched to using what they call ""presets"" to bundle many transformations together to ensure maximum compatibility.  You'll probably want to use these presets in all circumstances for at least another six months and then likely the main browsers that you use for development will support certain ES2015-2017 features that need require significant changes to the transpiled code.  You can turn these on by using a different preset or by only adding the transformation that you would like to see happen.  Also Webpack 2 pre-release is including its own preset that will allow maximum compatibility on the front end but allows the ES6 module import syntax to be left alone by Babel so that Webpack can instead do this transformation and use extra information (since ES6 import is different than a commonJS  require ) to do ""tree shaking"" to reduce the bundle size by sometimes quite a lot.  tl;dr  Don't worry about it for another six months.  Let it work itself out. "
By not being picky about who you work for  you can find yourself in a viscous cycle that can damage or limit your career.,"I understand this point very well, but I disagree with it and here is why.  If you start looking for any job instead of the right job, you'll end up wasting your time by interviewing with the wrong companies.  The repeated demoralization of being rejected by multiple companies can impact your confidence and ultimately reinforce the ""pray and spray"" strategy, making it even more difficult to find a good fit.  If you ""get lucky"" and find yourself employed by the wrong organization, you may end up extremely unhappy or may even get fired.  I suppose you then have to ask, ""is it worth getting a paycheck today at the expense of my paychecks tomorrow?""  There is no wrong answer to this question, but it is a consequence that you'll likely have to live with for a while.  tl;dr: By not being picky about who you work for, you can find yourself in a viscous cycle that can damage or limit your career. "
list s are slow and fat.  vector s are lean and fast. So people prefer  vector s for most cases.,"Modern CPUs love big chunks of memory and constant pointer+variable offset addressing.  vector s fit that description quite nicely, whereas  list s are the opposite of it (read: lots of small chunks of memory that point to each other).  Also,  list s require an allocation+free per element, whereas  vector s generally only allocate/free memory  log n  times (given that  n  elements are inserted), and sometimes only once (if you size it ahead of time). People care because allocations+frees can get expensive.  Finally,  list s impose a per-element overhead of multiple pointers (otherwise, how would elements point to each other?).  vector s take a constant overhead of a pointer + a size + a capacity, regardless of how many elements they hold (though a vector may have ""dead"" space at the end if it's holding  N  elements, but has the capacity for  N+M ).  tl;dr:  list s are slow and fat.  vector s are lean and fast. So people prefer  vector s for most cases. "
Headless should work and if it doesn't it sounds like a bug that needs to be reported,">With NVIDIA's current Vulkan Linux driver it's not possible to achieve headless Vulkan rendering if the X.Org Server isn't running, but that limitation is said to be addressed by their next (post-375) driver release.  From back in 2016 they said running headless was broken and it should be fixed for Nvidia.  Is it still broken for AMD? I can't find anyone mentioning headless and AMD which usually means working as intended.  In vulkans case the intention is so long as you do not create a display surface you can still compute... so compute on Vulkan is the default and should just work.  But I don't have a way to test it.  TL;DR Headless should work and if it doesn't it sounds like a bug that needs to be reported "
Wit ha few dozen hours invested  your mind can be rewired into a stack manipulation engine. Sorry for the rant.,"Well, yes, BUT!  You've done similar feats many times before, when you learnt regular expressions or how to ride a bike. Try to read this whacky language for a week, and you  will  kinda-sorta do stack manipulation in your head. Our minds a capable of that.  It's a threshold thwarting initial and casual use, but not regular one.  --  This isn't to say this is always worthwhile endeavor - we often find out only years, decades after the fact that this curious fascination with this quirky language made it somewhat easier for you to read javascript bytecode which helped you debugging a weird performance problem once, a skill that finally gave you the edge over a dozen other interviewees, landing you the job that's now paying for the college education of your granddaughters.  (best case scenario, but really, most of the time it's just cost of opportunity.)  When we learn a skill, we tend to forget how it was before. After learning how to read assembly fluently, it becomes hard to recall your first look at this gobbledygook mess, trying to figure out what's going on.  Worth remembering, because it not only turns us into bad teachers, it also makes us reject concepts for the wrong reason. No, sire, riding a bicyle is such a silly idea - I could fall and hurt myself really badly, and if the brakes break downhill, I'm surely gonna crash to death. What's so bad about walking?  tl;dr:  Wit ha few dozen hours invested, your mind can be rewired into a stack manipulation engine. Sorry for the rant. "
Learned practical skills on the job  most of them nothing to do with data science.,"I taught myself to program in school, so it was more recalling than transition. After deciding that I wasn’t taking any career paths my education was suggesting (becoming a scholar/translator/journalist/diplomat) I started doing small dataviz projects. They landed me my first job, junior analyst at a game company.  I really knew very little at the time but they loved my enthusiasm and initiative. So I learned on the job and the most important things I learned were general IT skills and domain knowledge, not “data science”. So regexp, OOP and FP, linux, file systems,  web development, common programming patterns, abstraction, database internals and of course SQL — instead of sexy stuff like machine learning, NNs, clustering etc. Broad, not deep.  Why? Because if your company does real business, your integration skills are much more important than your knowledge of “data science” itself. You have to analyze the problem, talk to business people, talk to IT people, look at data and find a solution that’s simple, robust, transparent, fast to develop, cheap and makes money. More often than not, it ISN’T machine learning. Yes, some companies build their business around things like image recognition, but most don’t. So if you’re a math/R genius, but can’t figure out why this event isn’t logging with browser devtools, you’re useless to the company, or at least extremely dependent on other people.  tl;dr Learned practical skills on the job, most of them nothing to do with data science. "
monotonic clocks are not the solution to this problem because of developer overhead and interface guarantees,"I did some research on this recently. Linux systems have trivially implemented calls for monotonic time (just make a regular call to gettime but set the clocksource to MONOTONIC_RAW), but you're limited severely by hardware in some cases (2009 era hardware makes the time calls 1000x slower).  I tried to do similar things in the macOS implementation as well and that was nearly impossible. You might be able to do it but it's not something widely available to a system user.  The other point is that the monotonic clock source is reset (it's not actually Unix epoch) after every boot.  tl;dr monotonic clocks are not the solution to this problem because of developer overhead and interface guarantees "
Is it possible to debug in VS Code using this Rust plugin  with or without another helper plugin    If so  then I like it.,"I'm not sure if this is the same one -- but in the past, I used a Rust plugin for VS Code along with an lldb plugin (I needed to have both for the best experience).. and the combination provided support for runtime debugging (breakpoints, watch window, etc.).  It worked fine, but it was a little tricky to setup as they were two separate plugins and neither was very mature.  In contrast, IntelliJ IDEA's Rust plugin(s) does not have debugging support, and it is very unlikely that it will ever have it since the underlying native debugging support (gdb and/or lldb) is only intended to ship with CLion (not IDEA).  For me, any Rust IDE that doesn't have a debugger is a total non-starter.  tl;dr: Is it possible to debug in VS Code using this Rust plugin (with or without another helper plugin)?  If so, then I like it. "
OP just became  1 of a small practice section. You can google answers of SPOJ questions and get a good rank there too.,"No you did not become hackerrank #1 in two hours.You just became 'Java's' #1 rank which is just one of many separate domains in hackerrank. I would like to see you try in popular domains like 'algorithms' or machine learning.  Ranking of hackerrank are of two types, contest and practice.No sane company would ever hire just on the basis of your ranking in practice where googling the answer is easy. I can google the answer of SPOJ problems and get a really good rank, it is true for any practice site.The real ranking again is the contest rank, which is the one that really matters. I would like to see you try there.  > Any abomination of code will pass the tests as long as it outputs properly.> The site does not teach proper or idiomatic Java.  Yes, it never claims to. The site is mostly for algorithmic challenges like Topcoder, Codeforces. The main challenge is solving the problem not idiomatic code. It has added a few domains like Java, python, ruby just to help people new to the language learn. Some people like me find it easy to learn something through challenges. Maybe some day they might add challenges for idiomatic code, but currently it isn't there.  Anything you did here applies to SPOJ or Leetcode too.  TLDR: OP just became #1 of a small practice section.You can google answers of SPOJ questions and get a good rank there too. "
make sure you're correct before trying to correct others.,"I have literally programmed my own keyboard to support full n-key rollover, using delta-frames and sorta-interrupts. EDIT: this wasn't a keyboard meant for typing though, just messing around with some sort of hybrid between a soundpad and a musical keyboard. Not going to explain it, but lots of gaming keyboards also have full n-key rollover using some funky driver hacks. The only problem is, the BIOS might have a problem with that, so they do include a 6-key rollover switch for when you're messing around with the BIOS.  Full-speed USB devices can conform to HID standards fully, and send 64 byte packets 1000 times a second. Unless your ""keyboard"" has more than 512 keys, I don't see you not having full n-key rollover this way.  TL;DR: make sure you're correct before trying to correct others. "
Tea is often among the drinks served at parties.,"I'm certainly no expert, but I am a US American, so my impression is that tea parties have historically been attended by teetotalers or drunks (not mutually exclusive on an individual basis), and have a predilection against providing tea for consumption.  More typical parties, such as those hosted for friends in a home on an evening or weekend will gladly serve tea upon request, subject to the constraints of what the host has on hand.  British expats will serve  anyone willing to listen to their muttering about the lack of quality teas, dairies, and kettles for 10 minutes or so a nice cup whether requested or not. A summer weekend party in the south will generally have tea near the keg in a serve yourself fashion.  tl,dr:  Tea is often among the drinks served at parties. "
c  wouldn't be so bad if the people and platform didn't abuse it constantly  or as you might see it from a c  dev    TooLongDidntRead.C DoesntSuck ThePeopleandPlatformDoThough .AndTheirLinesAreSuperLongAndNeverEndAndGoOnForever  ,"I'm one of those people who you'll hear bashing c# and even I'll admit the language is fine, it's the people and platform that, more often than not (in my experience), ultimately suck. It just seems like people who were forced into .net, maybe it was the only thing at their disposal, don't follow basic tenants of programming when it comes to style, readability, or just plain old common sense. You'd be surprised how many of the largest companies are built on a fragile, barely working .net stack. I would rant about examples but I don't think readers like hearing about it like they used to. Tl;dr, c# wouldn't be so bad if the people and platform didn't abuse it constantly, or as you might see it from a c# dev:  TooLongDidntRead.C#DoesntSuck(ThePeopleandPlatformDoThough).AndTheirLinesAreSuperLongAndNeverEndAndGoOnForever() "
the decision makers' attitudes toward your ability to develop the organization's utilization of data will determine what you do.,"Been there, fuck that.  It's all about executive attitude to developing the infrastructure. I was met with a lot of ""this is how we've always done it"" and ""At BoA we used excel exclusively."" which really just translates to ""if it was good enough then it's good enough"".  That attitude is really frustrating and I had to leave eventually. It hurts watching the potential projects slip away because you don't have access to the information and management won't listen to you.  If they were more open to things like allowing python or R instead of requiring excel and vba, I'd probably still be there.  tl;dr the decision makers' attitudes toward your ability to develop the organization's utilization of data will determine what you do. "
C is trying to be inclusive and flexible so that people can use it as a high level assembler. GCC's perverse interpretation of the standard breaks that.,"> The compiler tends to assume that undefined behavior does not happen (because if it happens anyway it has no obligation to perform any particular behavior, so there is no harm in this assumption)  ...except that there is harm, since that's not what the programmer intended.  As the pdf points out, the C standard deliberately draws a distinction between ""conforming programs"" (that are acceptable to  some  conforming implementation of the language) and ""strictly conforming programs"" (that are acceptable to  all  conforming implementations).  In other words, the standard is trying to say that conforming programs are  allowed to  rely on undefined behaviour. ""Undefined"" is actually a misnomer: it's just platform-defined behaviour. It won't necessarily be portable to other implementations, but is entirely reliable, and you can write a valid conforming program to run on a specific hardware platform, using the specific semantics appropriate to that platform.  TL;DR - C is trying to be inclusive and flexible so that people can use it as a high-level assembler. GCC's perverse interpretation of the standard breaks that. "
This article is complete horseshit and it makes me more aggravated the more I read it.,"I completely disagree. The first point in the article isn't even Python-specific. News flash: programming is hard.  I recently re-specialized from PHP to Python and had very little trouble. Rather than bitching, I read through pages of docs, digested PEPs, learned the syntax and libraries, and have been extremely pleased with Python, the  thoroughness  of its documentation, and the experience as a whole. I'll probably never go back to PHP. Yes,  it's work . No, it's not ""get started in .2 seconds with PHP!"" or ""write your own Twitter clone with Node in 12 milliseconds!""  But guess what:  I've learned a ton , not only about Python, but about software development and the inner workings of computing. The experience I have gained has been, and will continue to be, extremely valuable. I measure, in no small part, the worth and skill of a programmer by his or her willingness and ability to read, synthesize, and apply new knowledge, especially when that knowledge isn't delivered in bite-size, pithy excerpts with examples that don't teach one to think about the context of the problem, different solutions, and the trade-offs of each. There is so much knowledge and so many examples of good Python out there, I don't understand why this author had even the slightest bit of trouble.  TLDR: This article is complete horseshit and it makes me more aggravated the more I read it. "
It's routing. It's not supposed to be exciting and have thousands of features. It's supposed to be boring  fast  replaceable and it shouldn't dictate your app architecture.,"It doesn't matter what you choose, or rather you should choose and integrate so it doesn't matter what you choose (make your router replaceable).  Make sure the router lets  you  create your controllers and doesn't perform magic like calling specific methods on a controller (this should be implemented at the controller level, which can read parameter ""action"" and call a same named method based on it). Avoid routers that tightly mix route matching with: handler creation, dependency injection, handler dispatching.  FastRoute in this case is a good example because it just takes any value for the ""handler"" input and returns it on match. What you do with it is entirely up to you then.  tl;dr  It's routing. It's not supposed to be exciting and have thousands of features. It's supposed to be boring, fast, replaceable and it shouldn't dictate your app architecture. "
You can obfuscate  but dedicated programmers can take a while and they'll eventually deobfuscate.,"Obfuscation. If you decompile obfuscated java- the code structure is generally all the same. All variables, methods, and classes are renamed to obscure names such as a, b, ... , aa, ab, ... and so on... Modders can still see the source code, and can edit/compile/run it, but it's fairly harder to modify, as you have to infer what each class does based on how they call libraries(which generally have the same names), resources (such as /res/img/bunny.png as this is preserved) or strings. Each update to the program may or may not change the obfuscation. And of course, as modders deobfuscate the code, it becomes easier to deobfuscate the rest over time.  TL;DR: You can obfuscate, but dedicated programmers can take a while and they'll eventually deobfuscate. "
I think Notch gets way too much hate.  What he did  and the people at Mojang did  was extraordinary from a delivery perspective.,"Hindsight is always 20/20.  I don't blame him in any way.  He had a proof of concept idea become an absolute sensation, and when that happened he didn't have the time luxury of going back and rewriting it from scratch or fixing poor architecture decisions.  I have a general rule in software development and that's ""Make it work first, then iterate"".  The problem is when there is very high adoption and demand for new features, people don't tolerate addressing technical debt because they see it as a waste of time.  I've followed this game since Alpha, and watched people attack him regularly for not adding new features fast enough, not fixing bugs, taking a vacation, etc... had he took a month to go back and rearchitect things he would have been even under more fire.  When you have something like this become a huge success, you roll with it.  And what he did do right was hire the right people to come in and help.  I would fully expect a new version of MC to be way more performant and better built than it's predecessor, simply because the new version has the benefit of hindsight that the first did not when it was discovering itself.  TL;DR: I think Notch gets way too much hate.  What he did, and the people at Mojang did, was extraordinary from a delivery perspective. "
women at the event  in the industry    good  women at the event only talking about being at the event  in the industry    good,"At first I wanted to be lazy and just say 'garbage men'. A tired, although arguably valid response to this sort of thing. But I'd like to think I'm better than that. So two things. First off, you have the biggest job here. Not the Conference runners nor it's guest. YOU. If you want to see a change, you have to be the one that shows your daughter what she can do with hard work. It's not the worlds job. And in the end, if she enters the field against all this circumstantial 'adversity' she'll know she earned it and didn't have it given to her. Which is essentially what you're suggesting we do by saying ""lets ignore merit and just GIVE spots to people with different genitals"". Did you know that penises and vaginas have nothing to do with code?  But my second point. If you must hand out pity speaker spots (which should and likely will insult anyone woman you give them to) please heavily imply that they come with something other than ""Women in the Industry"". I'm not saying that's not a subject worth talking about. But if that's all the person had to say... it kind of defeats the point right? You want women speakers showing up and giving it their all. Showing everyone the deep and important impact they're having right? So if and when this hypothetical woman shows up she needs to leave her 'disadvantage' at the door. And act like she belongs. Because honestly, everyone there already thinks she does. If a female speaker showed up talking about promises in JS or ES2017 I wouldn't even think of treating her any differently from anyone else. Just don't show up and tell me about my problematic existence in the office using examples from the 60s.  tl;dr: women at the event/ in the industry == good, women at the event only talking about being at the event/ in the industry != good "
Wow  the author does not care if you get hacked.,"Faster, Better, Cheaper, and  totally insecure .  There is no mention of a secure software development life cycle ([SDLC]( or really any kind of attention to risk management.  Test Driven Development is great, if you have a security expert that is able to write security tests.  The security industry is in massive demand, and you must expect to pay a kings ransom for a talented security team.  Even when the app is build and ready for deployment you still need a real penetration test. Not having a penetration test maybe a violation of your cyber liability insurance, and that means you might not have coverage for the cost of a breach (true story).  If you wait to have security involved until it is ready to deploy, then the risk of architectural security issues, or even systemic violations of policy will sky rocket.  tldr; Wow, the author does not care if you get hacked. "
think about what you're doing and deal with cases where something is null sensibly,"I mostly work in C# and think null is a perfectly valid way to represent the absence of data. The problem I usually see is there's a path through the code that doesn't get considered correctly and something is unexpectedly null. This results in a null reference exception and so they wrap the line in if (x != null) and thus create another branch in the code.  I try to keep control of the variables and fields and then there are no unexpected nulls and we don't get a null reference exception. I firmly believe that a null reference exception is a programming error and should never be thrown. The code should be modified to handle it.  If your method gets passed a null and you can't work with that, throw an argument null exception. If the caller is your code, go and fix it so you aren't passing null or just don't call it if it is null. If you code had an unrecoverable error, throw and exception rather than returning null.  TLDR: think about what you're doing and deal with cases where something is null sensibly "
It's amazing that things even work as well as they do  given the environment the web operates in.,"Web applications are running on the largest heterogeneous, distributed network ever constructed by mankind, combining both human and non-human interactions, using a multitude of devices, clients and operating systems, across an unreliable network spanning the entire planet.  These are the circumstances for which the web, HTTP and the REST architectural pattern were designed. The web being stateless is a  requirement . The architecture of the web pushes state management to the client precisely so that services can scale, clients and intermediaries can deal with network failures and the whole thing can survive and evolve over a span of decades.  Edge networks can cache HTTP content so that clients can get the data faster, without additional infrastructure required by the service provider. Web browsers can cache Javascript code and web resources, and rely on HTTP headers to decide whether or not to make expensive network calls.  On top of that, most standards are developed bottom up, not top down. Javascript sucks because it was a slap-dash effort by Netscape to gain market share, copied by everyone else, and then made into a standard. Microsoft developed AJAX, and it was copied by everyone else in the same way, and then standardized. Web standards suck the way biology sucks, they are ""designed"" by evolutionary forces. The technology that is adopted is the technology that survives.  Not to mention, users want things for free, and the only way that is happening is with advertising, which comes with its own nastiness as content providers find ways to monetize their content.  Any design process to fix the problems of the web will have to take into consideration the world in which the web lives and the manner in which web technology evolves.  TL;DR It's amazing that things even work as well as they do, given the environment the web operates in. "
yeah you should handle it properly but don't worry about it.,"So there are a couple of small overheads that you will incur if you do this;   Some JS engines won't optimize your code  The error object and backtrace etc. will be created, which they wouldn't if you checked for stuff rather than just allowing your code to fall over.   Despite this, I don't think there is anything wrong with what you are doing. Readable code always wins out over tiny performance gains and it sounds to me like you've taken a pragmatic approach.  What you want to avoid is ""swallowing"" errors needlessly;  So before tucking your code in I would maybe either log something to the console or put a comment in to indicate to future explorers why you've done what you've done.  Also maybe create a backlog ticket for a rainy day to come back and handle it properly. I don't know what ""properly"" is, I just mean something to differentiate an error from just having parsed an empty array.  Tldr; yeah you should handle it properly but don't worry about it. "
monorepos are not the problem. Our culture is the problem.,"> You create a dedicates directory for each of your services, and commit them all at once.  > Everyone reviews everything, and everyone takes part if something brakes.  I don't see a problem. We can develop as a monolith and deploy as microservices. If we can't, we don't need microservices.  Monorepos are good as long as monorepos does not mean that you can't ever git init. Some organizations practice waterfall while claiming to be ""agile"". You must allow all team members to clone the monorepos and to create a pull request on any part of the repo.  The unanswered question: who can give  lgtm  and who can merge a pull request into any given module is what will bring us to the solution of ownership.  There needs to be a certain change in mindset. We must accept that breakage is inevitable. We should try to involve everyone in the code review process. We can have expansive test suites but the fact of life doesn't change. Embrace it.  At least this is my not so humble opinion.  tl;Dr monorepos are not the problem. Our culture is the problem. "
Google most likely misses out on great programmers because they target computer scientists.,"I agree with you. I'm starting at Google this fall, but many of my friends have all failed the process, despite several of them being indisputably better than me. One guy in particular who is a ridiculously great programmer, was sent home before lunch.  Their approach is obvious: They test general intelligence related to computer science and programming, giving you semi-hard questions and a very stressful situation to solve the problem. You have to be calm, focused and for many questions, a bit of luck too (e.g. if there are 2-3 approaches that could solve the problem and you pick the one that is feasible to do in time).  Anyway, I think their bias against false positives seems... too high. They seem to ignore that a very large portion of the people who are enormously competent fail their interviews. This is a problem for them. They should provide other ways to conduct the interviews. Maybe, they could separate the process into one ""software engineer"" interview and one ""software developer"" interview focusing on computer science and programming respectively.  There are geniuses from the academic world that have never written proper code, only prototyping (proof of concept) without version control, solo, until it ""works"" good enough for a presentation. These people have no knowledge about the workflow and practices of a programmer and it scares me that they might get in a position to manage critical code if they were at my team. Academic geniuses are obviously great assets, and is probably a reason for Google's success, but they are not great programmers by default, and great programmers are absolutely critical. And many, many great programmers are great  because  they spent a majority of their life coding, not learning neural networks and prime factorization.  TL;DR: Google most likely misses out on great programmers because they target computer scientists. "
synthesizing a solution from scratch by knowing how to ask the right questions demonstrates a key skillset completely apart from being able to pound code.,">  If I don't know what the interviewer is asking for, I just ask for clarification.  This is the right attitude.  Attempting to define/refine the task's requirements, by asking basic questions, is Requirements Gathering 101.  It's the developer's responsibility to understand the problem and business space, in which they are attempting to improve, by furnishing a software solution to a problem.  You can't get there without asking questions, else you'd already be an expert in damn near everything.  If any interviewer balks at your needing more information in order to understand the goal (outside of how to code a solution), it's not a place where you want to work.  If anything, you  want  to be asked these kinds of questions in an interview, because it's impossible to demonstrate this capability any other way.  TL;DR; synthesizing a solution from scratch by knowing how to ask the right questions demonstrates a key skillset completely apart from being able to pound code. "
It sucked  and you wouldn't want to try it.  Appreciate the resources available to you today.,"My experience is of a similar vintage, and ignoring the pun, the ways we had to learn back then would not even make sense today.  Just getting access to a compiler could be hard for the young and self-taught.  Machines were DOS or Windows, typically, and Borland Turbo C was expensive.  Linux was a few years from even existing, the Web was barely a twinkle in Tim Berners-Lee's eye, ""open source"" was what that nutty Stallman kept ranting about.  Answers and tutorials weren't just out there for the Googling.  You needed to go down to Barnes & Noble and find a dead tree to tell you how things worked, and there wasn't even an Amazon to tell you which dead trees didn't suck, so you often ended up with three or four different variants of  Learn C Programming in 7 Days  that mostly overlapped, but each had its own shortcomings.  Then you often went to college and at least had access to the tools, but on shared lab machines.  TL;DR: It sucked, and you wouldn't want to try it.  Appreciate the resources available to you today. "
In some cases  scouts have done their jobs  tanks can roll in.,"I'd be careful with this. The article only talks about correlation.  What I means is that wether or not to do TDD depends on wether or not is it easily doable. A project which is a reproduction of an already somewhat successful project will :   Have a clear, non-changing,  exhaustive  spec; and thus, be easily TDDed.   Already have been successful (selection bias).   Already battle-tested (no surprises, no gotcha, etc).    In short: the exploration of the complexity will already have been done, and a successful path can be followed again.  Part of this correlation could be explained by a type of work influencing both failure rate and doing the TDD.  TLDR: In some cases, scouts have done their jobs, tanks can roll in. "
CSS doesn't have variables and has extremely limited computational capabilities. If you don't like programming without variables and math  you won't like writing CSS without variables and math either.,"Yes, and it makes your life much easier and your code more maintainable and consistent. Sass is the most common (and pretty much everyone uses the scss syntax when using Sass, so if you ever see a  .scss  file it's Sass). LESS is also popular.  For example, suppose your site has a standard margin of 10px. Every time you want something to use the standard margin, you need to explicitly put  margin: 10px . Then you have a page where you're using a grid layout, and you want the gutters to match the standard margin so you give each of them  margin: 5px  (because the gutter is on each side, so you divide by 2).  Later on you decide to increase the margin to 20px. You find and replace all the  margin: 10px  with  margin: 20px , but you totally forgot to update the gutters to match!  In Sass, you could do something like this:  $standard-margin: 10px;.some-item {    margin: $standard-margin;}.grid-item {    $gutter-width: $standard-margin / 2;    margin: $gutter-width;    // Or, if you prefer not having the intermediary variable...    margin: $standard-margin / 2;}  You update the margin size in one place, and everywhere else automatically compensates.  You could do other similar things, too. For example, you could define  $primary-color: #6495ed  to have a nice cornflower blue as your site's main color, and then have a palette that's based on that like...  $primary-color: #6495ed;$primary-dark: darken($primary-color, 20%);$primary-light: lighten($primary-color, 20%);  If you want to go with a two-color scheme, you might generate the complement and some shades based on that, too.  $complement: complement($primary-color);$complement-dark: darken($complement, 20%);$complement-light: lighten($complement, 20%);  Or even be a bit fancier with a three-color scheme  $complement-left: adjust-hue($primary-color, 120deg);$complement-right: adjust-hue($primary-color, 240deg);  etc. Now you can change the entire color scheme of your site just by modifying your primary color, and all the other colors on your site will adjust to match it.  TL;DR: CSS doesn't have variables and has extremely limited computational capabilities. If you don't like programming without variables and math, you won't like writing CSS without variables and math either. "
Because something is new does not mean it's better. Because you can do something  does not mean you should.,"> The web is becoming gummed up with random crap in order to satisfy the desire of people to use a square peg for every hole. It was never meant to be a platform for writing full blown client side applications and trying to turn it into one has made a horribly overcomplex, slow, exploit-laden horror story.  You summarized my feeling exactly.  It's crazy how much time and effort goes into reinventing  badly  what was already not that good to start with.  The  worst  is ergonomy. I cringe every time I see the direction web-apps are going. WTF? Desktop apps are already poor enough, despite dozens of years of progress. But the passage to web-apps has thrown us back at least 20 years. This is pathetic.  But I feel like I'm trying to stop the flow of a river (called stupidity). Common sense shall not prevail. The ""shiny"" & the blind race to the latest ridiculous UI fad will go on.  As for the users: Fuck them.  TL;DR: Because something is new does not mean it's better. Because you can do something, does not mean you should. "
We're all doing this to procrastinate because work is boring.,"So you have your average young, hip JS programmer who reads reddit and he thinks, ""gee it seems like blogging about programming is the thing to do, maybe I should give it a try. It could be like, good for my career and stuff."" So he opens up the Medium editor but he realizes that, darn it! he doesn't actually have anything interesting to say, or if he does, the market for a 1000 word article about relational models in Backbone.js is pretty small. How oh how will he ever make the front page of /r/programming?  Hmm, he thinks.  ""What is the one thing that programmers love more than anything else, even programming? Of course!""  ""Complaining about everything all the time!""  And so a low effort rant is born about ""OMG so many frameworks"" (or now we're on to ""OMG so many complaints about so many frameworks""), and because programmers love a good rant, and because reading a good juicy rant is a lot more fun than reading some boring crap about actual programming, it rises to the front page, where we all write rants in the comments complaining about everything. The circle of life continues.  TL;DR We're all doing this to procrastinate because work is boring. "
equal parts determination and being at the right place at the right time,"I started off as a self taught dev I suppose. When I was 16, I offered to help a friend with tech support for his (likely not legally operated) web hosting company for free. As a part of this, customers would ask me to fix their websites, most of which were using some sort of PHP app (usually vBulletin or phpBB, forums were popular at the time), so I had to learn how to be able to debug basic PHP+MySQL issues. Around this time, I decided I liked programming and an acquaintance was bragging about having a programming internship in high school, so I pestered him every single day for a month until he finally talked to his boss to bring me in for an ""interview"" where I was asked ""Do you like programming? Do you have reliable transportation to the office? OK, you start Monday."" From there, I got a couple different internships by interviewing over the phone, one of which led to a full time job at a big semiconductor firm. I've since left that full time job for a better paying full time job at a smaller company.  tl;dr: equal parts determination and being at the right place at the right time "
if you're using mocks  use them the way they were meant to be used.,"> I realized that it was unnecessary to define an interface just to end > up with a production and a test implementation.  This isn't the reason to use mocks in Java, as demonstrated by the authors of [Growing Object Oriented Software Guided By Tests]( who are also the authors of jMock mocking framework.  Mocks are meant to create a contract for external dependencies in a [Dependency Inversion]( scheme, for specifying strategies for generic algorithms, or designing an event handling scheme.  Otherwise, you should always test using concrete classes.  > if the implementation isn't offloading the function call to a > database/file/socket/etc  These are almost always in a different level of abstraction that business code, and the inner workings of database mappings or network serialization formats change for very different reasons than business rules. In this case, a Dependency Inversion scheme is good because you can set the expectations of the business code in business terms and let the implementation figure out how to work out the details in concrete terms.  This is also good because it allows you to test the implementation to verify it meets the terms of the contract independent of the rest of the system. Thus, it is important that if there is an interface separating business logic from databases/files/sockets, that it actually define a proper contract at the proper level of abstraction.  TL;DR - if you're using mocks, use them the way they were meant to be used. "
I agree with you. But here's a short story on an error handling project.,"I once worked on an error handling system that  had  (and it was stressed very strongly) to convert errors into resolvable steps e.g.  Error occurred when attempting to find control server for server group ""name"".Original Error message: xxxxxxxxxThis error is normally caused when the server group is misconfigured, to solve this:- List of- Possible solutions(more technical information like stack trace if debug is enabled).  We knew what operation was being attempted by updating a progress variable at each stage e.g. currentStatus = CONFIG_STATUS.FINDING_CONTROL_SEVERAnd the resolution steps were generated dynamically by having a custom exception type that stored all kinds of interesting information. We found a lot of things that could go wrong through testing and every time we did so we took full dumps or reprod the issue, by looking at the combination of errors, how they were nested, and which stage we were at, we could determine pretty accurately the cause of the problem and how it would be resolved. If we got an ambigous situation like a file not found error when attempting to load the config for example, then the steps to resolve the error would have multiple scenarios i.e. ""check the file at path x exists"", ""check that you have permission to access the file"".  This worked surprisingly well and both the support guys and our customers loved it, but it took a lot of time to maintain, when a new situation arose that was reported we'd have to go in and update the rules to build messages that were better suited to the new scenario. All of the information that could be relevant was stored in the message too, and if we couldn't figure out what the problem was, we reverted to a generic message saying something went horribly wrong and as much information as was feasible to understand without overloading the user, while dumping all the info we'd need to create a new rule into a log file.  Due to the cost of such a system however that requirement disappeared one day, it was all just replaced with a generic ""something went wrong, check the log for more info"", which was cheaper in dev time, but annoyed the support guys to no end.  I can't even remember the point I was going to make at the start of this now. TL;DR: I agree with you. But here's a short story on an error handling project. "
I am not able to replicate any new or  odd  behavior with the way conda creates virtual environments using the latest version of Miniconda on OS X.11.6.,"I use conda on a daily basis. If something has fundamentally changed as you say, that would be troublesome for some instructions I have written for coworkers and colleagues.  I will create a new user with a fresh conda installation on my OS X machine and report back if I can replicate the findings.  Edit:  On a fresh install of Miniconda with python3.6 on OS X.11.6 El Capitan, I see no difference in the way conda handles environments.  I did a fresh install of miniconda answering yes to all questions in the bash installer when prompted.  I then executed:  conda create -n TestEnv python=3.6 numpy  and the installer proceeded as per usual.  conda env list  outputs /Users/testuser/miniconda/envs/TestEnv and /Users/testuser/miniconda3  I am not sure about any of the other 20 odd commands listed in that ""Cheat sheet"" you posted as I don't tend to use them. However, the command I use to create new envs works exactly the same as always. You can execute that command from any location but conda places the env in the /envs/ subdirectory of your conda installation folder.  TL;DR:I am not able to replicate any new or ""odd"" behavior with the way conda creates virtual environments using the latest version of Miniconda on OS X.11.6. "
Anything not directly manipulating the bits and bytes of the input  like accessing external devices  hard drives  network adapters  serial ports  graphics cards  sound cards  is impure by definition.,"> I know that pure functions shouldn't mutate external state, and it should return the same output every time as long as it has the same input.  It needs to be deterministic yes, same input should equal the same result , result does not have to equal the return value tho, e.g.  function add(a, b, out) {  out[0] = a[0] + b[0];  out[1] = a[1] + b[1];}  Is still technically pure, language semantics means some languages have multiple return values, some don't. In some cases it may be way more performant to accept an output object.  But fundamentally rendering can't be pure functions, pure functions can't do anything except pure computation.  Anything that can't be expressed via the language itself is impure, e.g, a call to console.log first of all accesses the global  console  object, then it mutates that state by calling  log , since the log data is not observable its a side effect.  Same can be said for rendering since it mutates unobservable memory, with a high level retained API like canvas rendering doesn't even have to take place when the function is called.  Rendering however is not meant to be pure, your application state and logic can be pure, but your rendering is an observation of the changes made to that application state.  TL;DR - Anything not directly manipulating the bits and bytes of the input, like accessing external devices (hard-drives, network adapters, serial ports, graphics cards, sound cards) is impure by definition. "
Imho  we need a good license created by smart people to avoid any kind of  misuse  of the software we wrote.,"I agree, there is a big problem about this. That's why I think we need a good licence for this where people think about that are way smarter than I am.  Some practical examples:  Have you heard about the open source sniper rifle?  I'm sure many developers never thought their software might be used for something like this. Some might think ""oh wow"" since their software can do much more than they have thought. Others might hate this use, but they can't ""defend"" since the license the used allows this case.  You also might have heard about ""Hacking Team"", that Italian IT-Secure company.  Many people wrote software for different uses, they put them together to create surveillance software. The developers didn't know, until Hacking Team got hacked and everything was leaked.  I ""know"" one of these originally developers, and he got really upset about what they did to his work. But he simply couldn't do anything about this issue.  Oh wow, I kinda feel like this post is horrible to read since my english sucks.  But I hope I could explain my point in this.  TL;DR  Imho, we need a good license created by smart people to avoid any kind of ""misuse"" of the software we wrote. "
software vendors should NEVER assume that their users' computers have access to internet. Especially at work.,"> Will this affect you in practical terms?  Yes, it will.  At work  1 . try bypassing a Microsoft web proxy... You'll understand the meaning of pain.  (I know the Jetbrains' IDEs have Proxy settings, but that's always a pain, especially from a Linux host, arg!)  I've had a lot of problems in the past to get over that, it's always a misery even just for a web browser! I ended up being banned because of login issues (which blocked my Windows session!).And you're lucky if some idiot did not blacklist Jetbrains as ""productivity tools"" at the proxy level, meaning that you can't connect! You have to sideload your tools. See point 2.  But now, you cannot even install them, since you must activate online.  2 . what do you do when Jetbrains site is blocked?  Blocking ""productivity tools"", that's rich! And you're lucky if you are not identified as porn (WTF? I've had legit Java websites blocked for 'porn', no kidding!).  Now, that's the TOP of stupidity from the customer (I worked as a contractor), but they don't understand that programmers  need  a complete internet connection. I can't list the incredible number of times when I've been stuck because of that. In some cases we had to illegally (it was strictly forbidden) side-load the tools required to do our jobs!  => no longer possible with this new system.  Note that I've always worked for technical companies (Telecom mainly). Mind-boggling. (I have plenty of horror stories about them)  In addition to that, some computers were not even allowed to connect to internet. Too bad...  At home  At home, technically, I don't have a problem, there is no firewall/proxy BS.  But, I also have some VMs & Portable computers that may stay months without internet connection (that's voluntary).  And what about when I cannot pay? I've had some serious pb a few years ago, and could not have afforded an update. With this new method, I would have lost a good tool...  TL;DR: software vendors should NEVER assume that their users' computers have access to internet. Especially at work. "
There's nothing wrong with being where you are  and ultimately being perpetually an expert and a newbie is  at least for me   the point .,"New York tourist to native: ""How do I get to Carnegie Hall?""  New York native: ""Practice, man. Practice!""  To me, the great thing about programming is that it encompasses both timeless knowledge and constant innovation. You're never going to go wrong keeping your Knuth or Sedgewick or Skiena close at hand, and you should refresh your recollection of them often, just as an adherent of a religion who is very familiar with its scripture should nevertheless study it often.  At the same time, the tools of our trade change—or at least opportunities to make different choices arise—fairly often, and even a given tool (language, framework, etc.) evolves fairly often. This is both exciting and challenging, which keeps the blood pumping, if you don't overdo the chasing-the-next-big-thing thing (one reason expertise in the fundamentals is crucial: snake-oil detection) and burn out.  tl;dr There's nothing wrong with being where you are, and ultimately being perpetually an expert and a newbie is, at least for me,  the point . "
worry more about your power supply and hard drives than the GPU,"> Also what will happen when GPU dies, will it just show nothing?  It'll fail to boot.  >Do CPUs die too?  Yeah.  When? Quite a long time really. I can't put a figure on it: 15 years of continuous use??  I've not had one go personally, but have read others have it happened, though usually it's something on the board (e.g. a capacitor) rather than the CPU/GPU chip itself, or they fried it with too many volts somehow.  The rest of your system, especially the PSU, will most likely die before your CPU or GPU chip does, and when the PSU dies that can often cause lots of other problems that then cause your CPU/GPU to fail and your HDD to write corrupted data etc.  tl;dr worry more about your power supply and hard drives than the GPU "
RC path is incorrect and making it correct is not the job of cmake so help can't help.,"Here let me go into detail as it'll explain why cmake help simply can't help. Alright so my mingw didn't have things like threads and mutex so I couldn't compile llvm with it. Alright so we go to the tested target visual studio. So you assume working with cmake normally it'll just work? No your wrong. unlike the compiler cl which you point to directly rc simply requests ""rc"". That means you've gotta do something else. Well it turns out that something else is you have to start up the visual studio command line and run the cmake from there so that rc works. You are free to run cmake-gui from there as well as I did.  TL:DR RC path is incorrect and making it correct is not the job of cmake so help can't help. "
What you did is light grey hat  not white hat. Your intention what you believe don't matter. I guess you already learned some of it.,"> that the university might have recognized a mistake in their thinking. I touch on the danger of assumptions and this is certainly a dangerous assumption that I made and I now recognize it as such.  A few extra notes: IANAL, but poking around many systems is  illegal . You did not just assumed they would react nicely to a vulnerability disclosure, you assumed from the very beginning that they would react nicely to an admission of guilt (though they are obviously also at fault).  For some, ethical hacking requires agreement with the target. This is to avoid risking perturbations of the service, etc. during the hacking attempt.  As for the law (in most countries), it's a requirement: See  There are negative consequences to have poking around illegal: citizens cannot test and report vulnerabilities, leaving system with our data exposed because companies/administration skimp on security.  But there also good reasons to make that illegal (Copying-editing-pasting from one of my previous comment):  -the hacker could disturb the service during his/her hack, as he/she is poking around in a  production  system. Damage can be tremendous (like, wiping or corrupting the production database). Even if no actual damage occur, even if the hack failed, the company may spend a large amount of worker-time (thus money) to analyze afterwards the attack and makes sure no damage was done (they will likely not know the attacker was a white hat, nor the extend of the hack)   any lawsuit against someone trying to intrude your network would become useless. ""I was trying to see if it could be hacked!"" said the nephew of your competitor.   it could happen at the wrong time, like right before a migration to a safer system. ""Ok, you've shown previous system was terrible at security, but we were in the middle of changing it. Now the new system is up and running, we would have prefer this new system to be pen-tested"".   if a hacker manage to penetrate a system containing sensitive data (like SSN, medical records -which is what you did-), you would still have someone getting unauthorized access to sensitive data - which is bad in itself. You may have been able to modify the query to return only some of your friend data, but opt out to read everything. The University is taking the proper measure.    TL:DR: What you did is light-grey-hat, not white-hat. Your intention/what you believe don't matter. I guess you already learned some of it. "
Consider your audience and develop sane websites applications that supports a reloading and doesn't need to load things from a crazy number of domains.,"Former NoScript user here. Please don't treat users with Javascript disabled as outdated old-timers.  The web is filled with some horrible code and some horrible websites. When I started using NoScript: pages tended to load faster, use less CPU, be less annoying (no autoplay, less popups/modals), and be more safe. (I don't use NoScript anymore due its simplistic approach in allowing script sources. As a content-consumer it was great. As an application user/shopper, it was pretty bad)  Javascript is fine, but do consider your audience. Progressive enahancement is likely better for a blog than for a slideshow/presentation. As a NoScript user, allowing javascript was (ideally) just a click a way--  I cared more about reloads working properly and how many script domains there were. (CDNs were fine, but [this]( looked like a mess.  A [minority]( of users can be influential, especially if they're your target audience:   IE6 users? They're running outdated browsers on outdated operating systems (probably on outdated hardware-- unwilling to pay more for newer hardware).   NoScript users? They likely care about what they're browser do-- likely technical or power users with a bit of technical influence.   tl;dr  Consider your audience and develop sane websites/applications that supports a reloading and doesn't need to load things from a crazy number of domains. "
ArrayDeque  is a more complete version of  Stack with better design decisions and intent behind it.   Hoped that helped       EDIT  Here's the visitor pattern I tend to implement ,"Stack  is based off of  Vector , and came with Java 1.0, before the Collections API, so by convention, you should probably ignore them.  Stack / Vector  were built in a time when they thought making everything thread safe was a good idea, but most for most use cases, it's really not necessary.  (It does this by marking mutation methods as  synchronized .) The thread-safe aspect isn't necessary for a single-threaded visitor. If you want to make the visitor parallelisable, I tend to pass around a  Context  as a parameter which contains scoping information and the like. When I enter a new scope, spawn a new task, with a child  Context  and fire and forget.  Taken from the Javadoc of  Stack : > A more complete and consistent set of LIFO stack operations is provided by the Deque interface and its implementations, which should be used in preference to this class. For example: Deque<Integer> stack = new ArrayDeque<Integer>();  Taken from the Javadoc of  ArrayDeque : > This class is likely to be faster than  Stack  when used as a stack, and faster than  LinkedList  when used as a queue.  TL;DR:   ArrayDeque  is a more complete version of  Stack with better design decisions and intent behind it.  Hoped that helped! :)  EDIT: Here's the visitor pattern I tend to implement: "
Rejection propagation is a horrible thing in promises  their specification implementations .,"Exception handling is an abused word. You dont handle exceptions in 97% of your code, you only log/display them before you terminate your application.  To get an exception really handled you would need to recover from it and this alone is harder than anything you have wtitten to the point. Now the way promises go about exceptions doesn't do any good to it in either case: you can't crash early and report the exact place that failed due to rejection propagation neither can you attempt to recover because being propagated the exception loses the information about where and consequently what has failed.  TLDR  Rejection propagation is a horrible thing in promises (their specification/implementations). "
REST don't solve many of actual usual dev problems  and nothing ever does it elegantly.,"If you tell me Swagger should only be used to present APIs, but not design/define them, I'd have to tell you Swagger is not a good development tool, as it does not help me to generate client and servers interfaces in compliance to each other.  I can do it, for ages, with WSDL and WADL, and I can do it with RAML, albeit only with Mulesoft's products for now.  That said, I also don't find some of REST's constructs to be that useful when developing systems; you are going to break the ""RESTful way"" if you, e.g.,  try to return multiple resources that share the same sub-resource, and not have copies of that sub-resource all over the returned graph.  EmberJS will elegantly use ""sideloaded relationships"", which don't use hrefs at all, and kinda break the linked data dogma, reducing the payload.  Facebook Relay will also do that when caching records, but GraphQL is not made to deliver flattened results, which defeats the efficiency purpose.  Netflix Falcor will do that with JSON Graph, but with a weird syntax.  TL;DR: REST don't solve many of actual usual dev problems, and nothing ever does it elegantly. "
 Like null exclusions can help eliminate unessicary code by  attaching an assertion to the type  and allowing the implicit assertion exception hndle it  so too can this help in general.,"> It looks like this is allowing you to add runtime type constraints to a type, when in reality what I think the language (or any language) should strive for is stronger tools for declaring compile time type constraints.  Oh, I agree that compile-time is far better to have a constraint than runtime — that said, there are situations where you cannot avoid runtime checks. (Trivial example: user input.)  There's some good reasons that you shouldn't dismiss runtime checks out of hand — for one, they can gratly cut down on debugging by indicating whre the bug was. (In Ada, since the constraint is on the subtype, it will be checked on parameter-passing and assignment [though there's a lot that can be optimized away, especially if the optomizer does flow analysis].)  A dead simple example to make impossible values impossible is SSNs and DB consistancy — by using a string subtype we can ensure DB values going to the DB are properly formatted/valid, and assuming no changes to the formatting-check and no tinkering manually with the DB that all SSN values coming from the DB are valid. (This would have been a lifesaver on a project I was on several years ago, perhaps literraly, as the program handled medical records and was plagued by inconsistant data in the DB.)  TL;DR — Like null-exclusions can help eliminate unessicary code by ""attaching an assertion to the type"" and allowing the implicit assertion/exception hndle it, so too can this help in general. "
there are uses for PHP and then there are places where PHP shouldn't be used. Also make sure your code is really solid before defending the language.,"I listen to their points and see if they are saying anything legitimate. Which sometimes it can be. I hear them complaining about certain code that others wrote or they have seen and I agree that it's pretty bad. But I hold my code to high standards, make sure it's in classes, type hinted, namespaced, business logic and views are separated, controllers are small, everything is dependency injected everything is SOLID and DRY. I give them no reason so say my code is bad.  I also realize when PHP is not the answer. PHP is great for API's and most normal sites. However PHP isn't great for queues, websockets and concurrency. Yes there are libraries to do all of that, however they are slower and require a lot more set up. For example in the same Go binary serving my API, I can have workers that can post-process all with simple code built into the language. If I were to do that in PHP, I'd have to set up RabbitMQ/Redis/SQS, then have another script constantly running that does the process, with something managing (like Supervisor) the workers since I'd need multiple ones as it's processing in serial not parallel. In the same place, PHP is also not great for creating large ETL pipelines where multiple processes have to run in parallel.  tl;dr: there are uses for PHP and then there are places where PHP shouldn't be used. Also make sure your code is really solid before defending the language. "
His boss was a rude jerk who demanded  almost constantly  he ask questions. He asks a question  gets an unsatisfactory response  and determines that he shouldn't ask questions.,"Because OP, in his own story, sounds like an unmotivated prick. The boss certainly made several critical mistakes in managing his onboarding, but those don't make up for the complete lack of effort on his part.  Calling his boss might not work for every boss, but it's something to bring up and ask a coworker. Many managers will take urgent work calls. He did talk to his coworkers, yes. That's step one, and takes a few hours, max. At some point you stop wasting your colleagues time and reach out to your boss. Even if your boss is a total shithead, sulking in the corner is what a toddler does.  Let's say you don't want to bother him while he's on vacation, so you talk to your coworkers for a week and a half... fine. After his boss returns, there were at least 3 whole days of nothing, where he made no effort to speak to his boss. I could understand one day for timing issues (although not sending a quick ""when will you be free"" chat is pretty inexcusable). 3 days?  It's pretty clear from the rest of his descriptions that he just didn't care enough to ask questions. Clearly, his boss sucked at explaining things, but his boss also demanded he ask questions, and he (after being scolded at least twice) never did.  In the new manager and interim sections, we get:   he first item on the meeting’s agenda was apparently to browbeat me for not communicating with him enough.  he next item on the agenda was to browbeat me for not communicating enough with my teammates.  Microsoft employees are expected to take initiative  should be asking questions  why haven’t you been asking me more questions?   Followed by:   I interpreted his response to mean that I would be better off continuing to search for the answer myself   Tell me he doesn't sound like a moron.  TL;DR His boss was a rude jerk who demanded, almost constantly, he ask questions. He asks a question, gets an unsatisfactory response, and determines that he shouldn't ask questions. "
Manager's fault  though OP could have maybe fixed the situation  with hindsight.,">At some point you stop wasting your colleagues time and reach out to your boss. Even if your boss is a total shithead, sulking in the corner is what a toddler does.  Not if he is AFK on vacation, no, you don't. This wasn't an 'urgent' call by any definition.  >After his boss returns, there were at least 3 whole days of nothing, where he made no effort to speak to his boss. I could understand one day for timing issues (although not sending a quick ""when will you be free"" chat is pretty inexcusable). 3 days?  You're being ridiculously judgmental. First, 3 days in a new work environment can go by in a scream. Second, we don't know if the projects his coworkers suggested were taking up time or if he was playing Solitaire.  More importantly, his manager was back from vacation for  three days  before he scheduled a 1:1 with his new hire that he (seemingly) hadn't prepped for at all(!!!). The same manager that didn't tell a teammate to be his mentor or to 'onboard the new guy' while the boss is AFK, or to have a first project. Nothing. Why aren't you more outraged  at that Manager  and not the new guy in a new environment that didn't respond perfectly to shitty circumstances?  >Tell me he doesn't sound like a moron.  He honestly doesn't sound like a moron.  He also mentions the next new-hire being onboarded in a reasonable way, which is what made OP feel alienated and, presumably, unsure in his footing on the team. The situation, to me, sounds like OP got on the wrong side of a shitty manager, and didn't go up the corporate ladder soon enough to stop the damage, so quit.  TL;DR Manager's fault, though OP could have maybe fixed the situation, with hindsight. "
Resume's are complete shit  and something like 60  of the jobs you're going to get are because of who you know  so get out there and meet people.,"As a somewhat recent grad and developer, here's what I've been seeing.  First, jobs that require, effectively, 1) a degree and 2) a pulse, will auto-filter based on GPA. The problem is that these may be the only jobs you're qualified for as a recent grad, so if your GPA isn't great, it doesn't matter how successful you are as a programmer or what you can do, you'll get filtered.  Second, there aren't many jobs online for new grads. Many of the postings you'll find want 2+ years, and a list of buzzwords, and you'll most likely find out that you're ""not the right candidate"" for the position if you don't have lots of those buzzwords on your resume.  So here's my advice. First, get a technical internship. Start applying right now to the big name places like Google, Dropbox, Facebook, Twitter, etc., and when you get through the names most normal folks have heard of, start looking through other companies. Any programming internship. Second, when you graduate, move to the coasts. Either California or New York. People like to bitch and moan about ridiculously easy it is to get funding in the Bay Area, or how Startups want to trade you equity for a lower salary and that's shit since the startup will probably fail, but fuck that. As a new grad, get into that roiling sea of money and action, and you'll get all of the experience and contacts you could hope for. Get active in meetups and hackathons, not only in areas you're already comfortable with but also outside of your comfort zone.  Tl;dr: Resume's are complete shit, and something like 60% of the jobs you're going to get are because of who you know, so get out there and meet people. "
I didn't know and ORMs don't encourage that sort of thing.,"A few reasons:   I didn't know it existed.  That's an interesting feature.  I'm a sysadmin that accidentally ended up doing software development, so I don't have much of a DBA background.  A lot of the Postgres docs are a difficult read for me.   Most ORMs, ActiveRecord included, don't support this sort of magic.  They're trying to run on a variety of database platforms and so target the lowest common denominator.  If it's not standard everywhere, it's not supported anywhere.   I'm not sure it could be made to do exactly what STI does in ORMs.  Looking at the [tutorial in the Postgres documentation]( you'd want City objects for things that are just cities and Capital objects for things that are capital cities.  It's not clear to me if it's possible disambiguate which rows are just Cities and which are Capitals when querying the cities table.  An ORM would need that to create objects with the correct class.    tl;dr: I didn't know and ORMs don't encourage that sort of thing. "
Intent and knowledge of your code's intended consequences have great bearing on your moral responsibility for said code.,"So if a person contributes to an open source project, then that project gets used to create gas chambers in a genocide, is that developer morally responsible for that gas chamber? How about the people who manufactured the pipes used to run the gas lines? Or the people who manufacture concrete mix, just because their mix was used to pour the foundation? What about the people that mined the raw materials that went into the concrete and pipes?  At some point, we must draw the line and say past this level of indirection, you are not responsible for the way your product was used. I am advocating this line to be drawn where intent is known. If you knowingly develop a program for use in a gas chamber, or make pipes designed around running deadly gas for use on humans, or mix and pour concrete knowing what the building is going to be used for, you bear a moral responsibility for that action. If you did not know how your work was going to be used, despite being inquisitive and thinking for yourself, you bear no responsibility for the way your code is used.  Your intent and state of mind are regularly used in criminal proceedings. If you kill a person in a car crash, you generally won't get sent to jail provided you weren't negligent. If you plan out a collision in advance with intent to kill the other driver, the outcome is the same: a person dies. But you are guilty of a  very  different crime, and it revolves around your intent and knowledge of your action's consequences.  TLDR: Intent and knowledge of your code's intended consequences have great bearing on your moral responsibility for said code. "
don't feel like you  have  to use  class .  If you love the syntax  go for it  but you're not missing much if you choose not to use it.,"Frankly, I'm in the camp that ES6 classes aren't the best idea.  (It's probably a small camp, but I've heard the same opinion expressed elsewhere, so I know I'm not just a lone voice in the wilderness here)  Particularly, I think they're a three-fold trap to newcomers to the language:   They abstract away the syntax of prototypical inheritance, but they're just sugar, and they don't actually free you from the necessity of actually understanding prototypical inheritance in order to understand what's actually going on, or to deal with some more complex situations.   They encourage trying to bring a very heavy OOP programming style to Javascript.  (The ""Java script "" programming style)  And I've never liked that style, and don't think it plays to the strengths of JS very well; it tends to produce more verbose code (more object definitions, unnecessary encapsulation, setters/getters, etc), certainly compared to a functional-style or even just compared to a more pragmatic mix of functional, imperative, and OOP style.   This is sort of an extension of point 2, but they encourage inheritance over composition, while it seems like popular opinion on best practice is moving strongly in the opposite direction.    You can definitely avoid these pitfalls: it's entirely possible to write great idiomatic Javascript with composition using  class ,  but the more widespread  class  is, the more newcomers are going to fall into those traps, and personally, I don't often find that the sugar is all that significant in the first place.  In part, I'm just waiting to see where ""best practice"" ends up.  TL;DR: don't feel like you  have  to use  class .  If you love the syntax, go for it, but you're not missing much if you choose not to use it. "
The defense doesn't have to  dis prove the data  only raise reasonable doubt as to its credibility  or convince the judge of it failing to meet admissibility standards.,"IANAL:  In our system of jurisprudence, the defendant is guaranteed the right to confront witnesses against him - primarily for the purpose of showing cause to discount or invalidate their testimony against the defendant.  The technical expert providing the statistical data, the software deriving that data, and the persons who wrote the software are legally witnesses against the defendant.  Ergo: The data and software, as well as the people who wrote or interpret them, must submit to open court scrutiny by the judge and the defense in order for the testimony to be legal.  Failing that, the testimony is inadmissible and may not be considered in determining guilt.  If inadmissible evidence is wrongly permitted by a judge, a mistrial may be found by a higher or appeals court.  TL;DR:  The defense doesn't have to (dis)prove the data, only raise reasonable doubt as to its credibility, or convince the judge of it failing to meet admissibility standards. "
Play with it  play with something else  choose what you like.,"I've used CakePHP in production and personal apps for about 5 years now. It boils down to what you're comfortable with (which all of these framework discussions do). I enjoy the framework and find it performs well once you tweak it. My main project at the moment, a full featured accounting platform, with API for iOS and Android apps, that's been going on 2 years now is based on CakePHP 2. I enjoy the speed at which you can get something out the door and into the consumers' hands.  My exposure to other frameworks is purely in testing them out for fun, but my entire business as a contractor has a foundation in Cake so I'm rather biased.  CakePHP 3 has incorporated the latest trends in PHP so if you learn Cake you aren't going to have an issue switching to any of the others.  TL;DR Play with it, play with something else, choose what you like. "
Those books  ISLR and ESL  they're actually a great place to start  if you're interested in becoming a competent practitioner.,"Well, you can get the basics about how to write the R / python / Matlab / whatever commands that do a thing, buy without some kind of intuition about what's going on mathematically, it's pretty certain that your analysis will be not really authoritative... Or correct.  On the other hand, the approach I have been taking, having started with the online courses available, is to go breadth + 'how to' first, then find an dataset that I can analyze and use that to motivate depth and understanding. If I had to do it again, I'd go depth first on glm, enough to make me competent in using the tool. That means knowing the guts of the math inside.  TLDR: Those books, ISLR and ESL, they're actually a great place to start, if you're interested in becoming a competent practitioner. "
React Native is about declarative  asynchronous UI using Javascript  ReactiveCocoa is about bringing FRP  declarative  functional  signal based programming  to Cocoa.,"Not sure why you're being downvoted; it's a good question.  React Native and ReactiveCocoa, while sharing the word 'React' in their titles are only somewhat similar in their goals and totally different in their implementations. They're similar because they both aim to make Cocoa programming more declarative and functional. For a nice intro on those subjects, look at ReactiveCocoa's [philosophy page](  React Native , and updates on-screen components based on a diff after rendering for a new state asynchronously.  ReactiveCocoa, on the other hand, expresses everything as a stream of signals. Using signal transformations, and evaluation of signal streams, the programmer can be explicit about how data flows through the app and how dependent components are updated. An example is interpreting a login screen as a set of signals to be evaluated that determine whether or not a login is successful, which UI components are active, etc.  TL;DR - React Native is about declarative, asynchronous UI using Javascript; ReactiveCocoa is about bringing FRP (declarative, functional, signal-based programming) to Cocoa. "
Some employers are willing to screw you hard  read the paperwork  you can give away far more than you can imagine in some of them.,"I had a contract a potential employer wanted me to sign that basically said that  anything  I produced related to programming (including videos, tutorials, books, etc etc etc the list was huggggge) would be the employers if it was created while I was employed with them. In addition the contract said that at the termination of my employment I would either require written permission from them to work in the industry of 'programming' or I wouldn't be able to compete in the industry of 'programming' for ten years.  I 'noped' the fuck out of that contract instantly. I had gotten all the way to the 'sign and you start work tomorrow' but one read through of the employee contract and I just said hell the fuck no.  tl;dr; Some employers are willing to screw you hard, read the paperwork, you can give away far more than you can imagine in some of them. "
saying you want the FP without the math stuff is like saying you don't want FP,"so like  I hear you and I think you have a good point  But there's a huge difference in the intellectual lineage between functional and imperative/object oriented programming. OO is modeled after a ""Turing Machine"" understanding of computation, while FP is modeled after a ""Lambda Calculus"" understanding of computation. OO/imperative programming is very concretely about modeling somewhat ambiguous Real World Things as  analogies  to software patterns and building software around those analogies. Then you get Observer pattern, dependency injection, MVC, etc... and none of these have any rules or laws to them. Just vague ideas of what might make things better. It is  so easy  to do OO wrong.  FP is more about taking ideas from math (algebra, category theory, etc), and applying them to software design. Ideas from math have really strong guarantees about their correctness -- if you want to make a data type a functor, there are two laws you have to obey for it to be correct, and from there, your functor will always work exactly like it should. Functor is a pretty weird word, but once you know what it means, you can see that there really aren't any better words.  Since the ideas come from math, it doesn't make any sense to obscure what they are or their origins. Any effort to reduce the abstractness of the name will necessarily confuse further learning and understanding of what the name really means. Calling a Functor a  Mappable  just punts the question: ""What does it mean to  map  over a  Mappable ?"" Calling it a  Container  completely obscures what a functor really is. Functions are functors, and thinking about functions as containers is a bit of a leap.  tl;dr  saying you want the FP without the math stuff is like saying you don't want FP "
It's OK to switch jobs for higher pay  but after a certain point pay is not as important as culture.   EDIT   grammar,"Lots of people recommend leaving a company to get a salary bump. This seems reasonable to me.  In your first job with no real experience you feel as though the company is taking a risk on you - so you're happy just to get an offer. A good place will give you yearly bumps and sweet bonuses because they want you to stay for a while, be happy and be productive. After you acquire some years of experience you can probably double your salary at another company, and negotiate your salary with more confidence.  Your current company is probably happy with this arrangement because they expect some people to leave naturally, giving them more room to hire new entry level people. They can keep overall salaries even, get fresh ideas, and give their seniors people to manage. However if you're exactly what the company needs then they can give you a promotion and a 'big' raise.  Then again some things are more important than pay - for me it's team culture. I'm generally more happy when I'm working with people I like and in a place that gives me autonomy. I can put a $ value on my happiness, and I've done it often while looking for work. For example certain consulting firms will pay an extra $30k/year, but I know I'll hate the work, the hours, the travel, the lack of autonomy and ownership, etc.  tl;dr - It's OK to switch jobs for higher pay, but after a certain point pay is not as important as culture.  EDIT - grammar "
If you are sufficiently obsessed with something  you can do better at it than most out of shear will power.,"Anyone can program yes, and I would even dare say that almost anyone  could  program adequately enough to be a professional if they  really really really really really really  wanted to.  The problem is usually not lack of human intelligence, but rather a lack of sufficient motivation and interest. I am not a talented developer. I have never had talent for programming, and it has definitely  not  come naturally to me. I almost Majored in  ART  because that is where my natural talents are, but chose CS instead because I figured it would be more practical. I struggled during college but managed to finish my CS degree because I worked my ass off and I was programming in my free time when I wasn't programming for school.  I do, however, have something that makes up for my lack of talent and natural ability, which is that I developed an endless fascination with programming. I think about it constantly, and it is always on my mind. I have been this way for over 15 years since I first started studying it. I worked at Microsoft for 7 years, and people there thought I was a decent developer, but would I spend significantly more time than most people take to solve the same problem. My mind struggles with the complexity of my own programs, and my short term memory isn't that great. But, I am a successful developer regardless, simply because I make up for my lack of innate problem solving abilities with experience gained from countless hours of just programming my own personal projects that a lot of developers don't do.  TLDR: If you are sufficiently obsessed with something, you can do better at it than most out of shear will power. "
here  heterogeneous collections appear everywhere in dynamic languages because it's just about the only tool in the box  not because they're fundamentally important.,"To (badly) paraphrase Conor McBride, if your type system is so stupid the compiler can write all your types for you, it can't be a very powerful type system, can it?  Type inference is great for the obvious types, but explicit types are essential to express more precisely what the program is intended to do, and complex type structures are in general undecidable given an expression.  Consider higher-kinded types, dependent types, existential qualifiers, etc.  > … that has most of the traditional dynamic language features.  What are dynamic languages bringing to the table here that we desperately need to emulate in typed languages, exactly?  We can rule performance out off the bat, right?  Avoiding the need to provide ""obvious"" types is there, but type inference fills this role effectively already.  Being able to run a program with known type errors is occasionally raised.  My eyebrows are likewise occasionally raised.  Nevertheless, many typed languages have an option to defer type errors to run time, or a bottom element that can fill any typed hole, and the usual purpose of this is to work on some unrelated bit of code, so ... work on that code, don't try to run the whole program?  Heterogeneous lists come up, too.  But, IMO, like type casts, these merely signal an inadequate type system rather than any general purpose need.  Invariably the contents of the list is either assumed to be a duck (ie, it has an implicit type anyway) or is tested to be one of several known types (ie, it's a member of a sum type).  In the very rare cases when a heterogeneous list might actually be the right solution to a problem, many typed languages have an ""Any"" type which you can blow up your program at runtime with all you like.  TL;DR here, heterogeneous collections appear everywhere in dynamic languages because it's just about the only tool in the box, not because they're fundamentally important. "
Unicode is complex because it is trying to address complex situations  unicode is also  more complex than needed  because it is a pile of kludges.,"\shrug\  -- Except I'm not arguing for a multilingual system... just looking at [methods of/for] representing/displaying text. That alone means there's less complexity and makes comparison mostly invalid.  Besides, I already showed how a single-codepoint system could indeed have a simpler realization than unicode's mismash of designs... or do you want a more specific layout?  How about this:  XX.YYYYYY  XX -- Two bits specifying case (upper, lower, title, RESERVED). YYYYYY -- 2^6 characters.  Now case-conversion may be done simply by altering XX to the desired case-enumeration. -- Of course this is oversimplified, and overly optimized for case-conversions... but the point is that mixing design-philosophies (like unicode does) makes things overly-complex. -- Heck, you  could  use a hybridized system that would be much simpler than unicode, if you had some over-arching design that was consistently applied. (One such system would be encoding each word [and possibly letter] in the language, and then have a glyph-building scheme for each word/letter -- you could do it in Forth by assigning the words [forth-functions] drawing the words/characters and simply passing through the encoded-word/letter.)  tl;dr -- Unicode is complex because it is trying to address complex situations; unicode is also  more complex than needed  because it is a pile of kludges. "
There are ways to render pages in a few hundred ms or less  but people tend to cast them aside due to readability browser compatibility issues library and framework choice.,"I've worked in frontend and backend systems, and generally, frontend devs don't always have the same time-based requirements placed upon them, so they just go with whatever works the easiest to get the front end complete to specifications. This isn't a reflection or judgment on their character, it's just they are doing what they must to get a lot done with the constraints they have.  Their constraints are usually things like browser compatibility, library/framework choice, etc., all of which can have vastly negative impacts on performance. Throw all of that on top of a bunch of rendering engines and languages ( cough cough JavaScript ) that also are not always the most performant, especially if you want your code/markup to be readable.  TL; DR: There are ways to render pages in a few hundred ms or less, but people tend to cast them aside due to readability/browser compatibility issues/library and framework choice. "
Just pick a framework that works well for you. When you need to care about milliseconds  the framework is not that important any more.,"You really shouldn't care about performance as much as you do. Phalcon has their performance as key selling point, but it really doesn't matter anyway. As long as the framework you choose is not  slow , it will do just fine. You should look at functionality and useability. Choose a framework that fits you and where you can develop with.  The reason I say this is quite simple: at the moment your website attracts lots of visitors, the speed of one framework on one instance isn't important any more. At that point, you need to look at load balancing, splitting web and database-servers; and probably application/instance cloud.  Thousands of requests per second on one instance is great, but with that many requests you should look to redundancy and load balancing.  TLDR; Just pick a framework that works well for you. When you need to care about milliseconds, the framework is not that important any more. "
If your Python 3 is reporting ascii as default encoding  your locale is probably not sane and needs fixing.,"Not sure what the situation with Python2 is, but Python3 defaults to the encoding of the current locale. For most sensibly-configured locales, this is utf-8 (or utf-16 on windows?):  $ pythonPython 3.4.3 (default, Mar 25 2015,     17:13:50) [GCC 4.9.2 20150304 (prerelease)] on linuxType ""help"", ""copyright"", ""credits"" or ""license"" for more information.&gt;&gt;&gt; import sys&gt;&gt;&gt; sys.getdefaultencoding()'utf-8'&gt;&gt;&gt; sys.getfilesystemencoding()'utf-8'  However, a) Not all locales are sanely configured, and b) in the case of filenames, a fixed encoding is not necessarily a real thing (some filesystems specify what encoding is to be used, but other filesystems (eg. ext4) simply store filename bytes without any encoding information or standardized encoding).  TLDR: If your Python 3 is reporting ascii as default encoding, your locale is probably not sane and needs fixing. "
There are Technical reasons for why some functions in PHP alter the parent data construct and others return a new data construct.,"There is a technical reasoning behind it. First array_push(), array_pop(), array_shift(), array_unshift(), and the various sorting methods are suppose to be used to change the parent data construct, the array. This is so you can use arrays in PHP to create a Stack or Queue, or Sort the array effectively in memory.  While the methods such as array_slice, is intended to take the parent data construct, the array, and return a subarray of that array for usage in whatever function. This is usually done when you want to do something with the subarray but you still want to keep the parent array intact. via some sorting or searching algorithm, for example.  TL;DR There are Technical reasons for why some functions in PHP alter the parent data construct and others return a new data construct. "
With weeks of manual regression you may have more intrinsic problems than whether or not you develop in a monorepo.,"I've been there and experienced that. I can't exactly speak for the project you're working one, but that's one huge red flag to me. Thinking back to monorepo project I was involved in with about 60 devs, the monorepo only made problems worse by forcing us all to release at the same time and share builds. There were far more intrinsic problems like lack of any automation and testing (leading to manual regression) that made releases extremely painful and consuming a huge chunk of time. We probably would have failed monorepo or not. Living in a world with one click releases (we now release many times per week, takes about 10 mins to run all the tests and publish), where all manual regression has been replaced by automated testing, I'm honestly surprised we managed to produce anything of value on the monorepo project (hint: we didn't). It was certainly impossible to move fast and react to change, not to mention thoroughly depressing to work on.  TL;DR With weeks of manual regression you may have more intrinsic problems than whether or not you develop in a monorepo. "
Streams are not a superset of promises. Promises are not a subset of streams.,"While from a very high level view it might seem reasonable to make that comparison, it's not really the case. You wouldn't use a stream instead of a promise, and vice versa. They solve fundamentally different problems, and using the wrong one is potentially going to cause more pain than it's worth.  You could technically use a stream with a single value to replace promises, but it wouldn't always work quite the way you're expecting.  A Promise represents the result of a computation that may complete sometime in the future. It's stateful, memoizes the result, will only ever fire once, and will propagate errors.  An RxObservable represents a value changing over time. Depending on the specifics of how the Observable is created, it may discard value if there are no subscribers, it may execute a side-effecting computation multiple times, it may potentially return multiple values when you're not expecting it. It also may cause memory leaks if you don't explicitly unsubscribe from a stream.  tl;dr Streams are not a superset of promises. Promises are not a subset of streams. "
I use style guides as  guides   not rules,"Easy to read is a human property that machines can't really judge. So instead, automated tools just enforce a set of fairly arbitrary rules about how you have to lay out your code, many of which don't benefit readability at all.  If every project doing this carefully configured the tools to check the specific things they care about, it would be fine - I'd rather a machine told me two minutes after I make the PR than a human told me two days later. But that never seems to happen. People just run the tool with the default options, because we've got to use PEP 8, in every tiny detail.  TL;DR: I use style guides as  guides , not rules "
You are disposable.  Always be figuring out how to move up or into a new career. You're fucking yourself if you're not planning.,"Every job can be automated or non-existent overnight. All it takes is one person to figure out the process to do it.  Once that happens and companies see it's a viable way to spend $50,000 on a piece of software rather than $150,000 on one employee, they will do it.  Enterprise software has disgusting pricetags on it because though it may be a piece of crap, it can do the exact same as 100 employees combined... for the price of one employee.  Almost every career has a lifespan.  They don't last more than a few decades.  They evolve into new roles.  If you aren't constantly thinking of what you'll move into if your career went extinct the next day, then you don't belong in a field of critical thinking and analyzing data to provide intelligent decisions.  You might as well file for unemployment now.  Or go fuck yourself if you're one of the complacent people who get comfortable in your job and do everything possible to make new hire's lives miserable since they are doing a more efficient job than you in a role that's like yours.  tl;dr : You are disposable.  Always be figuring out how to move up or into a new career. You're fucking yourself if you're not planning. "
JSON pointer has different goals  it's not designed to be a quick convenience syntax  so it's a poor match as one.,"The alternative I propose is an array of strings:  ['foo', 'bar', 'baz']  This doesn't need to be parsed. And dot notation is a useful convenience syntax for typical cases (i.e without dot in the key), which is easily converted to the array above:  if (is_string($path)) $path = explode('.', $path);  But the JSON Pointer, which has reasons to be the way it is (should be a string, should handle any characters in a key, should be usable in URLs etc.) is more complicated than dot syntax, and has to be parsed char by char to follow escaping rules and what not.  tl;dr  JSON pointer has different goals, it's not designed to be a quick convenience syntax, so it's a poor match as one. "
I downvoted your comment because I classified it as a shitpost.,"> Edit: apparently some people are annoyed that I prefer Vue and that I stated my opinion. Reddit being Reddit I guess. Thanks for the downvotes!  I didn't downvote you for liking Vue. I downvoted you because you didn't seem to have even read the article, but for some reason decided it was ok to try to intentionally derail the topic anyway.  Also all but one of your  objective  reasons are subject and even that's questionable because it comes back to comparing apples to oranges. Your subjective reasons contain zero information. They're equivalent to saying  Vue sux  without giving any actual reason.  TL;DR I downvoted your comment because I classified it as a shitpost. "
'Node isn't designed for X' is a bad response to the criticism 'Node is bad because it isn't designed for X'.,"Your analogy essentially boiled down to 'don't complain that tools don't work well when you use them for things they weren't designed to be used for', is that a fair assessment?  My analogy was that your comparison wasn't really fair. Defending node by saying it wasn't designed for CPU-bound work would be reasonable if you couldn't have a framework or library or language designed for both CPU-bound and IO-bound work, but you can. They exist, they're plentiful. So the argument that criticising Node for being bad for CPU-bound tasks is unfair because it's designed for IO-bound tasks misses the point that there's no reason that Node couldn't be good for CPU-bound tasks, given that lots of other similar projects/libraries/frameworks/languages do fine at both.  Your entire argument boils down to a false dichotomy, the ridiculous idea that in order to provide good performance for IO-bound tasks you need to sacrifice performance for CPU-bound tasks.  TL;DR: 'Node isn't designed for X' is a bad response to the criticism 'Node is bad because it isn't designed for X'. "
keep all the values and take the median instead of the mean,"It's important to understand what exactly you want to compare. For instance, you want to ""find out which side of the family was more generous on average."" So, you are really wanting the ""central tendency / expected value"" of the gift amount distribution for each family in order to see which is larger. The problem, as you stated, is that there are large outliers in the distributions (e.g. $2000 is much larger than any other amount). So, the few large amounts will skew the expected value to something that is not representative of the underlying distribution. For example, if Bill Gates was in a room, the average salary of people in the room would be much larger than you would expect by randomly picking a person from the group.  All that to say that the average (i.e. arithmetic mean) of a highly skewed distribution is not a good indicator of the expected value. A much more robust measure for highly skewed distributions is the median or even the harmonic mean.  TLDR keep all the values and take the median instead of the mean "
Looking solely at wages is myopic. Also  there's more to life than money.,"Hey man, you know what isn't shit in (northern) Europe?   Universal healthcare  Actual retirement benefits  Sick leave  Maternity and paternity leave for months  Labor unions  Affordable insurance  Low-cost to free secondary education  Job security (not in the ""no one else can maintain my code""-bullshit sense)  No earthquakes  Laws regulating working environment  No H1B system   But sure, you can go on and on about making 60-70k. I'm sure that makes it worth working 60-80 hour weeks, commuting for 2+ hours daily, obfuscating code and paying out the wazoo for healthcare.  Sure, northern Europe doesn't have the same amount of venture capital as the 'Valley. But claiming there are no startups here? Are you sure you lived in Stockholm, Sweden and not Stockholm, Canada? ( Ever heard of Skype or Spotify? Spotify was founded in Stockholm, while Skype was funded by a swedish entrepreneur.If you've ever, say, made a mobile phone call, chances are you've used several technologies made in Sweden, by companies such as 3S (inventors of the SIM card) and Ericsson (large scale manufacturers of automatic telephone switches and a certain language called Erlang). Oh, and you know that company called King that was bought by Activision-Blizzard for $5.9bn? Yup, that's a Swedish startup.  TL;DR: Looking solely at wages is myopic. Also, there's more to life than money. "
I will always hate using Angular instead of React until it gets JSX support.,"My main complaint with Angular is that it doesn't use TypeScipt  enough . Specifically having templates be strings is a huuuuge drawback coming from  React. The lack of static analysis on templates is brutal. I hate passing in properties to components in Angular and having no insight into whether they're correct or not. Nevermind simple refactorings like renaming freaking methods and properties goes back to the ol' JS ""find and replace and hope shit doesn't break"" pattern. (There exists a service to statically analyze templates in Angular but it doesn't work very well at all in my opinion. It's like being blind in one eye instead of both.)  If you're templates have code in them they should be code and leverage all the wonderful tooling and insights the language you're using can provide.  tl;dr: I will always hate using Angular instead of React until it gets JSX support. "
it wasn't really against  sociology  it was just to express something  not programming   let it go.,"eheheheh I think you really understand the point now. OP was just talking about ""something that doesn't involve coding"" and he said ""sociology"". Everybody understood (I think) that he was not bashing sociology, he just mentioned a topic that is on the opposite side from CS, it is probably a cliché, but most of the time I use sociology too, maybe because  many of us have  I had bad experiences with wannabe sociologists, I don't know, it's just the first that comes to mind.  It is just like when people think that CS students or programmers in general or people working with computers in general (which, BTW, is everyone in 2015!) are  nerds , often not knowing what that actually really mean. Should we specify what nerd mean every time? We just let it slide, because if we don't, we could start and endless discussion on literally  nothing .  BTW: two of my best friends are sociologists, they were studying at the building I mentioned above, some of the best stories about sociologists I've heard from them, and I told them stories about stupid CS students or bad jokes like ""engineers don't live, they just function"". I also had a sociologist girlfriend and she's literally a genius. So it's not really I have something against sociologists or sociology or engineers or someone else, we're just talking, not writing an essay for when we'll receive the nobel prize, it's obvious some generalisation kicks in, we're human after all, that doesn't mean really want to offend someone or something, we are just oversimplifying to make the  main  argument clearer.  The other BTW is that I know Elia personally, we worked together for a couple of years, he's a great coder, he taught me Ruby programming through Opal,  and I'll be always grateful for what he did. He also has his personal beliefs and I don't share some of them, but share  most  of them. Should we argue indefinitely about things we disagree? Is it useful? Should I stop using Opal or any other project he's involved because we disagree on something? My advice? Let it slide when you can especially when it's not clearly hurtful or hateful.  TL;DR: it wasn't really against  sociology, it was just to express something ""not programming"", let it go. "
Python is fine for most anything you need to write quickly  and some things you don't.,"Python is a great language for scripting and making small applications, it's so easy to write and read that it's worth a moderate hit to the speed 99% of the time. A few years ago, doing an algorithms module at university, I got called out for trying to do our classwork in Python by a guy doing his in C++, my code tended to blow his out of the water because his high level algorithm design was terrible, and 9 times out of 10 your algorithm design is going to be so much more important to speed than your language. And my code took less time to write.  tl;dr Python is fine for most anything you need to write quickly, and some things you don't. "
It is simple  Linus just wanted to automate something that he knew inside out by then.,"I might be wrong, but here is my take on why Linus was able to make the git we know and use.  It is not because he is very very smart  only : yes, he is a very experienced programmer who understands how the hardware works (on a mechanistic level!) and has a very good understanding of computer science theory. But these are the fundamentals.  Then: he had been maintaining the Linux source for many years pretty much by hand, so he had plenty of first hand experience of the workflows and the pitfalls and the gory details. Then, the number of contributors kept growing and growing. The workflows became more complex, too. Then, he used a pretty good SCM for a while, maintaining the ever growing Linux code base and growing number of contributors, and gathered plenty of experience with that, too, both positive and negative.  You cannot replace practical experience like this with anything. In education it is called ""time on task"", and we know by now that it is the single most important measure of how much better at something you get. I don't care if you have been ""thinking about"" three-way merges for a day or for a decade....  TL;DR It is simple, Linus just wanted to automate something that he knew inside-out by then. "
Be aware  Stuff breaks and you're not automatically covered for it  by design.,"This is not (necessarily) Digital Ocean's fault. They host a server, but hard-drives can still develop faults etc, the same as a self-hosted one could.  Unless they are providing a managed service where backups / redundancy are included (which would inevitably cost more, and put restrictions on what is possible on the server), then being in the cloud does not protect you from hardware failure by default.  It's impossible to offer this redundancy in the  general  case- the requirements of one app are so different from another that it's not always possible to instantly mirror & version a hard-drive or seamlessly load-balance over to another instance. This is why every application has its own way of adding redundancy- think DB replication, hadoop clusters, http load-balancers, S3 replication, etc. There's no one-size-fits-all solution, and DigitalOcean leave it up to you to decide how best to handle a server going down.  How you address this depends on the critical-ness of your application. Manual backups are a simple option, frequent automated backups are better, automatic replication can be even better, and if you can't even tolerate downtime then you need some live-live setup. Unfortunately this costs more and more to develop, set up and test as you go down the list, so if it's not a critical production app then it's not always worth it.  tl;dr: Be aware: Stuff breaks and you're not automatically covered for it, by design. "
I like Reason. You might like Reason too. Or not. There's still a lot of great options.,"As another anecdotal data point, I landed on Reason after being a similar situation to yourself. For me, it strikes a lot of sweet points.  It is based off OCaml/Bucklescript (OCaml as JS) so there is a pretty mature community behind it. It fully embraces that JS interop is necessary and tries to make it as seamless as possible.  It is backed by Facebook with all the good and bad that brings. There are probably few apps in the world that get as much use as Messenger which is written in React Reason and vanilla JS React. I believe a fairly large portion if not most of Messenger is now Reason. Reason also has the infamous Facebook licence so you'll need to decide how you feel about that.  It can run in a browserless context. I plan to use Reason on the client side, for toy scripts and ultimately for server development. There is promise for a really good isomorphic story as shown by this admittedly limited app (  As someone trying to learn it in their spare time, I get a feeling of momentum behind it. Some parts of the developer experience are still patchy. Yet if you follow the Reason Dischord you can see the intelligent people trying to improve things. And they're working quickly. I feel that over the next year the bootstrap and the language will get better documentation and the quality and availability of tutorials will improve. Something I hope to contribute to.  I think the important thing is to find one of the languages that appeal to your sensibilities the most and go with that. If you learn Purescript picking Reason will a lot easier than learning it from scratch anyway. Reason appeals to me and if I want to learn Elm later, there's nothing stopping me.  TL;DR I like Reason. You might like Reason too. Or not. There's still a lot of great options. "
blog post needs more information  and preferably a code sample.,"Without any details of, well, anything, it's hard to decide how much weight to give his opinion. Here's my questions if the author is reading this.  How quickly does he expect a query over 100TB to run, anyway? All we've got is this:> I do analytics where the response time is critical. For me, I give it up to 3 or 10 seconds, okay perhaps even up to 15 seconds and still consider this interactive mode.  Over what sized data-set?  > However, beyond 1 Tb, I noticed that the response time was extremely delayed  Delayed how?  Anyway, if this is too slow for interactive usage, what is faster, what is he comparing it to, Hive? Impala? Drill? Vertica? Greenplum?  Lastly, how was he using it. What was his source data - raw CSV/JSON from S3? If so, how was he parsing it? Was he loading Parquet files stored on HDFS? How are they structured? How was he using caching and serialization? Did he take advantage of in-memory columnar formats, for example. Because this:  > “where sum(), count() ” statements with 8 terabytes of data was 20-40 seconds  Sounds like it spent most of its time loading from disk. To take full advantage of Spark, you would cache your working set before counting, as counting will load all the data. Then the sum should be faster. If you cached in the columnar format, then the sum will be even faster, as it only has to retrieve the one column.  TL;DR - blog post needs more information, and preferably a code sample. "
React    easy API  complex ecosystem. Angular    complex API  easy ecosystem. In my opinion.,"Very basic differences between react and angular: Angular is a framework and gives you everything you need to build a front end application (view rendering, router, state management etc.) Whereas React is just a view library so if you need to partner it up with other libraries such as redux and react-router to get everything you would get from angular.  In terms of which to choose : my first piece of advice would be to disregard any comments about which is ""best"". Neither are ""best"" - it's just different tools for different situations and preferences. That said, if you are just starting out you might be better off with react or vue due to their shallow APIs. Much easier on beginners although it is still very possible to create a complex React app. In fact it is much easier to get yourself in a muddle with React than it is with angular because it's relatively unopinionated whereas angular has very strong opinions.  Tldr; React == easy API, complex ecosystem. Angular == complex API, easy ecosystem. In my opinion. "
I like JMP and I like GOTO. Showing ASM loops is always good times.,"Ya know, everytime I go and look at something even remotely similar to this (e.g., anything involving ASM, more specifically JMP, JNE, etc)  All I can think of is Dijkstra and his paper about how GOTO is bad and should feel bad, and how so many people I work with live in that same school of thought.  Then you show them something that has cmp ; je  [after explaining it to the ones that don't know any amount of asm] or just showing a jmp to skip a certain area of code to go to the logical end of the program. How is that any different than GOTO, why does GOTO bring such inspired hate.  I've accepted that while I code C# at work I can't use goto, and C# goto does look a bit squiffy. We also have some old VB6, there's about 3 of us that can read/write it. We pretty much have free reign there and we all use goto for error handling / object removal.  tl;dr I like JMP and I like GOTO. Showing ASM loops is always good times. "
Talk to alums of the programs  get a loan that gives you enough deferral   payments time to find a job,"I've heard great things about Metis and Galvanize (but this is in SF). You could learn everything by yourself — the real questions is whether you have the motivation to. The network is another big part of the value-add, depending on their partners.  Something I would do is to try and find alum from the schools you're considering and see what their experiences were.  If you do take out a loan, make sure there's a 6 month deferral (ideally). Most programs are 3 months and the job search process tends to be 1-3 months.  If you decide to move forward, check out WeFinance.co (full disclosure: I work there). You can crowdfund a loan and pick your own interest rate, deferral, length, etc.).  TLDR: Talk to alums of the programs, get a loan that gives you enough deferral / payments time to find a job "
Many people at software don't like know what they do and never spend some time to improve on their skills. So that led to awful quality.,As an Indian I can approve the things you said above. The worst part of all these yet are service bonds. People who get out of college and got recruited by those companies need to stay there for atleast 2 years on most occasions along with the service notice period. Most people use this time not to get better at the job but to prepare for some competitive exams and get off after two years. Go to a competitive exam and you will find most people there to be software people. So people use it like stipend and get out in two years. The companies have terrible attention rates over the last few years which led to TCS to increase the service notice period from 30 to 90. So they hire next set of graduates fresh out of college and the loop goes.  tldr  Many people at software don't like/know what they do and never spend some time to improve on their skills. So that led to awful quality. 
Established business work differently that trendy financially dubious startups.,"For your points:   Implying the framework is so severely outdated and unpopular to be buggy or poorly documented enough to warrant scrapping your entire product. This is not the case for many large modern frameworks.   As for talent, I can definitely understand that. It's less of a drive to work on the ""newest and greatest"" and more of a drive to stay relevant to current technology so you are still employable.    This seems like a pipe-dream. Do game developers rewrite their games every time the engine updates to a new major version, or a competing engine does? No, because that's asinine and a waste of everyone's time and money. If you are awash in so many bugs that this is a financially viable route, then maybe the companies development processes and framework choices need to be brought into question?   SPAs have their place, static sites have their place. I don't know why you are making such a generalized and blanketed statement, it's like saying  ""No one needs a Truck because cars are cheaper, smaller, easier to drive, and more gas efficient. Trucks don't fit in many parking spaces."" . It's all about use-cases, that's how this works. As an example:   I develop enterprise SPAs. These tend to gain much higher adoption and user satisfaction that static sites for enterprise customers. These are replacements for what where native applications, and they should cater to the same user-case. You're users may use this SPA for their entire work-day without leaving it, it needs to be fast, responsive, dynamic, and data-driven. It's okay to have a slow initialization. Things static sites do very poorly. It doesn't matter how many requests it's making or it's power usage when you're running it on enterprise-grade hardware. Obviously you need to be cognisant of the performance, but that's a different topic.     TL;DR:  Established business work differently that trendy financially dubious startups. "
Industry default languages and frameworks are all trash. Longer learning curves can be worth it. Do functional programming,"It's a subjective creative field, so you won't get exact numbers. But claiming that some languages or libraries aren't better for maintainability or productivity than others beyond pure developer experience is kinda denying that there is any point to library or language design. I disagree.  But given the list of languages/frameworks you listed, I'll concur that there probably isn't too much difference. They are all imperative languages with MVC frameworks that roughly have slightly different function names to do the same thing.  The goofy static typing of C#/Java may give you some small long term maintainability benefits, but they are likely to overshadowed by the experience factor.  But add Haskell, and pretty much any popular Haskell web library to the list, and suddenly one of these things is not like the others. The type system actually helps you avoid bugs. The abstractions make sense. Code is unsurprising and does what it says, but concise  at the same time. The end result being that cost of change increases logarithmically instead of exponentially over time.  But it's not easy. It could take months to learn. But if you are doing a long-term project, ""easy to start with"" is a false economy. Upfront learning investment is worth it.  TLDR: Industry default languages and frameworks are all trash. Longer learning curves can be worth it. Do functional programming "
You won't create clean idiomatic code if you switch directly from PHP to Python. You will most probably write PHP code in python.,"I haven't read this in the comments so I feel the need to quote an [article about the migration from Java to Python]( Yes, this thread is about migrating from PHP to Python, but I guess there are quite some similarities:  > This is only the tip of the iceberg for Java->Python mindset migration, and about all I can get into right now without delving into an application's specifics. Essentially, if you've been using Java for a while and are new to Python,  do not trust your instincts . Your instincts are tuned to Java, not Python. Take a step back, and above all,  stop writing so much code .  tl;dr: You won't create clean/idiomatic code if you switch directly from PHP to Python. You will most probably write PHP code in python. "
It's fine for an example in a pod cast. It's probably fine here because it  is  in fact the smallest and simplest of projects.,> Also I'm not sure it's a good idea to expose an instanceof Model anywhere outside the service layer. Is this a common practice from Laravel users to manipulate ORM entities in their controllers and so on? It looks like a practice suitable for the smallest and simplest of projects.  I think for a screencast it's probably ok - I can't see what additional value more layers of indirection would have given to explain the technique.  Incidentally in his Laracon talk and in a conversation on his podcast with Kent Beck he talks a lot about only solving the immediate problem. The immediate problem here is removing conditionals to select a coupon. There are no other requirements so the easiest solution to understand is probably as presented.  As a system's complexity grows one could imagine that at some point this approach would break down - at which point you would refactor to service layers or repositories or something.  The theory goes that making accommodations for future design problems is pure speculation and even if the speculation is right you need to carry the burden of that speculation long before you get the pay off.  tl;dr: It's fine for an example in a pod cast. It's probably fine here because it  is  in fact the smallest and simplest of projects. 
Getting rid of the HAL costs AMD development time and keeping it costs kernel maintainer time.,"But that's not the minimal effort path. AMD has extremely limited resources right now. They simply can't afford to do everything the right way, so they have to make the most of the resources they have. One of the ways they're trying to do that here is to get their driver upstream to lower the maintenance cost to them. A second way they've tried to do that is with a HAL. Linux doesn't care what the lowest cost driver solution is for their driver vendors. Linux wants the right solution for the Linux kernel because that has the lowest maintenance cost to the kernel. That is the right and proper attitude for a kernel developer to have.  AMD understandably wants the lowest cost solution for AMD. Since they have to support both Windows and Linux, the lowest cost option for them is to use a HAL because that gives them a lot more code reuse. When you've spent most of the last 5 years losing money, avoiding duplication of effort is a good thing to be striving for.  TLDR - Getting rid of the HAL costs AMD development time and keeping it costs kernel maintainer time. "
Integer division should return an int.  Flooring division should return a staircase.,"Yes? That I'm perfectly aware of, and using as an example in this case.  What I, as a developer, DO NOT EXPECT is that a term called  Integer division   Would return a  float .   When working with data where type matters, I  expect  the result of an  integer  division to return an  integer . I do not expect it to be a  floating point division treated as integer  .  However, the  python3  manual doesn't call it  integer division   but  floor division   which is, subtly different. So when /u/xXxDeAThANgEL99xXx  and others call it ""Integer division"" this causes even more frustration and annoyance.  TL;DR;  Integer division should return an int.  Flooring division should return a staircase. "
Code written in any programming language is actually literature for two diverse audiences.  PhD's understanding of literature applies to psychology and sociology aspects of programmers work  too.,"Well, as you know programming is not only a set of instructions to direct a computer, but a series of sentences/paragraphs/chapters of human comprehensible language to show other programmers what was intended and how it was achieved.  It has form, meaning, and is context sensitive.  There's syntax, with nouns and verbs, adjectives and adverbs, pronoun and determiners, prepositions, conjunctions, and interjections, among others.  Programming can be accomplished in combinations of any of a hundred different human and computer languages, sometimes even mixing them ad-hoc.  The most practical use of metaprogramming: Being able to meta-analyze the entire sourcecode base of something like Windows 10 to pinpoint flaws in assumptions and 'spelling errors' by the writers.  And running the process in reverse, we could learn how to develop highly automated, safer and more effective frameworks and templates in our programming languages, so we can be more concise and consistent in expressing our thoughts and intent.  It also could have the side benefit of not needing to translate and rewrite  War and Peace  functions by hand with 10,000 volunteer code monkeys each time a fashionable new language crops up.  TL;DR:  Code written in any programming language is actually literature for two diverse audiences.  PhD's understanding of literature applies to psychology and sociology aspects of programmers work, too. "
People need to implement their own Forth  or get stuck with Forth circa 1980  and never really understand Forth's potential ,"I've been using Forth professionally for a few years now and I couldn't disagree more. People need to implement their own Forth's, even if they end up using or reusing an existing implementation. So much of Forth's unique power and flexibility comes from being able to tailor it to your requirements. It's a mistake to treat Forth like you do C. The simplicity of Forth makes it very practical to implement a custom Forth, for each problem you work on. In contrast with this, we can't possibly do that with C, so we don't. That doesn't mean that we wouldn't want to, or it wouldn't we valuable/useful to do so!  What makes Forth different is that nothing is off limits, and there are no black boxes. Understanding ever aspect of your system enables solutions which simply can't be imagined in other systems.  Now you do eventually have to build something of value with your Forth but that's not to say that you should ignore it's strengths, and just use an existing implementation. If you do that then any benefits that you discover will be artificially limited, while any drawbacks will be artificially enhanced, because the Forth you're using is unlikely to have been designed with your solution in mind. Perhaps more than any other language, the Forth you use,  becomes  your solution. Forth is a programmable programming language. If you're not implementing Forth, you're missing the point, in my opinion.  The company that I work at are now working on our 7th major revision of our Forth. Every time we rewrite it, we make it better; faster, simpler, and more suitable for the kinds of problems that we have to solve.  Forth is not a dead language, nor is it frozen, nor slow moving! Major innovations are happening all the time. The biggest problem I see is that these innovations rarely make it out in to the wider community... so the free Forth's you're likely to be using are almost universally outdated, by definition. This paints a bad impression of the state of Forth and it's relevance in today's world.  tl;dr People need to implement their own Forth, or get stuck with Forth circa 1980, and never really understand Forth's potential! "
I think among numpy's top 3 use cases is  familiar element wise math operations library  which has broad applicability and could be useful for something like Numscrypt here.,"While those might be two use cases that justify numpy's capabilities, I also use numpy simply because I'm more familiar with it than the  math  and  stats  libraries, it allows very simple vector math like  a = np.cos(np.array([1, 2, 3]) * np.pi / 2)  and whenever I try to use  math  for the odd  sqrt  or whatever I always find something I need/prefer numpy for.  I'm generally not utilizing it's big matrix capabilities, but rather it's element-wise operations and healthy breadth of math functions.  On a related note... I'm familiar enough with pandas now, I use it whenever I need to read a csv file... the other day I was trying to figure out why my stupidly trivial test was taking 2 seconds to run... turns out it was importing my whole library which itself was importing numpy, scipy.interpolate, scipy.signal, scipy.integrate, pandas, and matplotlib... which combined takes a few seconds to import.  If I were more modular and made careful use of builtins like math and csv, I could avoid the bloat that comes with overuse of powerful modules.  So far the time it would take to refactor out the powerful modules is outweighed by their convenience and utility - which is similar to the ""why python when x is faster"" question.  tldr - I think among numpy's top 3 use cases is ""familiar element-wise math operations library"" which has broad applicability and could be useful for something like Numscrypt here. "
Virtual Intelligence for what we have now  AI for General AI,"Like other people have said, fake would imply that it's not real.  Limited though they may be, certain builds of Watson are better conversationalists than some people, not to mention can make insights very unusual for humans but far above just number crunching.  IMO Virtual Intelligence would be perfect, it's specific (AI could also mean something physical and possibly even biological), highlights the fact that these systems are fundamentally different from our own minds, and most importantly has not been co-opted by media to mean a general purpose AI more like us than alien.  It also seems like a better fit for the ""Intelligent Agents"" that we could be ordering pizza from next week...that name suggests the awesome OS from ""Her"" and not the glorified chat bots they are.  Definitely on board with your main point though, a near century of hype has spoiled the term AI.  TL;DR: Virtual Intelligence for what we have now, AI for General AI "
Everyone thinks they know better and their boss sucks. It's actually rarely the case.,"> No, we can't write good software because management doesn't give us the time, autonomy, or environment to write good software in and would rather pay us more to put out fires later than give us the chance to get things right in the first place.  Axiom: the ""time to get things right"" approaches infinity the more time you spend working on the solution you supposedly can ""get right"".  Hypothesis: if inept management was the only thing stopping the software development culture from producing excellent results, rogue developers quitting their job to open a company would be flourishing and being naturally selected in an economy where ""getting things right"" (if it's possible at all) is supposedly the most important factor for survival.  Corollary: if the above hypothesis is incorrect and ""getting things right"" (if it's possible at all) is not the most important factor, then it's unwarranted to ask for time to ""get things right"" as it wouldn't lead to better survival chances for the business, and so your job as a developer would be rather short lived.  Food for thought?  TL;DR  Everyone thinks they know better and their boss sucks. It's actually rarely the case. "
Moore's Law ending might be the best thing that has happened to computing since the Internet coming to everyone's home.,"Remember that Moore's Law != processing power.  There is a correlation, but it's important to keep them separate.  It is incredibly likely that cpu architecture, instruction sets, and compiler design have been solely focused on keeping up with Moore's Law.  By reaching the physical limits of our materials, we can now focus very smart people on these other problem spaces.  And I'm talking both optimizing for existing hardware as well as radical departure.  Perhaps trinary is worth exploring again (not likely).  But what about abandoning von Nuemann architecture?  The new wave of functional programming could be pushed down the stack to reactive cpu architectures.  Or we can put more components directly on the CPU itself as I/O speeds catch up.  And then we can return to separated components for some yet unbeknownst hardware optimization.  And then of course, what about our incredibly brittle software.  As /u/FeepingCreature points out the human brain is able to do many types of computing much, much better than computers.  This is as much due to specific decisions about the determinism of processing as it is about the incredible power of the brain.  What if we start from scratch knowing that many functions will return an answer if the call stack becomes too deep or a runtime exception occurs?  I don't care how much faster processors get, all software will become blazingly fast overnight.  They may not work as perfectly as they do now, but remember that people don't work on precision, and most apps are intended to improve people's productivity, so most apps do not need to work on the level of precision that von Nuemann architecture provides.  So what if the mouse is actually 1mm to the left of where I clicked?  That's less error than many websites that use custom buttons which aren't clickable on the entire box.  Computers were never designed for the people that use them, there are countless optimizations that have nothing to do with transistor density.  TL;dr Moore's Law ending might be the best thing that has happened to computing since the Internet coming to everyone's home. "
better to learn why you should do stuff  instead of interface all the things.,"I agree interface overuse can be an issue, but more often the opposite is the problem - little or no interfaces.  Best thing to do is to understand what interfaces can give you: uncoupling, nicer semantics​, polymorphism. Then keep your interfaces light (interface segregation) and depend on them over concretes where possible (dependency inversion).  As for avoiding interfaces. Things like structs or classes that only have getters and setters, I feel like interfaces can do more harm than good there. And internal/private components of a module or package, implementing interfaces is less crucial, but consuming them is still a good idea.  TLDR; better to learn why you should do stuff, instead of interface all the things. "
The fact that an ultimately untrustworthy person was trusted means that  someone else  will have the same chance to do the same thing.,"You vastly over estimate the ability of organisations to identify ""good actors"". While a similar attack is unlikely, the very idea that an organisation could chose be comprised of employees/actors/students/etc. that are exclusively ""good"" is impossible.  They will try, but inevitably they will fail.  And I hardly think that the rogue programmer applied for the job wearing jackboots and then plastered the room with Nazi propaganda posters while annoying everyone with a podcast extolling racial-privilege.  And even if the evidence is not subtler, most people will be too afraid to challenge the belief that a person is untrustworthy.  For example:  >My father has throughout my life used his profession and status to get the police (and other institutions) to act against my instructions (and the explicit letter of the law).  The presumption most people have is that because he is my father and ""important"" he would only be acting in my best interests.  Witnesses and physical recordings are/were consistently ignored and destroyed as evidence of an isolated ""out of character"" incident.  And worse, when I try to rebuff his abuse because of ""elder abuse"" and other  poorly implemented  overriding moral imperatives ""for [my] protection""- the police alternately get in my face or argue with each other instead of ejecting the blatantly gloating ass-hole they forced me to allow into my home/workplace away.  Imperatives that by design place trust on ""elders"" and  treaty privileged class  as a collective entity/concept; And do not allow discrimination trust on an individual basis, lest prejudice and other abuses occur.  Conversely;  >  [redacted]   The plethora of people who {lie,cheat, and steal because} they get away with it comes to mind.  Many people who challenge established assumptions are ignored/punished because that would require an authority rejecting a previous decision, and diminishing that would diminish that pool of trust they rely on as an authority.   TLDR; The fact that an ultimately untrustworthy person was trusted means that  someone else  will have the same chance to do the same thing. "
You take the lead in technical choices  let him do support and business side.   ps  Reminds me of  this  ,"Start from SQL. First check if it's relational (yes, all databases Can be relational, but it happens frequently this isn't imposed). Are there foreign keys present? You can check in the information_schema.'s (and find tons of other meta data there). If no foreign keys are present, pray they have used consistent field naming across tables.  Also check if there are usefull dt stamps in the records (dt created, dt changed).  Then start running these funny queries:   Check number of records in every table. this will greatly reduce the number of tables to check. Many will be tiny or empty.   if dt's are available, use this to determine activity rate of tables.   For the large tables, check the datatypes, and compare with count distinct per column.    At this point you should have an overview of the crucial contents of the database.  > he still doesn't know anything about SQL or databases. I was expecting to learn and evolve in my first job  Great, so he can do all the nasty presentations and documentations, and meetings you don't want to attend. If he's clueless he's also at your mercy. If he's unwilling to be reasonable expose him. I suspect he will be more than willing to play ball, as long as you show a tentative respect for his position.  TL;DR You take the lead in technical choices, let him do support and business side.  ps: Reminds me of [this]( "
JavaScript is fine  although this is a very biased sub. Just pick something and focus on learning some universal programming concepts and practices. Syntax is the easy part.,"Don't sweat the language too much.  When you grasp a programming paradigm, you can generally apply it in multiple languages as long as you've got the syntax documentation at hand.  Package managers, build tools and such have their own quirks for each, but when you know what you want to do, it's pretty straightforward to find the answers.  Javascript, Java, Python, C# and such aren't going anywhere and allow learning a variety of programming methods. At least the former also have a whole bunch of different flavours to choose from.  Naturally you'll get most comfortable with the syntax you write the most, but it can be beneficial to sometimes look at other languages and learn from them too. For example, if you mostly code in JavaScript, writing a simple program in Haskell or Clojure can open your eyes to functional programming practices and allow you to apply some of the really good stuff in JS as well.  TL;DR: JavaScript is fine, although this is a very biased sub. Just pick something and focus on learning some universal programming concepts and practices. Syntax is the easy part. "
Just try something  put it out there  and you will learn very fast as you go along. A computing background is very helpful.,"I'm not sure what you mean by taking object relationships and DOM into account. Js revolves around the DOM and you should already know how your methods interact when you make them.  There are lots of patterns out there you can use for larger projects. There's the mediator pattern, where you essentially have a control tower that all of your objects talk through, but this isn't worth it for smaller projects. Redux is popular now, isn't tied to React like many people think and scales well.  If you want to build a proper web app, with a back end and all, start by rolling a node backend, a Mongo db and whatever front end framework/library you want on the client. Stick with MVC until you know what you want, there is no need to add unnecessary complexity while you're learning.  A CS course would probably help too. An academic computing background  really  helps when it comes to these sorts of things and will make you a more complete developer, rather than one who just knows a few patterns. You should bear in mind runtime complexity (big O notation), which is essentially how many comparisons your functions/blocks will perform based on a given input. This becomes pretty essential when dealing with larger data sets.  Learning about computer networks is also very helpful, you will learn when it's better/possible to use a socket over ajax, serverside rendering vs clientside, app security. Basic HTTP knowledge will take you a long way with something like Express and is all you really need to know for most ajax programming.  State management is very important in large apps. Be considerate of what your app will be doing at any given time, in a large app you will need to be conscious of things like the event loop and call stack, it's easy to lock it up because javascript is single-threaded. Asynchronous programming is your bread and butter here.  TL;DR: Just try something, put it out there, and you will learn very fast as you go along. A computing background is very helpful. "
You can proof anything if you exclude any evidence that might run counter to your opinion.,"> The only two Office versions within the 5 year window are 2013 and 2016  The version you bought 5 years ago was 2010 which had half these, and five years before that it was again half. If you go exclusively by 2013 and 2016 its a three year increment and if you look at the requirements of the early versions you will see a similar stand still between versions, which might be caused by the rounding Microsoft mentions on the bottom of the requirements list.  > They also both require 2GB of ram (for the 64-bit versions).  Overly selective much? The 32 bit version apparently increased requirements quite a bit, but who cares about 32 bit any more right? its not like pointer bloat is a thing?  Note that the move from compiling 32 bit to 64 bit is a form of bloat. Software that could run fine as 32 bit requires twice as much memory to store pointer values and depending on compiler/language twice as much memory to store the default integer type. Hey on die memory is cheap so lets fill it with half empty pointers.  > And the vast majority of software written is custom stuff for specific purpose, not COTS software.  So nothing verifiable, comparable or relevant to 90% of end users?  TL;DR: You can proof anything if you exclude any evidence that might run counter to your opinion. "
It's a generalisation of writing to a pointer then incrementing it.,">Neither have positive connotations (well, maybe viral when used in a marketing context). 'viral' is heavily overloaded whereas 'infectious' communicates what we're talking about fairly clearly.  Fair enough. I see them as mostly equivalent but I can totally understand where you're coming from here.  >What does that do? Is first an address in memory that is being written to?  Ah it comes from C++. In C++, iterators are a generalisation of pointers. When in C,  first  would be a pointer into an array. In this code,  first  could be a pointer into an array or it could more generally be an iterator of another type into a range of another type. e.g. it could be a  std::vector&lt;int&gt;::reverse_iterator , where incrementing it is actually decrementing an internal  std::vector&lt;int&gt;::iterator .  In C++, a range is an iterator + a sentinel, where a sentinel is something that can be compared to an iterator to see if you're at the end of the range, but which can't be dereferenced. The sentinel for the C-style string  struct string { char *s; size_t len } s  is  s.s + s.len  i.e. the past-the-end pointer for the string, which also happens to be an valid iterator, but one you can't dereference (as it points  past  the end).  A range in C++ is something on which you can call  begin()  to get an iterator pointing at the first element of the range, and  end()  to get a sentinel.  TL;DR: It's a generalisation of writing to a pointer then incrementing it. "
You make a great point.  Have an upvote good sir ,"> Context is everything.  I agree one-hundred percent.  And even in programming I feel this is true.  For example, these days I use mostly Lua and C in my professional work.  A common complaint I've always heard about Lua is that table indices begin at 1 instead of 0, like they do in C.  But here is an example of context like you mentioned.  In the context of C it makes sense for array indices to begin at zero because the index represents an offset from a location in memory; the first element is at the beginning of the array in memory and thus requires no offset.  Meanwhile, ""arrays"" in Lua (i.e. tables), are not necessarily represented by a continuous chunk of memory.  In that context it makes more sense for the first element to be at the index of 1 because the indices do not reflect offsets in memory.  TL;DR You make a great point.  Have an upvote good sir! "
Thinking about anything you do in life as  tests  that can be failed is inaccurate.,"You're not failing on it if you are still improving it.  Failing is not even a good word in development, because typically things are ""abandoned due to priority changes"" and not ""failed"" like you would fail a test.  Educational tests are a thing that does not exist in nature, as in nature all you get are results, and the details of those results are what they are regardless of whether someone considers them correct or not, they are ""correct"" because they exist.  In software this is the same way, whatever the state of your code, data and implementation configurations, your project is ""at that state"".  It can only ""fail"" if you consider ""doing X with my project"", where X may be considering it ""done, being able to use it yourself for something you wanted to do, putting it up on github, or selling it as a software or service.  Only if you consider it a test can you ""fail"" it.  In every other circumstance, you merely deprioritize working on it over other things you have prioritized as higher.  This may mean you will never work on it again, because it's priority will never rise high enough, but that is not failure.  It's just spending your time on the things you prioritize as the most important to you now.  Which is how everyone lives, and is not a failure.  tl;dr: Thinking about anything you do in life as ""tests"" that can be failed is inaccurate. "
I think whatever usurps Java is probably going to be a JVM language.,"Maybe. But while Java the  language  might go away, I don't see Java the  ecosystem  going away. The best thing to compete with Java is newer JVM languages. Within the niche that Java addresses (which is a big niche), whenever something new crops up to give it a run for its money, that new concept makes it into an existing or new JVM language sooner rather than later. There's stuff JVM languages don't and never will do, such as low-level, native, non-managed code (places where C/C++ thrive and Rust is gaining popularity), but as computers become more powerful and tools become better, those problem spaces seem to continue to shrink (because it becomes more feasible to address more problems with higher-level code, and because better platforms that are done in those languages come out that allow you to build your own stuff in something higher-level and run on top of it).  The strongest contender is probably .NET now that it's open source, but it has a long way to go still, not in terms of features or functionality, maybe not even in terms of tooling (though from what I see Java's is still at least somewhat better), but in terms of cross platform support. As much as I love them, I don't see a language like Haskell making too large a dent in the JVM ecosystem primarily because by the time it gains a mature enough ecosystem of its own, enough of its best features will be available on the JVM (a lot already are via Scala), that it won't be attractive enough to switch to.  TL;DR  I think whatever usurps Java is probably going to be a JVM language. "
kinda like WinRAR  justs nags you but keeps working.,"I can answer you by own experience. I had a 365 subscription for some months, and while it was nice to have the complete (complete!) Office suite for some bucks a month, I felt I didn't really need it. So I cancelled the subscription but kept using it just to see what happened.  It just kept working, but had a warning everytime I opened an Office program saying that my subscription was expired, and that some features would be disabled. But I couldn't really see what features were disabled because everything just kept working. Maybe just the really advanced stuff (which I don't use) was disabled.  I kept using it for almost an year that way, until my HDD went full so I uninstalled it for the space.  TL;DR kinda like WinRAR, justs nags you but keeps working. "
The spec doesn't mention quality  speed and uniformity is where it's at 99  of the time  Crypto API's are there for the other 1 ,">Returns a number value with positive sign, greater than or equal to 0 but less than 1, chosen randomly or pseudorandomly with approximately uniform distribution over that range, using an implementation-dependent algorithm or strategy. This function takes no arguments.  Is what ECMA Specs had about Math.random() from the beginning. The spec is actually very specific in only the aspects that random() should return a uniform distribution of numbers where each number n is in 1 > n >= 0. How is left up to programmers and there is no garuntee of quality of the number generated.  With this is mind having the fastest and uniform implementation would be best. I would agree that in server-side applications (where security measures may actually be useful) overriding the default implementation may be of some interest but the better practice would still be to cleanse any critical code of using Math.random() in the first place.  TL;DR: The spec doesn't mention quality; speed and uniformity is where it's at 99% of the time; Crypto API's are there for the other 1% "
I enjoy my job quite a lot and try not to think about the implications.,"Good question. I try not to think about it. Sometimes it worries me, who we are selling the exploits to and what they are going to do with it.  I do have some idea of who we are selling to, but as far as I know, we always let the owner of the software know the kind of vulnerability we've found (including often, what could be done with it), and what we're being offered for the vulnerability and we allow them to make a counter offer. As far as I know, the software owner never outbid our clients.  My firm is a highly specialized boutique security firm with a few dozen employees and a handful of clients. There are much bigger firms doing the same thing. We focus on finding thorough exploits, not small stuff, like an array out of bounds exception here or a mismatched permission there. Those small errors you can sell to the software owner for $3k-$20k. These error don't generally lead to security breaches unless some other conditions are met and a few other errors are also found and exploited together. We find and sell such exploits where we can achieve arbitrary code execution by exploiting many such smaller bugs in succession.  Clarification: We also do other types of work, where our client will hire us on contract and have us work on a specific exploit in a very specific software, usually some obscure firmware because everybody is working on iOS and Chrome and whatnot and nobody will work on the obscure software unless you pay them to. We've also had contracts in the past  for companies who need their software to be exploit-free due to the nature of their work and they ask us to find exploits in their own software.  tl;dr I enjoy my job quite a lot and try not to think about the implications. "
every situation is different  but if you can  remember everyone is a human   try to treat them with respect dignity.,"I generally agree w/ you there too.  I suppose an important consideration is not every situation is black and white either.  In most cases, when you can, being helpful and understanding is likely the best way to get your cake and eat it too (which is to say, get the job done, while also having everyone on the same page / working cohesively).  There are also certainly situations (although I suppose this depends on what field you're in), where you have less choice.  I have been in incredibly tense situations where 'hundreds of millions of dollars' were on the line (debatable, but that's how management saw it) in the automotive industry - where deadlines can mean production delays, and serious bugs are worse (recalls are a best case scenario, human deaths are your potential worst case depending on what kind of component you're working on).  I can imagine similar 'high risk' scenarios in finance, medical, industrial, etc.  In critical situations like that, you have one extreme, and in more typical fields like business application / scientific / R&D / web dev - I think you have another extreme with a lot more freedom with how you approach things (due to potentially lower risks), and indeed you should (to make the most of your whole team, not just yourself).  And I'm sure there's a dozen middle grounds in-between my extreme examples that have different applicable approaches as well.  tl;dr - every situation is different, but if you can, remember everyone is a human - try to treat them with respect/dignity. "
BA in Econ   not STEM  BS MA MS   meh  maybe STEM  PhD   ermagerd  like  totes STEM.,"Technically, Finance is a sub-discipline within Economics, so I would include that as well.  For a bachelor's degree, it  definitely  depends on the school. Econ departments can vary a bit in their ""personalities."" Some are theory heavy, some are math heavy. It can also make a difference depending on how different the requirements are for a BA vs. a BS. At my alma mater, the Econ BS degree required an econometrics class, but the BA did not. Looking back on it 20 years later, I don't really see the point in an undergrad in economics  without  econometrics.  However, at the PhD level, nearly all Econ departments are math-heavy, and it's pretty rare to find a school offering a solo masters degree. The only people I've ever met with just a masters in economics were people who dropped out of PhD programs. Ironically, the best prep for an economics or finance doctoral program is not a BA/BA in economics, but a BS in math.  TLDR >> BA in Econ = not STEM; BS/MA/MS = meh, maybe STEM; PhD = ermagerd, like, totes STEM. "
before changing ownership permissions  first understand why they are the way they are and only change the files directories if you absolutely need to do so.,">So everything in the directory var/www        is off limits. I can't copy or past too. there is even a sub directory with var/www/html/   I can't write to any of it. How do I solve this.  Can you explain why it's off limits? Is that something a sysadmin did and you have no control over? Or is it merely because the owner/group is somebody other than the user you're logged in under?  There are a few possible ways to answer your question based in the situation. If it's because of a sysadmin, you will need to speak with them about installing your software so you may use it. If it's because of permissions, then you can either change them as another user suggested, or you can install the software as the user who owns /var/www.  Keep in mind that if the permissions were intentional, there probably was a good reason for that. So exercise caution before you blindly use chmod (permissions) and/or chown (ownership). I've seen developers destroy an entire server by chown'ing /usr recursively because they didn't understand the why some files were not writable - they switched between root and a normal user and some files were owned by root and not writable by normal users.  TL;DR before changing ownership/permissions, first understand why they are the way they are and only change the files/directories if you absolutely need to do so. "
Take the stats minor   you'll need it more than the CS knowledge  which you can easily make up with tutorials and online classes.,"As a CS major, I'd say go with stats. Data Science generally won't require the more complex data structures, algorithms, etc that a CS minor will focus on. You won't even really need to know good software engineering practices, since the data science code I've seen is generally in script format.  As others mentioned, your stats minor would most likely cover most of the programming skills you'll need, and there's always Stack Overflow, free/cheap online classes, and tutorials to make up the rest of the knowledge.  Of course, there are always caveats to this. Certain aspects of CS knowledge would be very useful for things like concurrency or distributed computing when dealing with large datasets, or making your own library/framework. Most people generally wouldn't do the second, however, and I believe there are libraries that handle the first aspect.  TL/DR: Take the stats minor - you'll need it more than the CS knowledge, which you can easily make up with tutorials and online classes. "
Never use  std  endl   it's harmful and longer to type  it has no saving grace.,"I am not sure about the quality of the overall site, but it certainly does not start well.  [A first look at cout, cin, and endl](  >  std::endl  > If we want to print things to more than one line, we can do that by using  std::endl . When used with  std::cout ,  std::endl  inserts a newline character (causing the cursor to go to the start of the next line)  Inserting a new line can be done by pushing the  '\n'  character, either on its own or as part of a string  ""Hello World!\n"" .  What  std::endl  does it  two-fold :   it appends a new line,  it  flushes  the underlying stream (as if streaming [ std::flush ](   Because flushing to stdout is so slow on most terminals, use of  std::endl  is one of the primary cause of slow C++ program among beginners (alongside compiling in debug mode).  TL;DR : Never use  std::endl ; it's harmful and longer to type, it has no saving grace. "
not my cup of tea  but don't have a problem with a choice being made.,"Chances are you're going to need something - given Laravel's game is to ""simplify things"", and given the absolutely mind bending array of choices in the front end framework space, it was ripe for simplification by opinionated selection.  Now, there are arguments to be made as to whether it's the best choice... (and for the kind of work I do, it's not.. but the kind of work I do is probably not representative of Laravel users in the main), but there isn't a problem with the choice being made in general I don't think.  Personally I'd like it if there was a clean way to opt out of it at project creation time (along with a myriad of other things I simply don't use), but that's a minor quibble.  tl;dr, not my cup of tea, but don't have a problem with a choice being made. "
Promises aren't going anywhere  and are fully backwards compatible with callback style code if desired. There's no reason not to use them.,"Promises are almost always the correct abstraction to use for a computation that may return a value later. They are effectively just a continuation monad with some extra tweaks. They aren't going to be replaced any time soon.  Rx Streams are an abstraction to represent zero or more values changing over time. They are different things, and while there's similarities they both have well defined and separate use cases, and aren't really something you'd chop and change between.  If you want to use a function in a callback style, then you can still do this with minimal fuss with promises.  Tl;Dr Promises aren't going anywhere, and are fully backwards compatible with callback style code if desired. There's no reason not to use them. "
Take some math  take some comp sci  and don't stress too much.,"Data Scientist of 2 years here - of course, my experience may or may not be atypical of data scientists.  The data science curriculum looks great to me. I kind of wish that I had had a program with as much cool applied math combined with computer science classes. A comp sci major is nice, but I think it de-emphasizes the mathematical component and that seems like the part that is the most difficult to pick up on your own. I would suggest emphasizing math in undergrad and picking up the coding to support it. I'm biased though - I was a math major.  > However, I am not entirely sure if it is the career path for me since I've been told that I would need to go to graduate school in order to be competitive candidate for a lot of jobs.  I would also like to emphasize that I only got a BA in math. I also got a BA in Economnics and a minor in Russian, and after that, I got a Master's in Nonprofit Leadership. It's perfectly normal to not be sure what you want to do, but as long as you do something math-y and something computer-y, I think you'll have the background to do pretty much whatever you want to do. Data science is a broad discipline that rewards breadth as well as depth.  TL;DR - Take some math, take some comp sci, and don't stress too much. "
Stop being condescending and answer the damn question if you wanna help.,"I really like Ned and have interacted with him on #python a few times -- really nice and very smart fellow. But I heavily disagree with him when it comes to this particular topic.  I ranted about it a few weeks ago in /r/programming in fact. I'll quote myself:  > Y'know, even on many prominent IRC channels, I've found lots of developers with this attitude. Holier than thou pedants who want to know everything and tell you that you ""are doing it wrong"" without actually getting into the problem.The unspoken assumption everyone in there makes is that everyone is stupid until proven otherwise. Sure, it sometimes leads to good answers or explore avenues I hadn't considered before, but 80% of the time, it's a fucking waste of time. It's the reason I've stopped interacting with most programmer communities for any ""real"" problem I have.  And  > This ""XY problem"" nonsense might make you feel smart but it isn't helping the guy asking the question. Most problems aren't that complicated, and neither do most answers have to be. If the answer leads to another problem, they'll ask the next question. Give them the benefit of the doubt. If the cycle continues for say 3 or 4 questions - only then is it fine to ask if they are wanting to a particular thing.Tl;dr: Stop being condescending and answer the damn question if you wanna help. "
comparing the capabilities of desktop layout to CSS layout is apples and oranges  but people really want to get orange juice out of their apples.,"> Except scalable to arbitrary screen dimensions, orientations, > resolutions, and input directions.  Java Swing had all of these in the 90s using the notorious GridBagLayout. Which I'm sure inherited design principles from the various Unix toolkits of the day. Far better than CSS until only a few years ago.  Windows and Apple generally had more fixed layouts based on RAD UI layout tools. People complain about CSS (and Java and Unix) not having these RAD tools.  > So way beyond desktop UI in the 90s, really.  When people say that CSS has ""caught up"" to desktop apps in the 90s, they mean with regards to  desktop style layout , which is the goal of recreating desktop application UIs using HTML/CSS.  The design goal of CSS was not to recreate desktop UI layout, but to augment HTML with style information to get configurable presentation that adapts to the capabilities and preferences of any user agent (desktop, mobile, text-only, robots). These are requirements for  the web , not desktop applications.  Creating desktop style layouts have typically been a pain in HTML/CSS because that's not what it was designed for. Additions to CSS like  flex  and  grid  give developers the layout tools they are accustomed to from the desktop world (while potentially breaking CSS layouts for the use cases it  was  designed for).  TL;DR - comparing the capabilities of desktop layout to CSS layout is apples and oranges, but people really want to get orange juice out of their apples. "
The tags are more integrated into the language. The language is also designed around features that's easy for the optimiser.,"Thanks for the feedback. I agree with you that the landing page doesn't really explain much at all yet.  The rendering works in a somewhat different way from regular virtual DOM implementations. I will write up a blogpost that goes into more detail about this. Instead of creating a virtual tree on every render, and then going through the dom to reconcile the changes, tags are efficiently cached and reused. Explanation by judofyr in an earlier thread on hacker news:  There's two parts of the ""very efficient rendering"":   It store the previous rendered tag and does a diff. (Just like React).   Every tag is compiled to JavaScript that's easy for the optimiser.  tag event &lt; div    def render        &lt;self&gt; &lt;h1 bar=""123"" baz=bar&gt; ""Hello world!""  Compiles to something along the lines of  tag$.defineTag('event', 'div', function(tag){    tag.prototype.render = function (){        return this.setChildren(            (this.$a = this.$a || tag$.$h1().setBar(""123""))                .setBaz(this.bar())                .setText(""Hello world!"")                .end()        ,2).synced();    };});  While React passes an object of props around, Imba will instead call setters. These setters are very easily inlined by modern browsers. Also note that it caches directly on the tag object itself which gives another additional boost. Since it caches the objects themselves it can also compare everything with === and/or indexOf.The  Imba.static -call marks an array as static which is a guarantee that the structure of the array won't change. In this case Imba will not do the full diff-algorithm, but instead compare each item element-by-element.And yes, once you've achieved the minimal amount of DOM changes needed (like React), the next step for performance is all of these little tweaks. Does it matter? It all depends. Imba is capable of rendering bigger applications than React when you use the ""render everything over again in the animation loop"".TLDR: The tags are more integrated into the language. The language is also designed around features that's easy for the optimiser. "
your suggestions end up becoming larger time wastes than a code challenge. And other alternative don't always give you the read you think your getting.,"The reason this dreaded ""coding test"" exists is to save time. A company can hand out the test and step away from the candidate and if the candidate fails to complete  no valuable time was lost ; however, if a candidate is brought in, knows nothing about the language and fails to learn from large code bases all the time spent pairing with another senior engineer is  wasted .  I also like the concept of a coding test to weed out attitudes. If you do not have enough humility to do a code challenge then your attitude probably doesn't work well with the team. If you think your time is too valuable to take part in the interview process for my company then I'll find a candidate more interested. The quality of engineers on the team is higher when the company includes a coding portion of their interview. I've worked a company that looked at ""code samples"" and the quality of engineers was much lower.  On that last point, the engineer who felt that coding tests were offensive and evil things promoted code samples and pointed to his ""genius"" code sample of some date manipulation in some XML language. It came out later in unrelated conversations that was the result of a team effort but he uses it to show off ""his"" coding abilities. So frankly I don't care what you  might have  done on your own, I want to see what you  can  do on your own.  tl;dr your suggestions end up becoming larger time wastes than a code challenge. And other alternative don't always give you the read you think your getting. "
Your emphasis on the  possibility  to emulate  break  in a  break  less setting is misleading.  Both are turing complete.  But one is much harder to reason about.,"It's all turing complete - you can emulate anything you want.  That doesn't make it easy.  All those steps you describe: they're not free.  In particular the fact that before introducing break you could reason about pre- and post-conditions without regards to context and afterwards you cannot (you need this virtual flag ""keep_going"") makes things non-composable (unless you use try...finally) and that makes things harder to reason about.  Of course, you can restore composibility if you restrict your composition operators to those that respect  keep_going  - but unsurprisingly, they're going to get a lot more tricky. You lost the conventional sequence operator  ;  - it's replaced by something that checks  keep_going , and in particular one where pre and post-conditions are affected by this  keep_going  flag. Note that in your keep_going model it's quite plausible that those implicit checks for  keep_going  outnumber all other branches in your code, and every single one of them has two outcomes both of which need to be reasoned about from the perspective of your invariant.  Your proof probably just increased in size and complexity by  at least  a factor two.  In human terms: this is much harder.  And to emphsize once again: this is about humans, not automated proof systems. Your  keep_going  flag sounds like lots of extra work  for humans .  TL;DR: Your emphasis on the  possibility  to emulate  break  in a  break -less setting is misleading.  Both are turing complete.  But one is much harder to reason about. "
the course is great for math engineering types  and really difficult for those of us without that background.   EDIT  corrected spelling,"I took this last time around and dropped out in the 4th week. The course is geared towards the math/engineer type and I found it really challenging to come to it as a musician. The assignments felt arcane and disjointed from any material with which I'd previously been familiar, so I was basically shooting in the dark whether or not my solutions worked, as I was expected to translate an arcane array of numbers to another arcane array of numbers.  I bring this up only to hone your expectation. If the Fourier Transform is sensible to you, you may do well in the course; if the Fourier Transform is a bit beyond your experience level, you'll have to commit a  lot  of time and energy to keep up (which I just couldn't).  TL;DR - the course is great for math/engineering types, and really difficult for those of us without that background.  EDIT: corrected spelling "
Stay motivated  invest in yourself  knowledge   don't be bashful and keep putting yourself out there.,"This may not mean much coming from a random on Reddit; however, I would definitely hire you. Here's why:  1) You're humble and ask questions. It's a great quality to have as a developer. I can't tell you how many developers I've fired because they thought they were irreplaceable. Ok, also for very reasonable other reasons, but the moral is - stay hungry, stay humble.  2) You have drive and ambition. I'm an alumnus of a top 3 business school and run an analytics shop for a very large, global company. I've seen VERY smart people come and go, but the people that have that inner fire and can execute on their vision and plan ALWAYS come out ahead.  3) You have a reason to work hard. They say that necessity is the mother of invention. I'm the oldest of a large family of children whose parents could not afford  to send me to college, buy me a car or any of the other ""normal"" purchases I saw my friends' parents make for them (not complaining, just elaborating). This fueled my work ethic and I created my own path to achieve my goals. You reaching out like this is the beginning of your path...well, really the preparation you've done prior to this was the beginning. Remember: Luck = preparation + opportunity. You have to prepare (and you are) and you have to, sometimes, create your own opportunities.  Please do not get discouraged. Youth is the best thing you have going for you. I'm a little more than twice your age and I wish I had picked up programming when I was your age.  I have a few pet project ideas I could throw your way if you want to give it a shot. This might help you expand on your skill set while developing a portfolio of work that you can market to potential employers.  What's your experience with scraping websites? Also, a little database knowledge would go a long way. My thoughts, if you can learn Python then you could definitely get your head wrapped around SQL.  Also, look at fiverr. You can do small projects for a few bucks and as you get more work and reviews you can charge more, etc.  Ok, this turned in to a novel. Feel free to ping me with any questions, etc. Best of luck, young friend!  tl;dr Stay motivated, invest in yourself (knowledge), don't be bashful and keep putting yourself out there. "
article is kind of bullshit with small grains of truth.,"It kind of is, the article treats companies like they're either Google/Amazon/Whatever or two guys in their mom's basement trying to roll out a service for 100 people. Now let me just say, I've railed against the same thing in the companies I've worked for before, such as ""we already have hadoop/spark up and running, I'll just run this dataset through that because I know how to do it"" instead of using bash or awk which would have been faster and easier but with a small learning curve. But that's not what the guy is saying, he's trying to paint this picture that there are no smaller companies with problems that can be solved using distribution. That's a complete bullshit argument, I've worked at small companies with petabytes of data.  TL;DR article is kind of bullshit with small grains of truth. "
I am taking what you are saying with a grain of salt and trying to think of how I can value your input.,"There is no need for creating account just to bash an open source program.  I am taking what you are saying and trying to think of any way I can constructively take this to make my program better.  I get that I am not a professional python programmer, the program still does what it is meant to do.  On Fabric's website it states Fabric is a python library which users Paramiko. NetScript Assist is a GUI that uses Paramiko.  As far as learning SSH, here is what my code does:  ssh.connect(nodeip, username=username, password=password)  ssh.set_missing_host_key_policy(paramiko.AutoAddPolicy())  chan = ssh.invoke_shell()  On my 'website': www.netscriptassist.com, I do not have users enter any password. There is simply a download link and other pages to help users. The only place where they would enter a password gets called in the SSH connection function. I even made it a point to have the logs use the variable name instead of the variable content. This was done so that if the users use the variables in the scripts they won't see the variable content in the logs. Additionally, I have made the program open source so users can see what I use the password field and any other variable for in my code.  Ansible is extremely powerful and I think that it has a steep learning curve. Once you get into Ansible, it can do the same thing as this program and go way beyond what my program is trying to do. This program is not meant to do what ansible does.  TL;DR : I am taking what you are saying with a grain of salt and trying to think of how I can value your input. "
I'm learning Flask. Do you know of any good  beginner  up to date Flask tutorial using Python 3 ,"I'm creating a digital sign showing local train schedules. Getting the data was relatively easy - there was an API for that. My current challenge is finding a way to display it on a TV screen using a Raspberry pi.  I took this as a chance to learn how to create a website whichconstantly updates the info I want (train schedule, temperature, time of the day, etc).  As a result, I'm learning this week...   Flask (it seemed simpler to begin with vs Django) along with Apache and WSGI  Git and Amazon Web Services (yes, I'm new to it)   Question:Most of the Flask content I found for beginners either uses Python 2 (O'Reilly books), isn't up-to-date (tutorials on the web) or is more complex than expected (Flask tutorials). Does any one have food recommendations here?  TL;DR: I'm learning Flask. Do you know of any good, beginner, up-to-date Flask tutorial using Python 3? "
not all language constructs are related to solving run time issues.  Much of it is to communicate with other programmers.,"This question he keeps asking:  > Whose job is it to <do some thing>. The language? Or the programmer?  The programmer's obviously.  But by following his intent it's like saying that because it's the carpenter's job to make sure boards stick together, he should be using his hand to pound nails instead of a hammer.  Programming languages are  tools .  If you want minimal tools, you're free to write all of your software in assembly.  Then you don't even need to worry about shared structure of classes, inheritance, etc.; if you know the offset of the memory you want, it's yours!  He's also ignoring that many of the things he considers to be crutches (like   ""open""  in Kotlin) can actually be features for the  programmer  rather than just rules for the compiler.  They actually  give  the programmer information.  Just because a class  can  be extended doesn't mean it should be all the time.  Using an annotation like  open  immediately communicates to other programmers that this class was designed with the intention of being extended.  In C++, if your function isn't making changes to an argument reference, you mark it as  const .  It's not just about letting the compiler make sure the rule is respected, it's also about communicating to other programmers that they can call that function without concern of their objects being modified.  tl;dr: not all language constructs are related to solving run-time issues.  Much of it is to communicate with other programmers. "
if you sell software and make updates to my tool conditional on a retroactive subscription  you lose my business.,"I bought licenses year after year from them. I recommended their products.  They lost my loyalty because of retroactive subscriptions.  Bought a pyCharm upgrade to the next version and found out after the fact that I had four weeks left of tool updates. Next they pulled the same stunt on Resharper. No longer version based and with retroactive subscriptions. I was so delighted to receive that sales email with warnings about not renewing right away.  Def: retroactive subscription means that if you let your subscription lapse even for a bit of time, if you ever want to use it again you have to pay for all the time you weren't licensed. Or presumably buy the full version again and fuck you for buying the last 6 versions of the tool.  TLDR: if you sell software and make updates to my tool conditional on a retroactive subscription, you lose my business. "
it is one of the most important concepts in software engineering and the best way of organizing software functionality.,"A user story is a discrete task a user should be able to do with the software. A user story might be ""I want to be able to add a customer into the system."" From starting out understanding what the customer wants from the software, you can then add tasks to this user story in order to make this happen. Tasks might be ""create webpage/create customer table/create add customer query."" User stories usually have ""story points"" that are fibonacci's sequences numbers. So you might assign 1, 2, or 3 points to the easy user stories, 5 to moderate user stories and 8 or 13 to complex user stories. Tasks are usually given hours of estimation.  tl;dr it is one of the most important concepts in software engineering and the best way of organizing software functionality. "
I would personally like to know all 3 and then choose the one which is right for the work.,"Having recently spent a significant time evaluating Vue,  React and Angular.  I have realized all frameworks are very close and have their benefits.  I ended up concluding:   For small to medium complexity and sized webapps I would prefer Vue over both angular and React.  It has a small leaving curve and is pretty fast in the cycle of Prototyping to real app.  Performance and concepts were enough for me.   Anything from medium complexity to high,  would go with either react or angular.  If working on a multi module app with lots of teams involved, I would perhaps lean towards Angular.    TLDR - I would personally like to know all 3 and then choose the one which is right for the work. "
If you can't translate your skills into money  no one will give a flying fuck about your AUC Gini.,"This is a hot career right now but I bet in 10 years a lot of it will be automated to the point where we won't be touching code. Our skills are highly technical and demand somewhat of a wage premium so the pressure to automate is there.  Beyond the minimum, the skills you want to develop to be successful in industry are 1) the soft ones and 2) the ability to conduct a research problem. Can you explain to the senior partner why they should should care about the cool machine vision model you built? Can you work with a team of engineers to get it implemented correctly? Did the training set you used have enough black people in it? No? Well congratulations it's now classifying black people as gorillas and you've just been fired to save face.  Almost no one is going to pay you 500k a year to do actual data science. What you want to do is figure out how to apply those skills in a business context and then lead a team in order to leverage the experience into upper management.  Tl;dr: If you can't translate your skills into money, no one will give a flying fuck about your AUC/Gini. "
This article is old but you should still take care choosing the 'right tool' because distributed computing is hard.,"So I can agree 'too big for excel is not big data' but this article is super old, Spark wasn't even created yet! With PySpark you will feel right home if you know Pandas. With Impala/Hive you can write your SQL queries the same as you would on an RDBMS. So a lot of his arguments are kind of weak since so much has been abstracted away.  While I can agree with the idea of using the 'right' tool I think size is not a good measure sometimes for big data. For example you may be doing complex calculations a take a long time to do but can be done in parallel. This way your analysis now takes minutes instead of hours/days because you have it running it massively parallel.  That said that debugging in Hadoop is a pain in the butt. You are working with cutting edge stuff so it may not be you but the system itself not working as intended. If it is you, you are looking at Python errors that are throwing Scala errors that are throwing JVM errors. Good luck with that.  Tuning is likewise hard to do because you have to think about how and when things get distributed.  TLDR; This article is old but you should still take care choosing the 'right tool' because distributed computing is hard. "
Learn principles  not patterns.  Separation of concerns is awesome  MVC is more of an interesting idea.   ,"The overriding principle should be ""separation of concerns"".  Depending on the type of app you're writing, and how you're approaching it, and what libraries or frameworks you're using, it's possible that all (or most) of your problem could be separated out into ""data"", ""presentation"", and ""business logic"", and you could call the code dealing with those parts ""models"", ""views"", and ""controllers"", and then you kinda have MVC.  Of course MVC is actually a concrete pattern which was developed to refer to desktop apps.  Your ""MVCish"" webapp may look superficially similar on some ways, but given the enormous technical differences between a PHP app and, eg, a classical C++ GUI-based app...the resemblance is going to be quite superficial.  Plus, if you look at a lot of MVCish webapps, you'll find there's a lot if  mixing  of concerns.  And if you collect 10 different MVCish webapps, you'll probably find they break things down in 10 different ways, both of which are signs that MVC is probably not the right abstraction.  > Are there other ""well tested"" patterns that I should learn about?  I would say:   MVC, as applied to PHP webapps, is not really a ""pattern"" so much as it is a fond dream.  And as such it's not tested at all.  On the other hand, there aren't any other patterns which  are  tested either.  MVC, MVVM, ADR, MVP, MV* are all terms used to describe ""let's break this down in ways which kinda seem logical"", and depending on you, your problem, your team, and your tools, some of them may be more or less appropriate, but none rise to the level of ""well tested pattern"" in the way a desktop app programmers mean when they talk about MVC.   TL;DR:  Learn principles, not patterns.  Separation of concerns is awesome; MVC is more of an interesting idea. :) "
you really do have to know what you're doing to have a happy outcome.  You can't build good  secure  performant systems by blindly following  best practice .,"> You still should not dismiss a best practice without good reason.  You should question  everything , nothing is law in this regard (well, except the actual law if you're working on banking/defence/etc. systems!).  There's so many (and contradictory) ""best practices"".  They can't all be right.  Every policy a team might adopt needs a specific provable/demonstrable reason for existing, ""it's best practice"" is just folklore, it's saying something's true because everyone knows it to be true...  > This includes things like encrypting passwords (we might need it) to rolling one's own hashing algorithm (NIH).  Encrypting passwords is a good example of how there's many contradictory ""best practices"".  These days it's BCrypt, so was it wrong to use BCrypt when the ""best practice"" was a hashed password with a unique salt?  Of course not.  The benefit of BCrypt has remained the same regardless of what the hive-mind thinks...  ""Yeah, but it's a good rule for developers who aren't experienced.""  No, it's a good argument to not hire a team exclusively made up of junior developers to do mission-critical components.  If junior developers (but no so junior that they hadn't had ""passwords should not be saved in plain text"" drilled into them) were to venture out on their own, based on that ""best practice"", a did a Google search to find out how, they'd happen upon an article telling them that MD5 and SHA1 are the most commonly used password hashing measures:  ""Not invented here"" is a terrible, terrible argument either for or against a thing.  But that's another story.  TL;DR - you really do have to know what you're doing to have a happy outcome.  You can't build good, secure, performant systems by blindly following ""best practice"". "
the reality is that you can't  sell  anything to programmers without being hands on on the subject you discuss.,"PIP and NPM are two different things. Python has a packaging philosophy and implementation that are built into the language itself with respective modules in its standard library, and further served by an extremely rich ecosystem that dwarfs the comparable capabilities of Node + NPM (I believe that Javascript is intentionally more ""lightweight""). Within that ecosystem PIP covers  some  aspects of deployment, leveraging the setup.py  script  of the project. NPM will never be able to provide these services for Python - not even just PIP's - because NPM is language agnostic and blind to anything the Python language prepares for its toolchain. I'm not quite sure what exactly your friends told you about requirements.txt, but it sounds to me like they either had a very limited Python experience, or you didn't quite understand the context of whatever they said, or maybe they worked with Python very long time ago.  As a side note, even though we are in /r/python, I'd add that the way you discuss make is also IMHO, well, wrong.  TL;DR the reality is that you can't ""sell"" anything to programmers without being hands-on on the subject you discuss. "
You hard coded a language  you were lucky it ever worked. Use the locale name   one the operating system supports,"The problem is that you are:   hard coding languages  not using an available language   See [Language Strings]( for a list of languages.  (Where you can see that Azerbaijani is not actually listed.)  But note:  > For a list of supported languages by operating system version, see [National Language Support (NLS) API Reference](  You should also follow the recommendations in setlocale:  > We recommend the locale name form for performance and for maintainability of locale strings embedded in code or serialized to storage. The locale name strings are less likely to be changed by an operating system update than the language and country/region name form.  TL:Dr: You hard coded a language; you were lucky it ever worked. Use the locale name - one the operating system supports "
the actual concrete things learned in school are a tangential byproduct of schooling.,"Education isn't designed to do anything other than produce people who are capable of learning. The better educated a person is, the more learning can be accomplished independently.  Computer Science degrees don't produce programmers or computer scientists, just like math degrees don't produce mathematicians. All a 4 year degree does is introduce students to a more concentrated study of one subject than is found in k-12 education.  Education is complicated. I think the basic idea is that in k-12 the goal is to teach how to learn a disparate array of subjects. That is a useful skill in life, since life has a way of presenting many different types of challenges. Those who chose to pursue a four year degree, or higher, learn in increasing measures how to learn more and more specific thinks in the same field.  The increase in specificity of learning helps when students enter the workforce in a few ways. At the simplest, most careers have a finite knowledge base that needs to be fully understood for mastery. Experience in learning a field's intricacies also makes it easier to repeat the process in a different job or field. That couples with k-12 introducing so many different types of things to learn about.  What is learned in school is less important than how it is learned. How many PhD's do you expect spend a lifetime studying or working in the limited confines of their dissertation?  tldr; the actual concrete things learned in school are a tangential byproduct of schooling. "
overall no   you've got experience and that should be enough. If you find it isn't  get interview feedback and decide if it's worth the investment from there.,"When I read the title, my first reaction was 1,000% yes. The I saw you said you have 6 years experience and now I'd say almost certainly not.  I'm in the UK so a Masters is the level above the standard undergraduate degree (BSc). No job I've ever seen wants more than an undergraduate degree, though some may stipulate how solid the degree is (2:1, for example). Masters degrees are more or less doorways into a PhD and very much send you down the academic route. In fact you can be viewed as overqualified if you try to get a dev job and have one.  I have a BSc, and I can tell you a few things. First, it was the biggest shower of shit you can imagine. I'm fairly sure I came out of that degree knowing less than I went in with. Secondly, it's the most important shower of shit I've ever had to endure. I view my degree as an invitation to interviews. A door opener if you like. From there it's up to you to stand out over the other candidates, but anyone hiring a recent grad with little to no experience will look to paw low and expect you to learn on the job as a junior. Your experience should really count for a lot more than a degree now, depending on what sort of companies you've worked with.  TL;DR, overall no - you've got experience and that should be enough. If you find it isn't, get interview feedback and decide if it's worth the investment from there. "
teach using effective means  and don't listen to this guy.,"Couldn't disagree more.  First, you use Venn diagrams to explain joins to someone that doesn't understand because most likely they have already seen and understood a Venn diagram.  If I have to first explain this new kind of diagram that I'm going to use to explain this other concept, then you are not succeeding at teaching.""Let me teach you one thing by first teaching you something new and useless""  Second, and this sounds crazy, but you can use Venn diagrams to explain  both  kinds of operations.  Try rotating your UNION diagram 90 degrees. Crazy.  Third, if someone is struggling to understand Join what would you assume they are already masters of set math and Cartesian products?  Again you don't teach by jumping to a more complicated topic and hope the student can see how it applies.  Forth, in the real world I had an engineer that just wasn't  getting it and the Venn diagram explanation using our database bridged the gap.  TLDR; teach using effective means, and don't listen to this guy. "
redux is awesome  just stick with it and only it and you'll enjoy it.,"At my company, we recently started rebuilding parts of our existing Marionette.js and Backbone.js SPA in React, Redux, Immutable.js, redux-promise, and a smattering of other packages.  React is easy - components, props, and JSX. Redux itself is easy, and easy to scale and test. Using a package like [redux-actions]( removes a lot of boilerplate, which is nice. It's also easy to plug Immutable.js into the mix.  What gets complex and  really  hard to manage is duplicate entities  and their relationships  in a redux store. Libraries like [normalizer]( help aggregate and reduce entities in a store by using ID references where appropriate, and [reselect]( using memo-ized functions to read something out of the store efficiently.  But using all this together with a standard (albeit well-documented and internally maintained) REST API gets crazy. Combining and managing two sets of entities in the store where they have a one-to-many relationship raises a lot of questions, simply ""how the hell do we do this"", and has resulted in some seriously messy spaghetti code, in my opinion.  Throw something like [redux-saga]( into the mix like we did recently and it's almost table-flipping time. We're slowly going through and refactoring what we have so that while we might not have a fully normalized store, we can at least read the code and easily do what we need to do with data before and after it gets merged into the store.  My advice? Stick to React, Redux, Immutable.js, and a Promise-based HTTP redux library (like redux-promise), and that's it. Sagas are cool, normalizr is a neat idea, and reselect promises some big efficiency gains, but using all of them together is a recipe for headaches right now. Give these tools more time to mature and I think there'll be some better patterns come out of it.  tl;dr; redux is awesome, just stick with it and only it and you'll enjoy it. "
Vulnerable servers aren't a  risk  they are a present  danger  to those placing trust in the security incompetent.,"This is more akin to seeing a children's hospital left wide open with no caretakers and no security, but full of patients (user data).  You're not scaling the analogy correctly.  The patients in the hospital are more important than the hospital's administration's concerns.  The company has legal and ethical obligations to secure their systems that are more important than their internal policies.  To correctly use the analogy from before (house is Intel) you would be morally obligated to charge in to the house to save those dependents (children, pets?) trapped in the house and reliant on their caretakers if the house was on fire.  TL:DR Vulnerable servers aren't a  risk  they are a present  danger  to those placing trust in the security incompetent. "
memorization is far from being every aspect of programming that is worth evaluating in a college course,"Pardon if I misinterpreted your following comment:  > So basically every aspect of programming is out then?  That seemed like a statement asserting that nearly every aspect of programming involves memorization. This has not been my experience and I studied computer engineering when laptops were a relative rarity and Google was not yet popular.  My school teaches Java... I envy Bekteley for using Python in their first year class. But even though I haven't written any Java for more than a decade since the classroom and couldn't run ""Hello world"" if you put a hub to my head, I've looked at the contemporary exams of my school's Java course and feel no doubt that I could ace them. The exams are necessarily written in a way that the problems, throughout the exam, provide enough example code to provide a reference for the extremely limited syntax knowledge that is asked of the student.  In fact, the hardest parts of the problems involve convoluted logic mixed with abstract/fanciful scenarios...the challenge isn't memorizing syntax, it's being able to see the core concepts through the noise of the specific code. For example, the Berkeley test attempts to test concurrent code concepts (altering a student database) using a nonsensical scenario with black box functions. If a student doesn't know enough syntax (e.g what a function call is) to answer those questions efficiently, then it is reasonable to say they've fallen short in understanding the concepts for reasons that have little to do with poor memorization.  I mentioned a list of programming standouts who achieved greatness before interactive keyboard-to-monitor tech existed, never mind Stack Overflow. They weren't renowned for having particularly exceptional memorization skills, not that there was much syntax for them to memorize. Woz didn't even have deep familiarity with programming and had to read up on computer science books while writing Apple Basic, IIRC.  tl;dr memorization is far from being every aspect of programming that is worth evaluating in a college course "
Don't hardcode table names  inject them. I use singular table names   have field names same as my objects.,"I give my RDBMS fields the same name that I plan to use across the stack (in PHP, in JSON API responses, in client-side JavaScript and so on). Mapping field names back and forth only because of differing naming conventions seems pointless. Plenty of good reasons to transform your data, but technology-specific naming conventions aren't one.  For tables, I use singular name which matches the class name that the data maps to (if any). The table names shouldn't matter because they should be injected from outside your business logic, and hence be easily configurable.  If you don't hardcode table names in your business logic, you don't need to think too much how to call them, as the module developers can use internal names that makes sense to them, and the developer configuring the module can use actual names that make sense to them (and fit the rest of the application, while avoiding table name conflicts).  tl;dr  Don't hardcode table names, inject them. I use singular table names & have field names same as my objects. "
learn computer science  not technology  like JavaScript . If you don't like the science part  do something else.,"So, I'm going to get out my soapbox for a sec. I've been in software development for over 20 years. I have a BS and MS in Computer Science from an Engineering school. I am in no way surprised that this inquiry comes in this particular sub.  The software development industry is flooded with ""self-taught"" people. I've known some really good ones. A formal degree is not necessary to be a good developer, but a formal education is critical, even if you learn it on your own.  I didn't learn languages in school (aside from Pascal, for learning data structures and C for Operating Systems.) I learned information science: data structures, algorithms, graph theory, set math, and all the ""stuff"" behind the languages. So now, when I want to learn a new language, it takes very little time, because I know how they all work.  If you call yourself a software developer and do not (at least) know the terms: Turing Machine, Big O, Binding Time, functional and procedural, you need to hit the books.  Too many ""self-taught"" developers are people who learn things about specific tech, languages, or things like HTML and CSS, but then never learn the underlying engineering principles. So they become irrelevant over time and write bad code when the task is complex or when there is no cut and paste code online.  All of the important books are available to purchase. Or for free online. There are free online classes from great universities that teach all these things.  You can't learn to drive by reading the manual on a specific car.  TLDR; learn computer science, not technology (like JavaScript). If you don't like the science part, do something else. "
Analyze all AJAX calls and determine what is cachable in cookies localStorage sessionStorage redux store etc in order to cut down on future calls in the same session.,"Here's a simple example:  Let's say you're running a SPA in React. You have a couple of components that make calls to a saga inside of componentDidMount().  Maybe one of these components makes a saga call each time: one to getAccountDetails(), one to getRewardsDetails(). In your first implementation, you may just have both saga calls made each time componentDidMount() runs. But what if accountDetails is unlikely to change, while rewardsDetails is more dynamic? Simply cache the accountDetails on first load, and on subsequent loads, first ensure the cache does not have accountDetails before making the saga call.  If each accountDetails request queries the DB, and you have millions of concurrent users, caching these details is a significant improvement.  tl;dr Analyze all AJAX calls and determine what is cachable in cookies/localStorage/sessionStorage/redux store/etc in order to cut down on future calls in the same session. "
I would use venv   docker to keep my base docker img clean as hell.,"I think they make sense too, some good points.Especially the post linking to this other site:  The problem I have with the second link [Why people create virtualenv in a docker container?], is that there are many if's.  What I mean by that is ""maybe you someday want to..."" etc. Which to me is wasting time, there was a good video on this subject on youtube, people creating big apps, defines whole classes that essentially ONLY contain a ""pass"" since they MAYBE will be implemented at SOME point in a few years or so...  My philosophy to get things done is don't make it complicated unless you NEED to.  TL;DR I would use venv + docker to keep my base docker img clean as hell. "
Long hours reduced my immune response to the point where a sore throat nearly killed me.,"> Eventually you learn all deadlines are negotiable. Most work in fear of missing them but never actually learn the consequences. Most of the time the consequences are the same whether you missed it or made it. In all situations you stand to gain the least.  I found this out 3 years ago... we had a hard and fast deadline but the pile of rubbish legacy system we were duct taping together took about 6 hours to install, and they wouldn't start the install until end of business. So with 2 hours of backup and snapshotting, the event was going to take 9 hours minimum.  But they  needed  me to come in and do my normal hours (I worked 7:30 to 4) this went on for weeks as every evening we'd be either bumped, roll back the install as the Ops guy was going off shift...  Leaving the house at 6:50am and getting back a 2am... and this sore throat wouldn't shift, week 2 this sore throat became an ear ache, this ear ache became a throbbing pain.  When I woke up in hospital I was told that I had meningococcal meningitis and that the past 36 hours had been touch and go until the antibiotics had started to win against the bacteria.  I was off for over a month, and returned to see how the deployment had gone. Turns out that they knocked it back 3 months as they'd had problems...  I stayed in that contract for 2 months, 2 weeks and left the day before the deployment, I heard one of my managers saying ""just before you go could you..."" and was glad to be able to say ""No"".  TLDR; Long hours reduced my immune response to the point where a sore throat nearly killed me. "
It has nice things  but it's strategically dangerous   the benefits are not substantial  anymore . I wouldn't.,"Hack exists because Facebook had a ton of PHP code, a monolithic architecture (not as much as of last 2-3 years, but previously) and they didn't like where PHP is going.  It's basically their insurance they have their fate in their hands.  As a third party developer, relying on Hack has the opposite result: your fate is in Facebook's hands.  It has a quite more fleshed out type system, but with PHP7 the gap is starting to close in many important areas. And if you don't drive yourself in a corner like Facebook did years ago, you can also introduce other languages in your app as microservices you can connect to from PHP.  tl;dr  It has nice things, but it's strategically dangerous & the benefits are not substantial (anymore). I wouldn't. "
if you aren't paying them  for support  your experience will suck.,"If they can't contact a person then they're not paying for support. That's on them.  I don't use cloud platform anymore but when I did late 2015 I paid for support (cheapest tier) and had to call three times. An actual person picked up in 4 rings each time (even at 3am!), and was surprisingly helpful for how crap I expected it to be, based on Google customer service in general.  If you're running mission critical systems, there's really no excuse for not having a real point of contact, even if you have to pay for it.  Edit: they even refunded us $600 in server charges when our public elasticsearch instance was compromised. Even though it was completely our fault for not upgrading after a critical security patch was out.  tl;dr if you aren't paying them  for support  your experience will suck. "
You are not entitled to free content. If you want an ad free experience online  pay a subscription fee.,"This looks like a fantastic idea and I love it. But I worry that people are missing the point because they don't understand how online advertising works and how ad blocking affects it.  When people go on espn.com or arstechnica.com they don't realize that they are being allowed to view content in exchange for viewing ads. If people refuse to view ads, then why should these websites keep their content online for free? If they aren't rightfully reimbursed for their content they'll just put up a paywall.  Imagine this:  you walk into a restaurant, order what you want, you eat, and then leave without paying. Does that sound right? Are you entitled to that food for free just because the waiter brought it to you when you ordered it? No because that is called theft. If a hundred people do the same thing every day, is that restaurant going to survive and stay open? Or what if they decide to charge you before they give you what you ordered?  Now replace the restaurant with your favorite website, and the food with the content you like. That's ad blocking.  Ad blocking  is  theft, but it's also an opportunity for market correction. People are (rightfully) pissed off because they hate the quality and content of a lot of ads out there. They hate how advertisers and marketers use their data. People notice ads and they don't like it when the ads create a bad user experience. And maybe they feel like there is no way for them to tell advertisers when their ads are shitty as fuck. Ad blocking is that response. At least now it looks like advertisers and websites are realizing this. Hopefully this creates an environment where advertisers, content producers, and users can have an open discussion on what kinds of ads are acceptable and what level of standards they want to see.  TL;DR: You are not entitled to free content. If you want an ad-free experience online, pay a subscription fee. "
Production level automation and distribution is not easy  fun  or consistent with Matlab or IDL in my experience.,"It not being free is irritating, but not the most irritating. Mainly it's the licensing and getting them to communicate with other existing software or languages.  Have you tried running Matlab or IDL in production or distributing them? I will say in my experience Matlab is easier to distribute than IDL if you buy the compiler. For IDL you can compile and release a small tarball with your software that comes with an executable, but it has to run in the ""IDL Virtual Machine"" which (because of the licensing) requires a graphical window to pop up and requires you to click it. If you automate this it goes against the license. So then if you decide to just run your stuff with the full version of IDL to not deal with that damn pop up you now need to have a license for every instance that is running.  For Matlab, I have never had much fun getting matlab to run in a production setting. We ended up just running it in a screen session because it randomly decides to screw up the terminal until you run sttysane. From what I've been told Matlab now has the ability to talk to shared C libraries, but when a coworker tried it it didn't work.  TL;DR: Production-level automation and distribution is not easy, fun, or consistent with Matlab or IDL in my experience. "
if a variable is undeclared  I'll just declare it  and Python should know that and therefore should do it for me.,"Wow, Slide sucks ass - I just lost a fairly lengthy reply. Sorry if this isn't particularly coherent.  In short, I think if I'm putting something in a box, I shouldn't have to tell the computer that the box needs to exist (I also think that I shouldn't have to tell the computer what type of thing I'm putting in the box, but that's a separate argument). More generally, I think it's useful to have the computer make decisions of its own accord whenever it's possible and useful for it to do so. IMO, Python's variable assignment behaviour is useful because it's very easy to understand and saves me typing/mental effort.  In other words, I think that much of the beauty of Python is that it does The Right Thing automatically, and that explicit variable declaration would go against that - if I get an error saying a variable wasn't declared, what will I inevitably do to fix it? Why can't Python do that for me? I'm sure there are issues you've seen that would have been solved by variable declaration, but I haven't, so this is my opinion, at least for now.  Anyway, your wish may come true at some point; I can't remember the details, but in Python 3.6 I think you can declare variables without initialising them, by annotating their type.  tl;dr: if a variable is undeclared, I'll just declare it, and Python should know that and therefore should do it for me. "
When prescriptive frameworks are bastardized by the community  it was the authors who were wrong  not the community.,"Sure, but that whole article is basically just purist masturbation. Your customers give  zero shits  about how  purely RESTful  your system is. Your downstream developers won't even care 99% of the time, all they care about is ""can I get it to do what I want?"" and ""is there documentation showing me how?""  There are some occasions where arguments from purity make sense, but this isn't one of them. I see this happen time and time again with technology: someone up in an ivory tower creates some golden, pure standard or concept, and the world takes it, mutates it to their own needs, and co-opts the original nomenclature. The original author kicks and screams because ""they're doing it all wrong!"" But they aren't, they changed it because it worked better that way for them, and because practical needs trump the desire for elegance and purity every single time.  What does this indicate? Well, usually, and this is especially true for REST, it means that your pure golden concept, while working excellently in your toy examples in your book, doesn't actually work in a lot of real life cases where things aren't as simple as you may have expected. Not everything ends up fitting as neatly as you'd hoped into your model. Some other classical examples: the semantic web, Flux/Redux, relational database normalization, regular expression engines... the list goes on and on.  TLDR: When prescriptive frameworks are bastardized by the community, it was the authors who were wrong, not the community. "
Generally we consider things like this and make a deliberate decision. Sometimes we miss things  and for that we welcome pull requests.,"Sorry for the delay! Your comment got buried a bit.  We have our own implementations of concepts like unique or shared pointers, and we use them sometimes. If the object lifetime is highly localized there's no need for the extra object allocations which can individually subtlely hurt performance, but which adds up to a lot if you do it everywhere. Also, part of the engine is a Garbage Collector for Javascript Objects, and so explicitly managing memory through standardized C++ constructs doesn't really apply in that scenario. We rely on the GC to properly collect objects allocated on the GC heap and we use vanilla pointers for those objects.  Also some of the code was written years ago and hasn't been modified since so it hasn't been brought up to a modern style (C++11, C++14, modern standard library additions). We try our best to incorporate new useful features (nullptr, lambdas) as much as possible and where applicable. Sometimes we didn't use the standard library because at the time the code was written, the Microsoft equivalent library was better. (And so on...) Every so often we go through active areas of the code base and update things.  Feel free to submit pull requests if you think you found a place we'd benefit from such changes (in terms of performance or memory leaks). If you think you've found a security issue, please responsibly disclose by reporting it as indicated in the [Security]( section of our README, instead of in a public issue or pull request, so that we can mitigate widespread impact of the issue.  There are several active conversations in the [issues]( and [Gitter]( on style related issues such as this.  TL;DR: Generally we consider things like this and make a deliberate decision. Sometimes we miss things, and for that we welcome pull requests. "
no  workers do not constantly spin up and down  they only die if they exit abnormally.,"Basically, that sits in front of the worker code. It uses Gearman (similar to plenty of other queues). Workers poll the Gearman server for jobs, grab one if available, and perform it. The process is not killed after.  Here's a simplfied version:  #!/usr/bin/env php&lt;?phpnamespace Firehed\Reader;use Firehed\ProcessControl\Daemon;use Firehed\ProcessControl\GearmanWorkerManager;use GearmanJob;use SimpleLogger\Syslog;require __DIR__.'/bootstrap.php';(new Daemon)    //-&gt;setSomeConfigs()...    -&gt;autoRun();$logger = new Syslog('RSS');$pm = new GearmanWorkerManager($logger);$pm-&gt;registerFunction(""updateFeed"", function(GearmanJob $job) {    Feed::findByID($job-&gt;workload())-&gt;update();});$pm-&gt;registerFunction(""addStoriesToUsers"", function(GearmanJob $job) {    UserStory::addStoriesToUsers();});$pm-&gt;setConfigFile(__DIR__.DIRECTORY_SEPARATOR.'gearman.ini');$pm-&gt;start();  The  new Daemon  bit handles daemonization like any other Unix process (without this, it would run in the foreground but doesn't otherwise affect forking). The GearmanWorkerManager is the master process that spawns worker children (according to gearman.ini), reaps them if they crash or are killed, sends SIGTERM on shutdown, etc.  tl;dr: no, workers do not constantly spin up and down; they only die if they exit abnormally. "
Do I  need  Yarn  Probably not. Do I  want  it  Hell yes.,"Lol this guy  First of all, Yarn is practically a drop-in replacement for npm. There's very little that you'd have to change about your project for you to be able to migrate to Yarn today. The very most you'd have to do would be to reconfigure some versions in your package.json if you use a npm-shrinkwrap.json as well.  In addition, Yarn accomplishes a very similar set of tasks to npm. They're both package managers, along with  brew ,  composer ,  bundle , and  apt . One you figure out how to use one package manager, it's generally not terribly hard to get the gist on how to use any other package manager. The only real difference in this particular case is  yarn add  replacing  npm install , which is completely trivial.  npm and Yarn are designed to solve the exact same problem regardless of the environment they are used in. There is no ""we should use npm in this scenario and Yarn in this other scenario"" because they both accomplish the same task regardless of where you use them. The only opinion piece that needs to be written on the subject of Yarn vs. npm is this: which one is better? And Yarn is superior in almost every conceivable manner, only losing out perhaps in longevity and familiarity.  But again, this isn't like classic JavaScript fatigue where you have to spend time migrating all your stuff to some new technology only to realize that you're behind again. This is about as drop-in of a replacement as you can get. There's nothing new to learn in order to use Yarn effectively, but the long-term gain from ""innovations"" like automatic lockfiles (which other platforms have had for years now) and extremely basic QOL improvements (like halfway decent install times) far outweigh the three minutes it would take for you to configure. Because that's literally how long it takes to get Yarn up and running. And half of it is waiting for npm to install the damn thing.  TL;DR: Do I  need  Yarn? Probably not. Do I  want  it? Hell yes. "
Variables declared outside of a function can be changed anywhere  global   variables inside a function can only be changed within the scope of that function  local ,"Hmm... if a javascript variable is defined locally, then it can be manipulated anywhere within the javascript code. So... walking through the code you have up there, not by actual lines but by relevant steps...   1 declare a global variable and assign to it the numerical value 7   2 declare another global variable and assign to it a function with one parameter   3 manipulate the first global variable within the function; it's saying the first global variable will be assigned a new value that is equal to a number passed to the function X 2   4 timesTwo(7) calls the function assigned to the timesTwo variable passing in the numerical value 7 as the local variable number (7 X 2 = 14)   5 JavaScript typically flows sequentially from top to bottom so both console.logs() should log 7 X 2 = 14 to the console    Does that make sense?  tl;dr: Variables declared outside of a function can be changed anywhere (global); variables inside a function can only be changed within the scope of that function (local) "
good engineers write good code TDD or sans TDD  and TDD is not better than writing tests after the fact if you create a simple design in the first place.,"Hey /u/button, since you've done a lot of TDD and also seem pretty open-minded about it, I'm curious about your thoughts on a few things that bother me about TDD.  I'll start by saying: Personally, I dislike TDD. I want to like TDD. I think it is a valid approach, but I don't see a clear benefit. It purports to result in a simpler design and better tests, but as Eric points out in the article, that rarely happens in the real world.  > The ones I know who are really good at this are already really good at design, seeing issues in code, and appreciating why those issues really are problems.  In my experience (TDD or Sans-TDD) the people you describe here are the type of people who write good code regardless of the development process they're required to follow. They aren't making a simpler design or better tests because of TDD, they do that by nature. You don't grow people like this by teaching them TDD, and you don't hone their skills by having them practice TDD. So I have always been curious what benefit there  actually is  to requiring TDD? I've seen more or less the same result from a group of good engineers who have built a culture around maintaining the test coverage.  > The places I've seen TDD work have been places where the entire culture is around building quality as the part of the development process, at the cost of getting work done more quickly.  I currently work at a place that definitely has a culture around quality, and our development process reflects that. We don't use TDD (although some of us have in the past), but we so see the same trade-off: we spend extra time to do things right. Would you say there's a benefit to using TDD if you already have a quality development process with engineers like the ones you/I describe above and a culture for maintaining test coverage? TDD has always felt like a process built to force you to do this, but why is it better than a process where tests are written after the fact? I don't see why writing tests first leads to a simpler design than just designing something simple in the first place and writing tests to accommodate that.  tl;dr good engineers write good code TDD or sans-TDD, and TDD is not better than writing tests after the fact if you create a simple design in the first place. "
if you aren't averse to something other than Javascript  give Haxe a go.,"Since the discussion has moved to include non-Javascript options as well, I have to add my 2 cents: Haxe  Cross compiles to native binaries for all three major desktop OSes, and Android plus iOS as well. These work quite reliably. It also has a HTML+Javascript transpiler target, but this, in my experience, has a way to go before I would consider it reliable.  Haxe's syntax is quite similar to Javascript as it derives from AS3, which derived from the proposed, but ultimately not adopted, ECMAScript 4 standard. I like to think of it as Javascript with strict types.  Owing to its history, its community and tooling and libs are all very game development oriented, but that is beginning to change recently.  tl;dr= if you aren't averse to something other than Javascript, give Haxe a go. "
Fuck equity. Shut up and pay me for my time and knowledge.,"Everyone that I've ever known that's taken such an agreement has regretted it. As one colleague of mine put it, ""I voluntarily took a pay cut in return for nothing."". Every company I've interviewed at that's had equity as part of their compensation package I've asked to have it removed and replace with market rate (since the salary is always at least 20% under if not more), because there's no guarantee that I'll still be here when your startup becomes the next facebook, but there is a guarantee that i'll still be here in two weeks when the next pay period comes around.  tl;dr: Fuck equity. Shut up and pay me for my time and knowledge. "
 Real time is exact time   Nope. Real time is ,"Real time has multiple definitions. Not just the one you consider the One True Definition.  I can almost agree with it having just one definition in programming, but it's perfectly within reason to reference a definition used in another domain. You have real time news feed for instance.  What's more if you you want to get truly pedantic there's no such thing as an actual RTOS/Program. It's all approaching 0, never reaching zero. Similarly, ""real time"" feeds (be they Reddit comments or news feeds) approach instantaneous, to the best ability of the technology used. The only difference is the delta between 0 and state change.  It's perfectly reasonable to talk about real time feeds/comments.  TL;DR:  >Real time is exact time  Nope. Real time is → "
psychology and biology are never even remotely simple. If you can possibly get away with it  use gender and categorise into male female nonbinary other.,"That actually makes the problem worse. Sure, 97% or whatever of people can be classified as male or female under that scheme, but that classification is hiding a whole bunch of dimensions along which people can vary. Like, you have at least penis/vagina/intersex, testicles/ovaries testosterone/estrogen/both/neither, presence or absence of breasts, as well as whether their body can actually process testosterone/estrogen (relevant for women with complete androgen insensitivity). Biology is fucking complicated and has way,  way  too many edge cases, don't go there unless you really need to (i.e. you are a hospital, but even then they track identified gender and mark down other stuff on your file as conditions to the best of my knowledge).  The question I'd ask the OP is ""why do you care?"". Specifically, knowing what purpose the data is to be used for tells you what distinctions are actually relevant to you. And 99% of the time you need either identified gender or (in places that had the concept) legal gender (though remember that this can be changed and even birth certificates are usually reissued with the gender changed for trans people, so this isn't a good proxy for assigned gender). Sometimes assigned gender is relevant too -- this can be summarised as ""what gender the doctor said you were when you were born"", but that's mostly relevant in like research surveys and such (and note that e.g. intersex people will often be assigned male or female).  Also there is increasing recognition of nonbinary and intersex people these days, and at least in some countries you can get your ID changed to reflect that, so you definitely can't get away with a simple male/female anymore.  TL;DR: psychology and biology are never even remotely simple. If you can possibly get away with it, use gender and categorise into male/female/nonbinary/other. "
lambdas are neat  but nothing more than syntactic sugar for anonymous inner classes at the moment.,"That is wrong. Look up InnerClassMetaFactory. While lambdas will generate an invoke dynamic in your bytecode, the runtime will actually generate a new class implementing the functional interface the lambda adheres to. The code of the lambda is proxied by that on-the-fly generated class. As such, lambdas currently are really nothing more than syntax sugar for anonymous inmer classes.  And lambdas are just as prone to leaks. If your lambda doesn't capture a local or field of the enclosing class, it's proxy class will get instantiated omce and cached statically. That's a very rare case. If your lambda captures a local or field, references to these effectively final things will be stored in the proxy class, hence the same stuff you'd get with an anonymous inner class.  Tl;dr: lambdas are neat, but nothing more than syntactic sugar for anonymous inner classes at the moment. "
TS      Good    Angular 1   2     Technical Debt,"I really like typed languages and honestly wish JS was typed, so I definitely see the benefit of TS. But one thing I just can't imagine or understand is AngularJS(1 of 2), Angular has so many useless and silly abstractions that offer ZERO benefit both in performance and speed of development. While I don't know too much about Angular 2, I definitely detest Angular1 and I personally call it Big-NG-Cubed since the digest loop is wildly inefficient, I always can tell when using an angular 1.x app by the flickering.  For Angular 2 I tried out their getting started and built a small sample app, and TS is the least of my worries. What worries me is all of the unneeded Angular Jargon and syntax, I mean whats the point of renaming native functionality? Why do we have to learn a whole new Syntax and language, when it offers no benefit? I mean I personally love learning new languages because it extends my capability as a developer, but Angular 1 or 2 do not, it's just learning something new to accomplish the same goal only 20 times slower with a 1MB dependency.  TLDR: TS === ""Good"" , Angular 1 & 2 === Technical Debt "
the two  PDFs  are just wrappers around JPGs  which each contain two independent image streams  switched by way of a variable length comment field.,"The visual description of the colliding files, at  is not very helpful in understanding how they produced the PDFs, so I took apart the PDFs and worked it out.  Basically, each PDF contains a single large (421,385-byte) JPG image, followed by a few PDF commands to display the JPG. The collision lives entirely in the JPG data - the PDF format is merely incidental here. Extracting out the two images shows two JPG files with different contents (but different SHA-1 hashes since the necessary prefix is missing). Each PDF consists of a common prefix (which contains the PDF header, JPG stream descriptor and some JPG headers), and a common suffix (containing image data and PDF display commands).  The header of each JPG contains a comment field, aligned such that the 16-bit length value of the field lies in the collision zone. Thus, when the collision is generated, one of the PDFs will have a longer comment field than the other. After that, they concatenate two complete JPG image streams with different image content - File 1 sees the first image stream and File 2 sees the second image stream. This is achieved by using misalignment of the comment fields to cause the first image stream to appear as a comment in File 2 (more specifically, as a sequence of comments, in order to avoid overflowing the 16-bit comment length field). Since JPGs terminate at the end-of-file (FFD9) marker, the second image stream isn't even examined in File 1 (whereas that marker is just inside a comment in File 2).  tl;dr: the two ""PDFs"" are just wrappers around JPGs, which each contain two independent image streams, switched by way of a variable-length comment field. "
Practicality is more important for the majority of developers.,"I expressed myself poorly I think. Think more of it as languages that covers the vast majority of code that is written. A lot of code isn't very exciting. Enterprise software isn't exciting. Basic web pages aren't exciting. The demands to express yourself extremely freely or incredible performance simply aren't there. I wasn't suggesting the one language to rule them all thing:)  And as I wrote in a different comment, crayon eaters are the people who shouldn't be developers in the first place. The idea that everyone can be a developer is kinda silly. I certainly can't do every job on this complexity level. And even if I theoretically could, I don't have the education nor experience anyways.  TL;DR Practicality is more important for the majority of developers. "
I prefer Sublime because I value smooth  responsive editors with the perfect balance of keyboard mouse usability. Also I bought Sublime long before Atom existed.,"> One has an absolutely massive amount of things built in, and the other is just a really lightweight text editor that I can't really say is any better than atom in terms of functionality. Why are they even close to the same price  I've tried on a few occasions to like any of JetBrains' editors, but I just can't. They feel clunky, slow and overly cluttered with UI - generally a bad UX experience. No matter how fast the computer. Just ick. I spent way too much time tuning the interface to try and get what I already got from TextMate or Sublime.  I've also tried Atom a few times. There are a few little things that just make me dislike it. For example, scrolling isn't per-pixel, it does this jumpy ""line (or three) at a time"" scrolling. OS X has really smooth, responsive scrolling ... I know it seems like a minuscule complaint, but it really makes it easier to track the lines and continue reading even when scrolling. Then Atom basically was unresponsive whenever I opened large files. I have to open large log files occasionally and I don't want to fight the editor when I do.  I was a bit grumpy paying my $70 for Sublime Text 2 back in the day, but I was coming from TextMate which was also commercial software (and similarly worth it). What I loved about Sublime at the time (look at me rhyme) was I could have the same ""light-weight"" text editor across all platforms, and that was/is worth supporting.  I think a lot of people, similar to myself, gave up on TextMate ""2.0"" ever coming out (which was going stagnant much as Sublime perhaps is now) and moved to Sublime, which, at the time, was very actively developed (like how Atom looks today).  $70 (or less for bulk licensing) ultimately isn't that much for professional developer, or company hiring developers.  I've never encountered a bug in Sublime, I use it every day for I don't know how many years now. For my needs it is ""finished"".  TLDR: I prefer Sublime because I value smooth, responsive editors with the perfect balance of keyboard/mouse usability. Also I bought Sublime long before Atom existed. "
the  reason  is that people who edit HTML use editors that don't word wrap automatically.,"Not necessarily. If I have something typed up in a mono-spaced editor without word wrap the 'paragraph' will need to either be a single excessively long line running off the end and adding a horizontal scroll bar, or, explicitly formatted to fit within a monospace character limit for the editor (using single newlines).  The former will show up nicely when displayed in a variable size area, but will be ridiculously annoying to modify with someone's html editor of choice. The latter will be easy to edit or read in the editor, but won't show up right in the variable-size word-wrapped display... unless single newlines are treated as a space.  tl;dr: the ""reason"" is that people who edit HTML use editors that don't word wrap automatically. "
If I have to choose between Scala and Spring  or Guice  or whatever  I'll take Scala any time.,"> one of the things that seriously cools me off to Scala (not sure yet about Kotlin) is that the DI integration seems to be sort of ""out of scope"".  Full-time Scala dev here for 7+ years.  That's actually one of the things that drew me to Scala, and have kept me loving it.  (Different strokes, and all that.)  DI is ""out of scope"" in Scala because it's not needed: built-in features handle app wiring just fine.  On my first big Scala project, we ported the existing codebase from Java class-by-class.  It felt  great  when we replaced all our Spring annotations and XML files with a (comparatively) simple class that laid out the wiring.  TLDR: If I have to choose between Scala and Spring (or Guice, or whatever) I'll take Scala any time. "
Minecraft is limited by the Java VM and it's definitely finite map size  not by the ability to load chunks.,"That's not entirely true.  The stock Minecraft chunk-loader is designed to keep as many chunks in memory as possible; barring memory, disk and network latency issues. You can configure the number of chunks with command-line parameters or from the options menu (although it is a slider limited to powers of two).  If the section of memory allocated to Minecraft is exhausted, and you move to the edge of the loaded chunks, Minecraft will unload and load new chunks. Chunks that are unloaded therefore receive no update or render ticks.  If you had a infinitely large amount of memory assigned to Minecraft by the Universal Java VM, and had a Universal CPU that could process all of that memory, Minecraft would never unload chunks. Therefore, Minecraft would update and render the entire map at once. Huzzah!  Although at that point, you would find that the Minecraft world is limited by 64bit integers, and its map is not actually infinite. That's where the true hard-coded limit lies.  tl;dr:  Minecraft is limited by the Java VM and it's definitely finite map size, not by the ability to load chunks. "
The React example works but does not reflect best practices in any way. The Angular example attempts to provide guidance towards proper application structure.," Because it's geared around creating large applications. It's overkill for a hello world example, but when dealing with real-world data models being worked on concurrently by large teams, it becomes very useful very quickly.   The 3 .ts files could easily be one file, and would be pretty much the same code length/complexity as the react example. Conversely, the react example could easily be structured as 3 different imports. Again, this is just an artifact of different approaches to a ""hello world"" example.   Your React example uses ES5. The Angular example is using Typescript transpilation. For one thing, this is generally done at build time, not runtime, so a lot of the scripts in index.html are really only to make it work in plunker. You are welcome to develop Angular2 apps in ES5, the code is just a lot easier to work with in Typescript (for example, I see you don't use JSX in your React example).    TL;DR : The React example works but does not reflect best practices in any way. The Angular example attempts to provide guidance towards proper application structure. "
Generators are for controlling iteration. They're synchronous  but can be used to implement async await like syntax.    1  ,"Though generators and asynchronous coroutines have similar execution models, they are different. Generators are functions that behave like iterators and are used to control iteration. They're used to increase expressiveness. If you look at the examples in the [wikipedia page I linked to above][1], you'll notice that none of them have anything to do with asynchronicity and they are not ""doing it wrong"".  Generators are sometimes implemented in langauges via the same underlying feature(s) as coroutines, which means they also sometimes become the mechanism powering that language's async/await features by yielding to the event loop. In JS, generators are synchronous, but can be used to implement asynchronous coroutines and something resembling async/await by calling  .next()  in an asynchronous function's callback, in which case they do  not  resolve to polling.  TL/DR: Generators are for controlling iteration. They're synchronous, but can be used to implement async/await-like syntax.  [1]: "
Don't mistake current bedazzling  IOT  bullshit  with what IOT will actually be about in the future.,"The IOT is not to blame, and it's not ""Planned"" either, just DGAF. Customers get dazzled by the shiny thing and they DGAF about how long it will work, so producers DGAF either, all they care about is making money by selling whatever shit their customers will be happy about in the short term.  With time, as people start to actually care, a true Internet Of Things fridge will be one that will have a standardized electrical interface for its components, a slot for a standards compliant processor/computer chosen by the customer, and maybe a slot for some standard replaceable screen. Producers like Samsung may sell them as a pack, or the customer may decide to buy a bare-bones fridge and put whatever processor unit and display in it.  TL;DR: Don't mistake current bedazzling ""IOT"" bullshit, with what IOT will actually be about in the future. "
If I find anyone using singletons in my code  I will kick them in the groin.,"The only valid description of Singleton is : Don't f*cking use it!!  Class's behavior and class's lifetime are two separate concerns and as such should be handled by separate classes. The lifetime will most probably be used by some kind of IoC container.  Singleton's static nature forces any code using it to make that dependency non-transparent. So it becomes PITA to unit test and reason about. We all know global state is bad. So why the hell are you using ""global state : the pattern"" ??  And finally, way too often it happens that ""there will only be single instance of this class"" might seem like a good idea at start, but later you figure out it is clearly not true. And due to it's global nature, it becomes impossible to refactor it into non-singleton version.  tl;dr; If I find anyone using singletons in my code, I will kick them in the groin. "
flow isn't just about hyper focusing or concentrating  you also have to be performing a challenging task that you enjoy.,"Lots of people here don't seem to realize that a level of passion and enjoyment also plays into flow. Achieving a flow state is generally easiest/most consistent/most effective when the task on which you're focusing is sufficiently challenging, interesting, and enjoyable such that it rewards your intense focus intrinsically.  Personally, I sorta think of it as a positive feedback loop; instead of rewarding yourself externally (though this can certainly be a part of the equation for many--I'm just focusing on my personal experience here), the challenge and interest in the task is its own reward. This is one way I knew computer science was my passion: the task of programming or outlining algorithmic solutions is something I can just kinda get lost in.  Additionally, if you really want to understand how this works, I highly recommend you at the very least skim through Mihaly Csikszentmihalyi's original book on the subject (he's the guy who coined the phrase and is generally considered its ""discoverer""). Pretty dense, but worth a look; otherwise, as with most things in psychology, you run the risk of getting the watered down pop version of the concept that may or may not be accurate.  Tl;dr flow isn't just about hyper-focusing or concentrating; you also have to be performing a challenging task that you enjoy. "
For Data Science  you're much better off learning R.,"If you're looking for a career as a Data Scientist rather than a more ""traditional"" predictive analytics professional, R seems like the way to go.  I'm a recruiter who works exclusively on DS roles, and R/Python are by far the two most requested tools I hear from my clients. In the last 2 years, I can count on one hand the DS roles that have preferred or required SAS.  On the other hand, our Predictive Analytics team still sees the SAS requirement all the time. Most of their roles (predictive modelers using structured data sets) are SAS-based, so there definitely is a market for it. It really depends on what you're looking to do.  TL;DR: For Data Science, you're much better off learning R. "
I like having a bootcamp grad on my team because they can relate to programmers and non programmers.,"A lot of people disagree with me on this, but this is part of why I am partial to considering Bootcamp grads, especially if they had real careers before learning basic web-dev.  What they lack in dev skills can sometimes be made up for in empathy and understanding the mindset of non-programmers. You ever talked to anyone who just randomly started a codeacademy course? They'll say things like ""I don't really see why having arrays and looping through them are important."" Whereas for us, this is fundamental to almost everything we do. We clearly see the world differently.  These bootcamp grads can relate to both worlds. They didn't know what an array was a year ago. In my experience, explaining these concepts often can be a source of miscommunication and friction. They can give really good feedback about whether I am explaining something in a way that makes sense to non-developers.  TLDR: I like having a bootcamp grad on my team because they can relate to programmers and non-programmers. "
Just because something is old doesn't mean it is good. Just because something is new and popular doesn't make it  shinny hipster candy .,"You didn't call it immature, you called it ""shinny hipster candy"".  Those are not the same concept. One of those implies that it's something that just needs to grow in stability a bit more. The other implies that it's a fad and only used by people because it's trendy.  You're also comparing a client side UI framework to server side languages.  You refer to ""non-JS devs"" like they are a group of wise old sages that are looking at people who happen to use JavaScript as if they are a bunch of young monks just starting their training.  Something is mature when you can convince the entire senior IT dev group of a Fortune 500 company that it is.  The things you listed have a specific role and place. They are mature in that space only. Modern languages and frames works have the benefit of learning from past mistakes to allow them to grow into maturity far far faster then languages that were written in the 50s and 60s. Perl 5 is a perfect example of how something can be mature and be horrible at the same time.  TL;DR: Just because something is old doesn't mean it is good. Just because something is new and popular doesn't make it ""shinny hipster candy"". "
Parent is right  but VMs are still useful. Use the right tool for the job.,"I think there's a lot of hate from people who see containers as a overly complicated way to build simple LAMP style web apps. In some circumstances, that may be true. If you've already got infrastructure to turn on a VM easily, and your app is never going hit more than ~25 concurrent users - then using docker just seems like an extra layer of needless complexity.  I still use vagrant to build the local dev environment for my current project, because I find it easier to experiment with different bits and pieces. Adding reddis, memcache, a profiling tool, or whatever else I might try to a full nix box is easier than dealing with docker images, and I can push the network configuration of those down the line until I'm sure I want to keep using them.  However, my CI pipeline uses docker images for testing, and staging and prod will probably go the same way once I get the app to that stage.  To the OP, if you really want to know how the pieces fit together, I'd recommend building a LAMP stack on a VM running Ubuntu (because of apt mainly), then automate the creation of that using Vagrant, and then when you want to start running Continuous Integration (CI) or Continuous Deployment (CD) then look at Docker. I recommend this approach, because each stage builds on what you learnt before.  tl;dr; Parent is right, but VMs are still useful. Use the right tool for the job. "
I'm more worried about the sad state of content curation than where the content is stored.,"This post only just scratches the surface. If we manage to get a fully distributed social network, what then? It still doesn't solve the problem of sock puppets and botnet voting rings. I started diving into that topic earlier this week and found out the practice of running hundreds of sock puppets to upvote, downvote or flag whatever they want is shockingly common. Even the US federal government gets in on the action to shape public opinion. There's very little that shows up on reddit that hasn't been engineered to some extent by powerful interests who either want you to buy their products or think the right thoughts.  tl;dr: I'm more worried about the sad state of content curation than where the content is stored. "
suck it up  inject javascript directly into your veins and pretend you like it baby,"What exactly would you replace it with? There is quite literally no language on the planet besides javascript which is capable of providing a dynamic web application experience (thankfully, because the previous contestants were Java Applets and Flash). Every single web delivered stack includes js at some point - I personally love developing the .NET stack (C# targeting the CLI with an IIS server using linq to connect to DBs) for web apps, but there comes a point where even that stack yields to js. Python with Flask/Django, Java EE, Rails, even old school PHP/Perl CGI.  The days when you could call yourself a programmer without the ability to deliver a web application are very close to done - embedded is really the only place left, and IoT is coming very soon to stab that in the back.  tl;dr suck it up, inject javascript directly into your veins and pretend you like it baby "
JWT can be used like cookies for a REST API and is better than other methods because it doesn't require a database hit on every request.,"You can't compare JWT to cookies as they serve different purposes. Instead, you should compare JWT to like, HTTP Basic Auth, or something like a API token string.  In a regular web application (like, not a REST API) you'd have cookies which persist session state. So, a user logs in, and a session is created for them on the server which keeps track of their user ID, username, when they logged in, etc. Then their browser is given a cookie to keep track of the session ID. The browser will send that cookie in each subsequent request and the server will look up that session and boom, the server knows who you are.  But in a REST API there is no persistent server state. There are no cookies. Each request to the server is treated as a completely new and independent request, so the server does not know one request from another. So we must pass something in the API request so that the server can look up the user. There's a few options here.  We could use HTTP Basic Auth (username+password) and the server would look up those values in a database. Or, we could assign every user a unique API access token and pass that. These are both viable options, but it means that we have to hit the database  on every single request . This is where JWT comes in. A JWT is a signed packet of data, so the server can reasonably trust the data that the client is giving it. So now the client can just send a JWT and the server knows who is making the request without having to do a database lookup.  This works because when the server first creates the JWT, it has to sign the packet with a secret key. Part of the JWT is a hash used for an integrity check. If any part of the JWT is changed after it was created, then the hash will not match and the server will know it is a forgery. So for example if I login as a regular user and then try to change permissions in the JWT, the server will know that I did that because the computed hash will be different.  tl;dr - JWT can be used like cookies for a REST API and is better than other methods because it doesn't require a database hit on every request. "
If you are concerned about performance  don't assume that passing by const reference is always better  or as good  compared to passing by value. Understand what is going on.,"Well you have to make up your mind. We're either talking about a case where the compiler is able to see across function calls and inline as it pleases, or we're not.  If we're talking about the former, then it doesn't matter whether you pass by const reference or value. The compiler can see when things are semantically equivalent. Note that this also involves cases with LTO.  For the latter, you're going to see that in most ABIs (if not all), passing by reference is implemented using pointers. This means that if you have your two-double point sitting nicely in a pair of registers and want to make a function call to a function taking that point by reference, the compiler has to spill that instance to memory (since you can't get a pointer to a register) and pass the pointer to that memory area into the function.  When you bring in aliasing, it can get even worse. Consider this silly example:  struct point { double x, y; };double add1(point&amp; p, point p2) {  p.x = 0; p.y = 0;  return p2.x+p2.y;}double add2(point&amp; p, const point&amp; p2) {  p.x = 0; p.y = 0;  return p2.x+p2.y;}  Now, look at the code generated by GCC 5.3 -O3, x64:  add1(point&amp;, point):       pxor    %xmm2, %xmm2       addsd   %xmm1, %xmm0       movsd   %xmm2, (%rdi)       movsd   %xmm2, 8(%rdi)       retadd2(point&amp;, point const&amp;):       pxor    %xmm0, %xmm0       movsd   %xmm0, (%rdi)       movsd   %xmm0, 8(%rdi)       movsd   (%rsi), %xmm0       addsd   8(%rsi), %xmm0       ret  For add1, the compiler can start clearing xmm2, add x and y into xmm0 (the return register) and finally copy the zero in xmm2 to the 16 bytes pointed to by rdi.  For add2, the compiler has to assume that p1 and p2 point to the same object, so it first clears 16 bytes that rdi points to, it then loads and adds the object from rsi. It has to do these things in sequence since it can't assume that they are separate.  TL;DR: If you are concerned about performance, don't assume that passing by const reference is always better (or as good) compared to passing by value. Understand what is going on. "
there are significant switching costs to replacing one software package by another  even if they have the same aim and are technically somewhat similar.,"I'm not sure in what idealized world you live in, but in practice, learning photoshop does not magically translate to proficiency in gimp.  Particularly  for the 99% of students that end up using this as part of a well-rounded background and not a full-time job, the effort of using a different tool isn't irrelevant.  Additionally, force of habit combined with network effects and incompatibilities mean that cooperation with those using another program is more difficult than using the same program.  This means that each individual is best off doing whatever everyone else is doing, and having a head start like ""everyone is using photoshop"" is quite something for a software package to have.  Finally, free software is improved substantially by a small group of end-users that learn how.  Even proprietary software is improved by users since people post how-tos and other helpful material.  Depriving your competition of users - even if they don't benefit you otherwise - is therefore valuable.  TL;DR: there are significant switching costs to replacing one software package by another, even if they have the same aim and are technically somewhat similar. "
If your interviews are like this guy's  you should really be questioning if you want to work for the people interviewing you.,"The thing a lot of people miss during interviews is that you are interviewing your managers and the company as well. To me, poorly conducted interviews are a sign of bad management or leadership. If I have offers on the table, I'm not going with that company.  There is a huge difference between the question ""Write a binary tree algorithm"" and ""When would you use a binary tree? What could be some potential issues when using a binary tree function?"" The first one relies on memorization, which is the lowest level of knowledge. The second relies on actual application of your knowledge, which makes it a much better question.  Companies that ask questions based on simple memorization worry me for a few reasons. First, are their engineers reinventing the wheel? If I'm being asked to write a binary search function, am I going to find a ton of unique search implementations in their code instead of a standard library function? I damn better not. Furthermore implementing a 3rd party library is a much different skill set than recreating an algorithm - and a much more used one as well.  Second, it makes the company seem like they care about being ""smart."" Not sure how man out there have dealt with people like this, but I call them the ""Wizards of Oz."" These are people that are highly revered in the organization for being ""smart"" but they can never articulate what they are doing, don't comment their code, and generally cause problems for the rest of the team. Many times these people are not doing anything particularly difficult for a standard programmer, but they way they talk and code, you'd think they're landing the Mars rover.  Personally, I know I'm going to fail standard knowledge questions because I know to use Google for those - like every other programmer. I also know if a company is relying on those questions to choose candidates, I'm probably going to be better off somewhere else with a better team.  tl;dr - If your interviews are like this guy's, you should really be questioning if you want to work for the people interviewing you. "
look at the job that's being interviewed for and what its  real  requirements are  and build your interview around that.  And be nice. It's not rocket science.,"If you use BFS (or whatever algorithm or data structure or design pattern) at work on a regular basis - and for all I know, that may be the case at Google - then yes, by all means it should be an interview question. As long as you signpost to candidates before the interview you'll be asking these kinds of questions so they're not ambushed coming through the door that's perfectly OK.  If you're just building a derpy CRUD app that wires together a bunch of tables in the backend to a PHP framework and pushes JSON back and forth to a JS framework on the front end and your daily problem is getting a div to render correctly or making sure the tables are properly indexed, then ask questions about those kinds of problems. If you ask a BFS question and you, personally, haven't touched BFS in a professional setting since college, then that's just dishonest and you're just hazing.  ""We want to see how you think under pressure"" is also BS. Typical day is more like your PM or dev lead says ""we need to do X"" and you ask a bunch of questions, go away and research the issue, have a meeting, ask some more questions and put together an estimate and maybe write some test code drinking your morning latte or whatever your development process does. You're not in a courtroom or Congressional hearing or the final round of Jeopardy where every second counts and every word is going to be pounced on and dissected. You're not a heart surgeon welding a scapel over a dying patient. Yet the interview process seems to assume that's the normal case.  And yes there are scary moments when it's 3 am and you get a phone call because the database is down and you can't SSH in. Ask the candidate about the worst moments about their career and how they coped with them and what they learned from the experience.  Show common courtesy - be polite throughout, be open about the hiring process, give feedback when possible, take the time to look over someone's resume before you invite them to spend precious vacation time travelling out to see you. Make it more of a conversation and less of an interrogation.  tl;dr; look at the job that's being interviewed for and what its  real  requirements are, and build your interview around that.  And be nice. It's not rocket science. "
I think there is and should be a middle ground  and neither option is it.,"I understand how IP laws work here, and starting out your evangelizing the GPL by assuming I'm an idiot isn't going to get you very far...  EEE was over a decade ago (Although I don't blame you for watching your back).  You're overlooking the fact that FreeBSD, and Clang/LLVM is at LEAST as good as linux and gcc (although linux does have more drivers)  The thing is, I don't give a shit if Apple or Microsoft or whoever uses the software I release under an open license. I'd be happy if someone found use for it, and if a company that big thinks by software is that good, shit I'd put that on my resume.  I also agree about the open hardware issue (although I'm more concerned with open firmware)  I just don't see it that way.  What I see, is that you're threatening users of your software with lawsuits if they use your products at all to make money (GPL 3 Tivoization clause), that's not creating a good community for software developers, or home users; and why should companies like Tivo have to develop their own OS from the ground up? Why should they have to buy shitty ass WinCE?  Also, from what I've seen (and it's probably not represenative of the gpl sub-community itself) but linux is pretty hostile to people that don't see everything there way, they're very quick to attack and belittle people for really dumb reasons (Not that I think the ""community guidelines"" or whatever nonsense github is trying to shove down people's throats is good either).  tl;dr I think there is and should be a middle ground, and neither option is it. "
showcasing the libraries creates a fundamentally unsound line of argument  even if Stroustrup uses it and regardless of the merit of the underlying point.,"The standard library is not the language in either C or C++. Comparing operations by cherry-picking the libraries employed is a spectacularly poor way to make an argument about a language. Even in the C example above, using strcat() is a simplification, and since std::string is going to be worlds worse in performance, there's no reason to treat '@' as a character... make it a string too. A quick macro will further simplify the process, but since std:string will probably allocate a larger buffer than necessary (and realloc arbitrarily behind the curtain anyway) why not simply preallocate a maximum buffer in the C example and use that? See how this just keeps going on? I won't ever claim that the C example will be as concise as the C++ example: it won't, but comparisons should be made with things that aren't library-dependent. I could write or probably find a C string management library to hide all that behind macros and/or functions.  tl;dr showcasing the libraries creates a fundamentally unsound line of argument, even if Stroustrup uses it and regardless of the merit of the underlying point. "
JS Python PHP have a way of doing things that makes never really learning to program ridiculously easy.,"Yes, this also my problem with Python (which also has a lot of non-programmers that clutter it's community.)  I'm all for libraries, as a mostly C programmer libraries save a lot of time.  But there still are things you need to implement yourself and the thing with this new library culture of the last 6 years or so is there's now a function to do pretty much everything you want in these interpreted languages (php is also a good example) it makes it so that people aren't even ""programming"" anymore just copy/pasting Stack Overflow answers and then using brute-force theory to throw javascript/php/python/w/e libraries into the mix until you have a working solution, instead of solving nearly any problems their selves.  Which could/can be a good thing I guess but usually ends up giving confidence to someone who should still be learning the basics so they have to learn defensive programming the hard way, if ever.  And also I can say from experience it is a right bitch taking over a code base written by someone who doesn't know how to actually implement algorithms or write things like a basic user system themselves.  I inherited a codebase 3 years ago that was begun in 2006 by a man who had never programmed in his life (it was an ecommerce company, and they had just began making enough money to actually higher a programmer full time.)  It was full of different styles of programming, half OOP as it used an old ass version of OScommerce from '05 or so as it's base and was ""hacked"" at by the man who owned it to get the features he needed over the course of 10 years (and his new ""features"" were not anything i'd call OOP, and were full of sql injection vulnerabilities) and the front end was full of JS libraries, and a number of functions that were failing but never removed.  TL;DR JS/Python/PHP have a way of doing things that makes never really learning to program ridiculously easy. "
Seams aren't the problem  the problem is how you are thinking about the problem. Also  read the book  not the blog.,"I think this post summarizes why blog posts are a bad teaching medium, and why books are still very useful.  It looks like this guy read some stuff and applied it, without reading the full instructions. For example, the OP quotes Uncle Bob, but misses the context which is fully explained in Uncle Bob's book [Agile Patterns, Principles and Practices](  For example, thinking that:  > depend on abstractions, not concretions  implies an explosion of interfaces and false abstractions.  The PPP book has a chapter [detailing]( what he means by this.  In essence, the business rules should be abstracted away from how they are implemented, because the business rules change for different reasons than the implementation (and thus why they should be separated, as per SRP).  In the example, he is implementing a Coffee Machine.  Uncle Bob  starts with the use cases  to determine what software behavior is required, and derives a software model of that behavior using OOP. This results in a coherent set of classes with well partitioned responsibilities.  Then, he creates  abstract classes  to expose seams which the concrete implementations use to connect the abstract model to the physical implementation of the Coffee Machine. You can use interfaces and the strategy pattern as well to achieve the same effect (the approach I prefer). This way, the physical implementation depends on the abstract definition (as per DIP).  Thus, the seams become  layer boundaries  driven by abstractions derived from use cases.  Note that the  goal  isn't testability, it is separation of concerns. The fact that testability is improved is a  side effect  (and testability is a  tool  for  measuring  how well concerns are separated). This is why chasing testability as a goal leads to false abstractions, because you are approaching the problem from the wrong direction.  Thus, if you apply Uncle Bob's advice  as he meant  and  as is explained in the book , the problem the OP demonstrates with lack of cohesion  doesn't exist . In fact, there is a very useful section in the PPP book providing some [useful metrics]( to determine how well your software model is organized, so you can move your abstract model from the ""bad abstraction"" to the ""good abstraction"" as demonstrated by the OP.  TL;DR - Seams aren't the problem, the problem is how you are thinking about the problem. Also, read the book, not the blog. "
It doesn't matter.  They have nothing to do with ML.,"Being that neither of those have to do with ML, it doesn't matter.  Your ML can be done with what you normally do.  Flask and Django are web frameworks.  Your ML will be done on a different service and your Flask/Django webapp will make calls to the ML services.  Now, if I had to choose? Well, it depends. Django is awesome for a traditional MVT/MVC webapp where it's all prepackaged with just about everything you need for a web app.  It  is  for a monolithic app though.  Flask is lightweight and the bare minimum for a web server.  It doesn't really include much. It's a  great  starting point though. It's been gaining a ton of traction in the past few years as microservices (Docker) becomes big.  Honestly, I'd say go the route of a Docker/containerized application with Flask.  Containerize your services.  It'll help scale  far  quicker and easier to throw more resources as needed at all your services. Traditional ones like Django will need to allocate too many resources to scale similarly. You  can  containerize Django, but it's a bit of work and a little hacky to get a monolithic framework to work in a microservice architecture.  tl;dr : It doesn't matter.  They have nothing to do with ML. "
I recommend starting today. You don't need to rotate  or even know  100  of your passwords to start increasing your security.,"I approached this by simply entering everything into the password manager as my first step. The one I'm using lets you categorize sites, so I put all the newly-imported stuff into its own category for sites with old, weak passwords.  Then I scanned through that list and picked the most critical sites and changed those first. That way I quickly reached a point where all the sites I care most about have new, strong passwords. If someone found out one of the passwords that I used to share between many sites, they'd only get access to the least important sites.  This way, you get 80% of the benefit for 20% of the work, and the other 80% of the work can be done gradually when you have a moment to kill. Even if you never did the remaining 80% of the work, you'd still be way ahead of where you are now security-wise.  Also, you might be at a point where you don't even know all the passwords for certain accounts you have. You can still enter them into the password manager with a blank password (perhaps in yet another separate category just to help you keep things straight later) as you think of them, then at least you are on top of what needs to be done eventually.  TLDR: I recommend starting today. You don't need to rotate (or even know) 100% of your passwords to start increasing your security. "
many DF players are programmers   if fact that's why I started    so you may see a few of us around here.,"I'm pretty sure Dwarf Fortress has the highest proportion of programmers to other players of any game since 1980 or so; it can't be  that  surprising.  I mean, [DFHack]( provides an API for three languages on every OS, with no first-party support or assistance - it's entirely reverse-engineered by live-editing memory in use by a closed-source and undocumented program.  Implementation is in lisp, shell, perl, python, lua, ruby, C++, C, asm, and I think a few other languages.  Or take Armok Vision, a real-time 3D viewer for Dwarf Fortress (uses DFHack API, or course).  The dev recently added key passthrough, an in-3D-world copy of the DF window, and  Oculus Rift support .  Yes, a community mod for a game using  ncurses  for graphics now provides full VR support.  And there are literally hundreds of other projects, so many that I had to [write a program]( to download and configure the ones I wanted.  TLDR; many DF players are programmers - if fact that's why I started! - so you may see a few of us around here. "
. Most comfortable  Keyboard   mouse  for me   personal preference.,"So like many editors, I think it is really a preference thing. I prefer using the command line over GUIs and keyboard shortcuts over using a mouse. I do not like the UI/UX and colors of most editors I have seen. There are many menus and nested traversal that is mind boggling.  Once I have learned a command, I can reuse it with  most  plugins or other commands. There was a good post about it on one of the programming subreddits awhile back that will do a lot better than this simple example:   d  - Starts deleting  w  - Moves the cursor forward a word  dw  - Starts deleting, then deletes the  w ord   Since it is running from the terminal, I can run any command straight from my editor.  :!npm run lint ,  :!ps -ef | grep npm , ...etc. This can probably be done in other editors as well though.  I normally work in linux-type environments and ssh-ing, so I know I will have an editor I am familiar with that off the bat.  I do agree that with the default  .vimrc  configuration, it might feel that it isn't very powerful. But once you get a [configuration]( you like, it is really powerful.  And finally, I've used it the most so I am most comfortable with it.  If I tried switching to a new editor now, I'd still try to keep switching to insert mode to type. I'd be very unproductive for a long time. I'm a slow typer outside of vim since I keep trying to switch input modes and use the familiar commands/shortcuts.  tl;dr. Most comfortable; Keyboard > mouse (for me); personal preference. "
Your idiosyncratic desires are not law. Keep them out of programming.,"> I still don't see how what you're saying makes sense. Keeping ""my politics"" out of programming isn't a coherent concept. My example is illegal because of politicians making laws. It's by definition a political idea and a political law.  Your example is illegal because of the law, but your example is not representative of what people really mean when they talk about 'inclusiveness', which really means any measures that transfer power from the qualified to the unqualified (i.e. establishing a non-meritocratic standard for evaluating contributions). Whether a contribution has merit and whether it should ethically be used are two completely separate issues which you, and those like you, are attempting to conflate. More to the point: You are attempting to conflate them so that your personal and idiosyncratic definition of ethics will be accepted by those who are opposed to your authoritarian morality. That is what we mean by keeping your politics out of programming. No one is interested in having their project hijacked so you can push your personal politics.  > Is your position that programmers should be not just apolitical, but also amoral and don't care about laws because they are political things?  I'm saying that your personal political concerns, which do not have the force of law, have no place in any project. While some people may or may not agree with your views of how things should be done, ultimately people participate in a project for the sake of the project,  NOT  to push your political agenda. Your question is off-topic, and people who raise such questions have proven themselves to be without merit, biased, idiosyncratic, and authoritarian...the very qualities which occasion developers to ignore you lest the project be derailed by political machinations.  Put another way: Since such peoples' concerns are class-based or otherwise political, not subject to any objective standard, and ultimately rooted in a desire for power -- no, no one is interested in yielding power to someone to make arbitrary decisions about their project.  TL;DR Your idiosyncratic desires are not law. Keep them out of programming. "
'Programming' is too broad a field of study to dictate how much mathematics should be required of a student.,"I think the question should really be ""What kind of 'Programming' am I trying to teach?""  Computer science has left the period of the one size fits all education. Look at Engineering for an example. Who actually gets a degree in Engineering anymore? No one. You need to look at the sub categories: Mechanical, Electrical, Chemical, Civil, Automotive, Software, Electrical, Environmental, Geological, etc.  There should be separate degree programs for web development, application development, implementation support, database management, mobile application development, embedded software development, etc. It is incredibly irrational to teach all of those things under the umbrella of computer science and expect a person to be an expert in all of them when they graduate from a university.  So yes, math should be a requirement in Programming if the student is looking to have a future in a programming discipline that requires it, but if the need for mathematics is less, such as in a software design type of career, the answer would be no.  tl;dr 'Programming' is too broad a field of study to dictate how much mathematics should be required of a student. "
doctors aren't leaving because they're money grubbers  they're leaving because conditions are shit and only going to get worse.,"The main pushback has been because this is part of a wider attack by Jeremy Hunt on doctors, he has been driving conditions and morale down for doctors, plus there is going to be a real drive on less foreign doctors in the NHS, when we dont currently have the British born doctors to replace them. As a result of this push by Hunt to stretch doctors to breaking point, a lot have been moving to Australia et al, where they get a better work/life balance, better pay and more sun. Rather than improve conditions in the NHS, this is seen more as another punishment, and personally i think we will see a lot of doctors sit it out to the 4 year mark, and then leave, but an even higher percentage than currently. All this will do is leave us with a serious skill gap between experienced and more junior doctors.  Tldr, doctors aren't leaving because they're money grubbers, they're leaving because conditions are shit and only going to get worse. "
They chucked him because other speakers would have left if they hadn't. The article conveniently forgot about that.,"Even assuming that this article is 100% correct, true, and tells the whole story, the author went from ""an inclusivity group made a bad decision"" to ""thus the entire concept is a joke!""  However, the actual reason they made the decision is (from [here](  > Andrew Albright [9:54 AM] > > I think to sum it up, many conference speakers have spoken with him at other conferences and generally didn't like his attitude and condescending nature, and have openly refused to speak at a conference with him in the future.  Maybe some of those speakers accepted the Nodevember invite, but then discovered he was also speaking and were going to back out?  So, in fact, they removed him so they could  include  a number of other speakers who would not have spoken without them making that decision.  They chose to trade his talk, and presumably some good will from him and others, for several other talks and goodwill from another group. Seems like a valid and rational decision to me, even if I wouldn't decide similarly.  I'm not here to argue about this: this article is logically unsound, and didn't present the whole story. It's fairly worthless.  TL;DR They chucked him because other speakers would have left if they hadn't. The article conveniently forgot about that. "
Don't keep static assets accessible to your machine s  that run code if you are so concerned about security.,"Alternative solution... Store your assets on a separate machine to the one with PHP; then PHP cannot physically access the files only return URL's to them. Sure it requires a better server setup, sure it isn't as simple to do, but.... IT'S MORE SECURE!  How PHP gets to that server? Well you can use league flysystem to access a network filesystem (proviso being that you don't read from it or download and execute code).  Another alternative... Why are you handling your uploads with PHP in 2017? There are projects in C and Java that allow you to upload to an amazon S3, WebDAV, sFTP. You could use some other non runtime language to handle uploads. Suggesting we use PHP for everything makes as much sense as using legos for everything. Suggesting it all has to be part of a single app that also cares for business logic is just long term a PITA!  You don't even need to use a web-client any more; with integrations with dropbox for command line you could manage the assets from your mobile device or desktop and claim through an automated server in near real-time the data; the options are endless.  TLDR; Don't keep static assets accessible to your machine(s) that run code if you are so concerned about security. "
Vue  is the fucking cheese and I love it and you should too.,"Yo. I faced this exact problem about two months ago. I was using Knockout and firebase as my go for everything. My problem was keeping my code organized enough to maintain, and clean enough that I could effectively move team members to and from projects on a whim. I couldn't do it. But two months ago I switched to Vue, and in combination with Vue and the AirBnB style guide (crucial, you need need need a style guide for this to work well), I can keep my whole project/view model organized like this:  let viewModel = new Vue({     el: ""#selector-for-view-context"",     data: {           // anything     },     methods: {            // functions to bind to elements     }});  And there's a lot more but I don't wanna write it. You can read a way better explanation on their site which is the second best thing about them. Their documentation is unparalleled! It's amazing. Only took my team two weeks for a full transition from knockout to Vue.  On top of that, it has fantastic scalability. You can create custom elements called components that can be simple or incredibly complex and act as their own micro view model. I have been able to scaffold and make huge design improvements, feature expansions, and bug fixes faster than ever before.  It is so much better, in fact, that I am having an intern convert past applications we've made into Vue from Knockout so we will have a more efficient flow.  TL;DR  Vue  is the fucking cheese and I love it and you should too. "
Neither of these courses seem worth the price. But I prefer the second degree for job prospects.,"I personally don't know much about Columbia's professional school but I doubt the return would be enough to justify 70k. If you pay that back in 10 years with NO interest. That's $7,000 annually. You'd have to increase your current salary by $10,000 to make it worthwhile. I like Sir_not_sir's option of a cheaper online degree. This would give you guided course work will also fulfilling the need some employers have for a Masters degree.  I know it can be harder, but I would rather have you go through the computer science focused course because that's going to give you skills that other people won't have. Which tends to result in higher pay.  A non-educational route could consist of working for boutique investment firms for a bit. This will be lower pay but you can start developing analysis skills. Buy a book on data science and python (they are overpriced) but they help give something physical you can refer to and should be more of a jumping point. SQL is relatively simple language. (It will be the relational database portion that gives you the most trouble, not the code's syntax). Python and SQL together should give you a decent head start as you start asking for projects at work or self-starting some with management approval.  TL;DR Neither of these courses seem worth the price. But I prefer the second degree for job prospects. "
Stop thinking in Java and instead think in the language you're actually using.,">Properties look similar to variables, but actually it is possible for a property to trigger other code. This is not immediately visible to the reader.  It's not really a problem in my experience (worked a lot with Scala and C#, which both have similar here). It seems like a big chunk of the problem is simply being used to Java and similar. Once you're used to the fact that you're working with a new language, you're more aware of the possibilities of what can happen.  That said, side effects are rather rare in getters and setters. When they do occur, they aren't typically a problem. And ideally should be documented. Pretty much the only kinds of side effects I see are caching, logging, lazy evaluation, and conversion, anyway. Those are typically non-issue.  The key point is simply that a Java program would likely have the same side effects in a getter or setter. Just have to remember that properties  are  the equivalent of Java's getters and setters.  TL;DR: Stop thinking in Java and instead think in the language you're actually using. "
if you understand the version range and know that the range you specify will not contain breaking changes  you're good. Be specific when unsure.,"As someone who has had to painfully debug issues coming from redeploying legacy code configured to use the ""latest"" OpenCV from pip (2.x -> 3.x made the app shit it's brains), I agree that people  should not use overly general versions in their dependency files. Major API changes can (read: will) happen and can (do) fuck up any downstream applications.  I see a lot of tutorials telling readers to just use the package name for the package. This means that people learn to use the overly generic syntax and never really consider it until it becomes a problem. Perhaps we, as the developer community, need to start specifying more specific and thought-out version strings in our dependency files.  Version ranges make sense as long the package maintainer has a consistent version scheme (ex. 2.3.* will all provide exactly the same function) and the package consumer understands that and specifies it when setting up dependencies.  Tl;Dr: if you understand the version range and know that the range you specify will not contain breaking changes, you're good. Be specific when unsure. "
Overall pretty good.     Add Unit Tests     Add Licence      Personally  Remove Reflective API dependency,"The code looks pretty good.  On the code base itself, you have one global function included and that's my only gripe. You can probably find a way to put it as a Static on one of your classes. Since it looks like a factory, it would make sense to put it on the Cluster object as a Static factory method.  On a package level you really should look into PHPUnit and create unit tests for your package. Also you should include a Licence.md for the licence.  I see you using Reflectives to peak at the function signature... you might consider some other method instead as I think some sort of DTO might be more appropriate but I think it might depend on preference. I only mention it as HHVM vs PHP on the reflection API can be different which will lead to unexpected things happening.  TLDR;   Overall pretty good.   Add Unit Tests   Add Licence   [Personally] Remove Reflective API dependency   "
part of  good taste  is knowing how complex it  should  be,"This ""good taste"" seems to be not wanting to make it more complex than it needs to be - an important part of this is making it general, so the same thing can do more.  For example, another way to do the  remove  example is to have an extra ptr (instead of using the  &amp;  operator, which isn't available in every language). Though of course this is wasteful, by using an extra ptr.  You get this in a tree too: every node has a ""parent"" but the root... just give it a parent.  Crucial for this definition of ""good taste"" is knowing how complex it ""should"" be. One aspect ( not  the whole story!) is how much work it's  supposed  to do. In the second example, only the edge grid-squares need to be touched, so ideally, you'd just loop over them. To sense that, is part of having ""good taste"". It provides a motivation (or irritation...) for what would be ideal... for what seems like it  should  be possible, even if you can't see how.  In the article, huge strides are made towards this goal, which is intuitively and informally sensed.  It is possible to reach the goal, but at a cost. The goal is to touch only what is needed (the edges), butthe solution given touches each corner twice... Knowing this, we can improve it by reducing the loop by one, and offsetting each of the indexing expressiins appropriately. But it's a bit complex and ""clever"", because each one must be different.  In my opinion, this cost isn't worth it, and so the solution given is ideal. Although it's a tiny bit inefficient, its simplicity (through regularity) make up for it.  In effect, we distort the problem slightly, to simplify it, and then solve that simpler problem.  tl;dr  part of ""good taste"" is knowing how complex it  should  be "
we have amazing tools these days  and we should use them.,">  I'll never understand the mentality that says, essentially: ""I can write sloppy code because a tool will come along after and clean it up for me"".  You'll never understand it because no one actually has that mindset.  If I cut and paste some code (something I do quite frequently during a refactor), the formatting is going to be messed up, and I want my tools to fix it for me. It really is that simple. I'm not writing ""sloppy code,"" I'm just using a tool to automate code cleanup. Reviewers never see my code in the intermediate sloppy state. If you are not taking advantage of this, you are missing out on a huge productivity boost.  > Do developers today really not think that having good, consistent habits, whatever style they might be, is worth something?  This sounds like a ""good old days"" type of argument. Sure, having consistent code is good. But if I understand the inconsistencies and know exactly which inconsistencies the tool is going to fix, why would I not take advantage of the tools?  > Sometimes they use good variable names, sometimes not. Sometimes they comment functions, sometimes not. All because they know a tool will deal with it.  Tools do not deal with these issues. You might just be dealing with low quality developers. Don't blame the tools.  TL;DR: we have amazing tools these days, and we should use them. "
3.1 compute shaders seem really cool  but no one uses them because they're slow.,"I wrote some compie shaders in 3.1, and they were much slower than using fragmemt shaders for compute. I spent a bit of time trying to optimise it (mainly in proportion to dispatch), but I didn't need to at all for fragment shaders.  I noted that the specification doesn't specify  performance , only that the API is implemented.  I read speculation online that if you can get your code into fragmemt shader form, you'll get better performance, because it's been honed over generations of ussge and competitive pressure. Whereas the compute shaders are new (and frankly seem unused).  I would add that fragment shaders are by nature ""embarassingly parallel"". All of the intricate complexity that besets multicore CPU is avoided. But compute shaders, because they are trying to be more general, bring it all back.  tl;dr  3.1 compute shaders seem really cool, but no one uses them because they're slow. "
if you want to master your tools  not worth it  if you want to advance human knowledge  worth it.,"If you want to pursue a masters, you should understand that it's largely about broadening your  skills  in pursuit of a  specialization . What I mean by that is there are academic areas with hard/unsolved problems and the tools being used to advance human knowledge in those areas might be in python, or they might involve leveraging AWS infrastructure, or N other tools that are nowhere near React-related.  If you want to become good at something mainstream like React, a masters won't do you much. However if you want to work on a more R&D capacity, that's where a masters would help you.  For example, my brother pursued a PhD in computational linguistics. He now works at Duolingo. There are people there doing js exclusively, whose job is to make sure js things are written well and maintained well. My brother does some js sometimes too, and he does python and a bunch of other things, but those are merely a means to goals related to computational linguistics.  TL;DR: if you want to master your tools, not worth it; if you want to advance human knowledge, worth it. "
I'm really not sure what to think of this thing  but it certainly seems interesting and worthy of further discussion.,"Wow, this guy's arrogance managed to piss me off before I made it past the first page. And this is coming from someone who regularly defends Jonathan Blow from similar criticism, and also mostly  agrees  with the guy.  Seriously, being confrontational in your paper's abstract, calling out criticisms of the first revision for being the same, and still having the gall to call this a  paper  seems to have ticked me off on a very fundamental level. That's how I write reddit posts while drunk, not something I would put on my resumee.  All that said, he does seem to know his stuff, which might be part of what makes reading this so frustrating.   Anyway, looking past the language here, I skimmed the article. The actual reasoning seems (mostly) solid, the approach to domain modeling (if you would call it that) is interesting, and the benchmarks are impressive (if accurate).  Still, I would like to read some more about this approach, using a different example, as it is not immediately obvious how you would generalize this modeling approach.  Maybe I missed it while skimming, but this also seems to be lacking a comparison to the ""naive"" procedural approach. All I see is the ""naive"" Haskell version, and his own approach in C#, C++ and Haskell, all with multiple levels of performance optimizations.  Finally, concluding that this modeling approach will consistently yield performance benefits because of this one example (with one important benchmark missing!) seems like dodgy logic at best, attempted deception at worst.   TL;DR : I'm really not sure what to think of this thing, but it certainly seems interesting and worthy of further discussion. "
stick with Git  you won't regret it. Cool concepts don't always translate into  actually better .,"Git is great SCM tool. Fossil is more like all-in-one tool that happens to have a bit of SCM tied on.  They try to sell it as a benefit that it's SQLite based, but Git's data structure is designed for its purpose, and superior in almost every way.  As for having a Github-light bundled with the SCM, I guess that sounds cool in theory, until you realise that it's a pain in the ass to self-host that kind of thing, and that the Fossil version doesn't integrate with your CI or most other online tools, and is in many ways inferior to the real Github, because it only has a tiny developer team behind you.  TL;DR: stick with Git, you won't regret it. Cool concepts don't always translate into ""actually better"". "
develop  is to develop and  master  is for the  users   user developers   production ,"So that master always has a consistent and tested state.  In Develop everyone can merge in their feature and call it a day. At the end of the week the features get merged into master.  If anyone would check out master during that period (for example if you create a library) then they get a stable master branch right of the bat without having to worry about accidentally compiling half-finished features into their code.  What they have is a consistent product. When the new develop branch is merged, everyone gets the sanctioned and planned feature set, tested to work together and blablabla.  Or alternatively, with having a seperate dev branch you can safely pull code directly into production without any interaction, just clone the repo and compile, no lookup for the latest version.  TLDR;  develop  is to develop and  master  is for the ""users""/""user developers""/""production"" "
you rebutted nothing  and fail to deliver on what YOU promise.    1    EDIT  woops forgot link for  1    EDIT 2  some    someone   EDIT 3  formatting of edits lol,"Instead of making appeals to authority (I've got professional experience in an ML lab, does that make me an authority? I don't think so), how about you rebut the actual problems I pointed out?  To respond to your points:   Of course matrix multiplication is required, it's core to how neural networks work (repeated application of linear transformations combined with a nonlinear activation function). My problem is that your figures are at best misleading, and at worst wrong. You fail to mention adding extra layers, the activation function, or address how you get multiple outputs; combine this with a matrix multiplication figure that makes NO SENSE [1], and it's very confusing. (To be clear, you are multiplying a 1xN matrix with a 5xM matrix and getting a 1x1 matrix out. So for it to be sensible, the actual dimensions need to be 1x5 and 5x1. Which they clearly are not.)   Show your researcher friend my complaints and ask him or her if they're valid. Show a professor my complaints and ask him or her if they're valid. You're teaching from  intuition  instead of from  math , and so it's a 2-step game of Chinese telephone. To people that know the correct results, it's easy to skim this article and say ""this LOOKS right"", but as soon as you dig in you find it's just... not.   Medium lists this as an ""18 minute read"". Do you really have the gall to sit there and tell me that, with no prior experience in AI/ML, you can have someone read an 18 minute article and then give a 30 minute lecture? That's just an unbelievable amount of arrogance.   ""doing projects"" and ""reading research papers"" is not all there is to it, especially when you say on your own blog that you're ""still a junior developer"". My own experience in ML comes from research in a lab under the supervision of somebody who could  make sure I was doing it right . I will also say that people without extensive backgrounds in mathematics have a tendency to get the details wrong.   ""You expected too much from the essay"". Well, telling people they can give a college lecture about it is a pretty big claim, which you fail to deliver on. You set the bar for this piece as high as it must be, and you should take responsibility for that.    TL;DR: you rebutted nothing, and fail to deliver on what YOU promise.  [1]  EDIT: woops forgot link for [1]  EDIT 2: some -> someone  EDIT 3: formatting of edits lol "
numbers don't work that way  and for good reason,"Well, empirical evidence: when the bug was discovered, we had a few (minor) news stories written about it. When the bug was fixed, we had a few (minor) news stories written about that. In none of those did I see people claim that it had actually broken the code.  Alternatively, argument by popularity: All other calculators we checked evaluated it as (2*1)+3i. In math, there's a good argument that semantics should follow popularity.  All that said . . .  I don't see a good argument that implicit parenthesis should be added. We all know that multiplication and division take precedence over addition and subtraction. I don't see any rationale behind changing this to ""multiplication and division take precedence over addition and subtraction unless the addition and subtraction are part of an imaginary number in which case those take precedence"".  Also, even if we were going to accept that, where does it end? What if I do ""2*1+(1+1)i""? I mean, ""1+(1+1)i"" is a complex number; should I put parenthesis around it? Are we really making the argument that I can change the precedence of an entire formula by just slappin' an ""i"" on the end?  And if we're  not  making that argument, where exactly is the dividing line? Evaluate the following formulas: ""2*1+i+i"", ""2*1+2i"", ""2*1+(2)i"", ""2*1+(1+1)i"". It would make me extremely uncomfortable if these do not all end with the same value.  (To say nothing of the wrench this throws into standard algebraic manipulations; I don't want to live in a world where (2) cannot, in all cases, be safely replaced by (1+1)!)  Overall, you're proposing an exception for the sake of user convenience, but that exception seems to simply add huge mental overhead to convenience. It's one of those things that may be convenient in small isolated cases, but applied globally, becomes a catastrophic source of complexity.  tl;dr: numbers don't work that way, and for good reason "
Everytime you make a call with 4  charge left on your phone  thank a developer battling these demons.,"Excellent question. There are three aspects: knowledge and performance (twice).  User Performance: Now yes, there's the golden adage of slow and right beating fast and wrong - but performance matters.  A CPU's speed is driven by prediction: knowing the next fistfull instructions and taking them apart. An indirect calls like in the example (calling a function through a pointer) is at least a brutal speedbump. Since they are also a common pattern (virtual functions are implemented that way), compilers do go a long way trying to replace them with a direct call.  Knowledge The underlying problem is that C and C++ are actually pretty optimizer unfriendly: to make such optimizations, the optimizer needs to know  everything  that happens to  Do .  However, in the general case, the optimizer cannot know or understand everything that goes on in a non-trivial C++ - program, there are just too many (useful, necessary nd common) mechanisms to modify variables or the execution path.  So the optimizer falls back to the next best thing we can give it: assume that the program is actually correct. This allows the optimizer to ""reason locally"", i.e. in the scope of a function, or a single source file, or a set of source files.  Performance, dev side Now yes, for a trivial program like that, such a complete analysis is possible - and indeed, any decent static code analyzer would warn you.  However, such code analysis is slow and requires excessive memory - something we cannot afford into the compiler.  It's also still incomplete in the general case, so you'd have a compiler that suggests it  can  detect UB, giving you a false sense of security.   tl;dr:  Everytime you make a call with 4% charge left on your phone, thank a developer battling these demons. "
Let your professor know youre doing this  because he or she will find out and might be quite annoyed.,"Programming teacher here (not yours, but I do give a BMI assignment later in the semester.)  Please be sure to clear this project with your professor.  He or she may take a dim view of this project.  While I’m sure your intentions are well-meaning, you’ve just created massive headaches for your teacher, because now you’ve created a dandy tool for people who don’t want to do the assignment themselves.  I see my assignments posted online all the time, and it simply adds to the database of code I have to check against to ensure students are doing their own work.  When the instructor discovers that you have (good intentions or not) simplified cheating and increased his or her workload to prevent cheating, this could be a problem for you.  TL;DR: Let your professor know you’re doing this, because he or she will find out and might be quite annoyed. "
Atom is not quite as fast as VSCode but it's a lot faster than it used to be.,"Yeah, it is. I did some benchmarking on my system on Ubuntu 16.04 with the latest Sublime, latest VSCode, and latest Atom and the results were interesting. Sublime launches in less than 1 second, VSCode launches in 2 seconds, and Atom launches in 3 seconds. To me, the extra telemetry found in the core and especially addons of VSCode (some that cannot be turned off, without even any comparable alternative non-telemetry addons) is not worth the extra 1 second faster performance compared to Atom. I like Sublime for large files and Atom for everything else, even though Atom is a lot better with large files than it used to be.  TL;DR: Atom is not quite as fast as VSCode but it's a lot faster than it used to be. "
As already specified in this thread  different jobs are different and require different skills.   What's good for you is bad for me.,"I can only speak for web dev (that's where I live), but I try to avoid ""algorithmic thinking"" as much as possible.  The code we write and maintain is so high level that ""plain and simple"" programming tends to be much more effective and much cheaper because it's easy to understand.  Yes, I could shave 90ms off this backend request by combining these loops and adding a special case - but that incurs the cost of making the code harder to understand and harder to modify. Over the lifetime of that code, this could literally costs me thousands of dollars in developer hours while not improving the service in a measurable way.  In a worst case scenario where multiple optimizations interact with each other in 5 year old code, hundreds of thousands of dollars is a reasonable estimate in what that fancy loop costs.  tl;dr:  As already specified in this thread, different jobs are different and require different skills.   What's good for you is bad for me. "
WebStorm does pretty much everything. Atom does what you want it to do.,"I use both WebStorm and Atom. WebStorm for applications, Atom for libraries/general coding.  WebStorm is a fully-fledged enviroment, I can set it up to automate everything in that project. Need something compiling? File Watchers. Need to debug two processes? Debug configs. Need to publish? Connect. It has many features that would support you through a sometimes laborious development process, especially with scabalable applications with about up to hundreds of files. It's designed to cope, it just falls short when you wanna get on with things. There's lots of fiddling with lots of menus, and there's not much chance you'll use ever feature.  Atom on the other hand, is kinda like the LEGO of IDEs. You're given freedom to add whatever you want. Want a console? Add one. Want icons? Sure. Want a live reloading localhost? Here ya go. You can add in peices to make it yours, but it will never be enough to support serious development workflows. On the other hand, it's beautiful for just plain writing code. I can write code for hours in that thing: it has beautiful syntax highlighting for anything JavaScript I need, or even things like SQL, Markup, HTML, CSS. You can be more productive because it doesn't complain if you've spelt a word wrong and doesn't care if you're referencing things that don't exist (this can be done later in WebStorm or something).  TL;DR - WebStorm does pretty much everything. Atom does what you want it to do. "
end your suffering with python  check out lisp or haskell.,"> You can't override other parts of the language though  yes, shit that is literally hardcoded into the syntax, like keywords and literals. The problem is that human readable class names don't belong there, they are on a lesser layer of abstraction where the variable names live. They are merely identifiers that can be repurposed like any other identifier. Reserving some names would introducing additional exceptions you love so much for the sake of it. Who and based on what criteria decides which default identifiers are untouchable? what about frozenset? OrderedDict? Count?  > If the argument is you want things to be highly dynamic, why can't I override what () means?  I don't want them to be highly dynamic, they are highly dynamic. If that's a recognized feature of the language you can safely assume it's here to stay. And the feature of overwriting default bindings is widely used in unit testing, mocking and monkey patching.() is hardcoded in the syntax itself to always point to the ""tuple the class object"", unlike the word ""tuple"" that the language merely sets up to be the handle of the class at startup, not unlike something like this  tuple = that_builtin_class_object_doing_tuple_stuff  > I also don't see how this is a good justification for a literal!  ""literals are performant and/or a nice, compact, convenient shortcut"" is not enough of a justification for their existence?  tl;dr: end your suffering with python, check out lisp or haskell. "
Nothing is safe from a idiot user with the root password  but there are good reasons for why Linux is considered to be much safer than Windows.,"UNIX has a quite different security model than at least the old-school Windowses: (1) The user typically has to manually confirm that any new executable is OK to run / no ""autorun"", and (2) you're not running as admin, so any change to the configuration of the machine has to be confirmed via password.  While nothing short of a tight walled garden can protect a machine from a stupid user with admin rights, given a semi-competent user it obviously helps a lot.  Then there is the culture: UNIX OS's and the software that runs on them are designed to exist in networked, multi-user environment. This forced a much larger focus on avoiding security holes. Also, most UNIX users don't install random stuff from the internet, as most software (except from some proprietary packages) are provided by a package system similar to Google Play etc - since sometime in the 90s.  TL;DR: Nothing is safe from a idiot user with the root password, but there are good reasons for why Linux is considered to be much safer than Windows. "
abstractions are  fine   choosing good ones is hard    ,"Eh, the problems with changes are not those of abstraction, are they? I mean,  what's abstract  about the new code? It's almost exactly the same as the previous one, except that   there's a bunch of only-once-used string literals which have been moved out of sight (that's just obfuscation, really),   that path function, which is only used in one file, is moved to another (which is also obfuscation).    It's a case of taking a good idea (DRY) and driving it into the ground.  Here's a good rule of thumb for those literals: if you use a literal only once, inline a value and maybe add a comment why is that value used. Once you use it more times, give it a mnemonic. Same goes for that ""root"" directory, that was in the end changed from ""output"" to ""reports"". See? Abstract thinking (it's a  root  for reports) => less need to change the code!  A rule of thumb for that function:  keep it where it is at first. If used in two places, reference it from another place.  Only if  used in more places, give it it's own place to live.  tl;dr abstractions are  fine , choosing good ones is hard :-) "
Kint focuses on squeezing out all the information it can get,"Check out the live demo on the first page.  Where vardumper outputs a string, Kint recognizes it as base64 and automatically decodes it for you.  Then it recognizes it's json and automatically decodes it for you.  Then it automatically recognizes a color in the json and it shows you the color and all the different ways to represent it. Along with exactly what line of code you need to get the color out of the json out of the base64.  That said, I'd personally use both - vardumper is better for heavy objects since it's fast without configuration. In kint if you don't blacklist your IoC container and dump it by accident it'll take a while.  tl;dr: Kint focuses on squeezing out all the information it can get "
same situation  it worked out well  do more tutorials.,"That was basically my exact experience: no education for it, and failed my stats class. I ran into wall after wall and felt super defeated because I felt like I was missing that special something.  What ended up working for me was doing just about every tutorial I could find off 'python beginner tutorial', even if I knew everything on there. Then I'd find a new tutorial that introduced a new word like 'class' or 'method' and then go to town on  those  tutorials 'python beginner class tutorial'.  Eventually I built up a basic enough  knowledge set that I started doing other things that I could think of, like downloading images off imgur and making simple calculators, and whatever else I found interesting.  Now, five or six years later, programming is still tough, but I've got it down to the point where I can approach arbitrary problems and have a reasonable idea of how to  approach it, which is a lot of fun. Never took a formal programming class and definitely don't plan on it.  tl;dr same situation, it worked out well, do more tutorials. "
Any tool will produce  SELECT    by default. What else should be the default ,"Well, if you think in terms of relational algebra, where the  projection function π  is an operation like any other. You may or may not apply it to any given relation.  Having said so,  SELECT *  is just a verbose version of not doing any explicit projection, which is the default in the algebra. Except that for the algebra, it doesn't matter, as it doesn't worry about implementation details, or real world problems (much like exponential time complexity isn't a problem  in theory ). In the real world, on the other hand, computational resources are limited, thus the default is a silly idea, even if reasonable conceptually.  TL;DR:  Any tool will produce  SELECT *  by default. What else should be the default? "
Really solid IDE for those who are new to programming. For more experienced users with different preferences its bit limiting at least out of the box.,"So I did some testing and played with it for while now and i have some feedback.  Well Thonny is really solid simplified IDE for  beginners . It has pretty much everything beginner would need. UI is simple and all features are easily available and easy to use. Code-Completion works well and highlighting is good. Package manager works well for both bundled and existing installations of python.  How ever I find lack of color/theme options or customization off-putting. Also I personally would enjoy more aggressive code-completion so i wouldn't need to press hotkey to prompt suggestion box. Code-completion could include templates for defining functions/methods/classes and support for snippets. I feel like there should be support for file templates too. Also there could be more options to syntax highlighting. So I could have my numbers in different color than my variables and class names different color than methods. Overall I'd like to have option to control these things, but I understand that it's meant for beginners. Also I really hoped to be able to have splitscreen for couple files. Filestabs should be draggable so I could reorder them.  This review should be taken with grain of salt as I only used it for short while so this is only first touch feelings.  TL;DR: Really solid IDE for those who are new to programming. For more experienced users with different preferences its bit limiting at least out of the box. "
While I agree  be sure not to use solution in need of a problem.,"> Many people will defend their code that has no design patterns, no DI, no tests, all in one file and using mysql_* functions, refusing to adapt to actually doing it in a more scalable, maintainable and cleaner way.  While I completely agree with you for my usual sized project, on the other hand, sometimes we use canons to hunt ducks...  If I am doing a one/two page + contact box website for a local sport assoc, I am not sure I can justify the whole overhead. Most of it is HTML/CSS(/JS?) and a simple 2 fields form valid + PDO + php mail function... For once, manual test would be faster/easier \^\^  One of my earliest paid job was such a website, only 15 pages + 2 dynamic ones. The company made over 500.000.00$ a year with that simple site, without issues (unlike the wordpress of the sister companies).  Sometimes I feel like people want to find THE solution for every problems... Spoiler, like everything in life, absolutes never works.  When two people disagree on the best solution, more often than never, they aren't talking about the same problem...   TL;DR; While I agree, be sure not to use solution in need of a problem. "
take a crappy architecture  turn it into microservices and up with  everything talking to everything else  ,">A couple of years ago, we decided to completely change the architecture of our system  Uh oh.  >We used to have a monolithic application written in PHP, with a bunch of maintenance scripts around it.  Oh no.  >We decided to switch to a microservices based architecture.  headdesk  More seriously, ""microservices"" isn't some magic word that will rescue you from having a terrible architecture with loose cohesion and tight coupling. Trying to use microservices to fix a bad architecture just means you end up with an application that runs  even slower , due to the overhead of network communication between the microservices, and ten times the deployment complexity, because your services are independent in name only. In reality there are still tight dependency couplings, which means you have to be very careful how you roll out new code.  tl;dr: take a crappy architecture, turn it into microservices and up with [everything talking to everything else]( "
OP had a psychotic break and is writing insanity code. Read his book for bonus hilarity.,"You folks should buy this guys book.  He's basically a schizophrenic autist who had a severe psychotic break years back, and is transferring his delusions into his code.  In other words, he's skilled at programming, completely insane, and has developed insanity code as a result.  Seriously, buy his book. Just buy the book. Just GLIMPSE at it. It explains everything.  If any of you read the documentation on P5, you'll understand. Ahem.  ""If you believe in women having value, you must purely logically believe that P5 have(sic) value."" - Ch 8, Documentation for Phosphorus 5.  That's just a small taste from the documentation, and doesn't even come close to the book, which is basically about conspiracy theories, mental illness, breakdowns, selling one's soul to Satan, paranoia, torture (I'm not making any of this up).  Paste of the book:  Have fun, folks. As a part-time troll, I could not sit and watch you guys get (unintentionally) trolled to shit by this guy, as many of you are actually trying to comprehend his methods.  TL;DR: OP had a psychotic break and is writing insanity code. Read his book for bonus hilarity. "
What is the best open source ETL tool for Windows  Are there any with both GUI and source code development options ,"Hello r/datascience, I'd like to tap the collective wisdom here. I find myself in the somewhat awkward position of being a new hire in a ""big data"" team and being asked to recommend an ETL tool. I come from an applied math background and am way deep into R and Python, but I've never heard of all these ETL tools (e.g. Lavastorm, Pentaho, etc.).  My team members have different backgrounds with less programming experience and want (also for business reasons) to use a tool with a graphical interface. I on the other hand would like to use a slightly lower-level tool, e.g. Apache Spark (pyspark).  Is there a tool out there which is built on Spark which has a GUI and allows the user to construct models purely by writing code?  TL;DR: What is the best open source ETL tool for Windows? Are there any with both GUI and source code development options? "
looking to apply to datascience jobs in the industry. want to understand the tool ecosystem.,"Over the last few weeks I have been taking courses on datascience / machine-learning and would like to get my feet dirty with some practical examples. However, I'm new to the ecosystem and have been hearing of a lot of tools for ML/Datascience purposes. Ideally I am looking to apply for jobs in this field and will probably have to adopt toolkits adopted by the company. But what are the favoured tools used by the datascience community and by the industry? I hear of Tensorflow which appears to be a hosted service by Google. There is also Databricks. Are these hard to setup? Expensive in the long run? What is a good comparison of features across these services?  It would be nice to use a managed service that provides me with the libraries I would like to pick up from github. Also github has lots of examples with Jupyter/Ipython notebooks. Is a laptop installed version of Jupyter sufficient to tackle industrial problems?  Any pointers would be extremely helpful. Thanks!  tl;dr: looking to apply to datascience jobs in the industry. want to understand the tool ecosystem. "
did some python magic and used nonfree software to create better google assistant support  maybe someone could improve upon what i did.,"How they do it EDIT:   Quick Rundown: Following this [tut]( i enabled google assistant for pc, but i found it to be lacking. For one it lacked the most basic feature: voice activation. Immediately i thought of the program Voiceattack. but the problem was hooking into it with python, which the assistant api is built on.  I took this [script]( from the assistant sdk and modified it like [so](  Next I added a little cmd command to the mix:  py -c ""from distutils.sysconfig import get_python_lib; from urllib.request import urlretrieve; urlretrieve('file:///C:/Users/austin/Downloads/assistant-sdk-python-master/googlesamples/assistant/__main__.py', get_python_lib() + '/googlesamples/assistant/__main__.py')""  (you can use this, substituting it with your own paths)  and then i set up voice attack. using some simple batch files i had voice attack open and close python/cmd/conhostI had previously had the python script run a keystroke: ctrl-shift-alt-L, to hook into voice attack, this ran a command that closed the aforementioned processes.  Finally I had it working, with some tweaks to the timing so that it never misfired, it was ready to go. and it works pretty good, albeit it is a little ghetto and could use a lot of simplification/refining. perhaps someone could use what i did and create a better application with its own voice recognition/hotword detection and even a GUI?  TLDR: did some python magic and used nonfree software to create better google assistant support, maybe someone could improve upon what i did. "
Almost a recent stats oriented grad unsure of what steps I should take in the next 4 5 months to prep myself to enter the tech  data science  field.,"Hey everyone, I am about to get my B.S in Applied Mathematics and Minor in Statistics in 5 months from one of the top schools in the University of California system and I am at a loss as to what sort of preparation I should be doing for any data science jobs that would qualify for after graduating. I have experience in R, Stata, some minor experience in C++ and a basic ability to write in SQL and Python. I also completed a 10 week internship with a utility company that taught me some, albeit basic, methods for implementing machine learning to produce predictions models for large datasets.  My questions are more specifically phrased as follows:   How should I improve on my Python skills with respect to statistics applications? Nearly all of the resources I find online about improving one's Python skills with respect to the tech industry focus more on the coding aspect than on statistical analyses. For example, most of my CS friends tout the Cracking the Coding Interview book as one that I should know cold yet it does not focus at all on statistical applications. Should I be utilizing these resources?   What kind of tech jobs with a statistics background do not require an MS or PhD? I am not sure that I would want to pursue either in the future as my theoretical math skills and understanding are rather shaky. Yet nearly all of the jobs I find on my university career site are oriented to individuals with MS or PhDs in Stats or CS.   What kinds of career fairs should I be attending? The general one by my university has companies that are generally not looking for very technical roles for their hires yet the CS career fairs naturally are looking for very technical CS students.    Overall, should I be doing the same things as my CS friends to make myself more attractive to tech companies (ie interview prepping coding problems, hackathons, attending technical career fairs oriented to CS majors, etc.) or should I be focusing on something else entirely?  Generally, I have an interest in predictive modeling but I am unsure of what I generally should be doing and internet, and university resources, give many conflicting answers on the topic.  Thanks for all your guys help!  TLDR: Almost a recent stats-oriented grad unsure of what steps I should take in the next 4-5 months to prep myself to enter the tech (data science) field. "
Pick popular frameworks for companies  or doesn't it matter that much ,"I'd like to start a discussion on what frameworks you would pick in a corporate environment. The developers I work with here are not always as passionate in exploring new technologies/frameworks as I'd like (no offense to them personally), so there's not much help from their side.  Currently our stack is AngularJS 1.5 + Spring Boot and that works fine for most projects that are currently being maintained, despite the typical angular legacy issues. This year a few new projects will start & we'd probably should start moving away from the 1.X angular branch... Not that I think there's something wrong with 1.X, but boilerplate wise there are better options now.  Now, personally I've no big issues with whatever component framework a company would use:   React (though I think it requires disciplined teams?),  Angular 2.0 (feels rather bloated for smaller projects, but cannot judge as I have not yet tried intensively),  Vuejs (Rather impressed by the tooling and ease when coming from angular 1.x)  Ember (Also impressed by CLI)  Aurelia   They all feel similar but put their focus/implementation on different areas, so there's always a learning curve but I think it's fairly easy to get going with either of them once you've tried a few. Though part of my question here is if you or your development team feels the same?  Do you as a dev or lead prefer to use the seemingly popular mainstream projects (Angular 2.0 / React), do you think it'll hurt your career or company in the long term when using one of the lesser known frameworks? e.g. You will not be hired because you never used X, or you won't find devs anymore that are willing to do Y?  Currently I'd like to introduce VueJS since I've used it on a project for another client, and while it was simple CRUD, I was impressed by the tooling & easy learning curve when coming from angular 1.X, so I think it would be easier to pick up for the other devs but I'd like to avoid getting the company stuck with a framework nobody would like to use in a year from now on.  TLDR: Pick popular frameworks for companies, or doesn't it matter that much? "
starting data science internship  what can I learn to  be ahead  of expectations ,"Hello, let me begin my saying I'm a junior in college currently working on a CS degree and I was offered an internship position for the summer in the Data Science department of a very large health organization. I've been doing a bit of research on what Data Science really is, as I was quite clueless before. To preface, I have a good knowledge of C and fairly good knowledge of Python (I've built a couple web crawlers and smallish scripts/petty programs for fun). So I have a somewhat decent grasp of programming but I know data science is a whole different ballgame.  I really want to do my best to not be the incompetent newbie who'll need everything explained to him(although I'm sure I'll still need alot to be explained) but I would like to have some miniscule advantage where I could at least say ""Oh, I've learned a bit about that"" so I'm not completely clueless. Basically, what books van I read, what projects can I work on, what software or languages should I learn, so my supervisor can look at me and say ""hey this kid's done his homework.""  I'd contact my supervisor directly to ask him ahead of starting this internship but the recruiter told me they'd give me that info a few weeks before the job starts in early June. So in the meantime I'd like to get a leg up,if possible, and I was hoping maybe sone fellow data scientists/supervisors could tell me what they'd recommend a new recruit to do/learn.  Any feedback is appreciated, thank you.  Tldr: starting data science internship, what can I learn to ""be ahead"" of expectations? "
Inexperienced data scientist at a fintech company with small data  recipe for disaster ,"I recently got an offer from a young fintech startup looking to expand their machine learning capabilities in making predictions about clients. I would be the sole data scientist (working with the CTO). The previous data scientist is heading back to academia.They seem to be doing well and have raised funding.  I, however, only have ML experience from a master's degree where I used Bayesian techniques and some ML. They also only have access to data in the range of 100s of clients' data (clients are companies).  I'm afraid that they will be expecting magic. How much of a risk is it for an inexperienced guy like me to tackle this challenge? They seem to have a lot of domain knowledge (and an existing model they want to improve) and Bayesian methods may be the way to go with the small data set.  TLDR; Inexperienced data scientist at a fintech company with small data, recipe for disaster? "
Is there a way to block bad crawlers  Is there a script to send fake information to crawlers  Thanks ,"My friend has a company that gathers information in a certain topic. He then noticed an enemy company scraping his information. Basically, his company is doing all the work and the enemy company is just leeching off. It seems like they're using bots/crawlers because we tried making a fake page and it automatically generated a page on theirs, too. Anyway, I suggested to him using ReactJS for the pages because I thought that their bots/crawlers wouldn't be able to handle them. Funny enough, it worked, a week and a half passed and none of his pages have been copied. We talked and thought that it's just a matter of time before they can find a way.  TL;DR: Is there a way to block bad crawlers? Is there a script to send fake information to crawlers? Thanks! "
Got caught introducing best practices without asking permission. How would you convince your boss that libraries  frameworks  and unit testing are good ideas ,"I work for a very small shop that does some in-house projects, but most of our bread and butter comes from client contracts.  My boss, our CTO and founder, has a very negative view of frameworks, unit tests, and other things that I would consider best-practices.  I have been pushing to get us to adopt best practices (2 years of pushing got us on git!), but I'm treated like an excited youngster swaying in the wind of Javascript's flavor of the month club.  I used Symfony's router and HTTP components for the MVP of a REST API I built for an in-house project that's not yet in production. I did it to save time, as well give us flexibility as the application grows (we're still defining it). I also wrote extensive documentation for anyone else that might have to work on it. When something broke, he went in to go fix it himself and then sent me this message:  > This is why I don't like frameworks. I really don't have time or desire to learn all this shit, so tag you're it.  There's no one in our organization that knows this except you.  All feelings about being disparaged and abandoned aside; I'd like to make an argument that always writing an untested, single-purpose router from scratch, or using the antiquated procedural GET-param router that one of our senior devs copies and pastes in his projects is insecure, inefficient, inflexible, and difficult to maintain.  I feel like continuing this way hurts our ability to compete, or attract talented developers. I also feel like developing in this fashion is proving all the PHP critics correct, which turns my stomach.  Have you ever had to argue for something like this?  How would you handle it?  Can you recommend some good resources to bolster my argument?  TLDR;  Got caught introducing best practices without asking permission. How would you convince your boss that libraries, frameworks, and unit-testing are good ideas? "
got my first data science job finding white space. I'm tired of going home with nothing to do. Help me out please ,"Hi guys. Long time lurker first time, poster here. I just got my first full time job and it's as a CRM Analyst. Basically what i do is I'm the admin of a lot of tools the company has to find whitespace and enrichment of data. Read: I've always got to be clever in the way i pull lists.  I'm also tasked with filling in whitespace on my own, and let's just say it's a lot to do by hand. I love my job and they didn't throw me a shit assignment to be mean. They knew i could innovative.  I was wondering if there are skills i need to be brushing up on at home. We use Qlik View at work and Obviously Excel and I'm the only one with some R and Python experience. Qlik is a pain in the ass, but i  I found some Udemy courses that will help. i turned my hobby into a job and now I'm in a rut of what I need to be doing to stay sharp and above the fray.  Anyway all help is appreciated. Tldr; got my first data science job finding white space. I'm tired of going home with nothing to do. Help me out please? "
What is your favouite data science tool and why ,"I'm an undergrad seeking a career in Big Data Engineering/Data Science/ Big Data Analytics/ (Insert any buzzword floating around on the internet here), and just wanted to know which of these tools are your favourites.  Excel: So I personally don't use excel much for data analytics in general, but the IT major at my university SWEARS by Excel. I don't know if it's an ego thing or what not, but the professors usually aren't CS people, and swear on everything that Excel is all you need for big data analytics.  R: My STAT classes usually require us to use R. I personally love R, as it's my tool of choice early in my undergrad career.  Python: So I've heard that Python is on the up and up for Data Science, and may soon overtake R. Do you guys think this is true, and what are your thoughts on Python vs R?  SQL: So I've kind of had some exposure to MySQL, and created some basic tables and queries. But may I ask, what's the difference between Excel and MySQL exactly? I feel as if Excel does more, so if that's the case, why is SQL such a big deal in data science?  Any feedback, and addition of any tools I didn't mention would be great! Looking forward to great discussion!  TLDR: What is your favouite data science tool and why? "
Would Preact essentially be at risk with Facebook for sharing close similarities for copyright infringement ,"Hello Reddit,  this maybe a really dumb question and my knowledge of copyright is pretty limited (E.g: find the best license in my case and copy/paste into my project) and I don't know if this is the right subreddit.  Anyway, let's say hypothetically Facebook get's hell bent on Preact and want's to shut it down. Would we now have to worry about Preact possibly getting targeted by Facebook's Lawyers? I noticed at times it usually ends up being who ever has the most money wins. So I can maybe see this as a possibility, including that Preact and React share very close distinctions.  TL;DR:  Would Preact essentially be at risk with Facebook for sharing close similarities for copyright infringement? "
I have to pick a skills track for a webdev bootcamp in Java or Javascript. Which is a potentially better language to learn for an aspiring DA DS    Thanks in advance ,"I'm going through a webdev bootcamp that's offered for free by a nonprofit in my city. The course is taught in Python for the first two units and then for the last unit we pick either Java or Javascript.  My research on this subreddit leads me to believe the answer is Java, but I couldn't really find any reasons why not to pick Javascript mentioned on this sub (yet I'm sure there are plenty). Is it just a no-brainer which is why I only see Java mentioned as a potential pick?  I know neither of those two languages are really recommended for DA/DS, but I don't have a choice and it's free education so I'm not passing that up. I am marginally comfortable with Python and R and am continuing to learn with them. Will be tackling SQL in the future as well.  I really don't have an interest in being a front-end web developer for even short-term, but I do think it'd be nice to be able to make web apps, interactive websites, etc. for nice polished data visualizations at some point. But I really am just interested in building up my skills for working with data — data viz is not something I want to prioritize over analysis skills.  Also, I will likely pursue an MS in CS in the near future if that helps. So if you think one language over the other would be better for an application and just for preparing me for rigorous CS courses, let me know!  Oh and if it helps, at some point I would love to learn some data engineering skills just so I can be well versed and potentially able to create and maintain my own data infrastructure.  So keeping all that in mind, I'm curious what you much more experienced DA/DS programmers think I should do.  TL;DR I have to pick a skills-track for a webdev bootcamp in Java or Javascript. Which is a potentially better language to learn for an aspiring DA/DS?  Thanks in advance! "
How hard is it to take a script that relies on standard python packages and port it to run on Google's Cloud ,Hey all! First time poster long time python user. A couple months back a landed an interesting consulting gig that's transitioned into an ongoing partnership. I'm actually in the process of renegotiating the terms of the agreement which is was brought me here.  Basically the client has requested that future iterations of software I built (entirely in python) run in google's cloud. Now I have 0 experience using the cloud but from what I can tell it's at least possible. Just wondering if anyone here has experience in taking a project from your laptop to the cloud and all the headaches that come with that.  The tech uses another google api and most of the standard ML libraries so I'm pretty sure everything's supported. The script I use manages a small file directory and requires internet access as well but this seems like it's supported. What kind of time commitment should I estimate? Basically everything boils down to getting a single script to run but my instinct tells me this could be a nightmare.  TLDR; How hard is it to take a script that relies on standard python packages and port it to run on Google's Cloud? 
why are there so few data science positions in Austria ,"Hello guys. I'm an aspiring data scientist who would like to move to Austria and I've been lately looking for jobs related to data science but there seems to be very very few.  Does anyone here in data science in Austria and can tell me if it's normal that I can't find many positions? I've used LinkedIn to look for jobs and I can only find half a dozen at most. Seems quite strange, but maybe it's something that's not very mainstream in the country?  And also, in general, would you say you need to know German to be hired?  tl:dr why are there so few data science positions in Austria? "
how do you break into data science right out of college ,"Hey, guys! I'm a freshman at Rutgers University, double-majoring in Finance and Business Analytics & IT (BAIT - similar to MIS, except with a bigger focus on statistics,) and I've recently discovered my passion for data science.  I've pursued this by mastering many aspects of Python, R and SQL (some of which my major, BAIT, goes into, but the rest of which does not,) and am most recently taking Kirill Eremenko's Udemy course on Machine Learning. After completing this and obtaining the certificate, I plan to familiarize myself with descriptive and inferential stat theory, as well as Spark and Scala.  My only concern is that I won't be able to break into the field right out of college. I've taken the most relevant major possible (double major in CS and stats would be more obvious, but more of this is theory than any real application to data - BAIT comes close to that,) and am doing everything I can with MOOCs, but will that be enough of an analytical background? Is there any way at all that I could become a data scientist at 22?  TL;DR: how do you break into data science right out of college? "
Master's in EE  only have a couple of real courses of data science under my belt. Feel completely inadequate for any job listings.,"I'm a recent grad with a master's degree in EE. In my undergrad years (also as EE) I never really knew what I wanted to do. I got an offer to continue on into the masters program at my school, which I took. It wasn't until the past year I ""discovered"" that I was interested in data science through a class I took, so I took any relevant classes I could at school while finishing my degree. Most of my degree consisted of theoretical classes (optimization, LP, stochastic processes, etc.) that I did not particularly enjoy. I much preferred the more practical/hands-on project-based courses of the couple of data science courses I took.  Well now I'm graduated, and looking for a job like many people are. I look at the requirements for data science jobs and just feel woefully inadequate. If I'm lucky they're asking for only 1-2 years experience, but I look at my data science experience: I took 2 courses focused on data science, and in those courses I completed 4-5 assignments/projects. And that's pretty much it. I feel I'm familiar with many aspects of the field, but no expert in anything. I've looked at data science interview sample questions, and I feel after some studying/review I can at least give some sort of reasonable response for a good number of them. I've taken classes on statistics and probability and dabbled with machine learning algorithms. I have used Python/R but have never used SQL, so I've been spending time trying to sharpen my coding. I read all sorts of blogs and stuff saying you have to know/use github, Hadoop, jupyter, etc. which I have also never used (I did my school projects just using spyder and Rstudio).  I've looked into data science workshops/bootcamps, but I've also heard that programs like Springboard merely compile information/resources you can access online. I've looked into Kaggle competitions and feel overwhelmed. I know I need more experience, so I guess my question is, how much experience are we expected to even have to actually get an interview/job? Should I just go for something like a data analyst just to get some experience before going for a data scientist position further down the line? I've considered internships but the prospects for an advanced-degree grad don't seem to promising.  tl;dr: Master's in EE, only have a couple of real courses of data science under my belt. Feel completely inadequate for any job listings. "
I want the domain my script is loaded from to be allowed to access my API  not necessarily whatever domain it is loaded from.,"I'm working on a script thats embeded on a page (any domain, not always owned by me, call it devdomain.com) that is loaded from my server (load.devdomain.com) that I need to allow XHR calls to api.devdomain.com. It seems like the API call works in the network tab, but then fails in the console with this:  > XMLHttpRequest cannot load  The 'Access-Control-Allow-Origin' header has a value 'api.devdomain.com' that is not equal to the supplied origin. Origin ' is therefore not allowed access.  I want to allow access to the API from origins api.devdomain.com and load.devdomain.com. My understanding is that since the script is loading from load.devdomain.com, I should be able to allow that origin and it work, but it looks like its trying to get permission on the domain not owned by me (devdomain.com). Is there a way around this? Its my assumption that this is how stripe does it's javascript api calls, only allowing calls from javascript loaded from their domain. Am I doing CORS wrong or is it maybe only a problem because I'm not using HTTPS in dev?  TL;DR: I want the domain my script is loaded from to be allowed to access my API, not necessarily whatever domain it is loaded from. "
need to be ping 2000  hosts  want to do it the fastest and best way in python3.,"So the problem I am looking to solve -I want to send a ping to 2000~ devices every 10 seconds, and then store that data in a database.  I'm good on the database storage side etc, but what would be the best way to go about this in python?  I am assuming one host probably can't keep up with that many operations because the GIL will be blocked?  For the pinging itself I could use subprocess/os.system or  Would this be a good opportunity for threading/multiprocessing?  I'm using Python3, and really don't want to move to Python2 for this project.  I hear twisted is good for distributed workloads so I suppose I could put this into a pub/sub type of system.  Thoughts and guidance is appreciated!  TLDR:  need to be ping 2000+ hosts, want to do it the fastest and best way in python3. "
Looking for Flask Django exercise prompt project to show off my Python skills that I could finish over the weekend.,"I just had a first-round interview that asked me to follow up with some Python code I've used. I followed up with a recent JavaScript project I did that I think they will find impressive, but the position is for mainly Python so I want to show my skills there too.  Most of the Python coding I've done has been for internal use at the company, not production level, and the HR guy I talked to was concerned that my Python experience was too data science oriented. I know I have solid Python skills and want to prove it with a project.  I've built web apps with Flask before, but don't have them anymore because they're all property of the company I used to work for. I was wondering if there were any good Django or Flask exercises out there that I could do to show I know what I'm doing. Thanks!  TL;DR - Looking for Flask/Django exercise/prompt/project to show off my Python skills that I could finish over the weekend. "
how do I put labels on axvspan commands and how do I put them in my plot's legend ,"Hi all, I'm fairly new with Python and would really appreciate some help with something that may be very basic, but I haven't found any way of doing so far. I apologize in advance if this is too much of a ""newbie question"", but I don't really know where else to look for help.  I am plotting two curves and using  axvspan  to shade two sections of said plot; and I would like to put a legend with both my curves and the two shading sections too. How can I do this? I tried searching for it but couldn't find anything very helpful...  Thank you so much in advance to anyone that offers some help to this beginner!!  tl;dr; how do I put labels on axvspan commands and how do I put them in my plot's legend? "
Are there any Python IDEs can be run on a server  accessed through a web browser  AND are integrated with git    Thank you so much ,"I'm trying to get more people to use python where I work, so I'm looking to figure out the best software environment. The necessities are this: all code is stored on remote servers and never gets stored on the drives of local machines, there is integrated ability to version control (specifically git), and the programming happens inside an IDE.  I have been using Jupyter running on a remote server, then ssh port binding to that instance of Jupyter from my local machine to edit notebooks from a local port, but there is no version control integration with jupyter and git doesn't play nice with .ipybn files. My thoughts now are to mount the remote server to the local machine with sshfs, run pyCharm on my local machine, and access the .py files via the mount, doing version control through pyCharm. So, use .py instead of .ipynb.  Ideally, we would have it set up like RStudio-Server is right now where RStudio is always running from the remote server, we can access it via a browser, log in with our user credentials, and start working with files on the server through the web browser, using git through RStudio. Are there any Python IDEs that allow this remote access via a web browser AND contain a version control system?  TL;DR: Are there any Python IDEs can be run on a server, accessed through a web browser, AND are integrated with git?  Thank you so much! "
Do I demonstrate the skills one would expect for predictive analytics  or what do I need to develop ,"Quick context: I have been involved in higher education research through internships, graduate school and working at two community colleges over the last 10 years. In that time, I conducted a logistic regression as part of a program evaluation for my master's thesis, helped determine significant factors of enrollment using logistic regression, used multiple regression to create residuals that would be used in part of a process to predict enrollment for the fiscal year, and employed multiple regressions as part of my path analysis in my dissertation examining factors that may impact online community college students' decision to remain enrolled in college. However, when one of the interviewers asked me yesterday if I had ever tested any of these models, I did not really have an answer. I was always focused on r-squared values for model fit as oftentimes I did not have other data available to test out the results other than what I used to create the model.  I am concerned that I overstated my ability to do predictive analytics and am wondering if I should remove it from my resume. What do you expect someone who lists predictive analytics as an area of expertise to be able to demonstrate? One of the problems they would expect me to answer on this job would be to look at factors that predict college enrollment and to be able to test the model. Is my knowledge of how to conduct regression analyses going to be enough, or would I need a full data science background through extended training?  My passion is to use data to help tell a story to make students successful and I thought that enrollment management analyst would be a great step in my career, but after the interview yesterday I am wondering if I am promoting someone I am not, but I may also be overreacting as well. Thanks for any advice you can provide.  TLDR: Do I demonstrate the skills one would expect for predictive analytics, or what do I need to develop? "
Receiving my masters in chemistry and want to move into data science. Would this boot camp be beneficial ,"As mentioned in the title, I am receiving my masters in physical chemistry this December. The past few months I have become increasingly interested in programming and transitioning into data science and have been teaching myself python (pandas, matplotlib, NumPy), unix and SQL through MOOCs.  This boot camp interests me specifically because it seems cheaper than most, it is online so I don't have to wait to start or forgo working to participate and they guarantee you a job in 6 months or your tuition is returned (probably some fine text here). Would this be beneficial to pursue considering I already have an advanced degree in a scientific background? Anyone have any experience with this particular program?  Tldr: Receiving my masters in chemistry and want to move into data science. Would this boot camp be beneficial? "
Veteran educator wondering if Python is better than Ruby for marketability outside of classroom.,"My question is problematic but I'm going to ask because when Google sees ""educational"" and ""programming"" it thinks I want to learn programming.  And I do... I've been dabbling with Ruby for a month (my last language was Basic on a C=64 30 years ago).  Lately, when I'm in the classroom, I really can't get around how anachronistic our profession is.  Technology will (and should) absolutely eat our lunch.  Right now schools are in bed with Google.  We buy the Chromebooks.  We've got everyone on Google Docs.  We've stopped buying MS Office licenses.  We've shut off over half our local servers (urban district: 70 buildings, 20k students).  So my thinking (as a neophyte) is go the Python route.  I'm not interested in start-ups (Ruby's strength?).  It's likely leading Tech Ed companies are going to use Python because Google does.  I might never be a full stack developer, but with a M.Ed and a strong understanding of tech and a working understanding of Python I feel like I would be marketable.  I should also add that, since I need to learn computational thinking and the basics of CS, I'd like to avoid JavaScript for this first go-around.  If that's unrealistic, please advise but I've heard the syntax and logic alone tends to bury newbies (and my brain isn't exactly suited to math, computation, logic to begin with).  I concede that I have no idea what I don't know.  Your thoughts are appreciated.  tl;dr - Veteran educator wondering if Python is better than Ruby for marketability outside of classroom. "
I need to request data in the every few seconds   should I use threads to get around the otherwise performance impact ,"This application is supposed to monitor changes in data from an API server while providing full functionality. In the case of changes in the values of one of the many requests' response objects the view will be updated.  I've toyed around with using a timer to make the requests every n seconds. The issues with that obviously being a dramatic impact on performance.  Should I use threads in order to accomplish this? The issue I saw with a threading package I checked out is that it lacks the ability to send messages from child threads to the parent thread.  Any thoughts would be appreciated.  Tldr: I need to request data in the every few seconds , should I use threads to get around the otherwise performance impact? "
