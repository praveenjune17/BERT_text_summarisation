Summary,Document
"Predict service time from features such as service branch ID, technician ID, and location of service call.","We use features such as machine id, call priority level, hour of day, day, month, year, state, zip code, service branch code, customer type code, and representative id to predict a range for time required to complete the service call.We took a multi-class approach as we are interested in predicting a broad range—a 15-20 minute difference is acceptable—to get an approximate idea of service call duration. The duration ranges are: 0–179, 180–359, 360–719, 720–1439, 1440–2999, 3000+Moreover, among other things, the range of service call duration  was very large, and exact duration was not required. Therefore, we did not take a regression approach."
Predicting Flight Delay Demo Experiment - completed Preprocessing Stage to use within UK Azure ML workshop and end to end experiment ready to produce a web service,This is a completed Preprocessing Stage experiment that is used during the UK Azure ML workshop. The aim is to build on the clean data set to create an initial machine learning two class classification model.By using Flight and weather data to predict whether a flight will be delayed by more than 15 mins or not. Using Supervised learning and Binary classification we can start to say if a flight will be delayed. This experiment is part of a larger lab document delivered at Microsoft UK Data events  you may modify the HDInsight cluster template to decrease the number of workers provisioned. Azure Machine Learning WorkbenchFollow the quick start installation guide to install the program and create a workspace.
New Loan Granting Binary Classification,"After completing the courses in the Microsoft Professional Degree Data Science Curriculum, it’s time to put all that you have learned into practice. Before entering this competition, you should register for the Data Science Professional Project  course on edX and review the detailed information there.This competition concerns loan data. When a customer applies for a loan, banks and other credit providers use statistical models to determine whether or not to grant the loan based on the likelihood of the loan being repaid. The factors involved in determining this likelihood are complex, and extensive statistical analysis and modelling are required to predict the outcome for each individual case. You must implement a model that predicts loan repayment or default based on the data provided.The dataset used in this competition consists of synthetic data that was generated specifically for use in this project.The data is designed to exhibit similar characteristics to genuine loan data."
The experiment classifies whether a diabetic patient is likely to be readmitted or not based on the past record of the patient.,"The experiment uses a dataset containing columns like age,weight,medical_specialty,time_in_hospital,number_inpatient,and various medicines prescribed to them.First,the data is preprocessed and normalized then it is split into training and test.Then various classification algorithms are used on the data .They are Two class Logistic Regression,Two class Neural Network and Two class Boosted Decision Trees.Permutation Feature Importance is also used in the experiment to eliminate features with minimal importance and hence reduce the dimensionality of the data.CrossValidation is performed too. As it turns out from their AUC score,Neural Network and Boosted Decision Tree perform slightly better than Logistic Regression on this data."
Loan data analysis using a classification model in Azure ML,Here’s an AzureML model to analyze loan data. It uses a two class logistic regression algorithm for binary classification. This is based on Python based sample from learnds.com. It leverages various AzureML Studio components as well as custom R code for data cleansing and feature engineering.
This is a demonstration of how to use ML Studio to complete a sentiment analysis based on tweets data.,"We are gonna to use tweets data and stopword dataset to train a model to predict the sentiment. We will first use python script to preprocess the raw file and then use feature hashing to convert the data to numeric value. This is a binary classification problem, a two-class boosted tree will be a good choice as training algorithm"
"This experiment demonstrates how to use the ""Create R Model"" module to train and score a model, and use ""Execute Python Script"" to evaluate a model using breast cancer classification as an example","This experiment demonstrates how to use the Create R Model module to train, and score a naive bayes classification model using the breast cancer dataset, and use Execute Python Script to calculate performance and plot the performance curve. Similar to Execute R Script module, the Create R Model module allows a user to train a model using R scripts. In addition, it also allows the user to save the trained model similar to how other built-in machine learning models can be saved. This function enables the user to apply the trained model  into the scoring experiment.Create R Model module can be used if the user wants to train a model available in a R package. The user needs to provide two scripts, ""Trainer R script""  and ""Scorer R script"" . In the ""Trainer R script"" box, the label and feature columns can be extracted from the input dataset using get.label.column() and get.feature.columns() functions. In this sample experiment, we train a binary classifier using Naive Bayes classification method. In the ""Scorer R script"" box, the user should generate a data frame with predicted class labels with corresponding probabilities on the input dataset using the input model."
Blank template project for using Jupyter Notebooks in Azure Machine Learning Workbench.,"Welcome to Jupyter Notebooks in Azure Machine Learning. For more information go to How to Use Jupyter NotebooksGo to Notebooks tab and select the sample Notebook to view it. To edit and run cells, click Launch Notebook Server. Select a kernel, and wait for it to start to run cells.Note: Do not select Python 3 kernel. Instead, select one of the kernels named after your project.You can also launch Notebook server by opening File, Open Command Prompt, and entering az ml notebook start.You can explore past runs of Notebook cells using the Run History tab. Click the gear icon next to the list of runs, and select Cell as one of colums to find specific cell runs."
"This solution will address solving such problems. We will show how to train, evaluate and deploy your own image classification model using the Microsoft Cognitive Toolkit (CNTK) for deep learning. ","A large number of problems in the computer vision domain can be solved using image classification approaches. These include building models which answer questions such as, ""Is an OBJECT present in the image?""  where OBJECT could for example be ""dog"", ""car"", ""ship"", etc. as well as more complex questions, like ""What class of eye disease is evinced by this patient's retinal scan?""Example images are provided, but the reader can also bring their own dataset and train their own custom models.The key steps required to deliver this solution are as follows:1. Generate an annotated image dataset. Alternatively, the provided demo dataset can be used.2. Train an image classifier using a pre-trained Deep Neural Network.3. Evaluate and improve accuracy of this model.4. Deploy the model as a REST API, either to the local machine or to the cloud."
"Sentiment analysis is a well-known task in the realm of natural language processing. Given a set of texts, the objective is to determine the polarity of that text. The objective of this lab is to use CNTK as the backend for Keras and implement sentiment analysis from movie reviews.","The detailed documentation for this real world scenario includes the step-by-step walkthrough at scenario-sentiment-analysis-deep-learning hyperlink.For code samples, click the ""View Project"" icon on the right and visit the project GitHub repo.Key components needed to run this scenario:An Azure account .An installed copy of Azure Machine Learning Workbench following the quick start installation guide to install the program and create a workspace.For operationalization, it is best if you have Docker engine installed and running locally. If not, you can use the cluster option but be aware that running an Azure Container Service can be expensive.This Solution assumes that you are running Azure Machine Learning Workbench on Windows 10 with Docker engine locally installed. If you are using macOS the instruction is largely the same.Sentiment Analysis code samples located in the project GitHub repo."
"The detailed documentation for this document collection analysis scenario includes the step-by-step walk-through. Key components needed to run this example:1) An Azure account (free trials are available).2) An installation of Azure Machine Learning Workbench with a workspace created.3) This example could be run on any compute context. However, it is recommended to run it on a multi-core machine with at least of 16-GB memory and 5-GB disk space.","With a large amount of data collected every day, a significant challenge is to organize, search, and understand vast quantities of these texts. This document collection analysis scenario demonstrates an efficient and automated end-to-end workflow for analyzing large document collection and enabling downstream NLP tasks.The key elements delivered by this scenario are:1. Learning salient multi-words phrase from documents.2. Discovering underlying topics presented in the document collection.3. Representing documents by the topical distribution.4. Presenting methods for organizing, searching, and summarizing documents based on the topical content.The methods presented in this scenario could enable a variety of critical industrial workloads, such as discovery of topic trends anomaly, document collection summarization, and similar document search. It can be applied to many different types of document analysis, such as government legislation, news stories, product reviews, customer feedbacks, and scientific research articles.The machine learning techniques used in this scenario include:1. Text processing and cleaning 2. Phrase Learning 3. Topic modeling 4. Corpus summarization 5. Topical trends and anomaly detection"
"TDSP Project Dashboard.This is the project dashboard where you put key project information (for example, a project summary, with relevant links). In your actual project, replace the rest of the content with project-specific summary.","This is the project dashboard where you put key project information. In your actual project, replace the rest of the content with project-specific summary.This repository contains an instantiation of the Team Data Science Process from Microsoft for project Azure Machine Learning. The TDSP is an agile, iterative, data science methodology designed to improve team collaboration and learning. It facilitates better coordinated and more productive data science enterprises by providing:a lifecycle that defines the steps in project development, a standard project structure, artifact templates for reporting,tools to assist with data science tasks and project execution.When you instantiate the TDSP from Azure Machine Learning, you get the TDSP-recommended standardized directory structure and document templates for project execution and delivery. The workflow then consists of the following steps:modify the documentation templates provided here for your project, execute your project,prepare the Data Science deliverables for your client or customer, including the ProjectReport.md report.TDSP uses the data science lifecycle to structure projects. The lifecycle defines the steps that a project typically must execute, from start to finish. This lifecycle is valid for data science projects that build data products and intelligent applications that include predictive analytics. The goal is to incorporate machine learning or artificial intelligence models into commercial products. Exploratory data science projects or ad hoc/on-off analytics projects can also use this process, but in this case some steps of this lifecycle may not be needed."
The goal of this scenario is to guide a data scientist through the implementation and operationalization of the predictive maintenance solution using *Azure Machine Learning Workbench*,"Understanding fleet maintenance requirements can have a large impact on business safety and profitability.An initial approach is to rely on  corrective maintenance, where parts are replaced as they fail. Corrective maintenance ensures parts are used completely not wasting component life, but incurs expense in both downtime and unscheduled maintenance requirements off hours, or inconvenient locations.An alternative is a preventative maintenance schedule. Here a business may track or test component failures and determine a safe lifespan in which to replace that component before failure. For safety critical machinery, this approach can insure no catastrophic failures. The down side is components are replaced frequently, many with remaining life left.The goal of predictive maintenance  is to optimize the balance between corrective and preventative maintenance. This approach only replaces those components when they are close to failure. The savings in this case come from both extending component lifespans compared to preventive maintenance, and reducing unscheduled maintenance over corrective maintenance and improving safety associated component failure."
This example showcases various effective machine learning methods to match open ended queries to pre-existing FAQ question/answers pairs. This example demonstrates an easy development process for building such a solution using the Azure Machine Learning Workbench.,"This example addresses the problem of mapping user questions to pre-existing Question & Answer (Q&A) pairs as is typically provided in a list of Frequently Asked Questions  or in the Q&A pairs present on websites like Stack Overflow. There are many approaches to match a question to its correct answer, such as finding the answer that is the most similar to the question. However, in this example open ended questions are matched to previously asked questions by assuming that each answer in the FAQ can answer multiple semantically equivalent questions."
Our use case scenario focuses on how a large amount of unstructured unlabeled data corpus such as PubMed article abstracts can be analyzed to train a domain-specific word embedding model. Then the output embeddings are considered as automatically generated features to train a neural entity extraction model using Keras with TensorFlow deep learning framework as backend and a small amount of labeled data.,The aim of this real-world scenario is to highlight how to use Azure Machine Learning Workbench to solve a complicated NLP task such as entity extraction from unstructured text.Here are the key points addressed:How to train a neural word embeddings model on a text corpus of about 18 million PubMed abstracts using Spark Word2Vec implementation.How to build a deep Long Short-Term Memory  recurrent neural network model for entity extraction on a GPU-enabled Azure Data Science Virtual Machine  on Azure.Demonstrate that domain-specific word embeddings model can outperform generic word embeddings models in the entity recognition task.Demonstrate how to train and operationalize deep learning models using Azure Machine Learning Workbench.
How to use the Team Data Science Process template to create a project in Azure Machine Learning that classifies US incomes.,"This sample shows how to instantiate and execute a machine learning project using the Team Data Science Process  structure and templates in Azure Machine Learning. For this purpose, we use the well-known 1994 US Census data from the UCI Machine Learning Repository. The modeling task is to predict US annual income classes from US Census information for example, age, race, education level, country of origin, etc.The problem is to understand how socio-economic data captured in US Census can help predict annual income of individuals in US. Based on such Census features, the machine learning task is to predict if the income of an individual is above $50,000 or not binary classification task."
