Summary,Document
Compare performance of multiple binary classification algorithms by running a sample application with multiple evaluation metrics. Also view the feature importance scores per the algorithm.Email me your feedback.,"This experiment uses 8 binary classification algorithms to train models against your data. It outputs the performance result of each algorithm such as Accuracy, AUC, for comparing the algorithms' performance.It also outputs the list of features sorted by their importance score for each model using the Permutation Feature Importance.The experiment has been published as a web service, and can be used through an application to evaluate the results of training your data using the experiment.To get started.Get a dataset with a column for binary classification ,Rename the column name to ""target"" , Upload the data and run the app ,View the results .Feel free to email me and let me know if you have any suggestions or comments about this app or the usefulness of this scenario in general."
"This experiment demonstrates the use of the Execute R Script, Feature Selection, Feature Hashing modules to train a text sentiment classification engine using Azure Machine learning studio.An SVM model is used for training this.","In this article, we'll explain how to to build an experiment for sentiment analysis using Microsoft Azure Machine Learning Studio. Sentiment analysis is a special case of text mining that is increasingly important in business intelligence and and social media analysis. For example, sentiment analysis of user reviews and tweets can help companies monitor public sentiment about their brands, or help consumers who want to identify opinion polarity before purchasing a product. This experiment demonstrates the use of the Feature Hashing, Execute R Script and Filter-Based Feature Selection modules to train a sentiment analysis engine. We use a data-driven machine learning approach instead of a lexicon-based approach, as the latter is known to have high precision but low coverage compared to an approach that learns from a corpus of annotated tweets. The hashing features are used to train a model using the Two-Class Support Vector Machine (SVM), and the trained model is used to predict the opinion polarity of unseen tweets. The output predictions can be aggregated over all the tweets containing a certain keyword, such as brand, celebrity, product, book names, etc in order to find out the overall sentiment around that keyword. The experiment is generic enough that you could use this framework to solve any text classification task given a reasonable amount of labeled training data."
Cost-sensitive binary classification in Azure ML Studio to predict credit risk based on information given on a credit application.Two different datasets and models were used for training and evaluation,"This sample demonstrates how to perform cost-sensitive binary classification in Azure ML Studio to predict credit risk based on the information given on a credit application. The classification problem in this experiment is a cost-sensitive one because the cost of misclassifying the positive samples is five times the cost of misclassifying the negative samples.In this experiment, we compare two different approaches for generating models to solve this problem:Training using the original data set and Training using a replicated data set.In both approaches, we evaluate the models using the test data set with replication, to ensure that results are aligned with the cost function. We test two classifiers in both approaches: Two-Class Support Vector Machine and Two-Class Boosted Decision Tree."
The experiment aims to build some predictive models which help to predict Malignant & Benign cases of Breast tumors.The entire process of ML pipeline is discussed here,"Machine Learning, these days playing a pivotal role in Healthcare Analytics. Early prognosis of diseases specifically cancer finds ray of hope of getting cured of. The data on Breast Cancer was collected from an online source. The data was was understood from various aspects doing some visualization & other Statistical analysis. After that Data Wrangling was done to prepare the data set for final analysis & finally applied SVM of Azure ML Studio & received desired outcome"
"Document classification experiment using Multiclass Decision Forests, with data taken from the CFPB to model support ticket classification.This experiment models using multiclass decision forests to categorize documents. The data used is taken from a publicly-available dataset of consumer complaints about financial products; this is a close analogue to a common task of customer support ticket classification.","Data is taken from the Consumer Financial Protection Bureau's online database of consumer complaints about different financial products. The overall dataset contains over 600,000 complaints, over 90,000 of which contain a ""Consumer complaint narrative"" -- open-ended text describing the problem the consumer is reporting. These complaint span 11 different types of financial products. Because the number of complaints varies substantially across product types, here we limit ourselves to the 5 most frequent product types:Bank accounts, credit cards, Credit reporting, Debt collection and mortgages. Additionally, we restrict ourselves here to complaints with narratives at least 500 characters long, to ensure a sufficiently rich training set. This yields a dataset of 36,958 consumer complaints. In addition to columns containing the product type, complaint narrative, and complaint ID, there are numerous other metadata columns as well that are included in the dataset but not used here.Inspection of cross-validation results shows this model to achieve about 82% accuracy on this data. What is particularly interesting is to note the nature of the classification errors. As can be seen in the confusion matrix displayed by the model evaluation step, errors are concentrated in three highly plausible areas.Bank accounts are mistaken for credit cards.Credit cards are mistaken for credit reporting.Debt collection is mistaken for credit reporting .All three of these error types involve topic areas that overlap in real life. Many credit cards are bank-issued, credit reporting is an integral step in securing a card, and credit reporting is also almost always directly implicated in the debt collection process. These errors may well indicate that the labels being applied to this data are not entirely mutually-exclusive, an important consideration when doing classification; it is also interesting that the errors are not symmetrical which may indicate further interesting structural properties of the data."
Email classification experiment to assign an email to one or more class(es) of predefined set of classes or work queues.The goal of this experiment is to classify an email into one or more predefined classes or categories and to create a support ticket or assign it to correct support team.,"This experiment has two steps.Train model with data and Save trained models.Create an experiment using the trained models and publish it as web service .Next optional step is to Integrate Predictive Web service in CRM workflow.In this experiment, input dataset has two raw text columns: Email Subject and Email Body, and three label columns: Case Type, Case Subject, and Queue name. Once we train the model it should predict case type of the email, case subject, and queue name to which email belongs. This experiment has following steps
Import Data, Data Preprocessing, Text Preprocessing, Feature Engineering, Train and Evaluate Model and Save Trained Models."
This experiment is meant to train models in order to predict accuratly who survived the Titanic disaster.It has been a good first machine learning experience with Azure Machine Learning Studio. The purpose was to use the knowledge earned with Microsoft MOOC on Datascience (EDX).The results of the test data was published in the kaggle competition.,"A set of labelled data is available to train models and a set of unlabelled data is available to make and submit predictions.The training data contains about 900 observations and has 12 columns. Among these columns, there are 10 features and 1 label column. The last column is a data column for tuple identification.Some data like Survived and Pclass needed to be reinterpreted as categorical data. Others, like Age, Fare and Embarked had missing data. The missing data were replaced by the mean or the mode depending on the data is categorical or numeric. There was not any outlier.Some data were removed because it is hard to see how they would have an impact on survival like Name, PassengerId, Ticket. The Cabin feature was also removed because it contained too much missing value and there were no way to rebuilt missing information.Finally, numerical features like Age and Fare were grouped into bins because their distribution was totally skewed.There were two columns detailing the composition of family onboard. They were used to build two derived features : One that indicates if the passenger has some family aboard and another that indicates the number of family members aboard.The features were selected after performing a test (ANOVA, Chi-Square) that aims to verify the link between the label and the feature.For example, tests proved that having some family aboard the titanic has an impact on survival but the size of family does not. A chi-sqaure test showed that the embarked feature has a link with the Survived label.Several models based on various classification method (Neural network, Support Vector Machine, Naive Bayes, Logistic Regression, Boosted Decision Tree and Random Forest) were trained and tuned using cross-validation and parameter sweep.It seemed like the boosted decision tree had the best performance among the other models.A section in the experiment is dedicated to produce a .csv file which contains prédictions that can be directly submitted to Kaggle.Predictions were 77,033% accurate for the public leaderboaard (made with half of the prédictions. The other half will be used to make the final classement at the end of the competition)."
"Rank sales leads in a CRM system by predicting the likelihood that they turn into an actual sale.The use case described in this experiment is a prediction of likelihood that prospect customers buy a specific product, under certain conditions identified in the source dataset.","The prediction is based on customer-oriented features, such as location, age, gender, and product features like program type, duration, destination. Products considered in this experiment relate to an educational institution selling on-site language courses.The dataset is labeled, which allows for supervised learning algorithms, and the label column ""Count"" being trained contains numerical values, which are the total number of sales collected in the past for the indicated conditions. The following classification - i.e. prediction - algorithm is tested:Two-Class Boosted Decision Tree: This algorithm is an ensemble learning method in which the second tree corrects for the errors of the first tree, the third tree corrects for the errors of the first and second trees, and so forth. Predictions are based on the entire ensemble of trees together that makes the prediction.Generally, boosted decision trees are the easiest methods with which to get top performance on a wide variety of classification tasks. However, they are also one of the more memory-intensive algorithms, and therefore, it should not be used to process a very large datasets. If you are dealing with datasets in the range of millions of row, prefer a linear algorithm, such as Two-Class Support Vector Machine."
"Sample experiment that uses multiclass classification to predict the letter category as one of the 26 capital letters in the English alphabet.This experiment demonstrates how to build a multiclass classification model for letter recognition, using Azure ML Studio.","We will use the letter recognition data from UCI Machine Learning repository ( http://archive.ics.uci.edu/ml/machine-learning-databases/letter-recognition/letter-recognition.data ).In this data, a set of 20,000 unique letter images was generated by randomly distorting pixel images of the 26 uppercase letters from 20 different commercial fonts. The parent fonts represented a full range of character types including script, italic, serif, and Gothic. Each letter within these 20 fonts was randomly distorted to produce a file of 20,000 unique stimuli. The features of each of the 20,000 characters or stimulus were summarized in terms of 16 primitive numerical attributes (statistical moments and edge counts) which were then scaled to fit into a range of integer values from 0 through 15. The objective is to identify each black-and-white rectangular pixel display as one of the 26 capital letters in the English alphabet.In order to access this dataset, drag the Reader module to the experiment canvas. This module can be used to specify the data source for an experiment.In general, when preparing data for an experiment, you might need to perform tasks such as selecting a subset of relevant columns from the entire dataset, changing the data type, converting columns to categorical or continuous variables, taking care of missing values etc. Additional tasks might include normalizing the data or binning values, if necessary.
In this experiment, the uploaded data is already in a good format, so no pre-processing of the data is needed.Defining a good set of features for a predictive model requires experimentation and knowledge about the problem at hand. Some features are better for predicting the target than others. Also, some features have a strong correlation with other features, so they will not add much new information to the model and can be removed. When building a model we might use all the available features in the dataset, or we might use only a subset of the features. For this experiment, we will use all 16 columns (Col2-Col17) that contain features of the 20,000 characters.The next step is to train a model. First, split the data into training and testing sets. Select and drag the Split module to the experiment canvas and connect it to the second input port of the Train Model module. Set Fraction of rows in the both output dataset to 0.5. This way, we'll use 50% of the data to train the model, and the other 50% for testing.In this experiment we will use two different algorithms. Drag the following modules into the experiment workspace: Multiclass Decision Jungle, and the Two-Class Support Vector Machines module connected to the One-vs-All Multiclass module.Connect the output of the algorithm module to the input ports of the Train Model module, along with the training data.We are ready to run the experiment. The result is a trained classification model that can be used to score new samples to make predictions.Now that we've trained the model, we can use it to score the other 50% of the data and see how well the model predicts and classifies new data.

Add the Score Model module to the experiment canvas, and connect the left input port to the output of the Train Model module. Connect the right input port to the test data output (right port) of the Split module.

Run the experiment and view the output of the Score Model module, by clicking the output port and selecting Visualize*. The output shows the the scored labels and the probabilities for each of the 26 classes (""A""-""Z"").Finally, to test the quality of the results, drag the Evaluate Model module to the experiment canvas, and connect the left input port to the output of the Score Model module. There are two input ports because the Evaluate Model module can be used to compare two models. Therefore, we can compare the performance of the two different algorithms that we applied -- Multiclass Decision Jungle and the Two-Class Support Vector Machines used together with the One-vs-All Multiclass modules.Run the experiment and view the output of the Evaluate Model module, by clicking the output port and selecting Visualize. The following diagram shows the resulting statistics for the model.Each column of the matrix represents the instances in a predicted class, while each row represents the instances in an actual class. In this confusion matrix, for class A, the recall is 85.1%. The algorithm correctly classifies class A 85.1% of the time whereas class A was misclassified as belonging to class G 0.5% of the time. The confusion matrix also does a poor job of classifying G and O."
This experiment predicts if a customer will visit after the marketing campaign using boosted decision tree and support vector machine.This experiment demonstrates how to use binary classifiers to predict customer response to a direct mailing campaign based on historical data.,"The dataset contains 64,000 records, each having nine features and three response (or label) columns.The three responses that can be predicted are as follows:visit - Denotes that a customer visited the store after the marketing campaign.visit - Denotes that a customer visited the store after the marketing campaign. conversion - Indicates that a customer purchased something. spend - The amount that was spent.For this particular experiment, we are predicting whether a customer will visit a store after the marketing campaign, which is labeled by a value in the visit column. This is an example of an unbalanced classification problem, because the percentage of visits is only about 2% of the data.First, we did some simple data processing.The segment variable is converted to be categorical, using Metadata Editor.The *history_segment* column is removed because it is a simplification of history column.The label columns conversion and spend are eliminated because we are not using them in this experiment.The resulting data is split into training and testing datasets as following figure.From the training data, we built two models, one using Two-Class Boosted Decision Trees, and the other using Two-Class Support Vector Machine. To determine the optimal parameters for each classifier, we used the Sweep Parameters module. Sweep Parameters module uses the random sweep technique to find the parameters of the classification algorithms, which can give best performance on the test set.Our results found that the Two-Class Boosted Decision Trees model had better accuracy. The following graphic shows the lift chart for the model.The values for accuracy, precision, and recall were similar for both classifiers, when comparing at a threshold of .15, which is about the maximum f-score for both learners. Therefore, in order to determine whether the slightly increased precision of the Two-Class Boosted Decision Tree model was significant, we used custom R script to calculate McNemar's statistic.

The sequence of modules at the end of the experiment shows how we computed the scores for the labels at a threshold of .15 prior to calculating McNemar's statistic. (Normally the Score Model module uses a threshold of .5.)

From this, we learned that McNemar's p-value is close to zero, indicating that the improvement is significant. Therefore the Two-Class Boosted Decision Tree model is the one that we would use for any scoring workflow."
"This experiment uses the Heart Disease dataset (1988) from the UCI Machine Learning repository to train a model for heart disease prediction.This experiment is based on the original Heart Disease Prediction experiment created by Weehyong Tok from Microsoft, which is one of the Healthcare Industry solutions. This experiment uses the data set from the UCI Machine Learning repository to train and test a model for heart disease prediction. We will use this as a starting point to give you 7 ideas how to start and improve the Cortana Intelligence Gallery examples. Thanks Weehyong for creating and sharing your experiment!","The original database contains 76 attributes, but ML researchers tend to use a subset only containing 14 attributes. The ""goal"" field refers to the presence of heart disease in the patient (num). It is integer valued from 0 (no presence) to 4. With this experiment we will attempt to to distinguish presence (values 1,2,3,4) from absence (value 0).We used the processed Cleveland data, because of a warning from the author.In this case, we only miss 4 entries of ca (number of major vessels colored by flourosopy) and 2 entries of thal. In the original sample, all missing values were substituted with -1. But there are many more options. In this case, we decide to replace them with the mode (most occurring value). Replacing the missing values with the mean would be strange in this case, as thal is a categorical variable, and although I'm not a doctor, I think it would be hard to have 0.67 major vessel colored. This is actually also the reason why we use ca as a categorical variable.We use a stratified split to maintain a balanced training and test set read more on MSDN and set a seed so we can reproduce this experiment.As we have a lot of categorical variables, we decide to make dummy variables out of them. This can be easily done by using the ""Convert to Indicator Values"" module. It helps us to get more insights about the value of a specific categorical variable.We would suggest not to use the ""One-vs-All Multiclass"" module, as this creates a multiclass classification model from an ensemble of binary classification models. We are here dealing with a binary classification model, and not a multiclass classification model, so we don't see the added value.If you use the ""Tune Model Hyperparameters"" module to train your model, please make sure you use the ""Parameter Range"" option for the 'create trainer mode"" property.By using the ""Permutation Feature Importance"" module, you can gain insights. This module computes the permutation feature importance scores of the feature variables given a trained model and a test data set. From the left model, we can find that ""oldpeak"", ""sex"" and ""restecg"" have a relative high feature importance score.

A nice thing, as we have transformed the categorical values into dummies, is that we can actually gain more information by looking at the feature importance scores of the right model. Here we find also ""oldpeak"" with the highest score, second by cp-4, which means asymptomatic chest pain. This chest pain type variable didn't come up while just using the categorical variables as they were, so sometimes it can really help to transform them in order to gain more insight."
Develop a model that uses various network features to detect which network activities are part of an intrusion/attack.In this experiment we use various network features to detect which network activities are part of an intrusion/attack.,"We used a modified dataset from KDD cup 1999. The dataset includes both training and testing sets. Each row of the dataset contains features about network activity and a label about type of activity. All activities except one (with value 'normal') indicate network intrusion. The training set has approximately 126K examples. It has 41 feature columns, a label column and an auxiliary 'diff-level' column that is an estimation of the difficulty of correctly classifying a given example (see [1] for a detailed description of this column). Feature columns are mostly numeric with a few string/categorical features. The test set has approximately 22.5K test examples (with same 43 columns as in the training set).Note that to read from the public blob storage we choose authentication type 'PublicOrSAS'. Importing of the test set is done in a similar way.
The original label column, called 'class' has many values and of string type. Each string value corresponds to a different attack. Some attacks do not have many examples in the training set and there are new attacks in the test set. to simplify this sample experiment we build a model that does not distinguish between different types of attacks. For this purpose we replace 'class' column with the binary column that has 1 if an activity is normal and 0 if it is an attack. The Studio provides built-in modules to ease this preprocessing step. The binarization of 'class' column is achieved by using Metadata Editor to change the type of 'class' column to categorical, getting binary column with Indicator Values module and selecting 'class-normal' column with Project Columns module. This sequence of steps is shown below:We compare 2 machine learning algorithms: Two-Class Logistic Regression and Two-Class Boosted Decision Tree. Also we compare two different training sets, the first one with the original 41 features and the second one with 15 most important features that are found by Filter Based Feature Selection module.Having computed AUC's for all 4 combinations of learning algorithm and training set, we use Execute R to generate a table that summarizes all results.The final output of the experiment is the left output of the last Execute R Script module"
Text classification aims to assign a text instance into one or more class(es) in a predefined set of classes. This collection of experiments demonstrates the steps of the Text Classification Template on how to build and deploy a text classification model.,"The goal of text classification is to assign some piece of text to one or more predefined classes or categories. The piece of text could be a document, news article, search query, email, tweet, support tickets, customer feedback, user product review etc. Applications of text classification include categorizing newspaper articles and news wire contents into topics, organizing web pages into hierarchical categories, filtering spam email, sentiment analysis, predicting user intent from search queries, routing support tickets, and analyzing customer feedback. As part of the Azure Machine Learning offering, Microsoft provides a template to help data scientists easily build and deploy a text classification solution. In this document, you will learn how to use and customize the template through a demo use case.The following graphic presents the workflow of the template. Each step in the workflow corresponds to an Azure ML experiment. The experiments must run in order because the output of one experiment is the input to the next."
"The goal of text classification is to assign some piece of text to one or more predefined classes or categories. The piece of text could be a document, news article, search query, email, tweet, support tickets, customer feedback, user product review etc. "," Applications of text classification include categorizing newspaper articles and news wire contents into topics, organizing web pages into hierarchical categories, filtering spam email, sentiment analysis, predicting user intent from search queries, routing support tickets, and analyzing customer feedback. As part of the Azure Machine Learning offering, Microsoft provides a template to help data scientists easily build and deploy a text classification solution. In this document, you will learn how to use and customize the template through a demo use case.The data used in this use case is Sentiment140 dataset, a publicly available data set created by three graduate students at Stanford University: Alec Go, Richa Bhayani, and Lei Huang. The data comprises approximately 1,600,000 automatically annotated tweets.The tweets were collected by using the Twitter Search API and keyword search. The automatic annotation process works as follows: any tweet containing positive emoticon such as :),:-), :D or =D was assumed to bear positive sentiment, and tweets with negative emoticons such as :<, :-( or :( were supposed to bear negative polarity. Tweets containing both positive and negative emoticons were removed. Additional information about this data and the automatic annotation process can be found in the technical report written by Alec Go, Richa Bhayani and Lei Huang, Twitter Sentiment Classification using Distant Supervision, in 2009.

We sampled only 10% of the data and shared it as Blob in a public Windows Azure Storage account. You can use this shared data to follow the steps in this template, or you can get the full data set from the Sentiment140 dataset home page. In Azure Machine Learning, users can upload a dataset from a local file, or they can connect to an online data source, such as the web, an Azure SQL database, Azure table, , or Windows Azure BLOB storage, by using the Reader module or Azure Data Factory.

For simplicity, this template uses pre-loaded sample datasets. However, users are encouraged to explore the use of online data because it enables real-time updates in an end-to-end solution. A tutorial for setting up an Azure SQL database can be found here: Getting started with Microsoft Azure SQL Database.

The template steps 1-4 represent the text classification model training phase. In this phase, text instances are loaded into the Azure ML experiment, and the text is cleaned and filtered. Different types of numerical features are extracted from cleaned the text, and models are trained on different feature types. Finally, the performance of the trained models are evaluated on unseen text instances and the best model determined based on a number of evaluation criteria.

In steps 5A and 5B, the most accurate model is deployed as a published web service, using either RRS (Request Response Service) or BES (Batch Execution Service). When using RRS, only one text instance is classified at a time. When using BES, a batch of text instances can be sent for classification at the same time. By using these web services, you can perform classification in parallel, using either an external worker or the Azure Data Factory, for greatly enhanced efficiency."
"This experiment demonstrates the use of cross validation in binary classification.This experiment demonstrates how to use the Cross Validate Model module. We used the Adult dataset and trained multiple classification models using cross validation to predict whether an individual’s income is greater or less than $50,000.","Cross Validate Model takes two inputs: a machine learning model and a dataset. For this binary classification problem, we used the following four binary classification methods: Two-Class Averaged Perceptron, Two-Class Boosted Decision Tree, Two-Class Logistic Regression, and Two-Class Support Vector Machine.To configure Cross Validate Model, you must also specify the label column, or classification target. In this case, select the column income and leave the default value for the Random seed option as 0, to randomize the distribution of instances into the folds.There are two outputs from Cross Validate Model. The left-hand output contains the scored results on the training data; the right-hand output contains accuracy metrics for each model.For example, the following diagram shows the first output from the instance of Cross Validate Model that trained using Two-Class Averaged Perceptron. The Scored Labels column shows the predicted class label; the Scored Probabilities column shows the probabilities for the predicted class.Based on the cross validation results, you can tune the model parameters or decide which model to use in the scoring experiment."
"Document classification experiment using Multiclass Decision Forests, with data taken from the CFPB to model support ticket classification.This experiment models using multiclass decision forests to categorize documents. The data used is taken from a publicly-available dataset of consumer complaints about financial products; this is a close analogue to a common task of customer support ticket classification.","Data is taken from the Consumer Financial Protection Bureau's online database of consumer complaints about different financial products. The overall dataset contains over 600,000 complaints, over 90,000 of which contain a ""Consumer complaint narrative"" -- open-ended text describing the problem the consumer is reporting. These complaint span 11 different types of financial products. Because the number of complaints varies substantially across product types, here we limit ourselves to the 5 most frequent product types:Bank accounts, credit cards, Credit reporting, Debt collection and mortgages. Additionally, we restrict ourselves here to complaints with narratives at least 500 characters long, to ensure a sufficiently rich training set. This yields a dataset of 36,958 consumer complaints. In addition to columns containing the product type, complaint narrative, and complaint ID, there are numerous other metadata columns as well that are included in the dataset but not used here.Inspection of cross-validation results shows this model to achieve about 82% accuracy on this data. What is particularly interesting is to note the nature of the classification errors. As can be seen in the confusion matrix displayed by the model evaluation step, errors are concentrated in three highly plausible areas.Bank accounts are mistaken for credit cards.Credit cards are mistaken for credit reporting.Debt collection is mistaken for credit reporting .All three of these error types involve topic areas that overlap in real life. Many credit cards are bank-issued, credit reporting is an integral step in securing a card, and credit reporting is also almost always directly implicated in the debt collection process. These errors may well indicate that the labels being applied to this data are not entirely mutually-exclusive, an important consideration when doing classification; it is also interesting that the errors are not symmetrical which may indicate further interesting structural properties of the data."
"Search suggestions passes wrong previous result to form history  When using the search suggestion extension , the browser asks the user to confirm  the selection by clicking .","MattN noticed a problem with the WIP patch from bug 469443 applied. When typing in the search box, sometimes search-suggestion entries would be displayed above the divider (where entries for previous matching searches are). The problem here is that nsSearchSuggestions.js is passing the wrong previousResult to form history. Instead of it being the previous form history search result, it's the SuggestAutoCompleteResult result (which contains the union of the form-history and search-suggest entries). So, when form history refines its results as you time, it can actually add *more* entries as data leaks from the suggestions result into form history result, and it thus looks like the divider is being drawn in the wrong place. This bug wasn't visible before 469443, because nsFormFillController::StartSearch tries to QI the provided result to a nsIAutoCompleteSimpleResult. The search-suggestion result is only implements nsIAutoCompletResult (no \""Simple\""), so the QI fails, historyResult nee previousResult becomes null, and thus Satchel was doing a new search every time. EG: 1) type \""b\"" in the search field. 2) form history finds 1 entry (\""blah\""), search-suggestions finds \""baaa\"", \""bloop\"", \""bzzz\"", the autocompete menu shows these in order with a divider between \""blah\"" and \""baaa\"". 3) type an \""l\"" in the search field (\""bl\"") 4) startHistorySearch()'s previous result contains [\""blah\"", \""baaa\"", \""bloop\"", \""bzzz\""], Satchel filters this down to [\""blah\"", \""bloop\""] to match the new \""bl\"" search string 5) nsSearchSuggestions's onReadyState() change is called with updated search suggestions, builds up a new list of results, but sees that the form history result now has *two* entries. Created an attachment (id=380567) [details] Patch v.1 (WIP) This fixes the problem, but isn't quite correct... If you type \""a&lt;backspace&gt;b\"", satchel trying to use the results from the \""a\"" search for the \""b\"" search, and so nothing is found. I suspect nsSearchSuggestions needs to throw away the old form history result when the search string changes like this, but I'm not entirely sure it's responsible for doing so, maybe satchel should be smarter about throwing away a previous result when the previous result's search string doesn't have a common prefix. [That seems to be handled somewhere else for normal form field entries, oddly enough.] Created an attachment (id=383211) [details] Patch v.2 Ah. So, there's a ._formHistoryResult in the SuggestAutoCompleteResult wrapper (used to combine form history with search suggestions), and also a ._formHistoryResult in SuggestAutoComplete (the service itself, used to hold onto a form history result until a search suggestion is available). The simple fix it to just discard the service's form history result copy when startSearch() is called with a null previous result. Otherwise it's trying to use a old form history result that no longer applies for the search string. (From update of attachment 383211 [details]) Perhaps we should rename one of them to _fhResult just to reduce confusion? (In reply to comment #3) &gt; (From update of attachment 383211 [details] [details]) &gt; Perhaps we should rename one of them to _fhResult just to reduce confusion? Good point. I renamed the one in the wrapper to _formHistResult. fhResult seemed maybe a bit too short. Pushed http://hg.mozilla.org/mozilla-central/rev/097598383614                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                        "
" Session Restore and Tab Mix Plus disabled, the current patch that is suggested affects Tab Mix Plus so remove the browser.sessionstore.enabled pref","That pref was thought to be for extensions which wanted to completely replace our own Session Restore functionality. While this has worked somehow for Tab Mix Plus, we've had several issues with people ending up both Session Restore and Tab Mix Plus disabled (see bug 435055 and its duplicates). Furthermore, there are several code points which will also break when Session Restore has been disabled (such as the list of recently closed tabs). Instead of adding try-catch-blocks wherever we use Session Restore, I'd much rather encourage extensions authors to override both nsSessionStartup and nsSessionStore to provide the same API with their own functionality (or implementing a dummy-API and making sure for themselves that they've correctly replaced all known consumers). This would also make the lives of those other extension authors simpler who so far can't be too sure that the Session Store component actually works (through whatever implementation). Note that privacy concerned users will still be able to disable writing to sessionstore.js through the browser.sessionstore.resume_from_crash pref.        Created an attachment (id=332726) [details] remove the pref    (note: bug 448725 should be wontfixed if this is fixed)    Created an attachment (id=333820) [details] remove (buggy) note from API comments (From update of attachment 332726 [details]) a problem with this patch is that the session data is still stored in memory while the app is running, and by removing this pref, there's no way to disable that. some users do not want the recently-closed-tabs menu, and others don't want any session tracks stored in memory at all.    (In reply to comment #4) &gt; some users do not want the recently-closed-tabs menu, That's what browser.sessionstore.max_tabs_undo is for: setting it to 0 effectively disables the feature. &gt; and others don't want any session tracks stored in memory at all. Then again, we don't save any data that wouldn't be in memory, anyway, or do we? I'd rather introduce a different pref or different means to cater the privacy sensitive users than have this half-baked cut-it-all pref which AFAICT so far has produced more issues than it's solved. (In reply to comment #5) &gt; Then again, we don't save any data that wouldn't be in memory, anyway, or do we? Fair point. I'm not sure. &gt; I'd rather introduce a different pref or different means to cater the privacy sensitive users than have this half-baked cut-it-all pref which AFAICT so far has produced more issues than it's solved. Yes, agreed the pref is not ideal for this purpose. So max_tabs_undo=0 + resume_from_crash=false is fine for now, until proper \""private browsing\"" is supported. (From update of attachment 332726 [details]) &gt;+            // XXXzeniko should't we just disable this item as we disable &gt;+            // the tabbrowser-multiple items above - for consistency? &gt;+            this.mUndoCloseTabMenuItem.hidden = &gt;+              Cc[\""@mozilla.org/browser/sessionstore;1\""]. &gt;+              getService(Ci.nsISessionStore). &gt;+              getClosedTabCount(window) == 0; 1 r=me (In reply to comment #7) &gt; +1 That's bug 350731. Care to convince mconnor that he's wrong? ;-) Pushed as 17120:e712e96d7861 and 17121:adb1ef78dd21. onemen: This patch significantly affects Tab Mix Plus: Instead of disabling SessionStore, you'll now have to replace it by shipping a component which implements the same API. You should be able to keep that component minimal, though, and just call your own code whenever the API is used. This will make the lives of people using the SessionStore API (such as Session Manager) somewhat simpler, as they can just use the API without having to worry too much about the implementation behind it. If you want to offer the option of switching between your and our implementation, see e.g. the ignore-history component of the Torbutton extension for how to overwrite a component with the original still being available internally (so that you can either pass API calls forward or handle them yourself). Sounds like this has impact for extension authors and so should be documented on MDC no problem. current Tabmix dev-build already not disable SessionStore I currently have only one problem , how to disable the restore after restart. can you add a pref for this, or some other way to do it? (In reply to comment #12) &gt; I currently have only one problem , how to disable the restore after restart. You've got several options for that: * Set the prefs browser.sessionstore.resume_from_crash and browser.sessionstore.resume_session_once both to false as early as possible and make sure that browser.startup.page isn't 3. * Delete the file sessionstore.js as early as possible (e.g. when the profile-after-change notification is dispatched). * For Firefox 3.1: Respond to the sessionstore-state-read notification by setting the subject's data member to an empty string (cf. bug 448741 comment#6). Or is there a use case I'm missing?                                                                                                                                                                                                                                                                                                                                                                                                                                                              "
"Disable multitouch ""rotate"" gesture for cycling tabs  since it is causing issues in several browsers, the issue seems to be due to trigger zoom appearing more frequently than tab rotate hence this feature needs to disabled for the next update.","I've noticed that I frequently trigger the rotate gesture accidentally, usually while scrolling. Gestures have improved since the original landing (when triggering the wrong gesture was really easy), but this articular gesture is still problematic. The basic reasoning is that it's highly disruptive to be switched to another tab when you're not expecting it. When it happens, you don't know what just happened until you notice that you're on some entirely different page, that's randomly to the left or right (1 or more tabs) from the page you thought you were on. I'm don't think the rotate gesture for switching tabs is nearly as useful, discoverable, or a good fit as the other gestures are. So, given this problem, we should just disable it for 3.5. [I'd also be open to tweaking it to make it much harder to trigger accidentally, dunno if that's possible.]       See also bug 461376. Just play around with browser.gesture.twist.* in about:config as a temporary workaround.    Justin, the odd thing here is that I find I trigger zoom more frequently than tab rotate ... far far more frequently. And I also find that it's not just in Firefox, it's all over OSX. I wonder how much of this is us vs. odd timings causing misreads of the trackpad by OSX.  I think I've never accidentally switched tab because of rotate gesture. Actually I'd probably decrease the default threshold for tab switching. I've triggered zoom repeatedly too; usually I fix it right away but a quick browse though content-prefs.sqlite shows ~20 sites with zoom settings that I didn't know were set. That's probably because zoom is a subtle effect (for 1 step, relative to the distraction that an accidental tab switch causes). While playing with different browser.gesture.twist settings, I've found twisting through tabs often resulted in triggering a zoom. Probably because a twist is a very awkward gesture to make, especially if it's more than a quarter-turn... Around that point, my fingers want to spread or join, and that ends up read as a pinch. I've been using swipe left/right to change tabs and cmd-left/right to go back/forward, but I do have rotate as change tabs as well and I haven't accidentally triggered it. Maybe it's your rotating technique ;) I'm right handed and I position my index finger at 7oclock and ring finger at 1oclock. To rotate right I end up at 8/2oclocks.   The first time I came into contact with tab switching via gesture was accidentally. I wasn't sure what happened, I just appeared on another tab and couldn't figure out why. The tab-switching gesture right now isn't obvious and feels unexpected when I hit it.    Mardak: the way you've written this code, disabling the gesture is a pref-flip, right? Blocking here for decision and mulling, will probably take it though.        Yup, should be able to just unset the default value for the twist prefs: -pref(\""browser.gesture.twist.right\"", \""Browser:NextTab\""); -pref(\""browser.gesture.twist.left\"", \""Browser:PrevTab\"");  Would setting the value to empty-string (\""\"") work, or something similar? Since this is now implemented on Win7 with Multitouch screens please be sure not to disable it for Windows.     (In reply to comment #10) This bug is just rotate, but do we want to consider pinching too? If we take those out, we'll have swipe up/down as jump to top/bottom and swipe left/right as back/forward by default.   (In reply to comment #12) &gt; This bug is just rotate, but do we want to consider pinching too? Maybe; it certainly has been noted that people are accidentally hitting that gesture too. Though I think it's a separate decision: rotate is an unnatural gesture for switching tabs (nothing else uses it this way), with a high penalty for accidentally triggering. Pinching is a natural gesture for page zoom (and more widely used/known), with a low penalty for accidentally triggering. I think most people would consider it a bug if we claimed to support multitouch, but pinch didn't do anything. So, I'd strongly lean towards only removing the rotate. Pinchy would've wanted it this way. I guess that's part of why I'm not really fond of this gesture, and would just as soon turn it off. I'll try running with a threshold of 40, but I suspect there's an unavoidable conflict between making it easy to use on purpose, yet hard to invoke accidentally. It would be nice to leave the prefs visible for those who want to use them. &gt; Since this is now implemented on Win7 with Multitouch screens please be sure not to disable it for Windows. Fine by me; though I'd be a bit wary of having Windows users hit the same problem with multitouch trackpads... But Rob says such hardware/drivers doesn't exist today, so it shouldn't be a common problem. We should keep on eye out for problems with issues on touchscreens, andt from Rob demoing it on his touchscreen it would appear harder to trigger. And maybe Windows 7 is just better at discriminating between gestures. OTOH it wouldn't be a big deal to just disable it globally. Safari doesn't use it, and Rob confirmed that IE doesn't use it on Windows 7. (In reply to comment #12) &gt; This bug is just rotate, but do we want to consider pinching too? &gt; &gt; If we take those out, we'll have swipe up/down as jump to top/bottom and swipe left/right as back/forward by default. pinch should definitely be left on for win7. Tracking is turned off so you get a nice zoom in/zoom out effect. It's also nearly impossible to trigger without a definitive pinch with two fingers on the screen. If you want to consider disabling pinch, please file a separate bug, do not conflate. FWIW, I'm not nearly as keen on disabling that one, and am fairly-to-surely likely to WONTFIX such a bug. But go ahead ... try me ;) Created an attachment (id=376850) [details] v1 Don't do anything on os x when twisting. Alternatively, have people tried upping the threshold for twist? It would make it less likely to accidentally trigger. But I just noticed something interesting in os x's gesture detection. - If I use 2 hands (1 finger each) and place one at the top and one at the bottom, moving the top finger registers as a normal mouse move while moving the bottom finger is registered as a twist. I suppose the heuristic there is to assume the top finger is used for scrolling. - Also, if your fingers are too close to the edge, it won't treat it as a rotate. But you can still move the pointer from the edge. Can we get the patch reviewed - I think we want to remove the rotate gesture on both Windows and OSX. Created an attachment (id=378424) [details] v2 both Windows and OSX. (From update of attachment 378424 [details]) uir+r=beltzner http://hg.mozilla.org/mozilla-central/rev/d19424342b43 Give empty commands for twist gestures, so they do nothing, but still show up as prefs in about:config. http://hg.mozilla.org/releases/mozilla-1.9.1/rev/3329a3997d7b Verified fixed on trunk and 1.9.1 with builds on OS X and Windows like: Mozilla/5.0 (Macintosh; U; Intel Mac OS X 10.5; en-US; rv:1.9.2a1pre) Gecko/20090525 Minefield/3.6a1pre ID:20090525031110 Mozilla/5.0 (Macintosh; U; Intel Mac OS X 10.5; en-US; rv:1.9.1pre) Gecko/20090526 Shiretoko/3.5pre ID:20090526031155 Off the back of this, would there be any chance of getting... browser.gesture.swipe.left.shift added and set to Browser:PrevTab and browser.gesture.swipe.right.shift added and set to Browser:NextTab Having made this change, I must admit to now finding Firefox almost 'alien' to use on other multi-touch machines! https://bugzilla.mozilla.org/show_bug.cgi?id=502500                                                                                                                                                                                                                                                                                                                                                                                                               "
createExistentResourceFromHandle forgets to create children resources and returns  invalid status . In order to fix this issue the resource descriptions needs to be updated constantly to cope up with the changes that occurred,"Problem: The method FolderDescription#createExistentResourceFromHandle(IResource,IProgressMonitor) checks if the folder already exists and returns immediately if so. In cases where the FolderDescription has members which are not existing at that moment, that members will not be created. Use case: Assume you want to create a folder and a file in that folder and use two operations for this. I know it is not necessary to use two operations but my configuration is more complex than this example code. IFolder folder = ... // must not exist CreateFolderOperation op1 = new CreateFolderOperation(folder,...); IFile file = folder.getFile(...); // must not exist CreateFileOperation op1 = new CreateFileOperation(file,...); op1.execute(...); op2.execute(...); The execution of the op2 does not create the file because as stated above createExistentResourceFromHandle returns immediately because the parent folder of the file was created by op1. I checked the code in the ganymede release and the problem should exist there too.     Is there any reason you cannot execute op1 before creating op2? The basic problem is that the workspace file structure has changed since you first created the operation, so the change described by the operation is no longer valid.   &gt;Is there any reason you cannot execute op1 before creating op2? No there is not. I'll mark this one for M4. I think what needs to happen is that the javadoc better describes the lifecycle of these operations, and the isValid() method is tightened up so that the operation becomes invalid when the workspace changes to no longer match its recorded state. It is not the intention of these classes to stay in synch with workspace changes that happen independently of a shared, sequential workspace history.    Moving target as M6 has shipped.      Fixed in HEAD &gt;20090416. In order to understand better what the ramifications might be of actually fixing this bug vs. better documenting the limitations and usage, I started by implementing a trial fix for the problem as reported. It is pretty straightforward to rearrange the code to create the child resources (the file) even if the parent exists since the operation was created. However, the problems occur later when you try to undo such an operation. Since the operation \""remembered\"" that the folder didn't exist when it was first created, the undo of the create file operation tries to also delete the folder. This means that to truly fix the bug, we'd have to change the operations so that the resource descriptions were constantly updated to deal with changes that occurred. &gt; In addition, I created a stricter validity check on CreateFileOperation so that the operation will return an invalid status if portions of the resource tree already exist. What I meant to say is... I created a stricter validity check on CreateFileOperation so that the operation will return an invalid status if portions of the resource tree *that did not exist when the operation was created* already exist at the time of execution. It is expected/allowed that the containment tree already exists when the op is executed, as long they also existed when the op was created.      verified via automated test cases that run on each build. Also verified through source inspection                                                                    IMHO createExistentResourceFromHandle must check if members of a FolderDescription should be created. In the meantime I found a workaround in my code but I think it's very inconvenient that someone must know that op2 have to be created after the execution of op1. The API gave me no hint that I have to do so! Maybe the build of the FolderDescription could be delayed until the execution of op2. Think about cases where the to ops have nothing in common e.g. created and executed in different plug-ins. So I still believe it's better to document more clearly what the intentions of the API are. I've updated the javadoc for AbstractResourcesOperation, CreateFileOperation, and CreateFolderOperation. In addition, I created a stricter validity check on CreateFileOperation so that the operation will return an invalid status if portions of the resource tree already exist. Created WorkspaceOperationsTest.test250125() to simulate this condition. The CreateFileOperation is valid until the CreateFolderOperation is run, at which time the file operation knows it is invalid.                                                                                                                                                                                                                                                                                                                                                                                                     "
Provide an information about how many lines does the patch change using regular expressions which could be useful for everyone using the apply patch wizard ,"Inspired by Martin Oberhuber's mail about his \""lsc\"" script for counting lines in a patch[1], I though that it maybe be worthwhile to embed such thing in the Apply Patch wizard itself. imo using regexp should be enough here, especially due to the fact that we don't want to make any additional dependencies. So, my proposal is to allow to specify a regexp rule for lines that should be count as a \""real contribution\"". This is what first came to my head: /-\\\\+? (\\s*\\S+\\s*)+$ which means \""Count only lines starting with single '+' and a space. The line must also have at least one non-whitespace character\"". I think this is more/less what Martin's script does. All lines that match the above pattern would be sum up and the info would be displayed somewhere on the Apply Patch wizard. How does it sound? Martin? Again, I think it's a brilliant idea Martin. [1] sent to eclipse.org-committers  The info could be also shown in a patch outline view (see bug 190418).     Excellent idea! Having the line counter right in the apply patch wizard would finally give us a standard way of counting lines that everybody could use easily. My script also ignores \""empty comment\"" lines of the form /-\\\\+\\s+[#*]+\\s+$ And actually, if the wizard counts added lines, why shouldn't it also count removed lines? Giving two separate numbers e.g. Created an attachment (id=94358) [details] Patch Fix that matches all lines from a patch against given regexps (one for added lines and one for removed). Patterns can be changed on General &gt; Compare/Patch pref page. I decided to leave them blank by default, so I don't need to worry about different diff output formats which are out there. However, you can easily set it to a value most convenient to you (i.e. '/-\\\\+\\s+\\S'). Created an attachment (id=94360) [details] mylyn/context/zip     (In reply to comment #4) &gt; I decided to leave them blank by default, so I don't need to worry So this means it won't work out of the box? But then, the compare/patch feature already needs to analyze patches or it cannot apply them, right? So I don't quite understand why the same patterns that compare/patch already uses aren't the default for the line counting feature.   (In reply to comment #6) &gt; So this means it won't work out of the box? &gt; But then, the compare/patch feature already needs to analyze patches or it cannot apply them, right? &gt; So I don't &gt; quite understand why the same patterns that compare/patch already uses aren't the default for the line counting feature. I guess you're right Martin, but this will make the patch a little bit more complicated as the current patching mechanism is not very helpful in counting added/removed lines. I'll just need some extra time to do this.    Created an attachment (id=96838) [details] Patch 2   Created an attachment (id=96839) [details] The latest patch ensures, that when no regular expressions is provided, the patcher will use internal patterns to distinguish which lines have been added or deleted. However, this will work only for patches in unified and context diff output format. As reported in bug 227742, it appears that standard patch format is no longer supported (or it has never been). Moreover, I logged bug 228000 to make sure we add some automated tests to cover this newly added feature.  Created an attachment (id=96993) [details] The latest patch applied to CVS HEAD. Martin, would you like to try it out and let me know what do you think? Feel free to open a new bug if there is something I missed.  I tested this with I20080422-0800, on the attached patch from bug 227572 attachment 97084 [details]. Your dialog counts 207 added and 29 removed lines, but my script only counts 151 added lines. It looks like you are also counting empty lines, which doesn't seem overly useful to me.       mylyn/context/zip Patch 3 Previous patch with some minor adjustments.      Created an attachment (id=97398) [details] The preference page The \""internal\"" mechanism of counting added/removed lines is very simple, it sums up all lines with '+' and '-'. If you want to use your own patterns/script please take a look at the General &gt; Compare/Patch pref page. I've added there two fields where you can customize the way this simple mechanism works.   It's awsome that this is customizeable, but when I remember right, one reason for putting this enhancement into Eclipse SDK was such that there is a \""common standard\"" by which the projects count their lines in a patch. I think that this \""common standard\"" should be as good as possible by default. On the other hand, having some magic like removing empty lines from the count going on in the background is perhaps a problem... Martin, I see your point when you're saying that the \""common standard\"" should be as good as possible, however I would rather not to filter any lines from a patch when parsing. Here are the approaches so far: 1) Use only given filters, by default no counting will be made. This is how the patch from comment 4 worked. Rejected. 2) By default count lines using the simple patterns, which are currently used when parsing a patch (ie \""+\"" and \""-\""), this can be modified in the pref page. Patch in comment 13. In HEAD. 3) By default count lines using more sophisticated patterns like excluding empty lines. And again, this can be modified in the pref page. Verified in I20080429-0100. FYI, following regex suppresses lines in the patch from counting, which are empty or only contain non-wordchars (i.e. lines which only contain an } or only contain a * as continuation of an empty Javadoc comment: Added:     /-\\\\+[/-\\+]+[a-zA-Z0-9_!?\""|@~`$%&()+;,.:&lt;&gt;=+-]                10 lines added  20 lines removed This sounds familiar to me since I've seen similar numbers in RCS files before. I'd guess that the regex for removed lines could just be the same as for added lines but the + replaced by a -; and, files removed entirely should contribute the entire number of lines they had before (I believe that the old contents is not recorded in the diff, is it?)      but that's really neglectible and I'm fine with your approach of counting any non-empty added line in the patch. Actually, couldn't your regex even be simpler, say /-\\\\+\\s+\\S since you don't need to check till the end of the line? would it be possible to keep your current count (\""211 added / 53 removed lines\"") but add an additional count without empty lines e.g. Note that I'm only talking about empty lines here, e.g. Regex \""/-\\\\\\s*$\"". I agree that we cannot count empty comments here by default, because comment styles differ by language. Actually, it might be a good idea to have one additional kind of Regex pattern in the Preferences, which acts as a filter -- suggested tooltip behind //: Added Lines:    /-\\+ Removed Lines:  /-\\- Filter:         /-\\[+-]\\s*$     //Filter for lines to not count, e.g. empty lines providing an output such as   \""Patch contains 207 added and 29 removed lines (151 / 27 filtered)\"" If I understood you correctly this is what you meant in comment 15. My main concern here is: what if I would like to know total number of lines added in the patch (the filter idea looks to be an answer here but it's a pref value, it's not embedded in the parser itself, right?) 4) By default count lines using both simple and sophisticated patterns at the same time. The output would be something like this \""Patch contains X added and Y removed lines + Z added lines which are empty\"". Again, the user could provide his own patterns... which makes the whole mechanism overblown and not as intuitive as I would like it to be. Regarding the filter idea: it sounds good to me, but I think we should move the discussion about it to a separate bug. Would you mind opening one? btw, thanks for your feedback Martin. Martin are you willing to open new bugs to address your concerns? Removed:   /-\\-[/-\\-]+[a-zA-Z0-9_!?\""|@~`$%&()+;,.:&lt;&gt;=+-]                                                                                                                                                                                                                                                                                                                                                                              "
Incorrect deprecation comments in Platform class since they make the Javadoc confusing and the target milestone also needs to be updated,"According to the javaDoc in Platform.getResourceString(Bundle,String), this method has been deprecated and one should use the NLS class or the BundleFinder.find(). I was able to find the NLS class but I didn't find the BundleFinder class. Is the javaDoc correct? if it is a hint on where to find the BundleFinder class would help. The NLS class doesn't seem to provide me with the same functionality that Platform.getResourceString does, so I am assuming that BundleFinder would do the job that I need. Please provide me with more information on where to find the BundleFinder and improve the javaDoc.        I don't think that replacement functionality was ever implemented. Pascal, you added bunch of comments like that to the Platform ver. 1.92. As there are no current plans to act on them (and there has been no actions done in the last 2 years) I think we need to remove those comments - they make a rather confusing Javadoc. There are 21 \""XXX\"" comments that should be removed from Javadoc or acted upon.  Adding M7 as a milestone to remind to remove those comments.    (In reply to comment #1) &gt; I don't think that replacement functionality was ever implemented. The method Platform.getResourceString(Bundle,String) is not deprecated at this time; the comments should have been added to the implementation code or to a bug report, not to the Javadoc of the Platform class.     Please adjust the target milestone, so it does not point at a closed milestone in the past.      From bug 247983 other tagged methods: #NAME? #NAME? #NAME?   *** Bug 247983 has been marked as a duplicate of this bug. ***         Removed incorrect comments from the Platform class.    We collided, I just made the same changes.                    The only difference is that I moved the comments to InternalPlatform.                                              &gt; Pascal, you added bunch of comments like that to the Platform ver. 1.92. &gt; As there are no current plans to act on them (and there has been no actions done &gt; in the last 2 years) I think we need to remove those comments - they make a rather confusing Javadoc. &gt; There are 21 \""XXX\"" comments that should be removed from Javadoc or acted upon.                              So are you saying that the support for using getResourceString has just been droppped, and that no replacement will be available? that concerns me because WTP has a code that uses this method, and I am sure that there is plenty of adopters that are currently using that function as well. Also I deprecated Platform#endSplash since this has been replaced with IApplicationContext#applicationRunning().                                                                                                                                                                                                                                                                                                                                                                           "
the about:mozilla page is hardcoded to LTR in RC2 . Create a patch to complete all the localization work. The patch should be landed to mozilla-central before approval for 1.9.0.x,"the about:mozilla page in RC2 is right-aligned, but has LTR directionality instead of RTL. as a result, the dot at the end of the sentence is on the right side instead of the left, and both the parentheses around the \""10th edition\"" line are on the right side. i also think the quote source should be on the left side. perhaps the page should take CSS from intl.css, where localisers can override some of the classes or add a class to set direction.          Actually I would like not to add yet another entry in intl.css but to fix it in aboutMozilla.xhtml. Pike, can we think about it?    Same behavior also affect other internal about pages. * about:plugins pages that get RTL: * about:crashes the about:mozilla page used to be a file in the language pack, allowing the localizer to control it's look. moving this out of the language pack created this problem. the file dom/chrome/global.dtd control has direction control with the entity locale.dir. We can, actually, support RTL using something like &lt;body dir=&locale.dir;&gt;.     Created an attachment (id=324147) [details] patch Here is my patch. Tested on Hebrew locale. Please let me know if you prefer 'body{direction:&locale.dir;}' instead of the current '&lt;body dir=\""&locale.dir;\""&gt;'.  the W3C prefers markup and not CSS: http://www.w3.org/International/questions/qa-bidi-css-markup \""Because directionality is an integral part of the document structure, markup should be used to set the directionality for a document or chunk of information, or to identify places in the text where the Unicode bidirectional algorithm alone is insufficient to achieve desired directionality.\""    (In reply to comment #6) &gt; the W3C prefers markup and not CSS: Actually I'm not sure how we can do it using CSS. I have no idea if something like 'body{direction:&locale.dir;;}' is a valid.      To me, the trunk solution for this is to have a separate entity for each of those pages, to work around bugs like bug 427029. Once you localize a page we ship, you would then set the dir from ltr to rtl. That's not a solution for the 1.9 branch, though, there we should just re-use locale.dir from global.dtd. IMHO. Gavin, what do you think? (In reply to comment #8) In other words, isn't bug 427029 an edge case given the nature of about:robots and it's \""optional\"" status for localizers? Seems like we could just live with it not looking perfect for RTL locales that haven't localized it rather than adding complexity.    Well, that depends on how disciplined folks are. I'm adding another screenshot to bug 427029. Axel, Gavin - You are welcome to comment on that issue right there.   We are not far from 1.9.0.1 deadline. Please approve1.9.0.1. Thank you.       &gt; To me, the trunk solution for this is to have a separate entity for each of those pages, to work around bugs like bug 427029. We could make that call per page, like in the case of the robots page, it might make sense to have an open window for not translating it, for neterror pages probably less. There we could even get extreme and do it per div ;-).      Please get this landed on mozilla-central before getting approval for 1.9.0.x.       (From update of attachment 324147 [details]) Please re-request approval after getting this landed on mozilla-central. I'm not really sold on taking this given that it's more \""cosmetic\"" than anything else... http://hg.mozilla.org/index.cgi/mozilla-central/rev/d12488eef3f5                                     * about:robots - Has some open issues. See bug 427029 Pages that not translated, such as about:cache, about:credits and about:buildconfig should stay LTR.                         &gt; Once you localize a page we ship, you would then set the dir from ltr to rtl. &gt; That's not a solution for the 1.9 branch, though, there we should just re-use locale.dir from global.dtd. Sounds fine to me, I guess... do we really need to worry about the \""RTL locale that's falling back to en-US\"" though? Ideally we could just take this patch and not worry about having localizers update various different attributes based on localization state, and just rely on them completing all the localization work to make it look decent.                                                                                                                                                                                                                                                                                                                                                                     "
 AdapterManager.computeClassOrder Classimplementation does not match IAdapterManager JavaDoc and the specification of interface search order also needs to be fixed since it was observed that the superclasses take precedence over interfaces defined lower in the hierarchy,"Here is the JavaDoc for IAdapterManager.computeClassOrder(Class): Returns the class search order for a given class. The search order from a class with the definition class X extends Y implements A, B is as follows: - the target's class: X - X's superclasses in order to Object - a breadth-first traversal of the target class's interfaces in the order returned by getInterfaces (in the example, A and its superinterfaces then B and its superinterfaces) This comment is not perfectly clear. In particular, it does not complete the example by explicitly listing the exact order the example classes and interfaces are returned. I came up with a slightly richer example: interface A extends M, N interface B extends O class Y implements C, D class X extends Y implements A, B You are right on both counts. The implementation doesn't match the spec, and there is a contradiction in the spec itself. Now the question is what do we do about it. - For the contradiction, I think we should stick with the breadth-first search, and delete the parenthetic comment about the example (\""A and its superinterfaces then B and its superinterfaces\"") - For the spec not matching the behaviour, my instinct is to \""fix\"" the spec to match the implementation, to avoid the risk of breaking clients. It would be interesting to know what triggered the change in behavior. I guess the question is whether superclasses take precedence over interfaces defined lower in the hierarchy, i.e. does the implementation take precedence over the specification? I can see arguments for either way; but I guess I would probably come down on the side of giving specification precedence. So, I guess you argue further that all the interfaces could precede all the classes? It looks like this change was made inadvertently during a large batch of changes to add support for the adapters extension point (bug 32498). I think since this change was introduced accidentally, and this is a clear violation of a long-standing specification, I'm leaning towards fixing the implementation. Created an attachment (id=122293) [details] Fix v01     Created an attachment (id=122297) [details] Test case This is a test case using your richer example    Fix and tests released to HEAD.      Sounds reasonable. One tiny nit-pick though. The last entry in the bullet list now reads: a breadth-first traversal of each class's interfaces in the order returned by &lt;code&gt;getInterfaces&lt;/code&gt; (in the example, X and its superinterfaces then Y and its superinterfaces) &lt;/li&gt; It might be more accurate to say: a breadth-first traversal of each class's interfaces in the order returned by &lt;code&gt;getInterfaces&lt;/code&gt; (in the example, X's superinterfaces then Y's superinterfaces) Since X and Y are not actually included in the \""traversal\"". Thanks.  Good point, I have released this clarification to HEAD. Thanks again.                                                                    [The declarations for interfaces C, D, M, N, and O are trivial.] I noticed the spec also doesn't mention the interfaces of the target class' superclasses. I think the corrected spec should read: * &lt;ul&gt; * &lt;li&gt;the target's class: X      Argh...why did I even ask? :-) Anyway, I don't have any use cases at the moment to argue one way or the other. It's just that my curiosity got the better of me, after years of wondering, \""Just how does the Platform resolve which adapter to return?\"" :-)                     * &lt;li&gt;X's superclasses in order to &lt;code&gt;Object&lt;/code&gt; * &lt;li&gt;a breadth-first traversal of the target class's interfaces in the order returned by &lt;code&gt;getInterfaces&lt;/code&gt; (in the example, X and its superinterfaces then Y and its superinterfaces) &lt;/li&gt;        The order of classes and interfaces returned by AdapterManager.computeClassOrder(X) is: class X interface A interface B interface M interface N interface O class Y interface C interface D class java.lang.Object This conflicts with the JavaDoc. The class X is the first thing returned, but it is *not* followed by \""X's superclasses in order to Object\"". It is followed by a breadth-first list of X's interface hierarchy\"", *then* followed by X's superclass Y, which is then followed by a breadth-first list of Y's interface hierarchy; and so on, up to  java.lang.Object. Also, the JavaDoc has what reads to me as a contradiction: a breadth-first traversal of the target class's interfaces in the order returned by getInterfaces vs. in the example, A and its superinterfaces then B and its superinterfaces What actually happens *does* feel like a \""breadth-first\"" search; but that would not be accurately described as \""A and its superinterfaces then B and its superinterfaces\"". But maybe I'm reading it wrong.... It might be worth discussing which is the \""preferred\"" behavior: the behavior described by the JavaDoc or the behavior implemented in AdapterManager. In some ways having all the classes first, followed by the interfaces might make more sense than the current implementation.... But, then you have the whole \""backward-compatibility\"" thing to worry about.... This would be a very subtle change that could affect clients in unpredictable ways. On the other hand if there are really compelling reasons to change the implementation to match the spec, I'd be curious to hear them. I did some research and found that the implementation did match the spec until Febuary 20, 2004, when I change the implementation. At that time the spec wasn't part of the API. The spec was later added to the API (IAdapterManager#computeClassOrder) without checking that the implementation still matched. I'm going to do some more archeology to see if I can figure out why I changed the lookup order. * &lt;/ul&gt; Note that the comment changed to \""X and its superinterfaces then Y...\"" as opposed to \""A and its superinterfaces then B...\"". Thus this clarifies how the interfaces of superclasses are considered, while fixing the contradiction in the specification of interface search order.                                                                                                                                                                                                                                                                                                                                     "
Compare performance of multiple binary classification algorithms by running a sample application with multiple evaluation metrics. Also view the feature importance scores per the algorithm.Email me your feedback.,"This experiment uses 8 binary classification algorithms to train models against your data. It outputs the performance result of each algorithm such as Accuracy, AUC, for comparing the algorithms' performance.It also outputs the list of features sorted by their importance score for each model using the Permutation Feature Importance.The experiment has been published as a web service, and can be used through an application to evaluate the results of training your data using the experiment.To get started.Get a dataset with a column for binary classification ,Rename the column name to ""target"" , Upload the data and run the app ,View the results .Feel free to email me and let me know if you have any suggestions or comments about this app or the usefulness of this scenario in general."
"This experiment demonstrates the use of the Execute R Script, Feature Selection, Feature Hashing modules to train a text sentiment classification engine using Azure Machine learning studio.The data is trained using SVM model.","In this article, we'll explain how to to build an experiment for sentiment analysis using Microsoft Azure Machine Learning Studio. Sentiment analysis is a special case of text mining that is increasingly important in business intelligence and and social media analysis. For example, sentiment analysis of user reviews and tweets can help companies monitor public sentiment about their brands, or help consumers who want to identify opinion polarity before purchasing a product. This experiment demonstrates the use of the Feature Hashing, Execute R Script and Filter-Based Feature Selection modules to train a sentiment analysis engine. We use a data-driven machine learning approach instead of a lexicon-based approach, as the latter is known to have high precision but low coverage compared to an approach that learns from a corpus of annotated tweets. The hashing features are used to train a model using the Two-Class Support Vector Machine (SVM), and the trained model is used to predict the opinion polarity of unseen tweets. The output predictions can be aggregated over all the tweets containing a certain keyword, such as brand, celebrity, product, book names, etc in order to find out the overall sentiment around that keyword. The experiment is generic enough that you could use this framework to solve any text classification task given a reasonable amount of labeled training data."
This sample demonstrates how to perform cost-sensitive binary classification in Azure ML Studio to predict credit risk based on information given on a credit application.Two different datasets and models were used for training and evaluation,"This sample demonstrates how to perform cost-sensitive binary classification in Azure ML Studio to predict credit risk based on the information given on a credit application. The classification problem in this experiment is a cost-sensitive one because the cost of misclassifying the positive samples is five times the cost of misclassifying the negative samples.In this experiment, we compare two different approaches for generating models to solve this problem:Training using the original data set and Training using a replicated data set.In both approaches, we evaluate the models using the test data set with replication, to ensure that results are aligned with the cost function. We test two classifiers in both approaches: Two-Class Support Vector Machine and Two-Class Boosted Decision Tree."
Binary Classifier API is an example built with Microsoft Azure Machine Learning that fits a logistic regression model to data input by the user and then outputs the predicted value for each of the observations in the data,"Suppose you have a dataset and would like to predict a binary dependent variable based on the independent variables. Logistic Regression is a popular statistical technique used for such predictions. Here the   dependent variable is binary or dichotomous and 'p' is the probability of presence of the characteristic of interest.A simple scenario could be where the researcher is trying to predict whether the prospective student is likely to accept the admission offer to a university based on information (GPA in high school, family income, resident state, gender). The predicted outcome is the probability of a prospective student accepting their admission offer."
"This experiment uses the Heart Disease dataset (1988) from the UCI Machine Learning repository to train a model for heart disease prediction.This experiment is based on the original Heart Disease Prediction experiment created by Weehyong Tok from Microsoft, which is one of the Healthcare Industry solutions. This experiment uses the data set from the UCI Machine Learning repository to train and test a model for heart disease prediction. We will use this as a starting point to give you 7 ideas how to start and improve the Cortana Intelligence Gallery examples. Thanks Weehyong for creating and sharing your experiment!","The original database contains 76 attributes, but ML researchers tend to use a subset only containing 14 attributes. The ""goal"" field refers to the presence of heart disease in the patient (num). It is integer valued from 0 (no presence) to 4. With this experiment we will attempt to to distinguish presence (values 1,2,3,4) from absence (value 0).We used the processed Cleveland data, because of a warning from the author.In this case, we only miss 4 entries of ca (number of major vessels colored by flourosopy) and 2 entries of thal. In the original sample, all missing values were substituted with -1. But there are many more options. In this case, we decide to replace them with the mode (most occurring value). Replacing the missing values with the mean would be strange in this case, as thal is a categorical variable, and although I'm not a doctor, I think it would be hard to have 0.67 major vessel colored. This is actually also the reason why we use ca as a categorical variable.We use a stratified split to maintain a balanced training and test set read more on MSDN and set a seed so we can reproduce this experiment.As we have a lot of categorical variables, we decide to make dummy variables out of them. This can be easily done by using the ""Convert to Indicator Values"" module. It helps us to get more insights about the value of a specific categorical variable.We would suggest not to use the ""One-vs-All Multiclass"" module, as this creates a multiclass classification model from an ensemble of binary classification models. We are here dealing with a binary classification model, and not a multiclass classification model, so we don't see the added value.If you use the ""Tune Model Hyperparameters"" module to train your model, please make sure you use the ""Parameter Range"" option for the 'create trainer mode"" property.By using the ""Permutation Feature Importance"" module, you can gain insights. This module computes the permutation feature importance scores of the feature variables given a trained model and a test data set. From the left model, we can find that ""oldpeak"", ""sex"" and ""restecg"" have a relative high feature importance score.

A nice thing, as we have transformed the categorical values into dummies, is that we can actually gain more information by looking at the feature importance scores of the right model. Here we find also ""oldpeak"" with the highest score, second by cp-4, which means asymptomatic chest pain. This chest pain type variable didn't come up while just using the categorical variables as they were, so sometimes it can really help to transform them in order to gain more insight."
This sample demonstrates how to split the data set using external data; it also demonstrate how to perform binary classification to detect breast cancer using two-class boosted decision tree and compute customized performance metric.,"This sample demonstrates how to train binary classifier to detect breast cancer using Azure ML Studio. The data is from KDD Cup 2008 challenge. In this experiment, we focus on the problem of early detection of breast cancer from X-ray images of the breast. The performance metric in this experiment is the area under the ROC curve where probability of false positive is between 0.2 and 0.3.The Breast Cancer Info data set contains some meta data about the data set. Specifically, it contains 102,294 rows and 11 columns. We use the first 11 columns of this data set, including 1)label; 2)image-finding-id; 3)study-finding-id; 4)image-id; 5)patient-id; 6)leftbreast; 7)MLO; 8)x-location; 9)y-location; 10)x-nipple-location; and 11)y-nipple-location.Basically, this data set contains the label and many ID information for each examination: image-finding-id, study-finding-id, image-id, and patient-id.The Breast Cancer Features data set has 102,294 rows and 118 columns. It contains the features for each patient.There is a one-to-one correspondence relationship between each row of two data sets. In our experiment, we use the label and ID information in Breast Cancer Info data set to split the Breast Cancer Features data set into training and test data sets.First of all, we select the first 11 columns of the Breast Cancer Info data set by using the Project Columns module and then assign column names for these selected columns by using the Metadata Editor module. Then we split this data set into positive and negative subsets by using the Split module. Note that the splitting is based on patent ID, thus we use Remove Duplicate Rows to remain only one row for each patient ID. Then we further split the positive and negative patient ID's into training and test subsets by using the Split module. The following image shows the workflow of the preprocessing of the Breast Cancer Info data set.Since there is a one-to-one correspondence relationship between the Breast Cancer Info data set and the Breast Cancer Features data, we can use the Add Columns module to combine these two data sets together. After splitting based on patient ID in the Breast Cancer Info data set, we can map the splitting back the Breast Cancer Features data set by using the Join module. Therefore, we obtain the following 4 subsets:Positive training examples, Positive test examples, Negative training examples and Negative test examples.By using the Add Rows module, we can get the training data set by combining the positive training examples with the negative training examples. Similarly, we can get the test data set.Notice that this data set is an imbalanced data set: the number of positive samples is significantly less than the number of negative samples. In classification, we are more interested in correctly classifying positive samples. In order to increase the weight of positive samples, we replicate the positive samples for 93 times.In this experiment, we apply Two-Class Boosted Decision Tree. In this experiment, 4 different training data sets are used to train 4 different models:Original training data set.Replicating the positive sample for 93 times in the original training data set.Adding features created by quantizing the patient id feature in the original data set.After adding features created by quantizing the patient id feature in the original data set, the positive samples are replicated for 93 times.
We initialize the learning algorithm using the Two-Class Boosted Decision Tree module and then use the Train Model module to create the actual model. These models are used by the Score Model module to produce scores of test examples. Since 4 different predictions on the test data set are generated by 4 different models, we use Add Columns to combine these predictions together. The following image shows the workflow of the model training and scoring. It can be observed that the best performance is achieved by replicating positive samples and quantizing patient ID.
"
The experiment aims to build some predictive models which help to predict Malignant & Benign cases of Breast tumors.The entire process of Machine learning pipeline is briefly described.,"Machine Learning, these days playing a pivotal role in Healthcare Analytics. Early prognosis of diseases specifically cancer finds ray of hope of getting cured of. The data on Breast Cancer was collected from an online source. The data was was understood from various aspects doing some visualization & other Statistical analysis. After that Data Wrangling was done to prepare the data set for final analysis & finally applied SVM of Azure ML Studio & received desired outcome"
Customers dataset with and without categorization was used and two different sets of modelling techniques were carried out,"Consumer Lending Segment is very complex exercise for banks so they often offload to create profiling of different customers to different service provider and use their credit ratings of individuals to disbursement of loans.In this model we have extensive data of different customers from age to marital status to education to employment and so on . This model compares two different sets of modelling, one set select variables with out any categorization and one have categorization, followed by variables whom we think would matter most.It clearly shows the amount and spread of data determines the outcome of the model rather than what we think, what matter most. Suggestion are most welcome to prune it further and make it more useful to community.One more thing, reference of this data is from the internet itself."
Sample experiment that shows how to compare performance of multiple learning algorithms.,"We use 'Adult Census Income' dataset, with 'income' being a label column. This column indicated whether person's income exceeds $50K/yr. The dataset is prepopulated in your workspace and is originally downloaded from UCI Repository.We replace all missing values with ""0"" using Missing Value Scrubber module. Initially we use Split module to randomly split the dataset into training and test sets.We compare 4 binary classifiers: Two-Class Averaged Perceptron, Two-Class Bayes Point Machine, Two-Class Decision Jungle and Two-Class Locally-Deep Support Vector Machine. The comparison is done by performing the following steps:3-fold cross-validation over the training set, finding the best hyperparameters of each learning algorithm, training each learning algorithm over the training set using previously found algorithm's best values of hyperparameters ,scoring the test set,computing accuracy over the test set.The first three steps are done by Sweep Parameters module. By default Sweep Parameters module in cross-validation mode partitions the training set into 10-folds. To change the number of folds we use Partition and Sample module with the following parameters.We connect partitioned training set to 'training dataset' input of Sweep Parameters. Since we use Sweep Parameters in cross-validation mode, we leave module's right output unconnected. We use default values of Sweep Parameters that find the hyperparameters that optimize accuracy.The fourth and fifth steps in the above list are done by Score Model and Evaluate Model respectively. Evaluate Model computes many metrics. We extract Accuracy metric using Project Column module. Finally, we use Add Rows and Execute R Script to combine results of all learners into a single column and to add a column with the names of the algorithms. R code that is used in Execute R is given below: dataset <- maml.mapInputPort(1) Algorithm <- c(""Averaged Perceptron"",""Bayes Point Machine"", ""Decision Jungle"", ""Locally-Deep SVM"") data.set <- cbind(Algorithm, dataset) maml.mapOutputPort(""data.set"")"
Ensembled Prediction Model for Diagnosing Malignant and Benign cells and Ensembled Prediction Model to classify Recurrence / Nonrecurrence,"Prediction Model for Diagnosis (Malignant / Benign) Target: Diagnosis Predictors: Nuclear features of Breast Tumour mass Algorithms: Boosted Decision Tree, Decision Jungle, Locally-Deep SVM.Prediction Model for Recurrence (Recurrence / Nonrecurrence) Target: Outcome Predictors: Nuclear features of Breast Tumour mass, Tumour size Algorithms: Boosted Decision Tree, Decision Forest, Decision Jungle, Locally-Deep SVM"
This is a binary classification problem where we need to identify which customers who will make a specific transaction in the future.,"This is a binary classification problem where we need to identify which customers will make a specific transaction in the future, irrespective of the amount of money transacted. The data provided here is an anonymized dataset containing numeric feature variables, the binary target column, and a string ID_code column.
The task is to predict the value of target column in the test set.
"
"Driven by the SMLets Exchange Connector, this experiment will define Incident/Service Request, Classification, and Support Group for New Work Items in System Center Service Manager.","This experiment (1 of 2) is taken directly from the following Machine Learning Gallery Project leveraging a sample, but modified dataset from Microsoft/Endava seen here on GitHub in order to build a machine learning classification engine that can be consumed as a Rest API for the SMLets Exchange Connector.The SMLets Exchange Connector is an open source PowerShell based alternative to the stock Exchange Connector provided by Microsoft for System Center Service Manager that is used to parse an Exchange inbox and create or update Work Items in SCSM.
To configure these AML experiments for your Service Manager deployment, a full walkthrough on the project's Github can be found here."
The goal of this experiment is to classify the IMDB reviews dataset as positive or negative.Multiple predictive models were used for the problem out of which logistic regression performed better than the others.,"This experiment Extract N-Gram Features from the text and used the features to predict the sentiment of the reviews. The features are used to train multiple models .Two-Class Logistic Regression,Two-Class Neural Network (1 Hidden Layer with 2x2500 nodes - Cost Function: tanh), Two-Class Boosted Decision Tree ,Two-Class Locally-Deep Support Vector Machine,Two-Class Decision Forest.After running the experiment, you can see that Two-Class Logistic Regression has a higher accuracy and relatively lower training time than the other models."
Experiment to get inputs for Prescriptive Analytics on Consumer complaints for financial products.,"Two Class Boosted Decision Tree wins over Two Class Logistics Regression with AUC as 0.876 over 0.872. The web service derived from Azure ML could be integrated with Complaint logging system so that support executive could get the probability of whether the issue would be “Closed with monetary relief”. If yes, the company could take that issue as critical and start with balance maintenance and aim for non-monetary issue resolution.This model could be used in number of financial usecases which involves classification problem types."
The model used in this experiment  predicts whether  a customer has a probability to be put into default into payment based on customer data and payment history.This model can be used in Risk management related use cases,"This research aimed at the case of customers default payments in Taiwan and compares the predictive accuracy of probability of default among six data mining methods. From the perspective of risk management, the result of predictive accuracy of the estimated probability of default will be more valuable than the binary result of classification - credible or not credible clients. Because the real probability of default is unknown, this study presented the novel Sorting Smoothing Method to estimate the real probability of default. With the real probability of default as the response variable (Y), and the predictive probability of default as the independent variable (X), the simple linear regression result (Y = A + BX) shows that the forecasting model produced by artificial neural network has the highest coefficient of determination; its regression intercept (A) is close to zero, and regression coefficient (B) to one. Therefore, among the six data mining techniques, artificial neural network is the only one that can accurately estimate the real probability of default.This experiment also tried to compare the performance of Two-Class Neural Network model versus the Two-Class Bayes Point model in terms of predicting the risk of customer going into payment default.All data used in this experiment came from the University of California Irvine research team. "
The goal of this model is to form a better predictive model to classify wire breaks in prestressed concrete cylinder pipes .Various features of the cylinder pipes were used and four state of the art Machine learning algorithms were used in modelling the cyclinder data,"The following model utilizes a dataset of 2000 sections of PCCP including 10 features such as length, class, wire diameter, wire spacing, # of wire turns, thickness of cylinder, soil resistivity, potential value, line current discharge, and land use.A machine learning ensemble of four state-of-the-art algorithms was created to achieve best of breed results. These models were: a) two-class averaged perceptron, b) two-class bayes point machines, c) two class decision jungle, and d) two-class locally deep support vector machines. All performed similarly. Their accuracies were 0.9664, 0.9674, 0.9677, and 0.9557, respectively.
To qualify these results, these accuracies occurred at a recall value of 0. This indicates the model is saturated with false negatives (a wire break actually occurred, but the model predicted no wire break).
The level of recall determines how cautious the model should be in predicting no wire breaks. If you predict a break and it actually is fine you've wasted thousands of dollars rehabilitating a good pipe section.
If you predict a clean wire but it breaks, the pipe bursts costing millions of dollars in damage.As one can see, a high recall is desirable in situations where the cost of a false negative is tremendous. For small pipes the cost of a false negative may be acceptable, but the price of a false negative in large pressurized pipes is unacceptable. Therefore, this model will be reworked to achieve a recall of close 1 and bring false negative predictions close to zero.

This project is on an ""as-is"" basis and makes no warranties regarding any information provided on or through it. It disclaims any liability for damages resulting from the model's use. This model still is not ready for practical application."
"The goal of this predictive task is to predict the decay state of rotating equipment, modelling was done using Support Vector Regression and its varients like Basic Linear Regression and Decision Forest algorithms with SVMR were also used.","In the past years, we have witnessed dramatic drops in oil and gas prices. This has disrupted the industry players in moving to a “doing more with less without increasing the risk exposure significantly” business strategy. One of the key enablers is the digital disruption where big data and advanced analytics drives the decisions. In this experiment we are demonstrating prediction of decay state of rotating equipment using support vector machine regression (SVMR) algorithm. We have also tested Basic Linear Regression and Decision Forest algorithms with SVMR. The Gas Turbine Compressor state is being modelled as a performance decay state metric which is measured as 1 to 0, 1 meaning delivering 100% of the nominal performance.“The data was obtained from a numerical simulator of a naval vessel (Frigate) characterized by a Gas Turbine (GT) propulsion plant. The different blocks forming the complete simulator (Propeller, Hull, GT, Gear Box and Controller) have been developed and fine tuned over the year on several similar real propulsion plants. In view of these observations the available data are in agreement with a possible real vessel.” A series of measures (16 variables) which indirectly represents the state of the system subject to performance decay has been acquired and stored in the dataset over the parameter’s space: • Lever position (lp) • Ship speed (v) [knots] • Gas Turbine (GT) shaft torque (GTT) [kN m] • GT rate of revolutions (GTn) [rpm] • Gas Generator rate of revolutions (GGn) [rpm] • Starboard Propeller Torque (Ts) [kN] • Port Propeller Torque (Tp) [kN] • Hight Pressure (HP) Turbine exit temperature (T48) [C] • GT Compressor inlet air temperature (T1) [C] • GT Compressor outlet air temperature (T2) [C] • HP Turbine exit pressure (P48) [bar] • GT Compressor inlet air pressure (P1) [bar] • GT Compressor outlet air pressure (P2) [bar] • GT exhaust gas pressure (Pexh) [bar] • Turbine Injecton Control (TIC) [%] • Fuel flow (mf) [kg/s] • GT Compressor decay state coefficient • GT Turbine decay state coefficient A series of measures (16 features) which indirectly represents of the state of the system subject to performance decay has been acquired and stored in the dataset over the parameter's space.The dataset could be downloaded from the UCI website http://archive.ics.uci.edu/ml/datasets/Condition+Based+Maintenance+of+Naval+Propulsion+Plants# Detailed explanation could be obtained from the paper “A. Coraddu, L. Oneto, A. Ghio, S. Savio, D. Anguita, M. Figari, Machine Learning Approaches for Improving Condition?Based Maintenance of Naval Propulsion Plants, Journal of Engineering for the Maritime Environment, 2014, DOI: 10.1177/1475090214540874” The data is in csv format without the header names. The dataset has been imported to ML Studio and the column names has been added. Data is clean, there are no missing data. Two columns, GT Compressor Inlet Air Pressure and GT Compressor Inlet Air Temperature have constant numbers, these were removed from the data. Exploratory data analysis has been performed using PowerBI. The workbook could be found here http://bit.ly/1peBdIU . PowerBI Desktop has to be downloaded and the pbix file opened locally. Some of the features such as R Language capabilities needs the desktop version. Looking at the histograms, we do see skewed distributions in some of the variables, hence there is room for improvement with scaling. A R scatter plot reveals correlations between some of the variables. A feature extraction module has been used to extract 8 variables, Pearson correlation has been used. Other methods have not been tested, hence there is room for improvement. The features that has been chosen are as follows • GT Compressor outlet air temperature (T2) [[C]] • HP Turbine exit temperature (T48) [[C]] • Turbine Injecton Control (TIC) [[%]]
Gas Generator rate of revolutions (GGn) [[rpm]] • Fuel flow (mf) [[kg/s]] • GT Compressor outlet air pressure (P2) [[bar]],HP Turbine exit pressure (P48) [[bar]], Gas Turbine shaft torque (GTT) [[kN m]].We have selected 3 algorithms based on a priori experience of the author • Linear Regression (LR) • Decision Forest Regression (DFR) • Support Vector Machine Regression (SVMR) LR and DFR are supported out of the box by Azure Machine Learning Blocks, SVMR is being implemented using the R Model block. As the R Model block doesn’t support Evaluate Model block connection, an Execute R Script Module has been used to calculate the R Squared. Linear Regression LR is being used to uncover the dependencies using a parsimonious model. The model parameters were calculated as follows • Bias 2.16171 • Fuel flow (mf) [[kg/s]] -0.30294 • T Compressor outlet air pressure (P2) [[bar]] 0.0297791 • HP Turbine exit pressure (P48) [[bar]] -0.00537372 • GT Compressor outlet air temperature (T2) [[C]] -0.00280191 • HP Turbine exit temperature (T48) [[C]] 0.000423961 • Turbine Injecton Control (TIC) [[%]] -0.0000285644 • Gas Turbine shaft torque (GTT) [[kN m]] 0.00000583789 • Gas Generator rate of revolutions (GGn) [[rpm]] 2.18397e-7 Decision Forest Regression DFR is known for its robustness, hence this algorithm is expected to provide acceptable results with out of the box Azure Machine Learning blocks in the simplest way. The model has generated 8 separate trees and the forest is below.Support Vector Model Regression SVRM is not supported in Azure Machine Learning as an out of the box block. R Model block is being used to create the block. As the R Model doesn’t support Sweep Parameters Block, we have pre trained the algorithm in R Studio to find the best possible parameters for epsilon and cost. The below code ran for ran for 20 hours on a Lenovo W540 Laptop with 32GB RAM and 4 core (8 with HT) i7 CPU. library(e1071) tuneResult <- tune(svm, Y ~ X, data = data, ranges = list(epsilon = seq(0,1,0.01), cost = 2^(2:9))) tunedModel <- tuneResult$best.model The parameters of the best model was cost=512 epsilon=0.3 gamma=0.06666667

The SVMR R Model was created with the following code Trainer Script library(e1071) model <- svm(get('GT Compressor decay state coefficient') ~ ., dataset, type=""eps-regression"", cost=512, epsilon=0.3, gamma=0.06666667) Scorer Script library(e1071) scores <- data.frame(predict(model, dataset)) names(scores) <- c(""PredictedGTDecayCoefficient"") As the R Model doesn’t support Evaluate Model connection yet, we’ve used an Execute R Script block to calculate the R Squared / Coefficient of Determination dataset1 <- maml.mapInputPort(1) CoefficientofDetermination <- cor(dataset1$PredictedGTDecayCoefficient, dataset1[""GT Compressor decay state coefficient""])^2 data.set <- as.data.frame(c(""CoefficientofDetermination"", CoefficientofDetermination)) maml.mapOutputPort(""data.set""); R Squared values for the 3 methods are as follows • LR 0.67 • DFR 0.97 • SVMR 0.996 The Results are a very promising with the DFR and SVMR although it needs a more thorough analysis. Another way of looking at the accuracy might be comparing the GT Compressor Decay values with the predictions of the 3 models. SVMR provides the most precise predictions while the variation of the prediction is the lowest. The prediction of the SVMR needs further analysis with new data whenever possible to prevent overfitting. Also a K Folds Cross Validation could be added to the experiment."
This sample demonstrates how to use multiclass classifiers and feature hashing in Azure ML Studio to classify news into different categories,"This sample demonstrates how to use multiclass classifiers and feature hashing in Azure ML Studio to classify news into categories.We used the 2004 Reuters news dataset. The training set has about 23,000 examples, and the test set has 781,000 examples. The original dataset has 103 categories that are organized into four hierarchies . Corporate-Industrial (CCAT), Government and Social (GCAT),Economics and Economic Indicators (ECAT) and Securities and Commodities Trading and Market (MCAT).For this experiment, we used the names of the hierarchies as the label, or attribute to predict. Thus we were solving a multiclass classification problem with four classes. The original news articles might belong to one or more hierarchies. For those articles, a separate example was created for each combination of label and article, so that the articles had the same features but different label. For instance, if an article belonged to CCAT and GCAT, two examples would exist in the label data set, one for CCAT, and the other one for GCAT.
The training and test articles, as well as labels, were available as files stored in Azure public blob storage.Because the original Reuters data did not have column headings, after reading the data from storage we replaced the dummy column headings with meaningful column names, using Metadata Editor.For the label data, we used only the rows already tagged with hierarchy names (CCAT,ECAT,GCAT,MCAT). Then we joined the label data to tag the unlabeled train and test data by using the Join module. We also removed duplicate rows using the Remove Duplicate Rows module.We used the Feature Hashing module to convert the plain text of the articles to integers and used the integer values as input features to the model.The accuracy of the One-vs-All classifier was 71.7%, compared to accuracy of 69.6% for the native multiclass classifier (Multiclass Decision Forest).All accuracy values were computed and compared using custom script in the Execute R Script module. The following graphic showed the confusion matrices for the One-vs-All classifier on the left, and the Multiclass Decision Forest model on the right.From these results, you can see that the ""Economics and Economic Indicators (ECAT)"" category had the worst prediction accuracy."
"Combining fracking job data with oil/gas production data, we build a classification model to predict the success of secondary completions.","FracFocus, a national hydraulic fracturing chemical registry, is a site where oil/gas companies may voluntary submit information about frack jobs (e.g., chemicals used, depth of job, etc.). This machine learning experiment sources data from a subset of the jobs ran between 2010 – 2013. Specifically, we consider “secondary completions” in the state of Texas i.e. a frack job representing a completion on an existing wellbore. Using production output from the Railroad Commission of Texas, we can compare the average amount of oil/gas prior to and following the fracking job. To see a visualization of this data, download the sample PowerBI Desktop file from http://1drv.ms/1ozADp3 (Note, you will need to download the Power BI Desktop application in order to open this file).For our machine learning experiment, the fracking and production data is joined into a single table.To build a predictive model, we consider operator, country, water volume, vertical depth, a subset of popular chemicals, and whether this job was for oil or gas. We also attempt to identify the likely supplier of the frack job (TopNamedSupplier) based on information in the frack job’s chemical report. Production Change, expressed as a percentage, represents the average change in oil/gas production; Large Production Increase is set to 1 for production changes of 100% or more. This experiment attempts to predict the Large Production Increase value using the Two-Class Logistic Regression algorithm – a type of binary classification (note: the Production Change value could alternatively be used as part of a regression model). Holding back 20% of the data for testing purposes, we are able to make predictions with roughly 70% accuracy."
This solution uses SQL Server 2017 + ML Services with R or Python to train a machine learning model to categorize text.,"This solution describes how to train a machine learning model using SQL Server Machine Learning Services to categorize incoming text. This solution uses a preprocessed version of the NewsGroups20, containing a Subject (extracted from the raw text data), a Text, and a Label (20 classes). Note this has a similar structure to a support ticket data set which would also have two data fields: Title and Problem description."
This model was developed as a part of Chantilly High School science fair project with classification models.,"This project tries to identify cloud-based machine learning solutions for threats from terrorism and other malicious activities and to find out possible ways to quickly detect biochemical threats to the United States environment. The research in our project includes the water quality monitoring data from the Environmental Protection Agency (EPA) monitoring station, Microsoft Azure Machine Learning Studio, personal laptops, WebEx for web meetings and collaborations, and telephones for conference calls with EPA and Microsoft Subject Matter Experts. The steps of the experiment include data collection of free chlorine, pH, conductivity, turbidity, Ultraviolet Rays (UVA), and Oxidation- Reduction Potential (ORP’s), algorithm selection, training, scoring, validating, reviewing results, refining models and publishing trained models. The outcomes were classification based on machine learning algorithms to detect anomalies in water quality data. The Support Vector Machine (SVM) and K-Means cluster helped in classifying data sets into two or three clusters that can facilitate anomaly detection systems. The Two-Class classifiers helped in predicting potential values based on the time step data from the monitoring stations. It was concluded that Microsoft Azure Cloud-based Machine Learning mechanisms were easier to experiment with and to train models using various Machine Learning algorithms with large sets of data. These experiments did not require a huge set-up of many computers necessary for processing such large sets of numerical data using complicated machine learning algorithms such as K-means, Support Vector Machine (SVM), perceptron, Bayesian classifiers and neural networks. Therefore, we can simplify the implementation of complex machine learning techniques to understand patterns from millions and millions of sensor data received by the EPA and/or Homeland Security. This experimental methodology can be extended to support additional sensor data from homes, schools, community centers and other water quality data collection venues using contemporary water quality measuring instruments. The additional sensor data can be retrieved using the concept of Internet of Things (IoT) to quickly detect anomalies in water quality from potential biochemical terrorist or other malicious activities."
Churn model using NN and SVM for ‘F3: Fall Footbal Fanatica’ app with demographic breakdown for customers who do and do not churn.,"The online mobile app ‘F3: Fall Footbal Fanatica’ earns money by pushing adds to the subscribers. To these ends in support of real time add auctions, it is important to understand who the subscribers are and who does and does not stay with the app. The ‘HARMAN ANALYTICS: F3 Media Churn Analysis NN & SVM’ experiment illustrates a neural net and SVM within Azure ML that predicts churn.Specifically: the experiment:Has a common imputation process to fill in missing data by interpolation.Splitting dataset into a training (5%) and reserve test (95%) subsets.Training both a neural net and SVM with the training set.Evaluation: a) Confirming model predictivity using the training and reserve test set (excluded from the training) b) Use of R- for simple graphics to illustrate the age and gender demographics of the true-positive, false-negative, true-negative and false-positive subsets."
Template experiment for performing document classification using logistic regression. Includes cross-validation and training output,"This is a template experiment for performing document classification using logistic regression. It includes cross-validation and model output summary steps. The included data represents a variation on the common task of sentiment analysis, however this experiment structure is well-suited to multiclass text classification needs more generally as well.This dataset contains approximately 10,000 tweets that have been labeled using the CrowdFlower platform as conveying either Happiness or Sadness. That makes this dataset a unique perspective on the popular topic of sentiment analysis. While sentiment analysis typically focuses on expressions of positive or negative opinion, this data is alternatively more grounded in emotional states.The data contains 3 columns, two of which (label and features) are explicitly expected by the experiment as it is set up id_nfpu: This is a unique identifier for each piece of data. This is useful if you are only passing part of your data to the classifier, and want to be able to stitch predictions back together with other metadata later on. Here, labels are either ""happiness"" or ""sadness"", representing the two emotional states of posts being classified.features: This column contains the text to which the label applies. It will get transformed into features used by the model during training and prediction.This data is subjected to three standard transformation/cleaning steps:Converting the ""label"" column to be a categorical variable,Removing any rows for which the label column is missing a value,Stripping out non-alphanumeric characters and converting text to lower case,Feature extraction is done using AML's native Feature Hashing module, here set to fairly conservative parameters of unigram features and 12-bit hashing.,A logistic regression classifier is used. While the example data included with this experiment only contains two labels to predict, the model is created as one-vs-all multiclass.,In addition to training up a model, cross-validation is included (defaults to 5-fold). Summary statistics for cross-validation can be viewed directly via the output port of the Evaluate Model node, and predictions from the cross-validation run (averaged across folds) are also exported to CSV for inspection of model predictions"
Classifier that predicts activity class based on wearable sensor data. Human activity recognition dataset was used to train the model.,"This classifier predicts somebody’s activity class (sitting, standing up, standing, sitting down, walking). It is based on the Human Activity Recognition dataset. Human Activity Recognition (HAR) is an active research area, results of which have the potential to benefit the development of assistive technologies in order to support care of the elderly, the chronically ill and people with special needs. Activity recognition can be used to provide information about patients’ routines to support the development of e-health systems. Two approaches are commonly used for HAR: image processing and use of wearable sensors. In this case we will use information generated by wearable sensors (Ugulino et al, 2012).The complete description of this experiment can be found here: http://www.md2c.nl/how-to-build-a-human-activity-classifier-with-azure-machine-learning/.This experiment needs the -cleaned- HAR dataset"
This experiment uses the sample income dataset to compare the results of several different binary classification algorithms.,"The income dataset is split into 3 partitions: train (60%), test (20%), tune (20%). The Tune Model Hyperparameters module is then leveraged to get optimized results for several different binary classification algorithms. The best resulting models are then scored and compared against each other with a friendly output that makes it easy to view the varying results."
Analyzing consumer complaint data to determine the likelihood of customers raising dispute on a resolution,"Handling customer complaints and disputes is an integral part of the operations for any Company. If the process is not designed well, complaints could become a nightmare resulting in financial losses, lawsuits, employee dissatisfaction and brand damage. In this project we will be analyzing the customer complaint data from companies in the Financial services domain. We will be looking into a variety of parameters like Product (Mortgage, Credit Reporting, etc.) and the details of the complaints received from customers for these products. We will also be looking into the response from the Company to these complaints. The project objective is to create a prediction calculator which tells us whether the customer would dispute the resolution, after the Company has provided its response to the complaint."
This experiment uses a simple dataset to demonstrate the ability to predict soldiers resigning after their deployment,"Deployed Soldiers Classification Experiment.This experiment uses a simple fictitious dataset that represents US Soldiers who are deployed downrange and attempts to predict whether they will leave the Army after their deployment (represented in the training dataset by a 1 in the \""Active\"" column.\n\n\nData Flow\n---------\nThe experiment starts with a Convert to Dataset object (which is not entirely necessary for this dataset, but it's good practice to include it and remove missing values) and then uses the Project Columns object to remove unnecessary columns. The experiment then uses 2 split objects to create 3 datasets of 60%, 20% and 20% for Training, Validation and Testing. The experiment uses 2 models to train and then the Sweep Parameters object to find the best combination of parameters. Finally the models are scored and evaluated to determine which model performs best."
This experiment predicts if a customer will visit after the marketing campaign based on an unbalanced email marketing data,"This experiment predicts if a customer will visit after an email marketing campaign from a retailer. The training data was taken from this link: http://www.minethatdata.com/Kevin_Hillstrom_MineThatData_E-MailAnalytics_DataMiningChallenge_2008.03.20.csv. It is an unbalanced training data set with number of negative classes (visit = 0) far more outweighing the positive class instances. To overcome this limitation, Synthetic Minority Oversampling Technique (SMOTE) was used."
"Empirical Comparison of Decision Tree, Logistic Regression and Support Vector Machine algorithms used to predict the survivors in the Titanic dataset.","Three classification algorithms were considered (Decision Tree, Logistic Regression and Support Vector Machine) in the prediction of survivors in the Titanic disaster (focus is not on dataset). The performances of three models created were evaluated.The purpose of this research was to point out features of these learning algorithms that most influenced their individual predictive capabilities and to propose hypothetical combination of features of these three algorithms to form a hybrid algorithm with better predictive capabilities compared to the individual algorithms studied.A future research can make use of the result of this study to implement a new Learning Algorithm."
A very simple example of a classification model,This is a classification experiment based on the DAT101x course - Introduction to Data Science. I used the example of the gallery Lemonade Classification conducted by Jorge Guardado Cortés with some changes to the classification to be in agreement with the laboratory 4 of the course. Thank you to Jorge for his work.
"A classification model was created and  evaluated agains metrics like precision, recall and F1-score were. Few hyperparameters were also tuned.","A common problem in Machine Learning is model selection, which means determining which model performs the best with your data. You need to select the best model across different class of algorithms, like Decision Trees or Neural Networks, and different sets of hyperparameter like the Learning Rate or the Number of Iterations. Indeed picking the best algorithm is not enough, each algorithm make use of different parameters that need to be adjusted to produce the best performing model. This experiment makes use of the Tune Model Hyperparameters module to test out different combinations of parameters for a given algorithm. It performs a grid search on the set of hyperparameters.Stating that a model is the best performing model for a given classification task can be misleading, because the performance of a model can be evaluated with several metrics, for example Precision, Accuracy, Recall, or the F-Score. Depending on your use case, you might want to maximise one or the other. This experiments gives you the best model and parameters for each of the available metrics."
Prediction of wine quality using Multiclass Classification analysis,"The data contains quality ratings for a few thousands of wines (1599 red wine samples), along with their physical and chemical properties (11 predictors). We want to use these properties to predict a rating for a wine. This case study was addressed by Multiclass Classification analysis, where wine quality is modeled in from 0 (very bad) to 10 (excellent) so the number of classes were 11 .In this experiment we compare the performance of several Multiclass Classification algorithms available in Azure ML. "
This experiment demonstrates the use of cross validation in binary classification. The pipeline was desinged in R programming language .,"In this experiment, with the help of cross-validate module, we will decide which binary classifier algorithm to use for our dataset (adult census income binary dataset). This is an extension of the experiment Sample 3 : Cross Validation for Binary Classification: Adult Dataset. You can find the details of the experiment here.Using R script we can view the metrics from each classifier and select which classifier would perform the best. The output is shown below.Based on the results for maximum accuracy we would select Boosted Trees."
Analyzes PyPI publication statistics to determine the support of Python 3 over time. When will Python 3 be better supported than Python 2?,"Are you using Python 3 for your development? It has been out for 7+ years at this point. So, if you aren’t using it, why not? Since December of 2008, the initial release of Python 3, it seems the new version of Python has lived in the shadow of Python 2. And here we are, 7 years later, still looking at a world where people are using Python 2 and talking about how Python 3 doesn’t work for them. Python 3 has around 4 years to become the only support Python, as Python 2 Support is being discontinued as the version goes EOL (End of Life). Python 3 is gaining ground quickly and, before too long, will be better supported. And, if the data indicates correctly, it will be well before 2020.Read the Blog Post on Microsoft Python Engineering or view this notebook to see our analysis and use the data sources to come to your own conclusions.While getting the data was an interesting dev exercise, the real point is analyzing what is going on. We can use a Jupyter Notebook, Pandas, and the data we collected earlier to chart Python 2 and Python 3 package development to determine what trends may exist. The question we hope to answer is whether we are approaching the goal of the vast majority of active Python packages supporting Python 3 fast enough to meet the 2020 EOL of Python 2.Polyglot just means many languages which seems an appropriate word for packages that support two major versions of Python. It is also worth noting that, in a lot of ways, Polyglot is a necessary evil. In the beginning of Python 3, there were no Python 3 packages but there were Python 2 packages. It seemed more approachable by many to write a compatible package for both versions. This allowed them to provide Python 3 support in the early stages of adoption without developing a completely new package. As Python 3 continues, it becomes easier to justify writing Python 3 only packages that can leverage all of Python 3's features.We can fit trend lines to the data to attempt to find a fit line to use to predict the future. The data is fairly consistent so this should be good enough for getting an idea anyway. I again encourage you to try more advanced forecasting to see if you can't produce a better result.Let's start and try to find when Python 2 (Only) packages will drop to 10%. We will only look at the time period after the release of Python 3.3 since this is considered by many to be an inflection point for Python 3. This will make a linear fit line make more sense also and avoid early data from skewing our fit.If you use PyPI packages in your code consider showing the developers, both those that have ported and those that haven't, that developing on Python 3 is embraced by the community and their efforts are worthwhile. Thank the library developers for their efforts to support the ecosystem and provide you with libraries that help you complete and accomplish your tasks. Everyone appreciates being recognized for the work they do and it is a small thing to give thanks to those that provide us with a great ecosystem as Python developers."
The main objective of this model is to build a predictive model which is able to distinguish between main retail product categories.,"The data is collected from Otto Group which contains a dataset with 93 features for more than 200,000 products. It has a training dataset which has ID, Features and Target Class and a testing dataset which has ID and Features.Initially we are doing some preprocessing such as cleaning data and removing NAs like that.Then we are using Random Forest Classification Algorithm for classifying the products. There we used R Scripts for the Random Forest. We set the ntree as 100 for better classfication. The output will be the products with the classes which has to be there. The accuracy is 80%."
"The goal is to create a text multiclass classification experiment for a benchmark dataset 20newsgroups . The AML experiment created, using almost zero code, produces the similar accuracy (82-83 %) as other methods.","To make the dependences clear between training and scoring parts I put it all together. The experiment covers end to end solution where left part is used for training, parameters tuning and validation. The right branch is showing how to implement scoring experiment based on the trained model. In real deployment you would need to separate the right part and used saved training model and saved dictionary created by the training part. The data are provided as training and test sets, already split by the data provider. The data was converted to the csv files from the original format.In order to avoid model overfitting, the test dataset is split into two parts test and validation sets where the test set is used for hyperparameters tuning and validation set for the final model assessment.Note: The full run of the experiment can take upto x hours. In order to shorten the time you can remove the Tune model hyperparameters module and use Train Module instead."
Loan Granting Binary Classification experiment determining the likelihood of the loan being repaid,"After completing the courses in the [Microsoft Professional Degree Data Science Curriculum](http://aka.ms/lexdsc), it’s time to put all that you have learned into practice. Before entering this competition, you should register for the [Data Science Professional Project](https://www.edx.org/course/data-science-professional-project-microsoft-dat102x) course on edX and review the detailed information there.This competition concerns loan data. When a customer applies for a loan, banks and other credit providers use statistical models to determine whether or not to grant the loan based on the likelihood of the loan being repaid. The factors involved in determining this likelihood are complex, and extensive statistical analysis and modelling are required to predict the outcome for each individual case. You must implement a model that predicts loan repayment or default based on the data provided.The dataset used in this competition consists of synthetic data that was generated specifically for use in this project. The data is designed to exhibit similar characteristics to genuine loan data."
"Microsoft Azure Machine Learning provides a cloud-based platform for data scientists to easily build and deploy end-to-end machine learning solutions from the raw data input to consumable web service end point.This predictive maintenance template used the scenario of aircraft engine operation with failure conditions to illustrate the process of predicting future failure events. This template can be adapted to other predictive maintenance scenarios where the data representative of the asset is available in both operating conditions and failure conditions, and the failure probability shows an age related pattern.This experiment demonstrates the steps in building a predictive maintenance solution.","Predictive maintenance encompasses a variety of topics, including but not limited to: failure prediction, failure diagnosis (root cause analysis), failure detection, failure type classification, and recommendation of mitigation or maintenance actions after failure. As part of the Azure Machine Learning offering, Microsoft provides a template that helps data scientists easily build and deploy a predictive maintenance solution. This predictive maintenance template focuses on the techniques used to predict when an in-service machine will fail, so that maintenance can be planned in advance. The template includes a collection of pre-configured machine learning modules, as well as custom R scripts in the Execute R Script module, to enable an end-to-end solution from data processing to deploying of the machine learning model.The time units mentioned above can be replaced by working hours, cycles, mileage, transactions, etc. based on the actual scenario.This template uses the example of simulated aircraft engine run-to-failure events to demonstrate the predictive maintenance modeling process. The implicit assumption of modeling data as done below is that the asset of interest has a progressing degradation pattern, which is reflected in the asset's sensor measurements. By examining the asset's sensor values over time, the machine learning algorithm can learn the relationship between the sensor values and changes in sensor values to the historical failures in order to predict failures in the future. We suggest examining the data format and going through all three steps of the template before replacing the data with your own.
The template is divided into 3 separate steps with 7 experiments in total, where the first step has 1 experiment, and the other two steps each contains 3 experiments each addressing one of the modeling solutions.There are two general sampling methods to help balance the class distribution: sampling down the majority class, or sampling up the minority class. The first method is implemented in the Step 2B expriment (box 2); while not implemented here, the SMOTE module in Azure ML is one implementation of the latter method.we train and evaluate two multiclass classification models: Multiclass Logistic Regression and Multiclass Neural Network, and two ordinal regression models on Two-Class Logistic Regression and Two-Class Neural Network. Ordinal regression is a type of regression analysis used to predict an ordinal variable. An ordinal variable is the variable whose value can be ranked or ordered, but the real distance between these values is unknown. In the multi-class classification problem formulated here, the class attribute ""label2"" is an ordinal variable, as its value reflects the severity of the failure progress. Therefore, we consider it is appropriate to use ordinal regression, which takes into account of this relative ordering information, instead of only treating the class attribute as an categorical variable.When evaluating the ordinal regression models, the resulting metrics are similar to the results from regression models.In order to apply any trained models into the scoring experiment, these trained models have to be saved in the training experiment. The following figures show how trained models are saved during Step 2, and the saved models are shown in the ""Trained Models"" tab on the left-hand panel of the Azure ML studio.The trained model is now ready for deployment as a web service. To deploy a model, we need to connect all the data processing, feature engineering, scoring modules, saved transformations, and saved trained models to form one scoring experiment.Once a machine learning model is deployed as a web service, it can be consumed by a range of options, such as a mobile application, web site, Power BI dashboard, or even a Excel document. For details on consumtion, see http://azure.microsoft.com/en-us/documentation/articles/machine-learning-consume-web-services/.The web service input is set to be after the pre-processing steps where the data is aggregated to a single row to score for each unit. When the scoring experiment is published a web service, it thus expects the user to input the aggregated data rather than the raw features. This makes it easier to use RRS to score a given unit, as it is just a single row that is used for predictions. Alternatively, one could set the web service input to be before the pre-processing steps, and use batch scoring or multiple rows to predict for a given unit."
This experiment shows how to build ensemble of heterogeneous classifiers using stacking technique.,"The experiment shows how to implement stacking technique for building ensemble of classifiers. This technique was introduced in [1]. We use 4 base classifiers: averaged perceptron, decision forest, decision jungle and logistic regression. Our dataset is CRM upsell dataset from KDD Cup 2009. We would like to find a classifier that optimizes AUC.In the right part of experiment we build 4 base classifiers using the full training set and obtain baseline AUC performance. In the left part of experiment we build a stacked ensemble of classifiers. We split the training set into set 1 and set 2. Set 1 is used to train 4 base classifiers. Set 2 is used to find their best combination. Base classifiers and their ensemble are tuned to optimize AUC.The final results are output of the bottom Execute R module:"
"Classification Experiment involving Real Estate Data and if the home will sell for asking price, more, or not.",Insights into the Rhode Island Real Estate Market; and variables that might help sellers capture listing price or more - a classification experiment performed in R and Azure ML.This project was completed as a final project for Fundamentals of Data Science at Harvard University towards a Graduate Certificate in Data Science.
"This experiment demonstrates how we can build a binary classification model to predict income levels of adult individuals. The process includes training, testing and evaluating the model on the Adult dataset.","In this sample experiment we will train a binary classifier on the Adult dataset, to predict whether an individual’s income is greater or less than $50,000. We will show how you can perform basic data processing operations, split the dataset into training and test sets, train the model, score the test dataset, and evaluate the predictions.From these results, you can see that the Two-Class Boosted Decision Tree is fairly accurate in predicting income for the Adult Census Income dataset."
This experiment takes San Francisco crime data and evaluates which crimes are likeliest to occur at which places and times.,"In our minds today, San Francisco bustles with technology and innovation, not suffers from criminal activity that led to the creation of Alcatraz. With all of this growth, however, there comes a price: crime. This project uses a combination of Microsoft’s Azure Machine Learning Studio and Python’s Pandas data analysis library in order to predict the category of crime that could occur in San Francisco, given multiple parameters including location and time. This will allow for residents or visitors of a certain location to prepare themselves for the eventuality of a certain crime, as well as help police officers strengthen their protection of a certain area against this criminal activity.San Francisco recognizes 39 different categories. Many of these are differentiated on a technical basis; in this experiment, many similar categories were combined to create 6 instead of 39. In addition, there were 24,931 unique values in the “Address” column. A counting transform was used on this column with the “Category” column as the label. Finally, the “Resolution” and “Descript” columns were taken out because residents would not have access to them until after the crime takes place. They cannot enter it into the algorithm, and it must therefore be trained without them.The results had an overall accuracy of 38.99%. Perhaps with more data manipulation and experimentation, this could be increased to higher levels and provide significant aid to police and residents in thwarting crime."
This experiment uses the Heart Disease dataset from the UCI Machine Learning repository to train a model for heart disease prediction.,"The Heart Disease prediction dataset is used from the UCI Machine Learning repository to develop two models, a neural network and a two class boosted decision tree. The two class boosted decision tree yields a higher accuracy in predicting the readmission flag, which has been provided in the training data. Appropriate splits of the dataset to training, validation and test data are also taken into consideration.Accuracy achieved ~88%."
"One of the safeguards against over-fitting is to build multiple models over different partitions of the same data, a technique called cross validation","This version of the Titanic dataset can be retrieved from the Kaggle website, specifically their “train” data (59.76 kb). The train Titanic data ships with 891 rows, each one pertaining to an occupant of the RMS Titanic on the night of its sinking. Demo: Interact with the user interface of a model deployed as service.The dataset also has 12 columns that each record an attribute about each occupant’s circumstances and demographic. For this particular experiment we will build a classification model that can predict whether or not someone would survive the Titanic disaster given the same circumstances and demographic.First, some preprocessing. It is highly recommended that you read the detailed tutorial to understand the rationale behind each step:Drop the columns that do not add immediate value for data mining or hold too many missing categorical values to be a reliable predictor attribute. The following columns were dropped using the select columns in dataset module:PassengerID, Name, Ticket, Cabin, Identify ccategorical attributes and cast them into categorical features using the edit metadata module. The following attributes were cast into categorical values:Survived, Pclass, Sex, Embarked, Scrub the missing values from the following columns using the clean missing data module:All missing values associated with numeric columns were replaced with the median value of the entire column.All missing values associated with categorical columns were replaced with the mode value of the entire column Randomly split and partition the data into 70% training and 30% scoring using the split module.In this gallery experiment we show that how to build a single decision tree in Azure ML, much like that of the rpart package in R programming. We will take the two-class decision forest as the learning algorithm and set the number of trees to one. We use a 10-fold cross validation methodology to evaluate the mean accuracy the model will be expected to have, and its stability in the form of a standard deviation measure.We begin by running the model on default parameters to get a baseline. The model starts off with 76% mean accuracy and a standard deviation of 4.6% Min-sample-per-leaf node was set to 1 by default, which would naturally make the tree over-fit and learn from the all the data points, including outliers. We increase it to about ~1% of the data points to stop the tree from prematurely classifying these outliers. Mean accuracy saw an improvement however standard deviation shot up, showing a bias and variance trade-off.A decision tree depth of 32 is too large for a data set with only 7 predictors. We want to create a situation where almost all features have been given a chance to participate in becoming a decision node, but not too much so that we start splitting on arbitrary numeric cut off in numeric columns. Maximum tree depth was reduced to 6, and accuracy saw an improvement and so did standard deviation.Number of random splits per node matters a lot more in the context of a decision forest vs a decision tree. This controls how similar the trees will look toward one another. Reducing this will have marginal impact on the performance of the model, however will dramatically increase model build times. This number needs to be not so large that a true greedy approach is applied when learning, but not so small that good features are always excluded."
Predicting adult income (binary outcome of above or below 50k) and comparing results between a custom R model and SVM model,"This experiment predicts a binary outcome of class of income a person has (above or below 50k) using two models, one built with custom R code (basic naïve bayes classifier) and one built with a built-in SVM model to Azure ML with default parameters. The models are compared by accuracy, and the SVM model is found to predict better than the naïve Bayes model."
This experiment demonstrates the usage of the 'Multiclass Neural Network' module to train neural network which is defined in Net# language.,"In these experiments we will use the Multiclass Neural Network module to train models to recognize hand-written digits. We will use the Net# language to define network architecture, and consider several architectures ranging from simple 1 hidden layer to more advanced convolutional neural networks (""deep nets"").The data used in this experiment is a popular MNIST dataset which consists of 70,000 grayscale images of hand-written digits. The dataset is publicly available on the MNIST website.Our last example uses an advanced network architecture that can be considered an example of a ""deep neural network"", as it has several layers, including convolutional, max pooling, and fully connected layers. This network produces a model with error of approximately 1.1% which is a significant improvement over the simple neural network in the first experiment, which had an error of 2.33%."
Loan Granting Binary Classification,"After completing the courses in the [Microsoft Professional Degree Data Science Curriculum](http://aka.ms/lexdsc), it’s time to put all that you have learned into practice. Before entering this competition, you should register for the [Data Science Professional Project](https://www.edx.org/course/data-science-professional-project-microsoft-dat102x) course on edX and review the detailed information there.This competition concerns loan data. When a customer applies for a loan, banks and other credit providers use statistical models to determine whether or not to grant the loan based on the likelihood of the loan being repaid. The factors involved in determining this likelihood are complex, and extensive statistical analysis and modelling are required to predict the outcome for each individual case. You must implement a model that predicts loan repayment or default based on the data provided.The dataset used in this competition consists of synthetic data that was generated specifically for use in this project. The data is designed to exhibit similar characteristics to genuine loan data."
"In this experiment, we predict whether scheduled passenger flight is delayed or not using a Binary-classifier.","In this experiment, we use historical on-time performance and weather data to predict whether the arrival of a scheduled passenger flight will be delayed by more than 15 minutes.We approach this problem as a classification problem, predicting two classes -- whether the flight will be delayed, or whether it will be on time. Broadly speaking, in machine learning and statistics, classification is the task of identifying the class or category to which a new observation belongs, on the basis of a training set of data containing observations with known categories. Classification is generally a supervised learning problem. Since this is a binary classification task, there are only two classes.To solve this categorization problem, we will build an experiment using Azure ML Studio. In the experiment, we train a model using a large number of examples from historic flight data, along with an outcome measure that indicates the appropriate category or class for each example. The two classes are labeled 1 if a flight was delayed, and labeled 0 if the flight was on time.The time of the weather observation is rounded up to the nearest full hour, so that the column can be equi-joined with the scheduled flight departure time. Note that the scheduled flight time and the weather observation times are rounded in opposite directions. This is done to ensure that the model uses only weather observations that happened in the past, relative to flight time. Also note that the weather data is reported in local time, but the origin and destination may be in different time zones. Therefore, an adjustment to time zone difference must be made by subtracting the time zone columns from the scheduled departure time (CRSDepTime) and weather observation time (Time). These operations are done using the Execute R Script module.In machine learning, features are individual measurable properties of something you’re interested in. Finding a good set of features for creating a predictive model requires experimentation and knowledge about the problem at hand. Some features are better for predicting the target than others. Also, some features have a strong correlation with other features, so they will not add much new information to the model and can be removed. In order to build a model, we can use all the features available, or we can select a subset of the features in the dataset. Typically you can try selecting different features, and then running the experiment again, to see if you get better results.The various features are the weather conditions at the arrival and destination airports, departure and arrival times, the airline carrier, the day of month, and the day of the week.We created a model using the Two-Class Boosted Decision Tree module and trained it using the training dataset. To determine the optimal parameters, we connected the output port of Two-Class Boosted Decision Tree to the Sweep Parameters module.Now that we've trained the model, we can use it to score the other part of our data (the last month (October) records that were set aside for validation) and to see how well our model predicts and classifies new data.Add the Score Model module to the experiment canvas, and connect the left input port to the output of the Train Model module. Connect the right input port to the validation data (right port) of the Split module.After you run the experiment, you can view the output from the Score Model module by clicking the output port and selecting Visualize. The output includes the scored labels and the probabilities for the labels.Finally, to test the quality of the results, add the Evaluate Model module to the experiment canvas, and connect the left input port to the output of the Score Model module. Note that there are two input ports for Evaluate Model, because the module can be used to compare two models. In this experiment, we compare the performance of the two different algorithms: the one created using Two-Class Boosted Decision Tree and the one created using Two-Class Logistic Regression. Run the experiment and view the output of the Evaluate Model module, by clicking the output port and selecting Visualize.The boosted decision tree model has AUC of 0.697 on the validation set, which is slightly better than the logistic regression model, with AUC of 0.675.To make the results easier to analyze, we used the airportID field to join the dataset that contains the airport names and locations."
River Invertebrate Classification Tool (RICT) is a web application that implements the RIVPACS IV predictive model.,"This tool is maintained by the UK’s environment agencies; Scottish Environment Protection Agency (SEPA), Environment Agency (EA), Natural Resources Wales (NRW) and Northern Ireland Environment Agency (NIEA).This tool is hosted on Microsoft Machine Learning Studio and is freely available to access but requires the user to use an existing/or create a Microsoft account. This account requires an email address and password. These credentials are held solely by Microsoft and the usage is outlined in the Microsoft Privacy Statement (https://privacy.microsoft.com/en-gb/privacystatement). These login credentials are not accessible by the UK’s environment agencies. The UK’s environment agencies have no access to details of who has used the tool nor access to any inputs uploaded by the user or outputs generated by running the tool.The tool is provided as an open source solution for transparency reasons only and as such there is no support for external users of the tool but a user guide can be obtained from the RICT site."
"This sample demonstrates how to use the learning with counts modules for performing multiclass classification on the publicly available NYC taxi dataset. We use a Multiclass logistic regression learner to model this problem.We use the learning with counts approach to succinctly represent high-dimensional categorical variables in our modeling. This typically results in smaller models, faster run-times, and sometimes also in a better model performance. In particular, by employing these compact representations of high-dimensional categorical variables, the model performance on rarer classes is improved since the variance of the model is reduced. For a comparison of the models with and without count features, we refer the reader to this blog. Finally, we note that although for reasons of simplicity, we have shown how to perform multiclass classification on the NYC taxi dataset using a sample of the data, the technique of learning with counts is very scalable and has been demonstrated internally on very large datasets.","Learning with counts is a useful technique for efficient encoding of high-dimensional (also called ""high cardinality"") categorical variables.In this experiment, we demonstrate how to utilize the Learning with Count modules (Build counting transform, Modify count table parameters) and Apply Transformation module to generate compact representations of high dimensional categorical variables. These derived features are then used in a multiclass classification model to predict whether a passenger will tip and which bin the tip falls in.For each unique value of a selected column, the Build Counting Transform module counts the number of examples belonging to each class. The module then outputs a transform that can be used to featurize the categorical values with default parameters.The Modify Count Table Parameters module can be used to change the parameters in featurization of categorical values.The Apply Transformation module applies the transform to a dataset with the same schema as the input of the Build Counting Transform module and replace the original categorical values with features (such as log odds, counts of both classes, and the use of a backoff).For more information about using counts in machine learning, see the online help.We used the New York city taxi dataset in the experiment, freely available here. The dataset consists of two sets of data : the trip data and the fare data.We see that the trip data consists of driver details (medallion, hack_license, vendor_id) and trip details such as pickup and dropoff times, the number of passengers, trip time and distance, and the GPS coordinates of the pickup and dropoff.As mentioned in the introduction, in this experiment, we showcase how to produce a compact representation of high-dimensional categorical features by using the learning with counts approach. In our data, some of the high-dimensional categorical features are ""medallion"", ""hack_license"", and the GPS coordinates.As we see, the number of unique values (and hence the dimensionality) of these categorical variables is very large. We expect count features to help us by producing a compact representation of these high-dimensional data.To use the count features in our modeling, the first step is to use the Build Count Transform as shown below to generate the counting transform on our chosen categorical variables.We see that the prediction accuracy on the populous classes (class 0 and 1) is quite good. In addition to this, our prediction accuracy on the rarer classes 2 and 3 is also reasonable, given that we have fewer examples for learning from. We note that this performance can be improved further by two additional simple steps in the modeling process : i) Use the Clean Missing Data module to sanitize missing values in columns, ii) Use a Sweep Parameters module to run parameter sweeps so as to pick up the best multiclass logistic learner parameter values as opposed to the default settings we choose here.In this experiment, we demonstrated the use of the learning with counts technique by using Build Counting Transform, Modify Count Table Parameters and Apply Transformation modules to generate new count-based features for multiclass classification on the NYC taxi dataset."
This experiment demonstrates the usage of the 'Multiclass Neural Network' module to train neural network which is defined in Net# language,"In these experiments we will use the Multiclass Neural Network module to train models to recognize hand-written digits. We will use the Net# language to define network architecture, and consider several architectures ranging from simple 1 hidden layer to more advanced convolutional neural networks (""deep nets"").The data used in this experiment is a popular MNIST dataset which consists of 70,000 grayscale images of hand-written digits. The dataset is publicly available on the MNIST website."
Machine Learning poisonous and edible mushrooms to find out the defining characteristics,"Based on a dataset from UCI Machine Learning repository, we have run a two-class classifier to discover the most important characteristics which define whether or not a mushroom is edible. This should, by no means, be used as a final resource as to a mushroom's edibility, but it is a fantastic start!"
This is the experiment created using an example credit risk prediction walkthrough.,"This is the initial experiment that's created when you follow the first four steps in the article, Walkthrough: Develop a predictive analytics solution for credit risk assessment in Azure Machine Learning. Using available credit data, the experiment sets up two models to predict credit risk from credit application information, and then compares the results. The final two steps in the walkthrough show you how to deploy the model as a web service and generate predictions from new credit data.This experiment is a simplified version of Binary Classfication: Credit risk prediction in the Gallery.The training data in this experiment was derived from the ""UCI Statlog (German Credit Data) Data Set"" from the UCI Machine Learning repository: http://archive.ics.uci.edu/ml/datasets/Statlog+"
This model was developed as a part of Chantilly High School science fair project with classification models,"This project tries to identify cloud-based machine learning solutions for threats from terrorism and other malicious activities and to find out possible ways to quickly detect biochemical threats to the United States environment. The research in our project includes the water quality monitoring data from the Environmental Protection Agency (EPA) monitoring station, Microsoft Azure Machine Learning Studio, personal laptops, WebEx for web meetings and collaborations, and telephones for conference calls with EPA and Microsoft Subject Matter Experts. The steps of the experiment include data collection of free chlorine, pH, conductivity, turbidity, Ultraviolet Rays (UVA), and Oxidation- Reduction Potential (ORP’s), algorithm selection, training, scoring, validating, reviewing results, refining models and publishing trained models. The outcomes were classification based on machine learning algorithms to detect anomalies in water quality data. The Support Vector Machine (SVM) and K-Means cluster helped in classifying data sets into two or three clusters that can facilitate anomaly detection systems. The Two-Class classifiers helped in predicting potential values based on the time step data from the monitoring stations. It was concluded that Microsoft Azure Cloud-based Machine Learning mechanisms were easier to experiment with and to train models using various Machine Learning algorithms with large sets of data. These experiments did not require a huge set-up of many computers necessary for processing such large sets of numerical data using complicated machine learning algorithms such as K-means, Support Vector Machine (SVM), perceptron, Bayesian classifiers and neural networks. Therefore, we can simplify the implementation of complex machine learning techniques to understand patterns from millions and millions of sensor data received by the EPA and/or Homeland Security. This experimental methodology can be extended to support additional sensor data from homes, schools, community centers and other water quality data collection venues using contemporary water quality measuring instruments. The additional sensor data can be retrieved using the concept of Internet of Things (IoT) to quickly detect anomalies in water quality from potential biochemical terrorist or other malicious activities."
