{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "decoder_check_v2.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/praveenjune17/BERT_text_summarisation/blob/master/testset_eval/evaluation.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6Lq74MbqpCYY",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 136
        },
        "outputId": "43f35e3f-8fd0-4178-9dc3-ad4c02258f6b"
      },
      "source": [
        "import shutil\n",
        "try:\n",
        "  shutil.rmtree('/content/BERT_text_summarisation')\n",
        "except:\n",
        "  pass\n",
        "!git clone https://github.com/praveenjune17/BERT_text_summarisation"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Cloning into 'BERT_text_summarisation'...\n",
            "remote: Enumerating objects: 20, done.\u001b[K\n",
            "remote: Counting objects:   5% (1/20)\u001b[K\rremote: Counting objects:  10% (2/20)\u001b[K\rremote: Counting objects:  15% (3/20)\u001b[K\rremote: Counting objects:  20% (4/20)\u001b[K\rremote: Counting objects:  25% (5/20)\u001b[K\rremote: Counting objects:  30% (6/20)\u001b[K\rremote: Counting objects:  35% (7/20)\u001b[K\rremote: Counting objects:  40% (8/20)\u001b[K\rremote: Counting objects:  45% (9/20)\u001b[K\rremote: Counting objects:  50% (10/20)\u001b[K\rremote: Counting objects:  55% (11/20)\u001b[K\rremote: Counting objects:  60% (12/20)\u001b[K\rremote: Counting objects:  65% (13/20)\u001b[K\rremote: Counting objects:  70% (14/20)\u001b[K\rremote: Counting objects:  75% (15/20)\u001b[K\rremote: Counting objects:  80% (16/20)\u001b[K\rremote: Counting objects:  85% (17/20)\u001b[K\rremote: Counting objects:  90% (18/20)\u001b[K\rremote: Counting objects:  95% (19/20)\u001b[K\rremote: Counting objects: 100% (20/20)\u001b[K\rremote: Counting objects: 100% (20/20), done.\u001b[K\n",
            "remote: Compressing objects: 100% (20/20), done.\u001b[K\n",
            "remote: Total 631 (delta 9), reused 0 (delta 0), pack-reused 611\u001b[K\n",
            "Receiving objects: 100% (631/631), 6.61 MiB | 11.32 MiB/s, done.\n",
            "Resolving deltas: 100% (447/447), done.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cFaYRDfhpFDJ",
        "colab_type": "code",
        "outputId": "9067095e-5553-4cec-8528-2aa5e1f6003e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "!pip install bunch\n",
        "!pip install rouge==0.3.2\n",
        "!pip install bert_score \n",
        "!pip install --upgrade grpcio\n",
        "!pip install tensorflow==2.1\n",
        "!pip install tensorflow_addons"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&response_type=code&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /content/drive\n",
            "Collecting bunch\n",
            "  Downloading https://files.pythonhosted.org/packages/ef/bf/a4cf1779a4ffb4f610903fa08e15d1f4a8a2f4e3353a02afbe097c5bf4a8/bunch-1.0.1.tar.gz\n",
            "Building wheels for collected packages: bunch\n",
            "  Building wheel for bunch (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for bunch: filename=bunch-1.0.1-cp36-none-any.whl size=7076 sha256=4bc43faf6da17cf3531db2fc984d17ac10901c20616ec349b7b35a2fedcb22ba\n",
            "  Stored in directory: /root/.cache/pip/wheels/56/0f/19/fbbf81e5764e6d8b74501c4357a88c14c94466ec777c03734c\n",
            "Successfully built bunch\n",
            "Installing collected packages: bunch\n",
            "Successfully installed bunch-1.0.1\n",
            "Collecting rouge==0.3.2\n",
            "  Downloading https://files.pythonhosted.org/packages/63/ac/b93411318529980ab7f41e59ed64ec3ffed08ead32389e29eb78585dd55d/rouge-0.3.2-py3-none-any.whl\n",
            "Installing collected packages: rouge\n",
            "Successfully installed rouge-0.3.2\n",
            "Collecting bert_score\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/cd/0f/03a2b49125b420c3d6dea36122844f2308c0efd289bb447b046341439458/bert_score-0.3.0-py3-none-any.whl (48kB)\n",
            "\u001b[K     |████████████████████████████████| 51kB 2.5MB/s \n",
            "\u001b[?25hRequirement already satisfied: pandas in /usr/local/lib/python3.6/dist-packages (from bert_score) (0.25.3)\n",
            "Requirement already satisfied: torch>=1.0.0 in /usr/local/lib/python3.6/dist-packages (from bert_score) (1.4.0)\n",
            "Collecting transformers>=2.2.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/ee/fc/bd726a15ab2c66dc09306689d04da07a3770dad724f0883f0a4bfb745087/transformers-2.4.1-py3-none-any.whl (475kB)\n",
            "\u001b[K     |████████████████████████████████| 481kB 11.2MB/s \n",
            "\u001b[?25hCollecting tqdm>=4.31.1\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/cd/80/5bb262050dd2f30f8819626b7c92339708fe2ed7bd5554c8193b4487b367/tqdm-4.42.1-py2.py3-none-any.whl (59kB)\n",
            "\u001b[K     |████████████████████████████████| 61kB 7.4MB/s \n",
            "\u001b[?25hRequirement already satisfied: matplotlib in /usr/local/lib/python3.6/dist-packages (from bert_score) (3.1.2)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from bert_score) (1.17.5)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (from bert_score) (2.21.0)\n",
            "Requirement already satisfied: pytz>=2017.2 in /usr/local/lib/python3.6/dist-packages (from pandas->bert_score) (2018.9)\n",
            "Requirement already satisfied: python-dateutil>=2.6.1 in /usr/local/lib/python3.6/dist-packages (from pandas->bert_score) (2.6.1)\n",
            "Requirement already satisfied: boto3 in /usr/local/lib/python3.6/dist-packages (from transformers>=2.2.0->bert_score) (1.11.9)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.6/dist-packages (from transformers>=2.2.0->bert_score) (2019.12.20)\n",
            "Collecting sentencepiece\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/74/f4/2d5214cbf13d06e7cb2c20d84115ca25b53ea76fa1f0ade0e3c9749de214/sentencepiece-0.1.85-cp36-cp36m-manylinux1_x86_64.whl (1.0MB)\n",
            "\u001b[K     |████████████████████████████████| 1.0MB 20.0MB/s \n",
            "\u001b[?25hCollecting sacremoses\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/a6/b4/7a41d630547a4afd58143597d5a49e07bfd4c42914d8335b2a5657efc14b/sacremoses-0.0.38.tar.gz (860kB)\n",
            "\u001b[K     |████████████████████████████████| 870kB 29.9MB/s \n",
            "\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.6/dist-packages (from transformers>=2.2.0->bert_score) (3.0.12)\n",
            "Collecting tokenizers==0.0.11\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/5e/36/7af38d572c935f8e0462ec7b4f7a46d73a2b3b1a938f50a5e8132d5b2dc5/tokenizers-0.0.11-cp36-cp36m-manylinux1_x86_64.whl (3.1MB)\n",
            "\u001b[K     |████████████████████████████████| 3.1MB 40.1MB/s \n",
            "\u001b[?25hRequirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.6/dist-packages (from matplotlib->bert_score) (0.10.0)\n",
            "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib->bert_score) (2.4.6)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib->bert_score) (1.1.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests->bert_score) (2019.11.28)\n",
            "Requirement already satisfied: urllib3<1.25,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests->bert_score) (1.24.3)\n",
            "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->bert_score) (3.0.4)\n",
            "Requirement already satisfied: idna<2.9,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->bert_score) (2.8)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.6/dist-packages (from python-dateutil>=2.6.1->pandas->bert_score) (1.12.0)\n",
            "Requirement already satisfied: jmespath<1.0.0,>=0.7.1 in /usr/local/lib/python3.6/dist-packages (from boto3->transformers>=2.2.0->bert_score) (0.9.4)\n",
            "Requirement already satisfied: botocore<1.15.0,>=1.14.9 in /usr/local/lib/python3.6/dist-packages (from boto3->transformers>=2.2.0->bert_score) (1.14.9)\n",
            "Requirement already satisfied: s3transfer<0.4.0,>=0.3.0 in /usr/local/lib/python3.6/dist-packages (from boto3->transformers>=2.2.0->bert_score) (0.3.2)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers>=2.2.0->bert_score) (7.0)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers>=2.2.0->bert_score) (0.14.1)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.6/dist-packages (from kiwisolver>=1.0.1->matplotlib->bert_score) (45.1.0)\n",
            "Requirement already satisfied: docutils<0.16,>=0.10 in /usr/local/lib/python3.6/dist-packages (from botocore<1.15.0,>=1.14.9->boto3->transformers>=2.2.0->bert_score) (0.15.2)\n",
            "Building wheels for collected packages: sacremoses\n",
            "  Building wheel for sacremoses (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for sacremoses: filename=sacremoses-0.0.38-cp36-none-any.whl size=884628 sha256=8c85907b5cb70260b01e88eb81c0b2c3602808d116a87566c3ddeca6d889b134\n",
            "  Stored in directory: /root/.cache/pip/wheels/6d/ec/1a/21b8912e35e02741306f35f66c785f3afe94de754a0eaf1422\n",
            "Successfully built sacremoses\n",
            "Installing collected packages: tqdm, sentencepiece, sacremoses, tokenizers, transformers, bert-score\n",
            "  Found existing installation: tqdm 4.28.1\n",
            "    Uninstalling tqdm-4.28.1:\n",
            "      Successfully uninstalled tqdm-4.28.1\n",
            "Successfully installed bert-score-0.3.0 sacremoses-0.0.38 sentencepiece-0.1.85 tokenizers-0.0.11 tqdm-4.42.1 transformers-2.4.1\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "tqdm"
                ]
              }
            }
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "Collecting grpcio\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/17/8f/f79c5c174bebece41f824dd7b1ba98da45dc2d4c373b38ac6a7f6a5acb5e/grpcio-1.26.0-cp36-cp36m-manylinux2010_x86_64.whl (2.4MB)\n",
            "\u001b[K     |████████████████████████████████| 2.4MB 4.9MB/s \n",
            "\u001b[?25hRequirement already satisfied, skipping upgrade: six>=1.5.2 in /usr/local/lib/python3.6/dist-packages (from grpcio) (1.12.0)\n",
            "Installing collected packages: grpcio\n",
            "  Found existing installation: grpcio 1.15.0\n",
            "    Uninstalling grpcio-1.15.0:\n",
            "      Successfully uninstalled grpcio-1.15.0\n",
            "Successfully installed grpcio-1.26.0\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "grpc"
                ]
              }
            }
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "Collecting tensorflow==2.1\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/85/d4/c0cd1057b331bc38b65478302114194bd8e1b9c2bbc06e300935c0e93d90/tensorflow-2.1.0-cp36-cp36m-manylinux2010_x86_64.whl (421.8MB)\n",
            "\u001b[K     |████████████████████████████████| 421.8MB 35kB/s \n",
            "\u001b[?25hCollecting tensorboard<2.2.0,>=2.1.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/40/23/53ffe290341cd0855d595b0a2e7485932f473798af173bbe3a584b99bb06/tensorboard-2.1.0-py3-none-any.whl (3.8MB)\n",
            "\u001b[K     |████████████████████████████████| 3.8MB 34.9MB/s \n",
            "\u001b[?25hCollecting tensorflow-estimator<2.2.0,>=2.1.0rc0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/18/90/b77c328a1304437ab1310b463e533fa7689f4bfc41549593056d812fab8e/tensorflow_estimator-2.1.0-py2.py3-none-any.whl (448kB)\n",
            "\u001b[K     |████████████████████████████████| 450kB 40.9MB/s \n",
            "\u001b[?25hRequirement already satisfied: google-pasta>=0.1.6 in /usr/local/lib/python3.6/dist-packages (from tensorflow==2.1) (0.1.8)\n",
            "Requirement already satisfied: keras-applications>=1.0.8 in /usr/local/lib/python3.6/dist-packages (from tensorflow==2.1) (1.0.8)\n",
            "Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow==2.1) (1.12.0)\n",
            "Requirement already satisfied: wheel>=0.26; python_version >= \"3\" in /usr/local/lib/python3.6/dist-packages (from tensorflow==2.1) (0.34.2)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.6/dist-packages (from tensorflow==2.1) (3.1.0)\n",
            "Requirement already satisfied: scipy==1.4.1; python_version >= \"3\" in /usr/local/lib/python3.6/dist-packages (from tensorflow==2.1) (1.4.1)\n",
            "Requirement already satisfied: grpcio>=1.8.6 in /usr/local/lib/python3.6/dist-packages (from tensorflow==2.1) (1.26.0)\n",
            "Requirement already satisfied: wrapt>=1.11.1 in /usr/local/lib/python3.6/dist-packages (from tensorflow==2.1) (1.11.2)\n",
            "Requirement already satisfied: protobuf>=3.8.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow==2.1) (3.10.0)\n",
            "Requirement already satisfied: keras-preprocessing>=1.1.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow==2.1) (1.1.0)\n",
            "Requirement already satisfied: gast==0.2.2 in /usr/local/lib/python3.6/dist-packages (from tensorflow==2.1) (0.2.2)\n",
            "Requirement already satisfied: absl-py>=0.7.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow==2.1) (0.9.0)\n",
            "Requirement already satisfied: numpy<2.0,>=1.16.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow==2.1) (1.17.5)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow==2.1) (1.1.0)\n",
            "Requirement already satisfied: astor>=0.6.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow==2.1) (0.8.1)\n",
            "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.6/dist-packages (from tensorboard<2.2.0,>=2.1.0->tensorflow==2.1) (2.21.0)\n",
            "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /usr/local/lib/python3.6/dist-packages (from tensorboard<2.2.0,>=2.1.0->tensorflow==2.1) (0.4.1)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.6/dist-packages (from tensorboard<2.2.0,>=2.1.0->tensorflow==2.1) (3.1.1)\n",
            "Requirement already satisfied: setuptools>=41.0.0 in /usr/local/lib/python3.6/dist-packages (from tensorboard<2.2.0,>=2.1.0->tensorflow==2.1) (45.1.0)\n",
            "Requirement already satisfied: werkzeug>=0.11.15 in /usr/local/lib/python3.6/dist-packages (from tensorboard<2.2.0,>=2.1.0->tensorflow==2.1) (0.16.1)\n",
            "Collecting google-auth<2,>=1.6.3\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/1c/6d/7aae38a9022f982cf8167775c7fc299f203417b698c27080ce09060bba07/google_auth-1.11.0-py2.py3-none-any.whl (76kB)\n",
            "\u001b[K     |████████████████████████████████| 81kB 10.6MB/s \n",
            "\u001b[?25hRequirement already satisfied: h5py in /usr/local/lib/python3.6/dist-packages (from keras-applications>=1.0.8->tensorflow==2.1) (2.8.0)\n",
            "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests<3,>=2.21.0->tensorboard<2.2.0,>=2.1.0->tensorflow==2.1) (3.0.4)\n",
            "Requirement already satisfied: urllib3<1.25,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests<3,>=2.21.0->tensorboard<2.2.0,>=2.1.0->tensorflow==2.1) (1.24.3)\n",
            "Requirement already satisfied: idna<2.9,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests<3,>=2.21.0->tensorboard<2.2.0,>=2.1.0->tensorflow==2.1) (2.8)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests<3,>=2.21.0->tensorboard<2.2.0,>=2.1.0->tensorflow==2.1) (2019.11.28)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.6/dist-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.2.0,>=2.1.0->tensorflow==2.1) (1.3.0)\n",
            "Requirement already satisfied: rsa<4.1,>=3.1.4 in /usr/local/lib/python3.6/dist-packages (from google-auth<2,>=1.6.3->tensorboard<2.2.0,>=2.1.0->tensorflow==2.1) (4.0)\n",
            "Requirement already satisfied: cachetools<5.0,>=2.0.0 in /usr/local/lib/python3.6/dist-packages (from google-auth<2,>=1.6.3->tensorboard<2.2.0,>=2.1.0->tensorflow==2.1) (4.0.0)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.6/dist-packages (from google-auth<2,>=1.6.3->tensorboard<2.2.0,>=2.1.0->tensorflow==2.1) (0.2.8)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.6/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.2.0,>=2.1.0->tensorflow==2.1) (3.1.0)\n",
            "Requirement already satisfied: pyasn1>=0.1.3 in /usr/local/lib/python3.6/dist-packages (from rsa<4.1,>=3.1.4->google-auth<2,>=1.6.3->tensorboard<2.2.0,>=2.1.0->tensorflow==2.1) (0.4.8)\n",
            "\u001b[31mERROR: google-colab 1.0.0 has requirement google-auth~=1.4.0, but you'll have google-auth 1.11.0 which is incompatible.\u001b[0m\n",
            "Installing collected packages: google-auth, tensorboard, tensorflow-estimator, tensorflow\n",
            "  Found existing installation: google-auth 1.4.2\n",
            "    Uninstalling google-auth-1.4.2:\n",
            "      Successfully uninstalled google-auth-1.4.2\n",
            "  Found existing installation: tensorboard 1.15.0\n",
            "    Uninstalling tensorboard-1.15.0:\n",
            "      Successfully uninstalled tensorboard-1.15.0\n",
            "  Found existing installation: tensorflow-estimator 1.15.1\n",
            "    Uninstalling tensorflow-estimator-1.15.1:\n",
            "      Successfully uninstalled tensorflow-estimator-1.15.1\n",
            "  Found existing installation: tensorflow 1.15.0\n",
            "    Uninstalling tensorflow-1.15.0:\n",
            "      Successfully uninstalled tensorflow-1.15.0\n",
            "Successfully installed google-auth-1.11.0 tensorboard-2.1.0 tensorflow-2.1.0 tensorflow-estimator-2.1.0\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "google"
                ]
              }
            }
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "Collecting tensorflow_addons\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/e5/1a/988536d55ff814069d851731f31665b0a30989c496d64e0ff3a5efa42f63/tensorflow_addons-0.7.1-cp36-cp36m-manylinux2010_x86_64.whl (986kB)\n",
            "\u001b[K     |████████████████████████████████| 993kB 4.4MB/s \n",
            "\u001b[?25hRequirement already satisfied: tensorflow>=2.1.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow_addons) (2.1.0)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.6/dist-packages (from tensorflow>=2.1.0->tensorflow_addons) (3.1.0)\n",
            "Requirement already satisfied: tensorflow-estimator<2.2.0,>=2.1.0rc0 in /usr/local/lib/python3.6/dist-packages (from tensorflow>=2.1.0->tensorflow_addons) (2.1.0)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow>=2.1.0->tensorflow_addons) (1.1.0)\n",
            "Requirement already satisfied: gast==0.2.2 in /usr/local/lib/python3.6/dist-packages (from tensorflow>=2.1.0->tensorflow_addons) (0.2.2)\n",
            "Requirement already satisfied: protobuf>=3.8.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow>=2.1.0->tensorflow_addons) (3.10.0)\n",
            "Requirement already satisfied: absl-py>=0.7.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow>=2.1.0->tensorflow_addons) (0.9.0)\n",
            "Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow>=2.1.0->tensorflow_addons) (1.12.0)\n",
            "Requirement already satisfied: wrapt>=1.11.1 in /usr/local/lib/python3.6/dist-packages (from tensorflow>=2.1.0->tensorflow_addons) (1.11.2)\n",
            "Requirement already satisfied: scipy==1.4.1; python_version >= \"3\" in /usr/local/lib/python3.6/dist-packages (from tensorflow>=2.1.0->tensorflow_addons) (1.4.1)\n",
            "Requirement already satisfied: keras-preprocessing>=1.1.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow>=2.1.0->tensorflow_addons) (1.1.0)\n",
            "Requirement already satisfied: astor>=0.6.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow>=2.1.0->tensorflow_addons) (0.8.1)\n",
            "Requirement already satisfied: tensorboard<2.2.0,>=2.1.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow>=2.1.0->tensorflow_addons) (2.1.0)\n",
            "Requirement already satisfied: keras-applications>=1.0.8 in /usr/local/lib/python3.6/dist-packages (from tensorflow>=2.1.0->tensorflow_addons) (1.0.8)\n",
            "Requirement already satisfied: wheel>=0.26; python_version >= \"3\" in /usr/local/lib/python3.6/dist-packages (from tensorflow>=2.1.0->tensorflow_addons) (0.34.2)\n",
            "Requirement already satisfied: google-pasta>=0.1.6 in /usr/local/lib/python3.6/dist-packages (from tensorflow>=2.1.0->tensorflow_addons) (0.1.8)\n",
            "Requirement already satisfied: numpy<2.0,>=1.16.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow>=2.1.0->tensorflow_addons) (1.17.5)\n",
            "Requirement already satisfied: grpcio>=1.8.6 in /usr/local/lib/python3.6/dist-packages (from tensorflow>=2.1.0->tensorflow_addons) (1.26.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.6/dist-packages (from protobuf>=3.8.0->tensorflow>=2.1.0->tensorflow_addons) (45.1.0)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.6/dist-packages (from tensorboard<2.2.0,>=2.1.0->tensorflow>=2.1.0->tensorflow_addons) (3.1.1)\n",
            "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /usr/local/lib/python3.6/dist-packages (from tensorboard<2.2.0,>=2.1.0->tensorflow>=2.1.0->tensorflow_addons) (0.4.1)\n",
            "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.6/dist-packages (from tensorboard<2.2.0,>=2.1.0->tensorflow>=2.1.0->tensorflow_addons) (2.21.0)\n",
            "Requirement already satisfied: google-auth<2,>=1.6.3 in /usr/local/lib/python3.6/dist-packages (from tensorboard<2.2.0,>=2.1.0->tensorflow>=2.1.0->tensorflow_addons) (1.11.0)\n",
            "Requirement already satisfied: werkzeug>=0.11.15 in /usr/local/lib/python3.6/dist-packages (from tensorboard<2.2.0,>=2.1.0->tensorflow>=2.1.0->tensorflow_addons) (0.16.1)\n",
            "Requirement already satisfied: h5py in /usr/local/lib/python3.6/dist-packages (from keras-applications>=1.0.8->tensorflow>=2.1.0->tensorflow_addons) (2.8.0)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.6/dist-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.2.0,>=2.1.0->tensorflow>=2.1.0->tensorflow_addons) (1.3.0)\n",
            "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests<3,>=2.21.0->tensorboard<2.2.0,>=2.1.0->tensorflow>=2.1.0->tensorflow_addons) (3.0.4)\n",
            "Requirement already satisfied: idna<2.9,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests<3,>=2.21.0->tensorboard<2.2.0,>=2.1.0->tensorflow>=2.1.0->tensorflow_addons) (2.8)\n",
            "Requirement already satisfied: urllib3<1.25,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests<3,>=2.21.0->tensorboard<2.2.0,>=2.1.0->tensorflow>=2.1.0->tensorflow_addons) (1.24.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests<3,>=2.21.0->tensorboard<2.2.0,>=2.1.0->tensorflow>=2.1.0->tensorflow_addons) (2019.11.28)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.6/dist-packages (from google-auth<2,>=1.6.3->tensorboard<2.2.0,>=2.1.0->tensorflow>=2.1.0->tensorflow_addons) (0.2.8)\n",
            "Requirement already satisfied: rsa<4.1,>=3.1.4 in /usr/local/lib/python3.6/dist-packages (from google-auth<2,>=1.6.3->tensorboard<2.2.0,>=2.1.0->tensorflow>=2.1.0->tensorflow_addons) (4.0)\n",
            "Requirement already satisfied: cachetools<5.0,>=2.0.0 in /usr/local/lib/python3.6/dist-packages (from google-auth<2,>=1.6.3->tensorboard<2.2.0,>=2.1.0->tensorflow>=2.1.0->tensorflow_addons) (4.0.0)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.6/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.2.0,>=2.1.0->tensorflow>=2.1.0->tensorflow_addons) (3.1.0)\n",
            "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /usr/local/lib/python3.6/dist-packages (from pyasn1-modules>=0.2.1->google-auth<2,>=1.6.3->tensorboard<2.2.0,>=2.1.0->tensorflow>=2.1.0->tensorflow_addons) (0.4.8)\n",
            "Installing collected packages: tensorflow-addons\n",
            "Successfully installed tensorflow-addons-0.7.1\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hmZf4RXnpJA6",
        "colab_type": "code",
        "outputId": "bdd6bd94-d32e-47d1-e306-aaf93ce494aa",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 666
        }
      },
      "source": [
        "%%shell\n",
        "git clone https://github.com/praveenjune17/transformers.git\n",
        "cd transformers\n",
        "pip install ."
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Cloning into 'transformers'...\n",
            "remote: Enumerating objects: 17482, done.\u001b[K\n",
            "remote: Total 17482 (delta 0), reused 0 (delta 0), pack-reused 17482\u001b[K\n",
            "Receiving objects: 100% (17482/17482), 10.25 MiB | 14.41 MiB/s, done.\n",
            "Resolving deltas: 100% (12980/12980), done.\n",
            "Processing /content/transformers\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from transformers==2.3.0) (1.17.5)\n",
            "Requirement already satisfied: tokenizers==0.0.11 in /usr/local/lib/python3.6/dist-packages (from transformers==2.3.0) (0.0.11)\n",
            "Requirement already satisfied: boto3 in /usr/local/lib/python3.6/dist-packages (from transformers==2.3.0) (1.11.9)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.6/dist-packages (from transformers==2.3.0) (3.0.12)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (from transformers==2.3.0) (2.21.0)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.6/dist-packages (from transformers==2.3.0) (4.42.1)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.6/dist-packages (from transformers==2.3.0) (2019.12.20)\n",
            "Requirement already satisfied: sentencepiece in /usr/local/lib/python3.6/dist-packages (from transformers==2.3.0) (0.1.85)\n",
            "Requirement already satisfied: sacremoses in /usr/local/lib/python3.6/dist-packages (from transformers==2.3.0) (0.0.38)\n",
            "Requirement already satisfied: botocore<1.15.0,>=1.14.9 in /usr/local/lib/python3.6/dist-packages (from boto3->transformers==2.3.0) (1.14.9)\n",
            "Requirement already satisfied: jmespath<1.0.0,>=0.7.1 in /usr/local/lib/python3.6/dist-packages (from boto3->transformers==2.3.0) (0.9.4)\n",
            "Requirement already satisfied: s3transfer<0.4.0,>=0.3.0 in /usr/local/lib/python3.6/dist-packages (from boto3->transformers==2.3.0) (0.3.2)\n",
            "Requirement already satisfied: urllib3<1.25,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests->transformers==2.3.0) (1.24.3)\n",
            "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->transformers==2.3.0) (3.0.4)\n",
            "Requirement already satisfied: idna<2.9,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->transformers==2.3.0) (2.8)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests->transformers==2.3.0) (2019.11.28)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers==2.3.0) (7.0)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers==2.3.0) (0.14.1)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers==2.3.0) (1.12.0)\n",
            "Requirement already satisfied: docutils<0.16,>=0.10 in /usr/local/lib/python3.6/dist-packages (from botocore<1.15.0,>=1.14.9->boto3->transformers==2.3.0) (0.15.2)\n",
            "Requirement already satisfied: python-dateutil<3.0.0,>=2.1 in /usr/local/lib/python3.6/dist-packages (from botocore<1.15.0,>=1.14.9->boto3->transformers==2.3.0) (2.6.1)\n",
            "Building wheels for collected packages: transformers\n",
            "  Building wheel for transformers (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for transformers: filename=transformers-2.3.0-cp36-none-any.whl size=458557 sha256=a0ab0c04696e66b0bf01edbb6dd5188a36c42ae5ab18bbe267c5e470f7d5b65e\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-pw73yk7f/wheels/23/19/dd/2561a4e47240cf6b307729d58e56f8077dd0c698f5992216cf\n",
            "Successfully built transformers\n",
            "Installing collected packages: transformers\n",
            "  Found existing installation: transformers 2.4.1\n",
            "    Uninstalling transformers-2.4.1:\n",
            "      Successfully uninstalled transformers-2.4.1\n",
            "Successfully installed transformers-2.3.0\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              ""
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QNPMnp_LyPJK",
        "colab_type": "text"
      },
      "source": [
        "**Inference** **script**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XL2J0nIq1uVP",
        "colab_type": "text"
      },
      "source": [
        "Test set evaluation script"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vv7Bc-6W1tju",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "90c4ba29-7845-4b17-f489-afa01f6adce3"
      },
      "source": [
        "import sys\n",
        "sys.path.insert(0, '/content/BERT_text_summarisation/scripts')\n",
        "import tensorflow as tf\n",
        "tf.random.set_seed(100)\n",
        "import tensorflow_datasets as tfds\n",
        "import numpy as np\n",
        "import os\n",
        "from create_tokenizer import tokenizer\n",
        "from preprocess import map_batch_shuffle\n",
        "from configuration import config\n",
        "from input_path import file_path\n",
        "from preprocess import infer_data_from_df\n",
        "from metrics import convert_wordpiece_to_words\n",
        "from rouge import Rouge\n",
        "from bert_score import score as b_score\n",
        "from decode_text import *\n",
        "from tqdm import tqdm\n",
        "\n",
        "UNK_ID = 100\n",
        "CLS_ID = 101\n",
        "SEP_ID = 102\n",
        "MASK_ID = 103\n",
        "h_parms.batch_size = 40\n",
        "#ckpt = '59'\n",
        "rouge_all = Rouge()\n",
        "infer_template = '''Batch size <--- 30\\\n",
        "\\nDraft_decoder_type <--- {}\\nRefine_decoder_type <--- {}\\nROUGE-f1  <--- {}\\nBERT-f1   <--- {}\\nbeam-size   <--- {}\\nCheckpoint  <---{}'''\n",
        "\n",
        "\n",
        "def restore_chkpt(checkpoint_path):\n",
        "    ckpt = tf.train.Checkpoint(\n",
        "                               model=model\n",
        "                               )\n",
        "    assert tf.train.latest_checkpoint(os.path.split(checkpoint_path)[0]), 'Incorrect checkpoint directory'\n",
        "    ckpt.restore(checkpoint_path).expect_partial()\n",
        "    print(f'{checkpoint_path} restored')\n",
        "    \n",
        "if config.use_tfds:\n",
        "      examples, metadata = tfds.load(\n",
        "                                     config.tfds_name, \n",
        "                                     with_info=True, \n",
        "                                     as_supervised=True, \n",
        "                                     data_dir='/content/drive/My Drive/Text_summarization/cnn_dataset',\n",
        "                                     builder_kwargs={\"version\": \"2.0.0\"}\n",
        "                                     )\n",
        "      test_examples = examples['test']\n",
        "      test_buffer_size = metadata.splits['test'].num_examples\n",
        "      test_dataset = map_batch_shuffle(\n",
        "                                       test_examples, \n",
        "                                       test_buffer_size, \n",
        "                                       split='test',\n",
        "                                       batch_size=h_parms.batch_size\n",
        "                                       )\n",
        "      log.info('Test TF_dataset created')\n",
        "      test_dataset = test_dataset.take(1)\n",
        "else:\n",
        "  test_dataset = infer_data_from_df()\n",
        "for ckpt in range(73,74):\n",
        "  ckpt = str(ckpt)\n",
        "  try:\n",
        "    restore_chkpt('/content/drive/My Drive/Text_summarization/BERT_text_summarisation/created_files/training_summarization_model_ckpts/cnn/best_checkpoints/ckpt-'+ckpt)\n",
        "    \n",
        "    ref_sents=[]\n",
        "    hyp_sents=[]\n",
        "\n",
        "\n",
        "    for (doc_id, (input_ids, _, _, target_ids, _, _)) in tqdm(enumerate(test_dataset, 1)):\n",
        "      start_time = time.time()\n",
        "      draft, refined_summary, att = predict_using_beam_search(\n",
        "                                                              input_ids, \n",
        "                                                              beam_size=3, \n",
        "                                                              refine_decoder_sampling_type='greedy', \n",
        "    #                                                           k=k, \n",
        "    #                                                           p=p,\n",
        "    #                                                           temperature=temperature\n",
        "                                                              )\n",
        "      for tar, ref_hyp in zip(target_ids, refined_summary):\n",
        "        sum_ref = tokenizer.convert_ids_to_tokens([i for i in tf.squeeze(tar) if i not in [0, 101, 102]])\n",
        "        sum_hyp = tokenizer.convert_ids_to_tokens([i for i in tf.squeeze(ref_hyp) if i not in [0, 101, 102]])\n",
        "        sum_ref = convert_wordpiece_to_words(sum_ref)\n",
        "        sum_hyp = convert_wordpiece_to_words(sum_hyp)\n",
        "        #print('Original summary: {}'.format(sum_ref))\n",
        "        #print('Predicted summary: {}'.format(sum_hyp))\n",
        "        ref_sents.append(sum_ref)\n",
        "        hyp_sents.append(sum_hyp)\n",
        "      try:\n",
        "        rouges = rouge_all.get_scores(ref_sents , hyp_sents)\n",
        "        avg_rouge_f1 = np.mean([np.mean([rouge_scores['rouge-1'][\"f\"], \n",
        "                                        rouge_scores['rouge-2'][\"f\"], \n",
        "                                        rouge_scores['rouge-l'][\"f\"]]) for rouge_scores in rouges])\n",
        "        _, _, bert_f1 = b_score(ref_sents, hyp_sents, lang='en', model_type=config.pretrained_bert_model)\n",
        "        avg_bert_f1 = np.mean(bert_f1.numpy())\n",
        "      except:\n",
        "        log.warning('predicted output is empty so setting rouge and bert score to 0')\n",
        "        avg_rouge_f1 = 0\n",
        "        avg_bert_f1 = 0\n",
        "      print(infer_template.format('beam_search', 'greedy', avg_rouge_f1, avg_bert_f1, 3,ckpt))\n",
        "      print(f'time to process document {doc_id} : {time.time()-start_time}')\n",
        "      print(f'Calculating scores for {len(ref_sents)} golden summaries and {len(hyp_sents)} predicted summaries')\n",
        "  except:\n",
        "    print(f'{ckpt} checkpoint not found')\n",
        "    continue"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "setting default value to last_recorded_value\n",
            "INFO:tensorflow:Extracting pretrained word embeddings weights from BERT\n",
            "INFO:tensorflow:Embedding matrix shape '(30522, 768)'\n",
            "INFO:tensorflow:Loading Pre-trained BERT model for BERT SCORE calculation\n",
            "INFO:tensorflow:Extracting pretrained word embeddings weights from BERT\n",
            "INFO:tensorflow:Embedding matrix shape '(30522, 768)'\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:absl:No config specified, defaulting to first: cnn_dailymail/plain_text\n",
            "INFO:absl:Overwrite dataset info from restored data version.\n",
            "INFO:absl:Reusing dataset cnn_dailymail (/content/drive/My Drive/Text_summarization/cnn_dataset/cnn_dailymail/plain_text/2.0.0)\n",
            "INFO:absl:Constructing tf.data.Dataset for split None, from /content/drive/My Drive/Text_summarization/cnn_dataset/cnn_dailymail/plain_text/2.0.0\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Test TF_dataset created\n",
            "66 checkpoint not found\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\r0it [00:00, ?it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "/content/drive/My Drive/Text_summarization/BERT_text_summarisation/created_files/training_summarization_model_ckpts/cnn/best_checkpoints/ckpt-67 restored\n",
            "INFO:tensorflow:Building: 'Draft beam search decoder'\n",
            "INFO:tensorflow:Building: 'Refined greedy decoder'\n",
            "draft_summary [40 73]\n",
            "preds_refined_summary shape [40 72]\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "1it [01:34, 94.63s/it]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Batch size <--- 30\n",
            "Draft_decoder_type <--- beam_search\n",
            "Refine_decoder_type <--- greedy\n",
            "ROUGE-f1  <--- 0.21205842873627265\n",
            "BERT-f1   <--- 0.5131280422210693\n",
            "beam-size   <--- 3\n",
            "Checkpoint  <---67\n",
            "time to process document 1 : 93.86908054351807\n",
            "Calculating scores for 40 golden summaries and 40 predicted summaries\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "0it [00:00, ?it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "/content/drive/My Drive/Text_summarization/BERT_text_summarisation/created_files/training_summarization_model_ckpts/cnn/best_checkpoints/ckpt-68 restored\n",
            "INFO:tensorflow:Building: 'Draft beam search decoder'\n",
            "INFO:tensorflow:Building: 'Refined greedy decoder'\n",
            "draft_summary [40 73]\n",
            "preds_refined_summary shape [40 72]\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "1it [01:09, 69.79s/it]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Batch size <--- 30\n",
            "Draft_decoder_type <--- beam_search\n",
            "Refine_decoder_type <--- greedy\n",
            "ROUGE-f1  <--- 0.19475595025776457\n",
            "BERT-f1   <--- 0.49924755096435547\n",
            "beam-size   <--- 3\n",
            "Checkpoint  <---68\n",
            "time to process document 1 : 69.04167795181274\n",
            "Calculating scores for 40 golden summaries and 40 predicted summaries\n",
            "69 checkpoint not found\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "0it [00:00, ?it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "/content/drive/My Drive/Text_summarization/BERT_text_summarisation/created_files/training_summarization_model_ckpts/cnn/best_checkpoints/ckpt-70 restored\n",
            "INFO:tensorflow:Building: 'Draft beam search decoder'\n",
            "INFO:tensorflow:Building: 'Refined greedy decoder'\n",
            "draft_summary [40 73]\n",
            "preds_refined_summary shape [40 72]\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "1it [01:09, 69.63s/it]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Batch size <--- 30\n",
            "Draft_decoder_type <--- beam_search\n",
            "Refine_decoder_type <--- greedy\n",
            "ROUGE-f1  <--- 0.21007338814402837\n",
            "BERT-f1   <--- 0.5126749277114868\n",
            "beam-size   <--- 3\n",
            "Checkpoint  <---70\n",
            "time to process document 1 : 68.80205297470093\n",
            "Calculating scores for 40 golden summaries and 40 predicted summaries\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "0it [00:00, ?it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "/content/drive/My Drive/Text_summarization/BERT_text_summarisation/created_files/training_summarization_model_ckpts/cnn/best_checkpoints/ckpt-71 restored\n",
            "INFO:tensorflow:Building: 'Draft beam search decoder'\n",
            "INFO:tensorflow:Building: 'Refined greedy decoder'\n",
            "draft_summary [40 73]\n",
            "preds_refined_summary shape [40 72]\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "1it [01:09, 69.62s/it]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Batch size <--- 30\n",
            "Draft_decoder_type <--- beam_search\n",
            "Refine_decoder_type <--- greedy\n",
            "ROUGE-f1  <--- 0.20650209426682267\n",
            "BERT-f1   <--- 0.5106486678123474\n",
            "beam-size   <--- 3\n",
            "Checkpoint  <---71\n",
            "time to process document 1 : 68.8572678565979\n",
            "Calculating scores for 40 golden summaries and 40 predicted summaries\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "0it [00:00, ?it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "/content/drive/My Drive/Text_summarization/BERT_text_summarisation/created_files/training_summarization_model_ckpts/cnn/best_checkpoints/ckpt-72 restored\n",
            "INFO:tensorflow:Building: 'Draft beam search decoder'\n",
            "INFO:tensorflow:Building: 'Refined greedy decoder'\n",
            "draft_summary [40 73]\n",
            "preds_refined_summary shape [40 72]\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "1it [01:09, 69.51s/it]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Batch size <--- 30\n",
            "Draft_decoder_type <--- beam_search\n",
            "Refine_decoder_type <--- greedy\n",
            "ROUGE-f1  <--- 0.211377583027235\n",
            "BERT-f1   <--- 0.5072491765022278\n",
            "beam-size   <--- 3\n",
            "Checkpoint  <---72\n",
            "time to process document 1 : 68.7435212135315\n",
            "Calculating scores for 40 golden summaries and 40 predicted summaries\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WlsYPA_YBANc",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "6d145f46-9276-469d-f095-03c25bc82eed"
      },
      "source": [
        "start=125000\n",
        "ckpt_start=37\n",
        "step = 5000\n",
        "current_step = 230000\n",
        "for i in range(125000,300000,step):\n",
        "  \n",
        "  ckpt_start+=1\n",
        "  if i == current_step:\n",
        "    print(ckpt_start)\n",
        "    break\n",
        "  else:\n",
        "    continue\n",
        "  "
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "59\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HVlhGw1NAthk",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "1it [02:02, 122.77s/it]Batch size <--- 30\n",
        "Draft_decoder_type <--- beam_search\n",
        "Refine_decoder_type <--- greedy\n",
        "ROUGE-f1  <--- 0.20650209426682267\n",
        "BERT-f1   <--- 0.5106486678123474\n",
        "beam-size   <--- 3\n",
        "time to process document 1 : 121.24681329727173\n",
        "Calculating scores for 40 golden summaries and 40 predicted summaries\n",
        "ckpt :- 71"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6v_VX7rVHXz_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "Top-k's k   <--- 10\n",
        "temperature   <--- 0.7\n",
        "beam-size   <--- 5\n",
        "p <-- 0.7, 0.8, 0.9, 1"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sMUbKUBun9kf",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "0it [00:00, ?it/s]Batch size <--- 20\n",
        "Draft_decoder_type <--- beam_search\n",
        "Refine_decoder_type <--- topktopp\n",
        "ROUGE-f1  <--- 0.1690104807952041\n",
        "BERT-f1   <--- 0.48933693766593933\n",
        "Nucleus's p   <--- 0.7\n",
        "Topk's K  <--- 7\n",
        "temperature   <--- 0.9\n",
        "beam-size   <--- 4"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "92g-XfYX9eAS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# find best combo for top-k and top-p\n",
        "# combined top-k and top-p"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gSHmg2FcbNIJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "0it [00:00, ?it/s]Batch size <--- 20\n",
        "Draft_decoder_type <--- beam_search\n",
        "Refine_decoder_type <--- topk\n",
        "ROUGE-f1  <--- 0.16558894018493717\n",
        "BERT-f1   <--- 0.49077025055885315\n",
        "Top-k's k   <--- 10\n",
        "temperature   <--- 1\n",
        "beam-size   <--- 3\n",
        "time to process document 1 : 520.4894733428955"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7u-jb3iMbbKw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "0it [00:00, ?it/s]Batch size <--- 20\n",
        "Draft_decoder_type <--- beam_search\n",
        "Refine_decoder_type <--- topk\n",
        "ROUGE-f1  <--- 0.16919581924320998\n",
        "BERT-f1   <--- 0.4894334375858307\n",
        "Top-k's k   <--- 10\n",
        "temperature   <--- 0.7\n",
        "beam-size   <--- 5"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sKRpC6uXISc2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Nucleus sampling with batch\n",
        "# replace k by p and run the hyper parameters script"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}